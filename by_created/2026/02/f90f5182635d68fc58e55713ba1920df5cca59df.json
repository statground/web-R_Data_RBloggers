{
  "id": "f90f5182635d68fc58e55713ba1920df5cca59df",
  "url": "https://www.r-bloggers.com/2026/02/understanding-boosted-configuration-networks-combined-neural-networks-and-boosting-an-intuitive-guide-through-their-hyperparameters/",
  "created_at_utc": "2026-02-17T19:41:29Z",
  "crawled_at_utc": "2026-02-17T19:41:35Z",
  "html_title": "Understanding Boosted Configuration Networks (combined neural networks and boosting): An Intuitive Guide Through Their Hyperparameters | R-bloggers",
  "meta_description": "How BCN combine neural networks and boosting, explained through the knobs you can turn",
  "data": {
    "url": "https://www.r-bloggers.com/2026/02/understanding-boosted-configuration-networks-combined-neural-networks-and-boosting-an-intuitive-guide-through-their-hyperparameters/",
    "canonical_url": "https://www.r-bloggers.com/2026/02/understanding-boosted-configuration-networks-combined-neural-networks-and-boosting-an-intuitive-guide-through-their-hyperparameters/",
    "html_title": "Understanding Boosted Configuration Networks (combined neural networks and boosting): An Intuitive Guide Through Their Hyperparameters | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "How BCN combine neural networks and boosting, explained through the knobs you can turn",
    "meta_keywords": null,
    "og_title": "Understanding Boosted Configuration Networks (combined neural networks and boosting): An Intuitive Guide Through Their Hyperparameters | R-bloggers",
    "og_description": "How BCN combine neural networks and boosting, explained through the knobs you can turn",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "twitter_title": "Understanding Boosted Configuration Networks (combined neural networks and boosting): An Intuitive Guide Through Their Hyperparameters | R-bloggers",
    "twitter_description": "How BCN combine neural networks and boosting, explained through the knobs you can turn",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "Understanding Boosted Configuration Networks (combined neural networks and boosting): An Intuitive Guide Through Their Hyperparameters\nPosted on\nFebruary 15, 2026\nby\nT. Moudiki\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nT. Moudiki's Webpage - R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nDisclaimer:\nThis post was written with the help of LLMs, based on:\nhttps://thierrymoudiki.github.io/blog/2022/07/21/r/misc/boosted-configuration-networks\nhttps://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks\nhttps://docs.techtonique.net/bcn/articles/bcn-intro.html\nhttps://github.com/Techtonique/bcn_python\nPotential remaining errors are mine.\nWhat if you could have a model that:\nâœ… Captures non-linear patterns like neural networks\nâœ… Builds iteratively like gradient boosting\nâœ… Provides built-in interpretability through its additive structure\nâœ… Works well on regression, classitication, time series\nThatâ€™s\nBoosted Configuration Networks (BCN)\n.\nWhere BCN fits:\nBCN sits between Neural Additive Models (NAMs) and gradient boostingâ€”combining neural flexibility with boostingâ€™s greedy refinement. Itâ€™s particularly effective for:\nMedium-sized tabular datasets (100s to 10,000s of rows)\nMultivariate prediction tasks (multiple outputs that share structure)\nProblems requiring both accuracy and interpretability\nTime series forecasting with multiple related series\nIn this post, Iâ€™ll explain BCNâ€™s intuition by walking through its hyperparameters. Each parameter reveals something fundamental about how the algorithm works.\nThe Core Idea: Building Smart Weak Learners\nBCN asks a simple question at each iteration:\nâ€œWhatâ€™s the\nbest\nartificial neural network feature I can add right now to explain what I havenâ€™t captured yet?â€\nLetâ€™s break down this sentence:\n1. artificial\nâ€œNeural network featureâ€\nAt each iteration L, a BCN creates a simple single-layer feedforward neural network:\nh_L = activation(w_L^T Â· x)\nThis is just: multiply features by weights, then apply an activation function (tanh or sigmoid; bounded).\n2.\nBest\nBCN finds weights\nw_L\nthat\nmaximize how much this feature explains the residuals\n.\nSpecifically, it finds the artificial neural network whose output has the\nlargest regression coefficient\nwhen predicting the residuals. This is captured in the Î¾ (xi) criterion:\nÎ¾ = Î½(2-Î½)Â·Î²Â²_L - penalty\nwhere\nÎ²_L\nis the least-squares coefficient from regressing residuals on\nh_L\n.\n3.\nâ€œWhat I havenâ€™t captured yetâ€\nLike all boosting methods, BCN works on\nresiduals\nâ€“ the gap between current predictions and truth. Each iteration â€œcarves awayâ€ at the error.\n4.\nâ€œAddâ€\nOnce we find the\nbest\nh_L\n, we add it to our ensemble:\nnew prediction = old prediction + Î½ Â· Î²_L Â· h_L\nVisual mental model:\nImagine starting with the mean prediction (flat surface). Each iteration adds a â€œbumpâ€ (artificial neural network feature) where the residuals are largest, gradually sculpting a complex prediction surface.\nNow letâ€™s see how the hyperparameters control this process.\nHyperparameter Priority: The Big Three\nBefore diving deep, hereâ€™s how parameters rank by impact:\nTier 1 â€“ Critical (tune these first):\nB\n(iterations): Model complexity\nnu\n(learning rate): Step size and stability\nlam\n(weight bounds): Feature complexity\nTier 2 â€“ Regularization (tune for robustness):\nr\n(convergence rate, most of the times 0.99, 0.999, 0.9999, etc.): Adaptive quality control\ncol_sample\n(feature sampling): Regularization via randomness\nTier 3 â€“ Technical (usually keep defaults):\ntype_optim\n(optimizer): Computational trade-offs\nactivation\n(nonlinearity): Usually tanh because bounded\nhidden_layer_bias\n: Usually TRUE\ntol\n(tolerance): Early stopping\nHyperparameter 1:\nB\n(Number of Iterations)\nDefault:\nNo universal default (typically 100-500)\nWhat it controls:\nHow many weak learners to train\nIntuition:\nBCN builds your model piece by piece. Each iteration adds one artificial neural network feature that explains some of what you havenâ€™t captured.\nTrade-offs:\nSmall B (10-50):\nâœ… Fast training\nâœ… Less risk of overfitting\nâŒ May underfit complex relationships\nLarge B (100-1000):\nâœ… Can capture subtle patterns\nâœ… Better accuracy on complex tasks\nâŒ Slower training\nâŒ Risk of overfitting without other regularization\nRule of thumb:\nStart with B=100. If using early stopping (\ntol\n> 0), set B high (500-1000) and let the algorithm stop when improvement plateaus.\nWhatâ€™s happening internally:\nEach iteration finds weights\nw_L\nthat maximize:\nÎ¾_L = Î½(2-Î½) Â· Î²Â²_L - penalty\nwhere Î²Â²_L measures how much the neural network feature correlates with residuals.\nHyperparameter 2:\nnu\n(Learning Rate)\nDefault:\n0.1 (conservative)\nTypical range:\n0.1-0.8\nSweet spot:\n0.3-0.5\nWhat it controls:\nHow aggressively to use each weak learner\nIntuition:\nEven if you find a great neural network feature, you might not want to use it at full strength. The learning rate controls the step size.\nWhen BCN finds a good feature h_L with coefficient Î²_L, it updates predictions by:\nprediction += nu Â· Î²_L Â· h_L\nTrade-offs:\nSmall Î½ (0.1-0.3):\nâœ… More stable training\nâœ… Better generalization (smooths out noise)\nâœ… Less sensitive to individual weak learners\nâŒ Need more iterations (larger B)\nâŒ Slower convergence\nLarge Î½ (0.5-1.0):\nâœ… Faster convergence\nâœ… Fewer iterations needed\nâŒ Risk of overfitting\nâŒ Can be unstable\nWhy Î½(2-Î½) appears in the math:\nThis factor arises when we want to prove the convergence of residualsâ€™ L2-norm towards 0. Itâ€™s\nmaximized at Î½=1\n(full gradient step):\nf(Î½) = 2Î½ - Î½Â² \nf'(Î½) = 2 - 2Î½ = 0  âŸ¹  Î½ = 1\nThis ensures stability for Î½ âˆˆ (0,2) and explains why Î½=1 is the â€œnaturalâ€ full step.\nThink of it like:\nÎ½=0.1: â€œI trust each feature a little, build slowlyâ€ (like learning rate 0.01 in SGD)\nÎ½=0.5: â€œI trust each feature moderately, build steadilyâ€\nÎ½=1.0: â€œI trust each feature fully, build quicklyâ€ (can be unstable)\nHyperparameter 3:\nlam\n(Î» â€“ Weight Bounds)\nDefault:\n0.1\nTypical range:\n0.1-100 (often on log scale: 10^0 to 10^2)\nSweet spot:\n10^(0.5 to 1.0) â‰ˆ 3-10\nWhat it controls:\nHow large the neural network weights can be\nIntuition:\nThis constrains the weights\nw_L\nat each iteration to the range [-Î», Î»]. Itâ€™s a form of regularization through box constraints.\n# Tight constraints: simpler features\nfit_simple <- bcn(x, y, lam = 0.5)\n\n# Loose constraints: more complex features\nfit_complex <- bcn(x, y, lam = 10.0)\nWhy this matters:\nSmall Î» (0.1-1.0):\nNeural network features are â€œgentleâ€ (bounded outputs)\nLess risk of overfitting\nMay miss complex interactions\nâœ… Use for: Small datasets, high interpretability needs\nLarge Î» (5-100):\nNeural network features can be more â€œextremeâ€\nCan capture stronger non-linearities\nRisk of overfitting if not balanced with other regularization\nâœ… Use for: Complex patterns, large datasets\nWhatâ€™s happening mathematically:\nAt each iteration, we solve:\nmaximize Î¾(w_L)\nsubject to: -Î» â‰¤ w_L,j â‰¤ Î» for all features j\nThis is a\nconstrained optimization\n- weâ€™re finding the\nbest\nweights within a box.\nThink of it like:\nSmall Î»: â€œKeep the weak learners simpleâ€ (like Lâˆ regularization)\nLarge Î»: â€œAllow complex weak learnersâ€\nNote on consistency:\nIn the code, this parameter is\nlam\n(avoiding the Greek letter for R compatibility).\nHyperparameter 4:\nr\n(Convergence Rate)\nDefault:\n0.3\nTypical range:\n0.3-0.99\nSweet spot:\n0.9-0.99999\nWhat it controls:\nHow the acceptance threshold changes over iterations\nIntuition:\nThis is the\nmost subtle\nhyperparameter. It controls how picky BCN is about accepting new weak learners, and this pickiness\ndecreases\nas training progresses. Think of\nr\nas the â€œpatienceâ€ or â€œquality controlâ€ officer: high r means â€œOnly the best features get through the door early on.â€\nThe acceptance criterion:\nBCN only accepts a weak learner if:\nÎ¾_L = Î½(2-Î½)Â·Î²Â²_L - [1 - r - (1-r)/(L+1)]Â·||residuals||Â² â‰¥ 0\nThe penalty term\n[1 - r - (1-r)/(L+1)]\ndecreases\nas L increases:\nIteration L\nr = 0.95\nr = 0.70\nr = 0.50\nL = 1\n0.075\n0.45\n0.75\nL = 10\n0.055\n0.33\n0.55\nL = 100\n0.050\n0.30\n0.50\nL â†’ âˆ\n0.050\n0.30\n0.50\nInterpretation:\nThe penalty starts higher and converges to\n(1-r)\nas training progresses.\nTrade-offs:\nLarge r (0.9-0.99):\nEarly iterations: very picky (high penalty)\nLater iterations: more permissive\nâœ… Prevents premature commitment to poor features\nâœ… Allows fine-tuning in later iterations\nâœ… Better generalization\nâœ… Use for: Production models, complex tasks\nSmall r (0.3-0.7):\nLess selective throughout training\nâœ… Accepts more weak learners\nâœ… Faster initial progress\nâŒ May accept noisy features early\nâœ… Use for: Quick prototyping, exploratory work\nThe dynamic threshold:\nRearranging the acceptance criterion:\nRequired RÂ² > [1 - r - (1-r)/(L+1)] / [Î½(2-Î½)]\nThis creates an\nadaptive\nselection criterion that evolves during training.\nThink of it like:\nHigh r: â€œBe very careful early on (we have lots of iterations left), but allow refinements laterâ€\nLow r: â€œAccept good-enough features throughout trainingâ€\nHyperparameter 5:\ncol_sample\n(Feature Sampling)\nDefault:\n1.0 (no sampling)\nTypical range:\n0.3-1.0\nSweet spot:\n0.5-0.7 for high-dimensional data\nWhat it controls:\nWhat fraction of features to consider at each iteration\nIntuition:\nLike Random Forests, BCN can use only a random subset of features at each iteration. This reduces overfitting, adds diversity, and speeds up computation.\n# Use all features (no sampling)\nfit_full <- bcn(x, y, col_sample = 1.0)\n\n# Use 50% of features at each iteration\nfit_sampled <- bcn(x, y, col_sample = 0.5)\nHow it works:\nAt iteration L, randomly sample\ncol_sample Ã— d\nfeatures and optimize only over those:\nw_L âˆˆ R^d_reduced    (instead of R^d)\nDifferent features are sampled at each iteration, creating diversity like Random Forests but for neural network features.\nTrade-offs:\ncol_sample = 1.0 (no sampling):\nâœ… Can use all information\nâœ… Potentially better accuracy\nâŒ Slower training (larger optimization)\nâŒ Higher overfitting risk\nâœ… Use for: Small datasets (N < 1000), few features (d < 50)\ncol_sample = 0.3-0.7:\nâœ… Faster training (smaller optimization)\nâœ… Regularization effect (like Random Forests)\nâœ… More diverse weak learners\nâŒ May miss important feature combinations\nâœ… Use for: Large datasets, many features (d > 100)\nInteraction with B:\nColumn sampling as implicit regularization means you may need more iterations:\nHyperparameter 6:\nactivation\n(Activation Function)\nDefault:\nâ€œtanhâ€\nOptions:\nâ€œtanhâ€, â€œsigmoidâ€\nWhat it controls:\nThe non-linearity in each weak learner\nIntuition:\nThis determines the shape of transformations each neural network can create.\nCharacteristics:\ntanh (hyperbolic tangent):\ntanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\nRange: [-1, 1]\nSymmetric around 0\nGradient: 1 - tanhÂ²(z)\nGood for:\nMost tasks, especially when features are centered\nâœ…\nRecommended default\nsigmoid:\nsigmoid(z) = 1 / (1 + e^(-z))\nRange: [0, 1]\nAsymmetric\nGradient: sigmoid(z) Â· (1 - sigmoid(z))\nGood for:\nWhen outputs are probabilities or rates\nWhy bounded activations?\nBCN requires bounded activations for theoretical guarantees and stability of the Î¾ criterion. Unbounded activations like ReLU are\nnot recommended\nbecause:\nTheoretical issues: The Î¾ optimization assumes bounded activation\noutputs\nStability: Unbounded outputs can destabilize the ensemble\nWhile ReLU could\ntheoretically\nwork with very tight weight constraints (small Î»), tanh/sigmoid provide stronger guarantees\nRule of thumb:\nUse\ntanh\nas default. Itâ€™s more balanced, bounded and zero-centered.\nHyperparameter 7:\ntol\n(Early Stopping Tolerance)\nDefault:\n0 (no early stopping)\nTypical range:\n1e-7 to 1e-3\nRecommended:\n1e-7 for most tasks\nWhat it controls:\nWhen to stop training before reaching B iterations\nIntuition:\nIf the model stops improving (residual norm isnâ€™t decreasing much), stop early to avoid overfitting and save computation.\nHow it works (corrected):\nBCN tracks the relative improvement in residual norm and stops if progress is too slow:\nif (relative_decrease_in_residuals < tol):\n    stop training\nImportant clarification:\nEarly stopping is based on\nimprovement rate\n, not absolute residual magnitude. This means BCN can stop even when residuals are still large (on a hard problem) if adding more weak learners doesnâ€™t help.\nTrade-offs:\ntol = 0 (no early stopping):\nAlways trains for exactly B iterations\nMay overfit if B is too large\nâœ… Use for: Quick experiments with small B\ntol = 1e-7 to 1e-5:\nStops when improvement becomes negligible\nPrevents overfitting\nCan save significant computation\nâœ… Use for: Production models with large B\nPractical tip:\nSet B large (e.g., 500-1000) and tol small (e.g., 1e-7) to let the algorithm decide when to stop. The actual number of iterations used will be stored in\nfit$maxL\n.\nHyperparameter 8:\ntype_optim\n(Optimization Method)\nGradient-based.\nDefault:\nâ€œnlminbâ€\nOptions:\nâ€œnlminbâ€, â€œadamâ€, â€œsgdâ€, â€œnmkbâ€, â€œhjkbâ€, â€œrandomsearchâ€\nWhat it controls:\nHow to solve the optimization problem at each iteration\nIntuition:\nFinding the best weights w_L is a\nnon-convex optimization\nproblem. Different solvers have different trade-offs.\nAvailable optimizers:\nnlminb\n(default):\nUses gradient and Hessian approximations\nâœ… Robust\nâœ… Well-tested in R\nâœ… Works well in most cases\nâš ï¸ Medium speed\nâœ… Use for: General purpose, production\nadam / sgd:\nGradient-based optimizers from deep learning\nâœ… Fast, especially for high-dimensional problems\nâœ… Good for large d (many features)\nâš ï¸ May need tuning (learning rate, iterations)\nâœ… Use for: d > 100, speed-critical applications\nnmkb / hjkb:\nDerivative-free Nelder-Mead / Hooke-Jeeves\nâœ… Very robust (no gradient needed)\nâŒ Slow\nâœ… Use when: Other optimizers fail or diverge\nrandomsearch:\nRandom sampling + local search\nâœ… Can escape local minima\nâŒ Slower\nâœ… Use when: Problem is very non-convex\nRule of thumb:\nStart with\n\"nlminb\"\nIf training is slow and d > 100, try\n\"adam\"\nCan pass additional arguments via\n...\n(e.g., max iterations, tolerance)\nImportant insight:\nBecause BCN uses an ensemble,\nlocal optima are OK\n! Even if we donâ€™t find the globally optimal w_L, the next iteration can compensate. This is why BCN is robust despite non-convex optimization at each step.\nHyperparameter 9:\nhidden_layer_bias\n(Include Bias Term)\nDefault:\nTRUE\nOptions:\nTRUE, FALSE\nWhat it controls:\nWhether neural networks have a bias/intercept term\nIntuition:\nWithout bias, h_L = activation(w^T x). With bias, h_L = activation(w^T x + b).\nTrade-offs:\nhidden_layer_bias = FALSE:\nSimpler optimization (one less parameter per iteration)\nFaster training\nAssumes data is centered\nâœ… Use when: Features are already centered, want pure multiplicative effects\nhidden_layer_bias = TRUE:\nMore expressive (can handle shifts)\nCan handle non-centered data better\nOne additional parameter to optimize per iteration\nâœ…\nRecommended default\n- safer choice\nTypical choice:\nUse TRUE unless you have a specific reason not to (e.g., theoretical interest in purely multiplicative models).\nHyperparameter 10:\nn_clusters\n(Optional Clustering Features)\nDefault:\nNULL (no clustering)\nTypical range:\n2-10\nWhat it controls:\nWhether to add cluster membership features\nIntuition:\nBCN can automatically perform k-means clustering on your inputs and add cluster memberships as additional features. This can help capture local patterns.\nWhen to use:\nâœ… Data has natural groupings or modes\nâœ… Local patterns differ across regions of feature space\nâŒ Not needed for most standard regression/classification\nNote:\nThis is an advanced feature - start without it and add only if needed.\nPutting It All Together: Hyperparameter Recipes\nRecipe 1: Fast Prototyping (Small Dataset, N < 1000)\nfit <- bcn(\n  x = X_train, \n  y = y_train,\n  B = 50,              # Few iterations for speed\n  nu = 0.5,            # Moderate learning rate\n  col_sample = 1.0,    # Use all features (dataset is small)\n  lam = 10^0.5,        # ~3.16, moderate regularization\n  r = 0.9,             # Adaptive threshold\n  tol = 1e-5,          # Early stopping\n  activation = \"tanh\",\n  type_optim = \"nlminb\",\n  hidden_layer_bias = TRUE\n)\nWhy these choices:\nSmall B for speed\nHigh nu for faster convergence\nNo column sampling (dataset is small)\nStandard other parameters\nExpected performance:\nQuick baseline in minutes\nRecipe 2: Production Model (Medium Dataset, N ~ 10,000)\nfit <- bcn(\n  x = X_train,\n  y = y_train,\n  B = 200,             # Enough iterations with early stopping\n  nu = 0.3,            # Conservative for stability\n  col_sample = 0.6,    # Some regularization\n  lam = 10^0.8,        # ~6.31, allow some complexity\n  r = 0.95,            # Very selective early on\n  tol = 1e-7,          # Train until converged\n  activation = \"tanh\",\n  type_optim = \"nlminb\",\n  hidden_layer_bias = TRUE\n)\nWhy these choices:\nModerate B with early stopping safety\nConservative nu for stability\nColumn sampling for regularization\nHigh r for careful feature selection\nExpected performance:\nRobust model, may train 100-150 iterations before stopping\nRecipe 3: Complex Task (Large Dataset, High-Dimensional)\nfit <- bcn(\n  x = X_train,\n  y = y_train,\n  B = 500,             # Many iterations (will stop early if needed)\n  nu = 0.4,            # Balanced\n  col_sample = 0.5,    # Strong regularization for high d\n  lam = 10^1.0,        # 10, higher complexity allowed\n  r = 0.95,            # Adaptive\n  tol = 1e-7,          # Early stopping safety\n  activation = \"tanh\",\n  type_optim = \"adam\",  # Fast optimizer for high d\n  hidden_layer_bias = TRUE\n)\nWhy these choices:\nLarge B to capture complexity\nColumn sampling crucial for high dimensions (d > 100)\nAdam optimizer for speed with many features\nHigh r to prevent early overfitting\nExpected performance:\nMay use 200-400 iterations, handles d > 500 well\nRecipe 4: Multivariate Time Series / Multi-Output\nfit <- bcn(\n  x = X_train,\n  y = Y_train,         # Matrix with multiple outputs (e.g., N x m)\n  B = 300,\n  nu = 0.5,            # Can be higher for shared structure\n  col_sample = 0.7,\n  lam = 10^0.7,\n  r = 0.95,            # Critical: enforces shared structure\n  tol = 1e-7,\n  activation = \"tanh\",\n  type_optim = \"nlminb\"\n)\nWhy these choices:\nHigh r is critical\n: In multivariate mode, BCN computes Î¾_k for each output k and requires min_k(Î¾_k) â‰¥ 0 for acceptance. This ensures each weak learner contributes meaningfully across\nall\ntime series/outputs, creating shared representations.\nHigher nu because shared structure is more stable\nStandard B with early stopping\nNote on multivariate:\nBCN handles multiple outputs naturally through one-hot encoding (classification) or matrix targets (regression). The min(Î¾) criterion prevents sacrificing one output to improve another.\nExpected performance:\nStrong on related time series or multi-task learning\nUnderstanding Hyperparameter Interactions\nInteraction 1:\nnu\nÃ—\nB\nâ‰ˆ Constant\nTrade-off:\nSmall nu needs large B\n# Approximately equivalent final predictions:\nfit1 <- bcn(B = 100, nu = 0.5)\nfit2 <- bcn(B = 200, nu = 0.25)\nWhy:\nSmaller steps need more iterations to reach similar places.\nRule:\nFor similar model complexity,\nnu Ã— B â‰ˆ constant\n(approximately).\nIn practice:\nProduction (stability priority): nu = 0.3, B = 300\nPrototyping (speed priority): nu = 0.5, B = 100\nInteraction 2:\nlam\nâ†”\nr\n(Complexity Control)\nBoth control complexity:\nlam\n: How complex each weak learner can be\nr\n: How selective we are about accepting weak learners\n# More regularization\nfit_reg <- bcn(lam = 1.0, r = 0.95)   # Simple features, selective\n\n# Less regularization  \nfit_complex <- bcn(lam = 10.0, r = 0.7)  # Complex features, permissive\nBalance principle:\nIf you allow complex features (high lam), be selective (high r) to avoid noise.\nTypical combinations:\nHigh quality\n: lam = 10, r = 0.95 â†’ Complex but carefully selected features\nModerate\n: lam = 5, r = 0.90 â†’ Balanced\nFast/loose\n: lam = 3, r = 0.80 â†’ Simple features, permissive\nInteraction 3:\ncol_sample\nâ†”\nB\n(Coverage)\nColumn sampling as implicit regularization:\n# Fewer features per iteration â†’ need more iterations for coverage\nfit1 <- bcn(col_sample = 1.0, B = 100)\nfit2 <- bcn(col_sample = 0.5, B = 200)\nRough guideline:\nB_needed â‰ˆ B_baseline / col_sample\nIn practice:\ncol_sample = 1.0 â†’ B = 100-200\ncol_sample = 0.5 â†’ B = 200-400\ncol_sample = 0.3 â†’ B = 300-500\nThe Mathematical Connection: How Hyperparameters Appear in Î¾\nThe core optimization criterion ties everything together:\nÎ¾_L = Î½(2-Î½) Â· Î²Â²_L - [1 - r - (1-r)/(L+1)] Â· ||residuals||Â²\n      â””â”€â”¬â”€â”€â”˜   â””â”¬â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       nu    optimized over        r\n             w âˆˆ [-lam, lam]\nReading the formula:\nFind w_L (constrained by\nlam\n) that maximizes Î²Â²_L\nÎ²_L is the OLS coefficient: Î²_L = (h_L^T Â· residuals) /\nh_L\nÂ²\nScale by Î½(2-Î½) (controlled by\nnu\n)\nSubtract penalty (controlled by\nr\n)\nAccept only if Î¾ â‰¥ 0 for all outputs\nRepeat for\nB\niterations (or until\ntol\nreached)\nAt each step, sample\ncol_sample\nfraction of features\nThis unified view shows how all hyperparameters work together to control the greedy feature selection process.\nPractical Tips for Hyperparameter Tuning\nStart Simple, Add Complexity\nBegin with defaults:\nfit <- bcn(x, y, B = 100, nu = 0.3, lam = 10^0.7, r = 0.9)\nIf underfitting (train error too high):\nâ†‘ Increase B (more capacity)\nâ†‘ Increase lam (allow more complex features)\nâ†‘ Increase nu (use features more aggressively)\nâ†“ Decrease r (be less selective)\nIf overfitting (train Â«Â test error):\nâ†“ Decrease nu (smaller, more careful steps)\nâ†“ Decrease lam (simpler features)\nAdd column sampling (col_sample = 0.5-0.7)\nâ†‘ Increase r (be more selective)\nUse early stopping (tol = 1e-7)\nUse Cross-Validation Wisely\nMost important to tune:\nB\n,\nnu\n,\nlam\nModerately important:\nr\n,\ncol_sample\nUsually fixed:\nhidden_layer_bias = TRUE\n,\ntype_optim = \"nlminb\"\n,\nactivation = \"tanh\"\nExample CV strategy:\nlibrary(caret)\n\n# Grid search on log-scale for lam\ngrid <- expand.grid(\n  B = c(100, 200, 500),\n  nu = c(0.1, 0.3, 0.5),\n  lam = 10^seq(0, 1.5, by = 0.5)  # 1, 3.16, 10, 31.6\n)\n\n# Use caret, mlr3, or tidymodels for CV\nMonitor Training\n# Enable verbose output\nfit <- bcn(x, y, verbose = 1, show_progress = TRUE)\nWatch for:\nHow fast\nresiduals\n_F decreases (convergence rate)\nWhether Î¾ stays positive (quality of weak learners)\nIf training stops early and at what iteration (capacity needs)\nDiagnostic patterns:\nResiduals plateau early â†’ Increase B or lam\nÎ¾ often negative â†’ Decrease r or increase lam\nTraining very slow â†’ Try adam optimizer or increase col_sample\nQuick Reference: Hyperparameter Cheat Sheet\nHyperparameter\nLow Value Effect\nHigh Value Effect\nTypical Range\nDefault\nB\nSimple, fast, may underfit\nComplex, slow, may overfit\n50-1000\n100\nnu\nStable, slow convergence\nFast, potentially unstable\n0.1-0.8\n0.1\nlam\nLinear-ish, simple features\nNonlinear, complex features\n1-100\n0.1\nr\nPermissive, accepts more\nSelective, high quality\n0.3-0.99\n0.3\ncol_sample\nNo regularization\nStrong regularization\n0.3-1.0\n1.0\ntol\nNo early stop\nAggressive early stop\n0-1e-3\n0\nactivation\ntanh (symmetric)\nsigmoid (asymmetric)\n-\ntanh\ntype_optim\nnlminb (robust)\nadam (fast)\n-\nnlminb\nhidden_layer_bias\nSimpler, through origin\nMore flexible\n-\nTRUE\nWhen NOT to Use BCN\nWhile BCN is versatile, itâ€™s not always the best choice:\nâŒ\nUltra-high-dimensional sparse data (d > 10,000)\nTree-based boosting (XGBoost/LightGBM) may be faster\nColumn sampling helps, but trees handle sparsity natively\nâŒ\nVery large datasets (N > 1,000,000)\nTraining time scales roughly O(B Ã— N Ã— d)\nConsider subsampling or streaming methods\nâŒ\nDeep sequential/temporal structure\nBCN is static (no recurrence)\nUse RNNs/Transformers for complex time dependencies\nâŒ\nImage/text/audio from scratch\nConvolutional/attention architectures more suitable\nBCN works on extracted features (embeddings, tabular)\nâœ…\nBCN shines at:\nTabular data (100s to 10,000s of rows)\nMultivariate prediction (shared structure across outputs)\nNeeding both accuracy AND interpretability\nTime series with extracted features\nWhen XGBoost works but you want gradient-based explanations\nDebugging BCN Training\nSymptom\nLikely Cause\nFix\nÎ¾ frequently negative early\nr too high or lam too low\nDecrease r to 0.8 or increase lam to 5-10\nResiduals plateau quickly\nnu too small or B too low\nIncrease nu to 0.4-0.5 or B to 300+\nTraining very slow\ncol_sample=1 on wide data\nSet col_sample=0.5 and try type_optim=â€adamâ€\nHigh train accuracy, poor test\nOverfitting\nDecrease nu, increase r, add col_sample < 1\nPoor train accuracy\nUnderfitting\nIncrease B, increase lam, try different activation\nOptimizer not converging\nBad initialization or scaling\nCheck feature scaling, try different type_optim\nInterpretability Example\nOne of BCNâ€™s unique advantages is\ngradient-based interpretability\n.\nWhat makes this special:\nâœ… Exact analytic gradients (no approximation)\nâœ… Same O(B Ã— m Ã— d) cost as prediction\nâœ… Shows direction of influence (positive/negative)\nâœ… Works for both regression and classification\nâœ… Much faster than SHAP on tree ensembles\nConclusion: The Philosophy of BCN\nBCNâ€™s hyperparameters reveal its design philosophy:\n1. Iterative Refinement\n(via\nB\n)\nBuild the model piece by piece, adding one well-chosen feature at a time.\n2. Conservative Steps\n(via\nnu\n)\nDonâ€™t trust any single feature too much - combine many weak learners.\n3. Bounded Complexity\n(via\nlam\n)\nKeep individual weak learners simple to ensure stability and interpretability.\n4. Adaptive Selection\n(via\nr\n)\nStart picky (prevent early mistakes), become permissive (allow refinement).\n5. Randomization\n(via\ncol_sample\n)\nLike Random Forests, diversity through randomness helps generalization.\n6. Early Stopping\n(via\ntol\n)\nKnow when to stop - more iterations arenâ€™t always better.\n7. Explicit Optimization for Interpretability\nUnlike methods that require post-hoc explanations, BCN is designed with interpretability in mind through its additive structure and differentiable components.\nTogether, these create a model thatâ€™s:\nâœ…\nExpressive\n(neural network features capture non-linearity)\nâœ…\nInterpretable\n(additive structure + gradients)\nâœ…\nRobust\n(ensemble of bounded weak learners)\nâœ…\nEfficient\n(sparse structure, early stopping, column sampling)\nNext Steps\nTo learn more:\nğŸ“¦\nBCN R Package on GitHub\nğŸ“¦\nBCN Python Package on GitHub\nğŸ“\nResearch preprint on BCN\nTo contribute:\nBCN is open source! Contributions welcome for:\nNew activation functions\nAdditional optimization methods\nInterpretability visualizations\nBenchmark studies and applications\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nT. Moudiki's Webpage - R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-399006 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Understanding Boosted Configuration Networks (combined neural networks and boosting): An Intuitive Guide Through Their Hyperparameters</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 15, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/t-moudiki/\">T. Moudiki</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://thierrymoudiki.github.io//blog/2026/02/16/r/python/bcn-explained\"> T. Moudiki's Webpage - R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p><strong>Disclaimer:</strong> This post was written with the help of LLMs, based on:</p>\n<ul>\n<li><a href=\"https://thierrymoudiki.github.io/blog/2022/07/21/r/misc/boosted-configuration-networks\" rel=\"nofollow\" target=\"_blank\">https://thierrymoudiki.github.io/blog/2022/07/21/r/misc/boosted-configuration-networks</a></li>\n<li><a href=\"https://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks\" rel=\"nofollow\" target=\"_blank\">https://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks</a></li>\n<li><a href=\"https://docs.techtonique.net/bcn/articles/bcn-intro.html\" rel=\"nofollow\" target=\"_blank\">https://docs.techtonique.net/bcn/articles/bcn-intro.html</a></li>\n<li><a href=\"https://github.com/Techtonique/bcn_python\" rel=\"nofollow\" target=\"_blank\">https://github.com/Techtonique/bcn_python</a></li>\n</ul>\n<p>Potential remaining errors are mine.</p>\n<hr/>\n<p>What if you could have a model that:</p>\n<ul>\n<li>âœ… Captures non-linear patterns like neural networks</li>\n<li>âœ… Builds iteratively like gradient boosting</li>\n<li>âœ… Provides built-in interpretability through its additive structure</li>\n<li>âœ… Works well on regression, classitication, time series</li>\n</ul>\n<p>Thatâ€™s <strong>Boosted Configuration Networks (BCN)</strong>.</p>\n<p><strong>Where BCN fits:</strong> BCN sits between Neural Additive Models (NAMs) and gradient boostingâ€”combining neural flexibility with boostingâ€™s greedy refinement. Itâ€™s particularly effective for:</p>\n<ul>\n<li>Medium-sized tabular datasets (100s to 10,000s of rows)</li>\n<li>Multivariate prediction tasks (multiple outputs that share structure)</li>\n<li>Problems requiring both accuracy and interpretability</li>\n<li>Time series forecasting with multiple related series</li>\n</ul>\n<p>In this post, Iâ€™ll explain BCNâ€™s intuition by walking through its hyperparameters. Each parameter reveals something fundamental about how the algorithm works.</p>\n<hr/>\n<h2 id=\"the-core-idea-building-smart-weak-learners\">The Core Idea: Building Smart Weak Learners</h2>\n<p>BCN asks a simple question at each iteration:</p>\n<blockquote>\n<p>â€œWhatâ€™s the <em>best</em> artificial neural network feature I can add right now to explain what I havenâ€™t captured yet?â€</p>\n</blockquote>\n<p>Letâ€™s break down this sentence:</p>\n<h3 id=\"1-artificial-neural-network-feature\">1. artificial <strong>â€œNeural network featureâ€</strong></h3>\n<p>At each iteration L, a BCN creates a simple single-layer feedforward neural network:</p>\n<pre>h_L = activation(w_L^T Â· x)\n</pre>\n<p>This is just: multiply features by weights, then apply an activation function (tanh or sigmoid; bounded).</p>\n<h3 id=\"2-best\">2. <strong><em>Best</em></strong></h3>\n<p>BCN finds weights <code>w_L</code> that <strong>maximize how much this feature explains the residuals</strong>.</p>\n<p>Specifically, it finds the artificial neural network whose output has the <strong>largest regression coefficient</strong> when predicting the residuals. This is captured in the Î¾ (xi) criterion:</p>\n<pre>Î¾ = Î½(2-Î½)Â·Î²Â²_L - penalty\n</pre>\n<p>where <code>Î²_L</code> is the least-squares coefficient from regressing residuals on <code>h_L</code>.</p>\n<h3 id=\"3-what-i-havent-captured-yet\">3. <strong>â€œWhat I havenâ€™t captured yetâ€</strong></h3>\n<p>Like all boosting methods, BCN works on <strong>residuals</strong> â€“ the gap between current predictions and truth. Each iteration â€œcarves awayâ€ at the error.</p>\n<h3 id=\"4-add\">4. <strong>â€œAddâ€</strong></h3>\n<p>Once we find the <em>best</em> <code>h_L</code>, we add it to our ensemble:</p>\n<pre>new prediction = old prediction + Î½ Â· Î²_L Â· h_L\n</pre>\n<p><strong>Visual mental model:</strong> Imagine starting with the mean prediction (flat surface). Each iteration adds a â€œbumpâ€ (artificial neural network feature) where the residuals are largest, gradually sculpting a complex prediction surface.</p>\n<p>Now letâ€™s see how the hyperparameters control this process.</p>\n<hr/>\n<h2 id=\"hyperparameter-priority-the-big-three\">Hyperparameter Priority: The Big Three</h2>\n<p>Before diving deep, hereâ€™s how parameters rank by impact:</p>\n<p><strong>Tier 1 â€“ Critical (tune these first):</strong></p>\n<ul>\n<li><code>B</code> (iterations): Model complexity</li>\n<li><code>nu</code> (learning rate): Step size and stability</li>\n<li><code>lam</code> (weight bounds): Feature complexity</li>\n</ul>\n<p><strong>Tier 2 â€“ Regularization (tune for robustness):</strong></p>\n<ul>\n<li><code>r</code> (convergence rate, most of the times 0.99, 0.999, 0.9999, etc.): Adaptive quality control</li>\n<li><code>col_sample</code> (feature sampling): Regularization via randomness</li>\n</ul>\n<p><strong>Tier 3 â€“ Technical (usually keep defaults):</strong></p>\n<ul>\n<li><code>type_optim</code> (optimizer): Computational trade-offs</li>\n<li><code>activation</code> (nonlinearity): Usually tanh because bounded</li>\n<li><code>hidden_layer_bias</code>: Usually TRUE</li>\n<li><code>tol</code> (tolerance): Early stopping</li>\n</ul>\n<hr/>\n<h2 id=\"hyperparameter-1-b-number-of-iterations\">Hyperparameter 1: <code>B</code> (Number of Iterations)</h2>\n<p><strong>Default:</strong> No universal default (typically 100-500)</p>\n<p><strong>What it controls:</strong> How many weak learners to train</p>\n<p><strong>Intuition:</strong> BCN builds your model piece by piece. Each iteration adds one artificial neural network feature that explains some of what you havenâ€™t captured.</p>\n<p><strong>Trade-offs:</strong></p>\n<ul>\n<li><strong>Small B (10-50):</strong>\n<ul>\n<li>âœ… Fast training</li>\n<li>âœ… Less risk of overfitting</li>\n<li>âŒ May underfit complex relationships</li>\n</ul>\n</li>\n<li><strong>Large B (100-1000):</strong>\n<ul>\n<li>âœ… Can capture subtle patterns</li>\n<li>âœ… Better accuracy on complex tasks</li>\n<li>âŒ Slower training</li>\n<li>âŒ Risk of overfitting without other regularization</li>\n</ul>\n</li>\n</ul>\n<p><strong>Rule of thumb:</strong> Start with B=100. If using early stopping (<code>tol</code> &gt; 0), set B high (500-1000) and let the algorithm stop when improvement plateaus.</p>\n<p><strong>Whatâ€™s happening internally:</strong>\nEach iteration finds weights <code>w_L</code> that maximize:</p>\n<pre>Î¾_L = Î½(2-Î½) Â· Î²Â²_L - penalty\n</pre>\n<p>where Î²Â²_L measures how much the neural network feature correlates with residuals.</p>\n<hr/>\n<h2 id=\"hyperparameter-2-nu-learning-rate\">Hyperparameter 2: <code>nu</code> (Learning Rate)</h2>\n<p><strong>Default:</strong> 0.1 (conservative)<br/>\n<strong>Typical range:</strong> 0.1-0.8<br/>\n<strong>Sweet spot:</strong> 0.3-0.5</p>\n<p><strong>What it controls:</strong> How aggressively to use each weak learner</p>\n<p><strong>Intuition:</strong> Even if you find a great neural network feature, you might not want to use it at full strength. The learning rate controls the step size.</p>\n<p>When BCN finds a good feature h_L with coefficient Î²_L, it updates predictions by:</p>\n<pre>prediction += nu Â· Î²_L Â· h_L\n</pre>\n<p><strong>Trade-offs:</strong></p>\n<ul>\n<li><strong>Small Î½ (0.1-0.3):</strong>\n<ul>\n<li>âœ… More stable training</li>\n<li>âœ… Better generalization (smooths out noise)</li>\n<li>âœ… Less sensitive to individual weak learners</li>\n<li>âŒ Need more iterations (larger B)</li>\n<li>âŒ Slower convergence</li>\n</ul>\n</li>\n<li><strong>Large Î½ (0.5-1.0):</strong>\n<ul>\n<li>âœ… Faster convergence</li>\n<li>âœ… Fewer iterations needed</li>\n<li>âŒ Risk of overfitting</li>\n<li>âŒ Can be unstable</li>\n</ul>\n</li>\n</ul>\n<p><strong>Why Î½(2-Î½) appears in the math:</strong></p>\n<p>This factor arises when we want to prove the convergence of residualsâ€™ L2-norm towards 0. Itâ€™s <strong>maximized at Î½=1</strong> (full gradient step):</p>\n<pre>f(Î½) = 2Î½ - Î½Â² \nf'(Î½) = 2 - 2Î½ = 0  âŸ¹  Î½ = 1\n</pre>\n<p>This ensures stability for Î½ âˆˆ (0,2) and explains why Î½=1 is the â€œnaturalâ€ full step.</p>\n<p><strong>Think of it like:</strong></p>\n<ul>\n<li>Î½=0.1: â€œI trust each feature a little, build slowlyâ€ (like learning rate 0.01 in SGD)</li>\n<li>Î½=0.5: â€œI trust each feature moderately, build steadilyâ€</li>\n<li>Î½=1.0: â€œI trust each feature fully, build quicklyâ€ (can be unstable)</li>\n</ul>\n<hr/>\n<h2 id=\"hyperparameter-3-lam-Î»---weight-bounds\">Hyperparameter 3: <code>lam</code> (Î» â€“ Weight Bounds)</h2>\n<p><strong>Default:</strong> 0.1<br/>\n<strong>Typical range:</strong> 0.1-100 (often on log scale: 10^0 to 10^2)<br/>\n<strong>Sweet spot:</strong> 10^(0.5 to 1.0) â‰ˆ 3-10</p>\n<p><strong>What it controls:</strong> How large the neural network weights can be</p>\n<p><strong>Intuition:</strong> This constrains the weights <code>w_L</code> at each iteration to the range [-Î», Î»]. Itâ€™s a form of regularization through box constraints.</p>\n<pre># Tight constraints: simpler features\nfit_simple &lt;- bcn(x, y, lam = 0.5)\n\n# Loose constraints: more complex features\nfit_complex &lt;- bcn(x, y, lam = 10.0)\n</pre>\n<p><strong>Why this matters:</strong></p>\n<p><strong>Small Î» (0.1-1.0):</strong></p>\n<ul>\n<li>Neural network features are â€œgentleâ€ (bounded outputs)</li>\n<li>Less risk of overfitting</li>\n<li>May miss complex interactions</li>\n<li>âœ… Use for: Small datasets, high interpretability needs</li>\n</ul>\n<p><strong>Large Î» (5-100):</strong></p>\n<ul>\n<li>Neural network features can be more â€œextremeâ€</li>\n<li>Can capture stronger non-linearities</li>\n<li>Risk of overfitting if not balanced with other regularization</li>\n<li>âœ… Use for: Complex patterns, large datasets</li>\n</ul>\n<p><strong>Whatâ€™s happening mathematically:</strong></p>\n<p>At each iteration, we solve:</p>\n<pre>maximize Î¾(w_L)\nsubject to: -Î» â‰¤ w_L,j â‰¤ Î» for all features j\n</pre>\n<p>This is a <strong>constrained optimization</strong> - weâ€™re finding the <em>best</em> weights within a box.</p>\n<p><strong>Think of it like:</strong></p>\n<ul>\n<li>Small Î»: â€œKeep the weak learners simpleâ€ (like Lâˆ regularization)</li>\n<li>Large Î»: â€œAllow complex weak learnersâ€</li>\n</ul>\n<p><strong>Note on consistency:</strong> In the code, this parameter is <code>lam</code> (avoiding the Greek letter for R compatibility).</p>\n<hr/>\n<h2 id=\"hyperparameter-4-r-convergence-rate\">Hyperparameter 4: <code>r</code> (Convergence Rate)</h2>\n<p><strong>Default:</strong> 0.3<br/>\n<strong>Typical range:</strong> 0.3-0.99<br/>\n<strong>Sweet spot:</strong> 0.9-0.99999</p>\n<p><strong>What it controls:</strong> How the acceptance threshold changes over iterations</p>\n<p><strong>Intuition:</strong> This is the <strong>most subtle</strong> hyperparameter. It controls how picky BCN is about accepting new weak learners, and this pickiness <em>decreases</em> as training progresses. Think of <code>r</code> as the â€œpatienceâ€ or â€œquality controlâ€ officer: high r means â€œOnly the best features get through the door early on.â€</p>\n<p><strong>The acceptance criterion:</strong></p>\n<p>BCN only accepts a weak learner if:</p>\n<pre>Î¾_L = Î½(2-Î½)Â·Î²Â²_L - [1 - r - (1-r)/(L+1)]Â·||residuals||Â² â‰¥ 0\n</pre>\n<p>The penalty term <code>[1 - r - (1-r)/(L+1)]</code> <strong>decreases</strong> as L increases:</p>\n<table>\n<thead>\n<tr>\n<th>Iteration L</th>\n<th>r = 0.95</th>\n<th>r = 0.70</th>\n<th>r = 0.50</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>L = 1</td>\n<td>0.075</td>\n<td>0.45</td>\n<td>0.75</td>\n</tr>\n<tr>\n<td>L = 10</td>\n<td>0.055</td>\n<td>0.33</td>\n<td>0.55</td>\n</tr>\n<tr>\n<td>L = 100</td>\n<td>0.050</td>\n<td>0.30</td>\n<td>0.50</td>\n</tr>\n<tr>\n<td>L â†’ âˆ</td>\n<td>0.050</td>\n<td>0.30</td>\n<td>0.50</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Interpretation:</strong> The penalty starts higher and converges to <code>(1-r)</code> as training progresses.</p>\n<p><strong>Trade-offs:</strong></p>\n<p><strong>Large r (0.9-0.99):</strong></p>\n<ul>\n<li>Early iterations: very picky (high penalty)</li>\n<li>Later iterations: more permissive</li>\n<li>âœ… Prevents premature commitment to poor features</li>\n<li>âœ… Allows fine-tuning in later iterations</li>\n<li>âœ… Better generalization</li>\n<li>âœ… Use for: Production models, complex tasks</li>\n</ul>\n<p><strong>Small r (0.3-0.7):</strong></p>\n<ul>\n<li>Less selective throughout training</li>\n<li>âœ… Accepts more weak learners</li>\n<li>âœ… Faster initial progress</li>\n<li>âŒ May accept noisy features early</li>\n<li>âœ… Use for: Quick prototyping, exploratory work</li>\n</ul>\n<p><strong>The dynamic threshold:</strong></p>\n<p>Rearranging the acceptance criterion:</p>\n<pre>Required RÂ² &gt; [1 - r - (1-r)/(L+1)] / [Î½(2-Î½)]\n</pre>\n<p>This creates an <strong>adaptive</strong> selection criterion that evolves during training.</p>\n<p><strong>Think of it like:</strong></p>\n<ul>\n<li>High r: â€œBe very careful early on (we have lots of iterations left), but allow refinements laterâ€</li>\n<li>Low r: â€œAccept good-enough features throughout trainingâ€</li>\n</ul>\n<hr/>\n<h2 id=\"hyperparameter-5-col_sample-feature-sampling\">Hyperparameter 5: <code>col_sample</code> (Feature Sampling)</h2>\n<p><strong>Default:</strong> 1.0 (no sampling)<br/>\n<strong>Typical range:</strong> 0.3-1.0<br/>\n<strong>Sweet spot:</strong> 0.5-0.7 for high-dimensional data</p>\n<p><strong>What it controls:</strong> What fraction of features to consider at each iteration</p>\n<p><strong>Intuition:</strong> Like Random Forests, BCN can use only a random subset of features at each iteration. This reduces overfitting, adds diversity, and speeds up computation.</p>\n<pre># Use all features (no sampling)\nfit_full &lt;- bcn(x, y, col_sample = 1.0)\n\n# Use 50% of features at each iteration\nfit_sampled &lt;- bcn(x, y, col_sample = 0.5)\n</pre>\n<p><strong>How it works:</strong>\nAt iteration L, randomly sample <code>col_sample Ã— d</code> features and optimize only over those:</p>\n<pre>w_L âˆˆ R^d_reduced    (instead of R^d)\n</pre>\n<p>Different features are sampled at each iteration, creating diversity like Random Forests but for neural network features.</p>\n<p><strong>Trade-offs:</strong></p>\n<p><strong>col_sample = 1.0 (no sampling):</strong></p>\n<ul>\n<li>âœ… Can use all information</li>\n<li>âœ… Potentially better accuracy</li>\n<li>âŒ Slower training (larger optimization)</li>\n<li>âŒ Higher overfitting risk</li>\n<li>âœ… Use for: Small datasets (N &lt; 1000), few features (d &lt; 50)</li>\n</ul>\n<p><strong>col_sample = 0.3-0.7:</strong></p>\n<ul>\n<li>âœ… Faster training (smaller optimization)</li>\n<li>âœ… Regularization effect (like Random Forests)</li>\n<li>âœ… More diverse weak learners</li>\n<li>âŒ May miss important feature combinations</li>\n<li>âœ… Use for: Large datasets, many features (d &gt; 100)</li>\n</ul>\n<p><strong>Interaction with B:</strong>\nColumn sampling as implicit regularization means you may need more iterations:</p>\n<hr/>\n<h2 id=\"hyperparameter-6-activation-activation-function\">Hyperparameter 6: <code>activation</code> (Activation Function)</h2>\n<p><strong>Default:</strong> â€œtanhâ€<br/>\n<strong>Options:</strong> â€œtanhâ€, â€œsigmoidâ€</p>\n<p><strong>What it controls:</strong> The non-linearity in each weak learner</p>\n<p><strong>Intuition:</strong> This determines the shape of transformations each neural network can create.</p>\n<p><strong>Characteristics:</strong></p>\n<p><strong>tanh (hyperbolic tangent):</strong></p>\n<pre>tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n</pre>\n<ul>\n<li>Range: [-1, 1]</li>\n<li>Symmetric around 0</li>\n<li>Gradient: 1 - tanhÂ²(z)</li>\n<li><strong>Good for:</strong> Most tasks, especially when features are centered</li>\n<li>âœ… <strong>Recommended default</strong></li>\n</ul>\n<p><strong>sigmoid:</strong></p>\n<pre>sigmoid(z) = 1 / (1 + e^(-z))\n</pre>\n<ul>\n<li>Range: [0, 1]</li>\n<li>Asymmetric</li>\n<li>Gradient: sigmoid(z) Â· (1 - sigmoid(z))</li>\n<li><strong>Good for:</strong> When outputs are probabilities or rates</li>\n</ul>\n<p><strong>Why bounded activations?</strong></p>\n<p>BCN requires bounded activations for theoretical guarantees and stability of the Î¾ criterion. Unbounded activations like ReLU are <strong>not recommended</strong> because:</p>\n<ol>\n<li>Theoretical issues: The Î¾ optimization assumes bounded activation <em>outputs</em></li>\n<li>Stability: Unbounded outputs can destabilize the ensemble</li>\n<li>While ReLU could <em>theoretically</em> work with very tight weight constraints (small Î»), tanh/sigmoid provide stronger guarantees</li>\n</ol>\n<p><strong>Rule of thumb:</strong> Use <strong>tanh</strong> as default. Itâ€™s more balanced, bounded and zero-centered.</p>\n<hr/>\n<h2 id=\"hyperparameter-7-tol-early-stopping-tolerance\">Hyperparameter 7: <code>tol</code> (Early Stopping Tolerance)</h2>\n<p><strong>Default:</strong> 0 (no early stopping)<br/>\n<strong>Typical range:</strong> 1e-7 to 1e-3<br/>\n<strong>Recommended:</strong> 1e-7 for most tasks</p>\n<p><strong>What it controls:</strong> When to stop training before reaching B iterations</p>\n<p><strong>Intuition:</strong> If the model stops improving (residual norm isnâ€™t decreasing much), stop early to avoid overfitting and save computation.</p>\n<p><strong>How it works (corrected):</strong>\nBCN tracks the relative improvement in residual norm and stops if progress is too slow:</p>\n<pre>if (relative_decrease_in_residuals &lt; tol):\n    stop training\n</pre>\n<p><strong>Important clarification:</strong> Early stopping is based on <strong>improvement rate</strong>, not absolute residual magnitude. This means BCN can stop even when residuals are still large (on a hard problem) if adding more weak learners doesnâ€™t help.</p>\n<p><strong>Trade-offs:</strong></p>\n<p><strong>tol = 0 (no early stopping):</strong></p>\n<ul>\n<li>Always trains for exactly B iterations</li>\n<li>May overfit if B is too large</li>\n<li>âœ… Use for: Quick experiments with small B</li>\n</ul>\n<p><strong>tol = 1e-7 to 1e-5:</strong></p>\n<ul>\n<li>Stops when improvement becomes negligible</li>\n<li>Prevents overfitting</li>\n<li>Can save significant computation</li>\n<li>âœ… Use for: Production models with large B</li>\n</ul>\n<p><strong>Practical tip:</strong> Set B large (e.g., 500-1000) and tol small (e.g., 1e-7) to let the algorithm decide when to stop. The actual number of iterations used will be stored in <code>fit$maxL</code>.</p>\n<hr/>\n<h2 id=\"hyperparameter-8-type_optim-optimization-method\">Hyperparameter 8: <code>type_optim</code> (Optimization Method)</h2>\n<p>Gradient-based.</p>\n<p><strong>Default:</strong> â€œnlminbâ€<br/>\n<strong>Options:</strong> â€œnlminbâ€, â€œadamâ€, â€œsgdâ€, â€œnmkbâ€, â€œhjkbâ€, â€œrandomsearchâ€</p>\n<p><strong>What it controls:</strong> How to solve the optimization problem at each iteration</p>\n<p><strong>Intuition:</strong> Finding the best weights w_L is a <strong>non-convex optimization</strong> problem. Different solvers have different trade-offs.</p>\n<p><strong>Available optimizers:</strong></p>\n<p><strong>nlminb</strong> (default):</p>\n<ul>\n<li>Uses gradient and Hessian approximations</li>\n<li>âœ… Robust</li>\n<li>âœ… Well-tested in R</li>\n<li>âœ… Works well in most cases</li>\n<li>âš ï¸ Medium speed</li>\n<li>âœ… Use for: General purpose, production</li>\n</ul>\n<p><strong>adam / sgd:</strong></p>\n<ul>\n<li>Gradient-based optimizers from deep learning</li>\n<li>âœ… Fast, especially for high-dimensional problems</li>\n<li>âœ… Good for large d (many features)</li>\n<li>âš ï¸ May need tuning (learning rate, iterations)</li>\n<li>âœ… Use for: d &gt; 100, speed-critical applications</li>\n</ul>\n<p><strong>nmkb / hjkb:</strong></p>\n<ul>\n<li>Derivative-free Nelder-Mead / Hooke-Jeeves</li>\n<li>âœ… Very robust (no gradient needed)</li>\n<li>âŒ Slow</li>\n<li>âœ… Use when: Other optimizers fail or diverge</li>\n</ul>\n<p><strong>randomsearch:</strong></p>\n<ul>\n<li>Random sampling + local search</li>\n<li>âœ… Can escape local minima</li>\n<li>âŒ Slower</li>\n<li>âœ… Use when: Problem is very non-convex</li>\n</ul>\n<p><strong>Rule of thumb:</strong></p>\n<ul>\n<li>Start with <code>\"nlminb\"</code></li>\n<li>If training is slow and d &gt; 100, try <code>\"adam\"</code></li>\n<li>Can pass additional arguments via <code>...</code> (e.g., max iterations, tolerance)</li>\n</ul>\n<p><strong>Important insight:</strong>\nBecause BCN uses an ensemble, <strong>local optima are OK</strong>! Even if we donâ€™t find the globally optimal w_L, the next iteration can compensate. This is why BCN is robust despite non-convex optimization at each step.</p>\n<hr/>\n<h2 id=\"hyperparameter-9-hidden_layer_bias-include-bias-term\">Hyperparameter 9: <code>hidden_layer_bias</code> (Include Bias Term)</h2>\n<p><strong>Default:</strong> TRUE<br/>\n<strong>Options:</strong> TRUE, FALSE</p>\n<p><strong>What it controls:</strong> Whether neural networks have a bias/intercept term</p>\n<p><strong>Intuition:</strong> Without bias, h_L = activation(w^T x). With bias, h_L = activation(w^T x + b).</p>\n<p><strong>Trade-offs:</strong></p>\n<p><strong>hidden_layer_bias = FALSE:</strong></p>\n<ul>\n<li>Simpler optimization (one less parameter per iteration)</li>\n<li>Faster training</li>\n<li>Assumes data is centered</li>\n<li>âœ… Use when: Features are already centered, want pure multiplicative effects</li>\n</ul>\n<p><strong>hidden_layer_bias = TRUE:</strong></p>\n<ul>\n<li>More expressive (can handle shifts)</li>\n<li>Can handle non-centered data better</li>\n<li>One additional parameter to optimize per iteration</li>\n<li>âœ… <strong>Recommended default</strong> - safer choice</li>\n</ul>\n<p><strong>Typical choice:</strong> Use TRUE unless you have a specific reason not to (e.g., theoretical interest in purely multiplicative models).</p>\n<hr/>\n<h2 id=\"hyperparameter-10-n_clusters-optional-clustering-features\">Hyperparameter 10: <code>n_clusters</code> (Optional Clustering Features)</h2>\n<p><strong>Default:</strong> NULL (no clustering)<br/>\n<strong>Typical range:</strong> 2-10</p>\n<p><strong>What it controls:</strong> Whether to add cluster membership features</p>\n<p><strong>Intuition:</strong> BCN can automatically perform k-means clustering on your inputs and add cluster memberships as additional features. This can help capture local patterns.</p>\n<p><strong>When to use:</strong></p>\n<ul>\n<li>âœ… Data has natural groupings or modes</li>\n<li>âœ… Local patterns differ across regions of feature space</li>\n<li>âŒ Not needed for most standard regression/classification</li>\n</ul>\n<p><strong>Note:</strong> This is an advanced feature - start without it and add only if needed.</p>\n<hr/>\n<h2 id=\"putting-it-all-together-hyperparameter-recipes\">Putting It All Together: Hyperparameter Recipes</h2>\n<h3 id=\"recipe-1-fast-prototyping-small-dataset-n--1000\">Recipe 1: Fast Prototyping (Small Dataset, N &lt; 1000)</h3>\n<pre>fit &lt;- bcn(\n  x = X_train, \n  y = y_train,\n  B = 50,              # Few iterations for speed\n  nu = 0.5,            # Moderate learning rate\n  col_sample = 1.0,    # Use all features (dataset is small)\n  lam = 10^0.5,        # ~3.16, moderate regularization\n  r = 0.9,             # Adaptive threshold\n  tol = 1e-5,          # Early stopping\n  activation = \"tanh\",\n  type_optim = \"nlminb\",\n  hidden_layer_bias = TRUE\n)\n</pre>\n<p><strong>Why these choices:</strong></p>\n<ul>\n<li>Small B for speed</li>\n<li>High nu for faster convergence</li>\n<li>No column sampling (dataset is small)</li>\n<li>Standard other parameters</li>\n</ul>\n<p><strong>Expected performance:</strong> Quick baseline in minutes</p>\n<hr/>\n<h3 id=\"recipe-2-production-model-medium-dataset-n--10000\">Recipe 2: Production Model (Medium Dataset, N ~ 10,000)</h3>\n<pre>fit &lt;- bcn(\n  x = X_train,\n  y = y_train,\n  B = 200,             # Enough iterations with early stopping\n  nu = 0.3,            # Conservative for stability\n  col_sample = 0.6,    # Some regularization\n  lam = 10^0.8,        # ~6.31, allow some complexity\n  r = 0.95,            # Very selective early on\n  tol = 1e-7,          # Train until converged\n  activation = \"tanh\",\n  type_optim = \"nlminb\",\n  hidden_layer_bias = TRUE\n)\n</pre>\n<p><strong>Why these choices:</strong></p>\n<ul>\n<li>Moderate B with early stopping safety</li>\n<li>Conservative nu for stability</li>\n<li>Column sampling for regularization</li>\n<li>High r for careful feature selection</li>\n</ul>\n<p><strong>Expected performance:</strong> Robust model, may train 100-150 iterations before stopping</p>\n<hr/>\n<h3 id=\"recipe-3-complex-task-large-dataset-high-dimensional\">Recipe 3: Complex Task (Large Dataset, High-Dimensional)</h3>\n<pre>fit &lt;- bcn(\n  x = X_train,\n  y = y_train,\n  B = 500,             # Many iterations (will stop early if needed)\n  nu = 0.4,            # Balanced\n  col_sample = 0.5,    # Strong regularization for high d\n  lam = 10^1.0,        # 10, higher complexity allowed\n  r = 0.95,            # Adaptive\n  tol = 1e-7,          # Early stopping safety\n  activation = \"tanh\",\n  type_optim = \"adam\",  # Fast optimizer for high d\n  hidden_layer_bias = TRUE\n)\n</pre>\n<p><strong>Why these choices:</strong></p>\n<ul>\n<li>Large B to capture complexity</li>\n<li>Column sampling crucial for high dimensions (d &gt; 100)</li>\n<li>Adam optimizer for speed with many features</li>\n<li>High r to prevent early overfitting</li>\n</ul>\n<p><strong>Expected performance:</strong> May use 200-400 iterations, handles d &gt; 500 well</p>\n<hr/>\n<h3 id=\"recipe-4-multivariate-time-series--multi-output\">Recipe 4: Multivariate Time Series / Multi-Output</h3>\n<pre>fit &lt;- bcn(\n  x = X_train,\n  y = Y_train,         # Matrix with multiple outputs (e.g., N x m)\n  B = 300,\n  nu = 0.5,            # Can be higher for shared structure\n  col_sample = 0.7,\n  lam = 10^0.7,\n  r = 0.95,            # Critical: enforces shared structure\n  tol = 1e-7,\n  activation = \"tanh\",\n  type_optim = \"nlminb\"\n)\n</pre>\n<p><strong>Why these choices:</strong></p>\n<ul>\n<li><strong>High r is critical</strong>: In multivariate mode, BCN computes Î¾_k for each output k and requires min_k(Î¾_k) â‰¥ 0 for acceptance. This ensures each weak learner contributes meaningfully across <strong>all</strong> time series/outputs, creating shared representations.</li>\n<li>Higher nu because shared structure is more stable</li>\n<li>Standard B with early stopping</li>\n</ul>\n<p><strong>Note on multivariate:</strong> BCN handles multiple outputs naturally through one-hot encoding (classification) or matrix targets (regression). The min(Î¾) criterion prevents sacrificing one output to improve another.</p>\n<p><strong>Expected performance:</strong> Strong on related time series or multi-task learning</p>\n<hr/>\n<h2 id=\"understanding-hyperparameter-interactions\">Understanding Hyperparameter Interactions</h2>\n<h3 id=\"interaction-1-nu--b--constant\">Interaction 1: <code>nu</code> Ã— <code>B</code> â‰ˆ Constant</h3>\n<p><strong>Trade-off:</strong> Small nu needs large B</p>\n<pre># Approximately equivalent final predictions:\nfit1 &lt;- bcn(B = 100, nu = 0.5)\nfit2 &lt;- bcn(B = 200, nu = 0.25)\n</pre>\n<p><strong>Why:</strong> Smaller steps need more iterations to reach similar places.</p>\n<p><strong>Rule:</strong> For similar model complexity, <code>nu Ã— B â‰ˆ constant</code> (approximately).</p>\n<p><strong>In practice:</strong></p>\n<ul>\n<li>Production (stability priority): nu = 0.3, B = 300</li>\n<li>Prototyping (speed priority): nu = 0.5, B = 100</li>\n</ul>\n<hr/>\n<h3 id=\"interaction-2-lam--r-complexity-control\">Interaction 2: <code>lam</code> â†” <code>r</code> (Complexity Control)</h3>\n<p><strong>Both control complexity:</strong></p>\n<ul>\n<li><code>lam</code>: How complex each weak learner can be</li>\n<li><code>r</code>: How selective we are about accepting weak learners</li>\n</ul>\n<pre># More regularization\nfit_reg &lt;- bcn(lam = 1.0, r = 0.95)   # Simple features, selective\n\n# Less regularization  \nfit_complex &lt;- bcn(lam = 10.0, r = 0.7)  # Complex features, permissive\n</pre>\n<p><strong>Balance principle:</strong> If you allow complex features (high lam), be selective (high r) to avoid noise.</p>\n<p><strong>Typical combinations:</strong></p>\n<ul>\n<li><strong>High quality</strong>: lam = 10, r = 0.95 â†’ Complex but carefully selected features</li>\n<li><strong>Moderate</strong>: lam = 5, r = 0.90 â†’ Balanced</li>\n<li><strong>Fast/loose</strong>: lam = 3, r = 0.80 â†’ Simple features, permissive</li>\n</ul>\n<hr/>\n<h3 id=\"interaction-3-col_sample--b-coverage\">Interaction 3: <code>col_sample</code> â†” <code>B</code> (Coverage)</h3>\n<p><strong>Column sampling as implicit regularization:</strong></p>\n<pre># Fewer features per iteration â†’ need more iterations for coverage\nfit1 &lt;- bcn(col_sample = 1.0, B = 100)\nfit2 &lt;- bcn(col_sample = 0.5, B = 200)\n</pre>\n<p><strong>Rough guideline:</strong></p>\n<pre>B_needed â‰ˆ B_baseline / col_sample\n</pre>\n<p><strong>In practice:</strong></p>\n<ul>\n<li>col_sample = 1.0 â†’ B = 100-200</li>\n<li>col_sample = 0.5 â†’ B = 200-400</li>\n<li>col_sample = 0.3 â†’ B = 300-500</li>\n</ul>\n<hr/>\n<h2 id=\"the-mathematical-connection-how-hyperparameters-appear-in-Î¾\">The Mathematical Connection: How Hyperparameters Appear in Î¾</h2>\n<p>The core optimization criterion ties everything together:</p>\n<pre>Î¾_L = Î½(2-Î½) Â· Î²Â²_L - [1 - r - (1-r)/(L+1)] Â· ||residuals||Â²\n      â””â”€â”¬â”€â”€â”˜   â””â”¬â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       nu    optimized over        r\n             w âˆˆ [-lam, lam]\n</pre>\n<p><strong>Reading the formula:</strong></p>\n<ol>\n<li>Find w_L (constrained by <code>lam</code>) that maximizes Î²Â²_L\n    <ul>\n<li>\n<table>\n<tbody>\n<tr>\n<td>Î²_L is the OLS coefficient: Î²_L = (h_L^T Â· residuals) /</td>\n<td>Â </td>\n<td>h_L</td>\n<td>Â </td>\n<td>Â²</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>Scale by Î½(2-Î½) (controlled by <code>nu</code>)</li>\n<li>Subtract penalty (controlled by <code>r</code>)</li>\n<li>Accept only if Î¾ â‰¥ 0 for all outputs</li>\n<li>Repeat for <code>B</code> iterations (or until <code>tol</code> reached)</li>\n<li>At each step, sample <code>col_sample</code> fraction of features</li>\n</ol>\n<p>This unified view shows how all hyperparameters work together to control the greedy feature selection process.</p>\n<hr/>\n<h2 id=\"practical-tips-for-hyperparameter-tuning\">Practical Tips for Hyperparameter Tuning</h2>\n<h3 id=\"start-simple-add-complexity\">Start Simple, Add Complexity</h3>\n<ol>\n<li><strong>Begin with defaults:</strong>\n<pre>fit &lt;- bcn(x, y, B = 100, nu = 0.3, lam = 10^0.7, r = 0.9)\n</pre> </li></ol></div>\n<li><strong>If underfitting (train error too high):</strong>\n<ul>\n<li>â†‘ Increase B (more capacity)</li>\n<li>â†‘ Increase lam (allow more complex features)</li>\n<li>â†‘ Increase nu (use features more aggressively)</li>\n<li>â†“ Decrease r (be less selective)</li>\n</ul>\n</li>\n<li><strong>If overfitting (train Â«Â test error):</strong>\n<ul>\n<li>â†“ Decrease nu (smaller, more careful steps)</li>\n<li>â†“ Decrease lam (simpler features)</li>\n<li>Add column sampling (col_sample = 0.5-0.7)</li>\n<li>â†‘ Increase r (be more selective)</li>\n<li>Use early stopping (tol = 1e-7)</li>\n</ul>\n</li>\n<h3 id=\"use-cross-validation-wisely\">Use Cross-Validation Wisely</h3>\n<p><strong>Most important to tune:</strong> <code>B</code>, <code>nu</code>, <code>lam</code></p>\n<p><strong>Moderately important:</strong> <code>r</code>, <code>col_sample</code></p>\n<p><strong>Usually fixed:</strong> <code>hidden_layer_bias = TRUE</code>, <code>type_optim = \"nlminb\"</code>, <code>activation = \"tanh\"</code></p>\n<p><strong>Example CV strategy:</strong></p>\n<pre>library(caret)\n\n# Grid search on log-scale for lam\ngrid &lt;- expand.grid(\n  B = c(100, 200, 500),\n  nu = c(0.1, 0.3, 0.5),\n  lam = 10^seq(0, 1.5, by = 0.5)  # 1, 3.16, 10, 31.6\n)\n\n# Use caret, mlr3, or tidymodels for CV\n</pre>\n<h3 id=\"monitor-training\">Monitor Training</h3>\n<pre># Enable verbose output\nfit &lt;- bcn(x, y, verbose = 1, show_progress = TRUE)\n</pre>\n<p><strong>Watch for:</strong></p>\n<ul>\n<li>\n<table>\n<tbody>\n<tr>\n<td>How fast</td>\n<td>Â </td>\n<td>residuals</td>\n<td>Â </td>\n<td>_F decreases (convergence rate)</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>Whether Î¾ stays positive (quality of weak learners)</li>\n<li>If training stops early and at what iteration (capacity needs)</li>\n</ul>\n<p><strong>Diagnostic patterns:</strong></p>\n<ul>\n<li>Residuals plateau early â†’ Increase B or lam</li>\n<li>Î¾ often negative â†’ Decrease r or increase lam</li>\n<li>Training very slow â†’ Try adam optimizer or increase col_sample</li>\n</ul>\n<hr/>\n<h2 id=\"quick-reference-hyperparameter-cheat-sheet\">Quick Reference: Hyperparameter Cheat Sheet</h2>\n<table>\n<thead>\n<tr>\n<th>Hyperparameter</th>\n<th>Low Value Effect</th>\n<th>High Value Effect</th>\n<th>Typical Range</th>\n<th>Default</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>B</strong></td>\n<td>Simple, fast, may underfit</td>\n<td>Complex, slow, may overfit</td>\n<td>50-1000</td>\n<td>100</td>\n</tr>\n<tr>\n<td><strong>nu</strong></td>\n<td>Stable, slow convergence</td>\n<td>Fast, potentially unstable</td>\n<td>0.1-0.8</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td><strong>lam</strong></td>\n<td>Linear-ish, simple features</td>\n<td>Nonlinear, complex features</td>\n<td>1-100</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td><strong>r</strong></td>\n<td>Permissive, accepts more</td>\n<td>Selective, high quality</td>\n<td>0.3-0.99</td>\n<td>0.3</td>\n</tr>\n<tr>\n<td><strong>col_sample</strong></td>\n<td>No regularization</td>\n<td>Strong regularization</td>\n<td>0.3-1.0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td><strong>tol</strong></td>\n<td>No early stop</td>\n<td>Aggressive early stop</td>\n<td>0-1e-3</td>\n<td>0</td>\n</tr>\n<tr>\n<td><strong>activation</strong></td>\n<td>tanh (symmetric)</td>\n<td>sigmoid (asymmetric)</td>\n<td>-</td>\n<td>tanh</td>\n</tr>\n<tr>\n<td><strong>type_optim</strong></td>\n<td>nlminb (robust)</td>\n<td>adam (fast)</td>\n<td>-</td>\n<td>nlminb</td>\n</tr>\n<tr>\n<td><strong>hidden_layer_bias</strong></td>\n<td>Simpler, through origin</td>\n<td>More flexible</td>\n<td>-</td>\n<td>TRUE</td>\n</tr>\n</tbody>\n</table>\n<hr/>\n<h2 id=\"when-not-to-use-bcn\">When NOT to Use BCN</h2>\n<p>While BCN is versatile, itâ€™s not always the best choice:</p>\n<p>âŒ <strong>Ultra-high-dimensional sparse data (d &gt; 10,000)</strong></p>\n<ul>\n<li>Tree-based boosting (XGBoost/LightGBM) may be faster</li>\n<li>Column sampling helps, but trees handle sparsity natively</li>\n</ul>\n<p>âŒ <strong>Very large datasets (N &gt; 1,000,000)</strong></p>\n<ul>\n<li>Training time scales roughly O(B Ã— N Ã— d)</li>\n<li>Consider subsampling or streaming methods</li>\n</ul>\n<p>âŒ <strong>Deep sequential/temporal structure</strong></p>\n<ul>\n<li>BCN is static (no recurrence)</li>\n<li>Use RNNs/Transformers for complex time dependencies</li>\n</ul>\n<p>âŒ <strong>Image/text/audio from scratch</strong></p>\n<ul>\n<li>Convolutional/attention architectures more suitable</li>\n<li>BCN works on extracted features (embeddings, tabular)</li>\n</ul>\n<p>âœ… <strong>BCN shines at:</strong></p>\n<ul>\n<li>Tabular data (100s to 10,000s of rows)</li>\n<li>Multivariate prediction (shared structure across outputs)</li>\n<li>Needing both accuracy AND interpretability</li>\n<li>Time series with extracted features</li>\n<li>When XGBoost works but you want gradient-based explanations</li>\n</ul>\n<hr/>\n<h2 id=\"debugging-bcn-training\">Debugging BCN Training</h2>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Î¾ frequently negative early</td>\n<td>r too high or lam too low</td>\n<td>Decrease r to 0.8 or increase lam to 5-10</td>\n</tr>\n<tr>\n<td>Residuals plateau quickly</td>\n<td>nu too small or B too low</td>\n<td>Increase nu to 0.4-0.5 or B to 300+</td>\n</tr>\n<tr>\n<td>Training very slow</td>\n<td>col_sample=1 on wide data</td>\n<td>Set col_sample=0.5 and try type_optim=â€adamâ€</td>\n</tr>\n<tr>\n<td>High train accuracy, poor test</td>\n<td>Overfitting</td>\n<td>Decrease nu, increase r, add col_sample &lt; 1</td>\n</tr>\n<tr>\n<td>Poor train accuracy</td>\n<td>Underfitting</td>\n<td>Increase B, increase lam, try different activation</td>\n</tr>\n<tr>\n<td>Optimizer not converging</td>\n<td>Bad initialization or scaling</td>\n<td>Check feature scaling, try different type_optim</td>\n</tr>\n</tbody>\n</table>\n<hr/>\n<h2 id=\"interpretability-example\">Interpretability Example</h2>\n<p>One of BCNâ€™s unique advantages is <strong>gradient-based interpretability</strong>.</p>\n<p><strong>What makes this special:</strong></p>\n<ul>\n<li>âœ… Exact analytic gradients (no approximation)</li>\n<li>âœ… Same O(B Ã— m Ã— d) cost as prediction</li>\n<li>âœ… Shows direction of influence (positive/negative)</li>\n<li>âœ… Works for both regression and classification</li>\n<li>âœ… Much faster than SHAP on tree ensembles</li>\n</ul>\n<hr/>\n<h2 id=\"conclusion-the-philosophy-of-bcn\">Conclusion: The Philosophy of BCN</h2>\n<p>BCNâ€™s hyperparameters reveal its design philosophy:</p>\n<p><strong>1. Iterative Refinement</strong> (via <code>B</code>)<br/>\nBuild the model piece by piece, adding one well-chosen feature at a time.</p>\n<p><strong>2. Conservative Steps</strong> (via <code>nu</code>)<br/>\nDonâ€™t trust any single feature too much - combine many weak learners.</p>\n<p><strong>3. Bounded Complexity</strong> (via <code>lam</code>)<br/>\nKeep individual weak learners simple to ensure stability and interpretability.</p>\n<p><strong>4. Adaptive Selection</strong> (via <code>r</code>)<br/>\nStart picky (prevent early mistakes), become permissive (allow refinement).</p>\n<p><strong>5. Randomization</strong> (via <code>col_sample</code>)<br/>\nLike Random Forests, diversity through randomness helps generalization.</p>\n<p><strong>6. Early Stopping</strong> (via <code>tol</code>)<br/>\nKnow when to stop - more iterations arenâ€™t always better.</p>\n<p><strong>7. Explicit Optimization for Interpretability</strong><br/>\nUnlike methods that require post-hoc explanations, BCN is designed with interpretability in mind through its additive structure and differentiable components.</p>\n<p>Together, these create a model thatâ€™s:</p>\n<ul>\n<li>âœ… <strong>Expressive</strong> (neural network features capture non-linearity)</li>\n<li>âœ… <strong>Interpretable</strong> (additive structure + gradients)</li>\n<li>âœ… <strong>Robust</strong> (ensemble of bounded weak learners)</li>\n<li>âœ… <strong>Efficient</strong> (sparse structure, early stopping, column sampling)</li>\n</ul>\n<hr/>\n<h2 id=\"next-steps\">Next Steps</h2>\n<p><strong>To learn more:</strong></p>\n<ul>\n<li>ğŸ“¦ <a href=\"https://github.com/Techtonique/bcn\" rel=\"nofollow\" target=\"_blank\">BCN R Package on GitHub</a></li>\n<li>ğŸ“¦ <a href=\"https://github.com/Techtonique/bcn_python\" rel=\"nofollow\" target=\"_blank\">BCN Python Package on GitHub</a></li>\n<li>ğŸ“ <a href=\"https://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks\" rel=\"nofollow\" target=\"_blank\">Research preprint on BCN</a></li>\n</ul>\n<p><strong>To contribute:</strong>\nBCN is open source! Contributions welcome for:</p>\n<ul>\n<li>New activation functions</li>\n<li>Additional optimization methods</li>\n<li>Interpretability visualizations</li>\n<li>Benchmark studies and applications</li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://thierrymoudiki.github.io//blog/2026/02/16/r/python/bcn-explained\"> T. Moudiki's Webpage - R</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </article>",
    "word_count": 4094,
    "reading_time_min": 20.5,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/t-moudiki/",
        "text": "T. Moudiki"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://thierrymoudiki.github.io//blog/2026/02/16/r/python/bcn-explained",
        "text": "T. Moudiki's Webpage - R"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://thierrymoudiki.github.io/blog/2022/07/21/r/misc/boosted-configuration-networks",
        "text": "https://thierrymoudiki.github.io/blog/2022/07/21/r/misc/boosted-configuration-networks"
      },
      {
        "href": "https://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks",
        "text": "https://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks"
      },
      {
        "href": "https://docs.techtonique.net/bcn/articles/bcn-intro.html",
        "text": "https://docs.techtonique.net/bcn/articles/bcn-intro.html"
      },
      {
        "href": "https://github.com/Techtonique/bcn_python",
        "text": "https://github.com/Techtonique/bcn_python"
      },
      {
        "href": "https://github.com/Techtonique/bcn",
        "text": "BCN R Package on GitHub"
      },
      {
        "href": "https://github.com/Techtonique/bcn_python",
        "text": "BCN Python Package on GitHub"
      },
      {
        "href": "https://www.researchgate.net/publication/332291211_Forecasting_multivariate_time_series_with_boosted_configuration_networks",
        "text": "Research preprint on BCN"
      },
      {
        "href": "https://thierrymoudiki.github.io//blog/2026/02/16/r/python/bcn-explained",
        "text": "T. Moudiki's Webpage - R"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [],
    "lang": "en-US",
    "crawled_at_utc": "2026-02-17T19:41:35Z"
  }
}