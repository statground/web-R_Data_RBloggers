{
  "id": "79f296e06bfd5735b20729830d0dc446f4712484",
  "url": "https://www.r-bloggers.com/2026/02/getting-to-the-bottom-of-tmle-influence-functions-and-perturbations-2/",
  "created_at_utc": "2026-02-06T23:17:28Z",
  "crawled_at_utc": "2026-02-06T23:17:28Z",
  "html_title": "Getting to the bottom of TMLE: influence functions and perturbations | R-bloggers",
  "meta_description": "I first encountered TMLE—sometimes spelled out as targeted maximum likelihood estimation or targeted minimum-loss estimate—about twelve or so years ago when Mark var der Laan, one of the original developers who literally wrote the book, gave a talk ...",
  "data": {
    "url": "https://www.r-bloggers.com/2026/02/getting-to-the-bottom-of-tmle-influence-functions-and-perturbations-2/",
    "canonical_url": "https://www.r-bloggers.com/2026/02/getting-to-the-bottom-of-tmle-influence-functions-and-perturbations-2/",
    "html_title": "Getting to the bottom of TMLE: influence functions and perturbations | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "I first encountered TMLE—sometimes spelled out as targeted maximum likelihood estimation or targeted minimum-loss estimate—about twelve or so years ago when Mark var der Laan, one of the original developers who literally wrote the book, gave a talk ...",
    "meta_keywords": null,
    "og_title": "Getting to the bottom of TMLE: influence functions and perturbations | R-bloggers",
    "og_description": "I first encountered TMLE—sometimes spelled out as targeted maximum likelihood estimation or targeted minimum-loss estimate—about twelve or so years ago when Mark var der Laan, one of the original developers who literally wrote the book, gave a talk ...",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "twitter_title": "Getting to the bottom of TMLE: influence functions and perturbations | R-bloggers",
    "twitter_description": "I first encountered TMLE—sometimes spelled out as targeted maximum likelihood estimation or targeted minimum-loss estimate—about twelve or so years ago when Mark var der Laan, one of the original developers who literally wrote the book, gave a talk ...",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "Getting to the bottom of TMLE: influence functions and perturbations\nPosted on\nFebruary 4, 2026\nby\nouR data generation\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nouR data generation\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nI first encountered TMLE—sometimes spelled out as\ntargeted maximum likelihood estimation\nor\ntargeted minimum-loss estimate\n—about twelve or so years ago when Mark var der Laan, one of the original developers who literally wrote the\nbook\n, gave a talk at NYU. It sounded very cool and seemed quite revolutionary and important, but it was really challenging to follow all of the details. Following that talk, I tried to tackle some of the literature, but quickly found that it as a challenge to penetrate. What struck me most was not the algorithmic complexity (which it certainly had), but much of the language and terminology, and the underlying math.\nRecently, I inherited a project from a colleague who had proposed using TMLE to analyze a cluster randomized trial using a stepped-wedge design. In order to decide whether we would continue with this plan, I needed to revisit the literature to see if I could make more progress this time around. There are certainly more tutorials available as well as improved software and documentation, so it is much easier to get up and running to generate estimates. I was even able to build a model for the stepped-wedge design (that I hope to share on the blog some point soon). Beyond this, I really wanted to get a deeper understanding of the mathematical model that underlies the method without getting too far into the weeds (and proofs).\nI think I have made some progress in developing my understanding, and I wanted to write it down here for my future self (and for anyone else who wants to join along). This post is definitely not a tutorial nor is it literature review. It’s my attempt to encode my understanding of the conceptual ideas that support the underpinnings of the TMLE algorithm.\nWhat problem is TMLE trying to solve?\nAt its core, TMLE addresses a tension that appears throughout modern causal inference and semiparametric statistics. On one hand, we often want to estimate parameters that depend on complex features of the data-generating process, such as causal effects that require modeling both outcomes and treatment assignment mechanisms. On the other hand, more and more we rely on flexible machine learning methods to estimate these components, which can require large amounts of data before their estimates stabilize and may be difficult to analyze using classical statistical theory.\nClassical statistical inference typically relies on relatively simple, structured models where bias, variance, and uncertainty can be derived directly from model assumptions. Flexible machine learning models break many of those assumptions. They can be highly adaptive, involve many tuning choices, and may not correspond to smooth parametric likelihoods. As a result, while they can produce excellent predictions, their statistical behavior is often difficult to characterize analytically.\nTMLE provides a framework for resolving this tension. Rather than building estimators directly from outcome or treatment models, which are scientifically important but statistically treated as nuisance components, TMLE constructs estimators whose statistical behavior is governed by a known influence function. Once an estimator has the correct influence function, its large-sample bias, variance, and uncertainty can be characterized using general asymptotic theory, even when nuisance components are estimated flexibly.\nWhat follows is a description of four key elements that I think synthesize the ideas that are starting to make TMLE make more sense: (1) viewing parameters as functionals of probability distributions, (2) describing sampling as small perturbations of the data-generating distribution, (3) linearizing parameters using influence functions, and (4) constructing estimators that are orthogonal to nuisance estimation error.\nEstimators as functionals\nAn estimator, such as a mean or causal effect, is not merely a formula applied to data, but can be thought of more formally as a\nfunctional\n, a mapping that takes a probability distribution\n\\(P\\)\nin a family of distributions\n\\(\\mathcal{P}\\)\nand returns a number\n\\(T(P)\\)\n:\n\\[ T : \\mathcal{P} \\to \\mathbb{R} \\]\nHere\n\\(\\mathcal{P}\\)\njust denotes the collection of probability distributions under consideration. Writing\n\\(T : \\mathcal{P} \\to \\mathbb{R}\\)\nemphasizes that a statistical parameter is not a property of a particular data set, but of the\ndata-generating distribution\n. The data set enters only through the empirical distribution\n\\(P_n\\)\n.\nFrom this perspective, the key question becomes how\n\\(T\\)\nchanges when the data-generating distribution changes slightly—for example, by comparing\n\\(T(P_1)\\)\nwith\n\\(T(P_2)\\)\n, where\n\\(P_1, P_2 \\in \\mathcal{P}\\)\n.\nAs an example, the\nmean\nis a functional:\n\\[T(P) = \\int z\\ dP(z).\\]\nIf\n\\(P_0\\)\nis the true data-generating distribution, then the target parameter is\n\\(T(P_0)\\)\n.\n\\(P_n\\)\nis the empirical distribution (i.e., the distribution induced based by the observed sample),\n\\[P_n = \\frac{1}{n}\\sum_{i=1}^n \\delta_{Z_i},\\]\nand the estimator is simply\n\\(T(P_n)\\)\n. The notation\n\\(\\delta_z\\)\ndenotes a\npoint mass\nat\n\\(z\\)\n: a probability distribution that assigns probability 1 to the single value\n\\(z\\)\n. Writing the empirical distribution as an average of point masses simply formalizes the the idea that the observed data place equal weight on each sampled point. The empirical distribution is therefore a discrete approximation to the true data-generating distribution\n\\(P_0\\)\n, placing mass\n\\(1/n\\)\non each observed value.\nUltimately, estimation is about quantifying how far our estimate\n\\(\\hat{\\theta}\\)\nbased on the empirical distribution deviates from the target parameter\n\\(\\theta_0\\)\ndefined by the true distribution:\n\\[\\hat{θ} − \\theta_0 = T(P_n)−T(P_0).\\]\nWritten this way, estimation error is simply the difference between evaluating the same functional at two nearby distributions: the true distribution\n\\(P_0\\)\nand its empirical approximation\n\\(P_n\\)\n. Everything relies on understanding how\n\\(T(P)\\)\nchanges when\n\\(P\\)\nchanges from\n\\(P_0\\)\nto\n\\(P_n\\)\n.\nSampling as a perturbation\nThe empirical distribution\n\\(P_n\\)\ndiffers from\n\\(P_0\\)\nby many small deviations. As we just saw, each observation contributes a point mass of size\n\\(1/n\\)\n. It is tempting to write this difference as\n\\[P_n − P_0,\\]\nbut that isn’t really all that helpful. The object\n\\(P_n − P_0\\)\nis not itself a probability distribution but a signed measure that records where the empirical distribution over- or under-represents the truth. One way to conceptualize\n\\(P_n − P_0\\)\nis as a balance sheet: where the empirical distribution places more mass than the truth, the difference is positive; where it places less mass, the difference is negative. The total of these differences always sums to zero, but the pattern of positive and negative deviations determines how functionals of the distribution fluctuate.\n\\(P_n − P_0\\)\nis not used on its own. It only appears through its action on functions. For any function\n\\(f\\)\n,\n\\[(P_n −P_0)f \\equiv \\int f(z) \\ dP_n(z) − \\int f(z) \\ dP_0(z).\\]\nThis quantity measures how much the empirical average of\n\\(f(Z)\\)\ndeviates from its population mean.\nIn this context, we can think of\n\\(P_n − P_0\\)\nheuristically as a sum of many small perturbations to the true distribution: regions of the sample space where the empirical distribution places slightly too much mass contribute positively, and regions where it places too little mass contribute negatively. The total deviation integrates to zero, but its interaction with specific functions can be nonzero.\nThis brings us a little closer to the question we ended with in the last section:\nhow sensitive is the functional\n\\(T(P)\\)\nto small changes in the underlying distribution\n? To answer this question, we need a precise way to describe an infinitesimal change in a probability distribution, one that preserves total probability mass and allows differentiation. Hampel’s contamination model provides exactly such a construction.\nHampel’s contamination model and the influence function\nAnswering that question requires an understanding of the\ninfluence function\n. After reading about TMLE for a few weeks, I stumbled on Frank Hample’s important\npaper\n, written in the early 1970’s and provides a nice explanation of the influence function in the context of robustness.\nIn that paper, Hampel formalized what is meant by a “small change” in a distribution by introducing a specific directional perturbation of the data-generating distribution. This construction allows the parameter\n\\(T(P)\\)\nto be differentiated with respect to the underlying distribution itself. In other words, Hampel provides a way to define derivatives of statistical parameters with respect to the underlying distribution itself.\nAs before, if\n\\(P_0\\)\nis the true distribution, and\n\\(\\delta_z\\)\ndenotes the point mass at\n\\(z\\)\n, then we can consider a perturbed distribution that is a mixture of the two:\n\\[P_{\\epsilon} = (1−\\epsilon)P_0 + \\epsilon \\delta_z,\\ \\ \\ 0< \\epsilon < 1.\\]\nThis doesn’t correspond to adding a full new data point. Instead, it represents adding an infinitesimal amount of probability mass at\n\\(z\\)\n, defining a smooth path through the space of distributions along which derivatives can be taken.\nThe influence function of\n\\(T\\)\nat\n\\(P_0\\)\nis\n\\(\\phi_{P_0}\\)\n, defined as\n\\[\n\\phi_{P_0}(z) \\equiv\n\\left.\n\\frac{d}{d\\epsilon}\nT(P_\\epsilon)\n\\right|_{\\epsilon = 0} =\n\\left.\n\\frac{d}{d\\epsilon}\nT\\!\\left((1-\\epsilon)P_0 + \\epsilon \\delta_z\\right)\n\\right|_{\\epsilon = 0}.\n\\]\nIn words\nthe influence function measures the first-order effect on\n\\(T(P)\\)\nof “nudging” the distribution at point\n\\(z\\)\n.\nThe goal, however, is not to compute\n\\(T(P_n)\\)\nexactly, but to approximate how far it is from\n\\(T(P_0)\\)\n. This difference\n\\[T(P_n) − T(P_0)\\]\nis the estimation error. Describing its behavior is what allows us to understand bias, variability, and ultimately statistical uncertainty.\nBecause\n\\(P_n\\)\nconverges to\n\\(P_0\\)\nas the sample size grows, the difference between\n\\(P_n\\)\nand\n\\(P_0\\)\nbecomes smaller. Rather than approximating\n\\(T(P_n)\\)\ndirectly, we approximate the\nchange\nin the functional as we move from\n\\(P_0\\)\nto\n\\(P_n\\)\n:\nThis is analogous to ordinary calculus, where we approximate the change\n\\(f(x+h)−f(x)\\)\nby a linear term involving the derivative of\n\\(f\\)\nat\n\\(x\\)\n. Here, the role of the “increment”\n\\(h\\)\nis played by the signed difference\n\\(P_n−P_0\\)\n, and the role of the derivative is played by the influence function. The influence function provides the best linear approximation to how T(P) responds to small perturbations of the underlying distribution.\nThis leads directly to a first-order (linear) expansion of the estimation error\n\\(T(P_n) − T(P_0)\\)\n.\nLinearization via the influence function\nIf\n\\(T\\)\nis sufficiently smooth, the first-order expansion is:\n\\[\nT(P_n) − T(P_0) = (P_n−P_0)\\phi_{P_0} + R_n,\n\\]\nwhere the leading term is linear in the perturbation\n\\(P_n − P_0\\)\n, and\n\\(R_n\\)\ncollects higher-order terms. This expansion should be read as a\nfunctional Taylor expansion\n: the influence function\n\\(\\phi_{P_0}\\)\nplays the role of a derivative, and\n\\((P_n−P_0)\\phi_{P_0}\\)\nis the linear approximation to the change in\n\\(T\\)\ninduced by replacing\n\\(P_0\\)\nwith its empirical approximation\n\\(P_n\\)\n.\nEarlier, I defined\n\\((P_n – P_0)f\\)\nfor a general function\n\\(f\\)\n. Applying this definition when\n\\(f\\)\nis the influence function\n\\(\\phi_{P_0}\\)\ngives\n\\[\n(P_n−P_0)\\phi_{P_0}\n\\equiv\n\\int \\phi_{P_0}(z) \\ dP_n(z)\n−\n\\int \\phi_{P_0}(z) \\ dP_0(z)\n=\n\\frac{1}{n} \\sum_{i=1}^n \\phi_{P_0}(Z_i)\n–\nE_{P_0}[\\phi_{P_0}(Z)].\n\\]\nThe second term vanishes because influence functions are defined to satisfy the centering condition\n\\[\nE_{P_0}[\\phi_{P_0}(Z)] = 0.\n\\]\n\\(R_n\\)\nis a remainder term and plays a crucial role in asymptotic theory. If\n\\[\nR_n = o_p(n^{-1/2}),\n\\]\nthen multiplying both sides of the expansion by\n\\(\\sqrt{n}\\)\nyields\n\\[\n\\sqrt{n}\\big(T(P_n) – T(P_0)\\big)\n=\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\phi_{P_0}(Z_i)\n+\no_p(1).\n\\]\nThe appearance of the\n\\(\\sqrt{n}\\)\nscaling is not accidental — it reflects a universal feature of averaging noisy data. Each observation carries some randomness, and when we average\n\\(n\\)\nindependent observations, the noise partly cancels out. The variability of the average shrinks as the sample grows, but it shrinks at a very specific rate: the standard deviation decreases proportionally to\n\\(1/\\sqrt{n}\\)\n. Multiplying the estimation error by\n\\(\\sqrt{n}\\)\ntherefore puts it on a scale where the random fluctuations remain visible rather than collapsing toward zero. This is why\n\\(\\sqrt{n}(\\hat{\\theta} – \\theta_0)\\)\noften settles into a stable, typically Gaussian or normal, distribution in large samples, and why the\n\\(\\sqrt{n}\\)\nrate appears so consistently across statistical theory.\nBecause the influence function has mean zero, the central limit theorem implies that the right-hand side converges in distribution to a normal random variable. This is what ultimately allows us to characterize the sampling variability of the estimator. TMLE is designed so that its estimation error admits exactly this kind of\n\\(\\sqrt{n}\\)\n-scaled linear expansion, even when parts of the model are estimated using flexible machine learning.\nOrthogonality under estimation of the influence function\nThe mean-zero property of the influence function ensures that the leading term\n\\[\n(P_n−P_0)\\phi_{P_0}\n=\n\\frac{1}{n} \\sum_{i=1}^n \\phi_{P_0}(Z_i)\n\\]\nfluctuates randomly around zero rather than drifting systematically. In other words, the influence function describes sampling variability, not bias. Without this centering condition, the first-order term would contain a persistent deterministic component, and the expansion would fail to characterize uncertainty correctly.\nHowever, this discussion implicitly assumes that\n\\(\\phi_{P_0}\\)\nis known. In most problems of interest, particularly causal inference, the influence function depends on additional\nunknown\ncomponents of the data-generating process, such as conditional outcome models or treatment assignment mechanisms. These are often referred to as\nnuisance functions\nbecause they are not themselves the primary target of inference but are required to construct the influence function. These quantities must be estimated from the data, often using flexible statistical or machine learning methods.\nIf the influence function depends on unknown nuisance functions, we must replace the true influence function with an estimated one. This introduces an additional source of error. If this nuisance estimation error enters the leading linear term of the expansion, it can be amplified by the\n\\(\\sqrt{n}\\)\nscaling and distort the asymptotic behavior of the estimator. In particular, bias introduced by nuisance estimation may not vanish even in large samples.\nWhat goes wrong without orthogonality\nTo see this more concretely, note that in practice we replace the true influence function\n\\(\\phi_{P_0}\\)\nwith an estimated version\n\\(\\phi_{\\hat P}\\)\n. The leading term of the expansion then becomes\n\\[\n(P_n – P_0)\\phi_{\\hat P}\n=\n(P_n – P_0)\\phi_{P_0}\n+\n(P_n – P_0)(\\phi_{\\hat P} – \\phi_{P_0}).\n\\]\nThe second term reflects nuisance estimation error. Because\n\\((P_n – P_0)\\)\nfluctuates at order\n\\(1/\\sqrt{n}\\)\n, this term can dominate the asymptotic behavior unless\n\\(\\phi_{\\hat P}\\)\nis constructed to be orthogonal to nuisance directions. Orthogonality ensures that this term is absorbed into the remainder and asymptotically negligible.\nMathematically, this protection often appears as a\nconditional\nmean-zero property such as\n\\[\nE_{P_0}[\\phi_{P_0}(Z \\mid X)] = 0.\n\\]\nThis condition implies that the influence function is deliberately constructed to be insensitive to errors that arise from estimating functions of the covariates\n\\(X\\)\n. Since nuisance components like outcome regressions and propensity scores depend on\n\\(X\\)\n, small errors in estimating these quantities tend to vary across values of\n\\(X\\)\n. The conditional mean-zero property ensures that when these errors interact with the influence function, their contributions cancel out when averaged across the sample. In effect, nuisance estimation errors add noise but do not systematically push the estimator away from the truth.\nThis orthogonality is a key mechanism behind the robustness of TMLE. It allows flexible, slower-converging nuisance estimators to be used without contaminating the primary asymptotic behavior of the target parameter. More on how that influence function is constructed will hopefully follow in the next post that looks more closely at TMLE.\nNext steps\nIn the next post, I’ll take the same approach to walk through the main elements of the TMLE algorithm, focusing on how they operationalize the ideas developed here.\nReferences:\nVan der Laan, Mark J., and Sherri Rose. Targeted learning: causal inference for observational and experimental data. Vol. 4. New York: Springer, 2011.\nHampel, Frank R. “The influence curve and its role in robust estimation.” Journal of the american statistical association 69, no. 346 (1974): 383-393.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nouR data generation\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-398791 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Getting to the bottom of TMLE: influence functions and perturbations</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 4, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/our-data-generation/\">ouR data generation</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://www.rdatagen.net/post/2026-02-05-getting-to-the-bottom-of-tmle-1/\"> ouR data generation</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>I first encountered TMLE—sometimes spelled out as <em>targeted maximum likelihood estimation</em> or <em>targeted minimum-loss estimate</em>—about twelve or so years ago when Mark var der Laan, one of the original developers who literally wrote the <a href=\"https://www.google.com/books/edition/Targeted_Learning/RGnSX5aCAgQC?hl=en\" rel=\"nofollow\" target=\"_blank\" targt=\"_blank\">book</a>, gave a talk at NYU. It sounded very cool and seemed quite revolutionary and important, but it was really challenging to follow all of the details. Following that talk, I tried to tackle some of the literature, but quickly found that it as a challenge to penetrate. What struck me most was not the algorithmic complexity (which it certainly had), but much of the language and terminology, and the underlying math.</p>\n<p>Recently, I inherited a project from a colleague who had proposed using TMLE to analyze a cluster randomized trial using a stepped-wedge design. In order to decide whether we would continue with this plan, I needed to revisit the literature to see if I could make more progress this time around. There are certainly more tutorials available as well as improved software and documentation, so it is much easier to get up and running to generate estimates. I was even able to build a model for the stepped-wedge design (that I hope to share on the blog some point soon). Beyond this, I really wanted to get a deeper understanding of the mathematical model that underlies the method without getting too far into the weeds (and proofs).</p>\n<p>I think I have made some progress in developing my understanding, and I wanted to write it down here for my future self (and for anyone else who wants to join along). This post is definitely not a tutorial nor is it literature review. It’s my attempt to encode my understanding of the conceptual ideas that support the underpinnings of the TMLE algorithm.</p>\n<div class=\"section level3\" id=\"what-problem-is-tmle-trying-to-solve\">\n<h3>What problem is TMLE trying to solve?</h3>\n<p>At its core, TMLE addresses a tension that appears throughout modern causal inference and semiparametric statistics. On one hand, we often want to estimate parameters that depend on complex features of the data-generating process, such as causal effects that require modeling both outcomes and treatment assignment mechanisms. On the other hand, more and more we rely on flexible machine learning methods to estimate these components, which can require large amounts of data before their estimates stabilize and may be difficult to analyze using classical statistical theory.</p>\n<p>Classical statistical inference typically relies on relatively simple, structured models where bias, variance, and uncertainty can be derived directly from model assumptions. Flexible machine learning models break many of those assumptions. They can be highly adaptive, involve many tuning choices, and may not correspond to smooth parametric likelihoods. As a result, while they can produce excellent predictions, their statistical behavior is often difficult to characterize analytically.</p>\n<p>TMLE provides a framework for resolving this tension. Rather than building estimators directly from outcome or treatment models, which are scientifically important but statistically treated as nuisance components, TMLE constructs estimators whose statistical behavior is governed by a known influence function. Once an estimator has the correct influence function, its large-sample bias, variance, and uncertainty can be characterized using general asymptotic theory, even when nuisance components are estimated flexibly.</p>\n<p>What follows is a description of four key elements that I think synthesize the ideas that are starting to make TMLE make more sense: (1) viewing parameters as functionals of probability distributions, (2) describing sampling as small perturbations of the data-generating distribution, (3) linearizing parameters using influence functions, and (4) constructing estimators that are orthogonal to nuisance estimation error.</p>\n</div>\n<div class=\"section level3\" id=\"estimators-as-functionals\">\n<h3>Estimators as functionals</h3>\n<p>An estimator, such as a mean or causal effect, is not merely a formula applied to data, but can be thought of more formally as a <em>functional</em>, a mapping that takes a probability distribution <span class=\"math inline\">\\(P\\)</span> in a family of distributions <span class=\"math inline\">\\(\\mathcal{P}\\)</span> and returns a number <span class=\"math inline\">\\(T(P)\\)</span>:\n<span class=\"math display\">\\[ T : \\mathcal{P} \\to \\mathbb{R} \\]</span>\nHere <span class=\"math inline\">\\(\\mathcal{P}\\)</span> just denotes the collection of probability distributions under consideration. Writing <span class=\"math inline\">\\(T : \\mathcal{P} \\to \\mathbb{R}\\)</span> emphasizes that a statistical parameter is not a property of a particular data set, but of the <em>data-generating distribution</em>. The data set enters only through the empirical distribution <span class=\"math inline\">\\(P_n\\)</span>.</p>\n<p>From this perspective, the key question becomes how <span class=\"math inline\">\\(T\\)</span> changes when the data-generating distribution changes slightly—for example, by comparing <span class=\"math inline\">\\(T(P_1)\\)</span> with <span class=\"math inline\">\\(T(P_2)\\)</span>, where <span class=\"math inline\">\\(P_1, P_2 \\in \\mathcal{P}\\)</span>.</p>\n<p>As an example, the <em>mean</em> is a functional:</p>\n<p><span class=\"math display\">\\[T(P) = \\int z\\ dP(z).\\]</span>\nIf <span class=\"math inline\">\\(P_0\\)</span> is the true data-generating distribution, then the target parameter is <span class=\"math inline\">\\(T(P_0)\\)</span>.</p>\n<p><span class=\"math inline\">\\(P_n\\)</span> is the empirical distribution (i.e., the distribution induced based by the observed sample),\n<span class=\"math display\">\\[P_n = \\frac{1}{n}\\sum_{i=1}^n \\delta_{Z_i},\\]</span>\nand the estimator is simply <span class=\"math inline\">\\(T(P_n)\\)</span>. The notation <span class=\"math inline\">\\(\\delta_z\\)</span> denotes a <em>point mass</em> at <span class=\"math inline\">\\(z\\)</span>: a probability distribution that assigns probability 1 to the single value <span class=\"math inline\">\\(z\\)</span>. Writing the empirical distribution as an average of point masses simply formalizes the the idea that the observed data place equal weight on each sampled point. The empirical distribution is therefore a discrete approximation to the true data-generating distribution <span class=\"math inline\">\\(P_0\\)</span>, placing mass <span class=\"math inline\">\\(1/n\\)</span> on each observed value.</p>\n<p>Ultimately, estimation is about quantifying how far our estimate <span class=\"math inline\">\\(\\hat{\\theta}\\)</span> based on the empirical distribution deviates from the target parameter <span class=\"math inline\">\\(\\theta_0\\)</span> defined by the true distribution:</p>\n<p><span class=\"math display\">\\[\\hat{θ} − \\theta_0 = T(P_n)−T(P_0).\\]</span>\nWritten this way, estimation error is simply the difference between evaluating the same functional at two nearby distributions: the true distribution <span class=\"math inline\">\\(P_0\\)</span> and its empirical approximation <span class=\"math inline\">\\(P_n\\)</span>. Everything relies on understanding how <span class=\"math inline\">\\(T(P)\\)</span> changes when <span class=\"math inline\">\\(P\\)</span> changes from <span class=\"math inline\">\\(P_0\\)</span> to <span class=\"math inline\">\\(P_n\\)</span>.</p>\n</div>\n<div class=\"section level3\" id=\"sampling-as-a-perturbation\">\n<h3>Sampling as a perturbation</h3>\n<p>The empirical distribution <span class=\"math inline\">\\(P_n\\)</span> differs from <span class=\"math inline\">\\(P_0\\)</span> by many small deviations. As we just saw, each observation contributes a point mass of size <span class=\"math inline\">\\(1/n\\)</span>. It is tempting to write this difference as\n<span class=\"math display\">\\[P_n − P_0,\\]</span>\nbut that isn’t really all that helpful. The object <span class=\"math inline\">\\(P_n − P_0\\)</span> is not itself a probability distribution but a signed measure that records where the empirical distribution over- or under-represents the truth. One way to conceptualize <span class=\"math inline\">\\(P_n − P_0\\)</span> is as a balance sheet: where the empirical distribution places more mass than the truth, the difference is positive; where it places less mass, the difference is negative. The total of these differences always sums to zero, but the pattern of positive and negative deviations determines how functionals of the distribution fluctuate.</p>\n<p><span class=\"math inline\">\\(P_n − P_0\\)</span> is not used on its own. It only appears through its action on functions. For any function <span class=\"math inline\">\\(f\\)</span>,</p>\n<p><span class=\"math display\">\\[(P_n −P_0)f \\equiv \\int f(z) \\ dP_n(z) − \\int f(z) \\ dP_0(z).\\]</span>\nThis quantity measures how much the empirical average of <span class=\"math inline\">\\(f(Z)\\)</span> deviates from its population mean.</p>\n<p>In this context, we can think of <span class=\"math inline\">\\(P_n − P_0\\)</span> heuristically as a sum of many small perturbations to the true distribution: regions of the sample space where the empirical distribution places slightly too much mass contribute positively, and regions where it places too little mass contribute negatively. The total deviation integrates to zero, but its interaction with specific functions can be nonzero.</p>\n<p>This brings us a little closer to the question we ended with in the last section: <em>how sensitive is the functional <span class=\"math inline\">\\(T(P)\\)</span> to small changes in the underlying distribution</em>? To answer this question, we need a precise way to describe an infinitesimal change in a probability distribution, one that preserves total probability mass and allows differentiation. Hampel’s contamination model provides exactly such a construction.</p>\n</div>\n<div class=\"section level3\" id=\"hampels-contamination-model-and-the-influence-function\">\n<h3>Hampel’s contamination model and the influence function</h3>\n<p>Answering that question requires an understanding of the <em>influence function</em>. After reading about TMLE for a few weeks, I stumbled on Frank Hample’s important <a href=\"https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10482962\" rel=\"nofollow\" target=\"_blank\">paper</a>, written in the early 1970’s and provides a nice explanation of the influence function in the context of robustness.</p>\n<p>In that paper, Hampel formalized what is meant by a “small change” in a distribution by introducing a specific directional perturbation of the data-generating distribution. This construction allows the parameter <span class=\"math inline\">\\(T(P)\\)</span> to be differentiated with respect to the underlying distribution itself. In other words, Hampel provides a way to define derivatives of statistical parameters with respect to the underlying distribution itself.</p>\n<p>As before, if <span class=\"math inline\">\\(P_0\\)</span> is the true distribution, and <span class=\"math inline\">\\(\\delta_z\\)</span> denotes the point mass at <span class=\"math inline\">\\(z\\)</span>, then we can consider a perturbed distribution that is a mixture of the two:</p>\n<p><span class=\"math display\">\\[P_{\\epsilon} = (1−\\epsilon)P_0 + \\epsilon \\delta_z,\\ \\ \\ 0&lt; \\epsilon &lt; 1.\\]</span>\nThis doesn’t correspond to adding a full new data point. Instead, it represents adding an infinitesimal amount of probability mass at <span class=\"math inline\">\\(z\\)</span>, defining a smooth path through the space of distributions along which derivatives can be taken.</p>\n<p>The influence function of <span class=\"math inline\">\\(T\\)</span> at <span class=\"math inline\">\\(P_0\\)</span> is <span class=\"math inline\">\\(\\phi_{P_0}\\)</span>, defined as\n<span class=\"math display\">\\[\n\\phi_{P_0}(z) \\equiv\n\\left.\n\\frac{d}{d\\epsilon}\nT(P_\\epsilon)\n\\right|_{\\epsilon = 0} =\n\\left.\n\\frac{d}{d\\epsilon}\nT\\!\\left((1-\\epsilon)P_0 + \\epsilon \\delta_z\\right)\n\\right|_{\\epsilon = 0}.\n\\]</span>\nIn words <em>the influence function measures the first-order effect on <span class=\"math inline\">\\(T(P)\\)</span> of “nudging” the distribution at point <span class=\"math inline\">\\(z\\)</span></em>.</p>\n<p>The goal, however, is not to compute <span class=\"math inline\">\\(T(P_n)\\)</span> exactly, but to approximate how far it is from <span class=\"math inline\">\\(T(P_0)\\)</span>. This difference\n<span class=\"math display\">\\[T(P_n) − T(P_0)\\]</span>\nis the estimation error. Describing its behavior is what allows us to understand bias, variability, and ultimately statistical uncertainty.</p>\n<p>Because <span class=\"math inline\">\\(P_n\\)</span> converges to <span class=\"math inline\">\\(P_0\\)</span> as the sample size grows, the difference between <span class=\"math inline\">\\(P_n\\)</span> and <span class=\"math inline\">\\(P_0\\)</span> becomes smaller. Rather than approximating <span class=\"math inline\">\\(T(P_n)\\)</span> directly, we approximate the <em>change</em> in the functional as we move from <span class=\"math inline\">\\(P_0\\)</span> to <span class=\"math inline\">\\(P_n\\)</span>:</p>\n<p>This is analogous to ordinary calculus, where we approximate the change <span class=\"math inline\">\\(f(x+h)−f(x)\\)</span> by a linear term involving the derivative of <span class=\"math inline\">\\(f\\)</span> at <span class=\"math inline\">\\(x\\)</span>. Here, the role of the “increment” <span class=\"math inline\">\\(h\\)</span> is played by the signed difference <span class=\"math inline\">\\(P_n−P_0\\)</span>, and the role of the derivative is played by the influence function. The influence function provides the best linear approximation to how T(P) responds to small perturbations of the underlying distribution.</p>\n<p>This leads directly to a first-order (linear) expansion of the estimation error <span class=\"math inline\">\\(T(P_n) − T(P_0)\\)</span>.</p>\n</div>\n<div class=\"section level3\" id=\"linearization-via-the-influence-function\">\n<h3>Linearization via the influence function</h3>\n<p>If <span class=\"math inline\">\\(T\\)</span> is sufficiently smooth, the first-order expansion is:</p>\n<p><span class=\"math display\">\\[\nT(P_n) − T(P_0) = (P_n−P_0)\\phi_{P_0} + R_n,\n\\]</span>\nwhere the leading term is linear in the perturbation <span class=\"math inline\">\\(P_n − P_0\\)</span>, and <span class=\"math inline\">\\(R_n\\)</span>\ncollects higher-order terms. This expansion should be read as a <em>functional Taylor expansion</em>: the influence function <span class=\"math inline\">\\(\\phi_{P_0}\\)</span> plays the role of a derivative, and\n<span class=\"math inline\">\\((P_n−P_0)\\phi_{P_0}\\)</span> is the linear approximation to the change in <span class=\"math inline\">\\(T\\)</span> induced by replacing <span class=\"math inline\">\\(P_0\\)</span> with its empirical approximation <span class=\"math inline\">\\(P_n\\)</span>.</p>\n<p>Earlier, I defined <span class=\"math inline\">\\((P_n – P_0)f\\)</span> for a general function <span class=\"math inline\">\\(f\\)</span>. Applying this definition when <span class=\"math inline\">\\(f\\)</span> is the influence function <span class=\"math inline\">\\(\\phi_{P_0}\\)</span> gives\n<span class=\"math display\">\\[\n(P_n−P_0)\\phi_{P_0}\n\\equiv\n\\int \\phi_{P_0}(z) \\ dP_n(z)\n−\n\\int \\phi_{P_0}(z) \\ dP_0(z)\n=\n\\frac{1}{n} \\sum_{i=1}^n \\phi_{P_0}(Z_i)\n–\nE_{P_0}[\\phi_{P_0}(Z)].\n\\]</span>\nThe second term vanishes because influence functions are defined to satisfy the centering condition\n<span class=\"math display\">\\[\nE_{P_0}[\\phi_{P_0}(Z)] = 0.\n\\]</span>\n<span class=\"math inline\">\\(R_n\\)</span> is a remainder term and plays a crucial role in asymptotic theory. If\n<span class=\"math display\">\\[\nR_n = o_p(n^{-1/2}),\n\\]</span>\nthen multiplying both sides of the expansion by <span class=\"math inline\">\\(\\sqrt{n}\\)</span> yields\n<span class=\"math display\">\\[\n\\sqrt{n}\\big(T(P_n) – T(P_0)\\big)\n=\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\phi_{P_0}(Z_i)\n+\no_p(1).\n\\]</span>\nThe appearance of the <span class=\"math inline\">\\(\\sqrt{n}\\)</span> scaling is not accidental — it reflects a universal feature of averaging noisy data. Each observation carries some randomness, and when we average <span class=\"math inline\">\\(n\\)</span> independent observations, the noise partly cancels out. The variability of the average shrinks as the sample grows, but it shrinks at a very specific rate: the standard deviation decreases proportionally to <span class=\"math inline\">\\(1/\\sqrt{n}\\)</span>. Multiplying the estimation error by <span class=\"math inline\">\\(\\sqrt{n}\\)</span> therefore puts it on a scale where the random fluctuations remain visible rather than collapsing toward zero. This is why <span class=\"math inline\">\\(\\sqrt{n}(\\hat{\\theta} – \\theta_0)\\)</span> often settles into a stable, typically Gaussian or normal, distribution in large samples, and why the <span class=\"math inline\">\\(\\sqrt{n}\\)</span> rate appears so consistently across statistical theory.</p>\n<p>Because the influence function has mean zero, the central limit theorem implies that the right-hand side converges in distribution to a normal random variable. This is what ultimately allows us to characterize the sampling variability of the estimator. TMLE is designed so that its estimation error admits exactly this kind of <span class=\"math inline\">\\(\\sqrt{n}\\)</span>-scaled linear expansion, even when parts of the model are estimated using flexible machine learning.</p>\n</div>\n<div class=\"section level3\" id=\"orthogonality-under-estimation-of-the-influence-function\">\n<h3>Orthogonality under estimation of the influence function</h3>\n<p>The mean-zero property of the influence function ensures that the leading term\n<span class=\"math display\">\\[\n(P_n−P_0)\\phi_{P_0}\n=\n\\frac{1}{n} \\sum_{i=1}^n \\phi_{P_0}(Z_i)\n\\]</span>\nfluctuates randomly around zero rather than drifting systematically. In other words, the influence function describes sampling variability, not bias. Without this centering condition, the first-order term would contain a persistent deterministic component, and the expansion would fail to characterize uncertainty correctly.</p>\n<p>However, this discussion implicitly assumes that <span class=\"math inline\">\\(\\phi_{P_0}\\)</span> is known. In most problems of interest, particularly causal inference, the influence function depends on additional <em>unknown</em> components of the data-generating process, such as conditional outcome models or treatment assignment mechanisms. These are often referred to as <em>nuisance functions</em> because they are not themselves the primary target of inference but are required to construct the influence function. These quantities must be estimated from the data, often using flexible statistical or machine learning methods.</p>\n<p>If the influence function depends on unknown nuisance functions, we must replace the true influence function with an estimated one. This introduces an additional source of error. If this nuisance estimation error enters the leading linear term of the expansion, it can be amplified by the <span class=\"math inline\">\\(\\sqrt{n}\\)</span> scaling and distort the asymptotic behavior of the estimator. In particular, bias introduced by nuisance estimation may not vanish even in large samples.</p>\n<div class=\"section level4\" id=\"what-goes-wrong-without-orthogonality\">\n<h4>What goes wrong without orthogonality</h4>\n<p>To see this more concretely, note that in practice we replace the true influence function <span class=\"math inline\">\\(\\phi_{P_0}\\)</span> with an estimated version <span class=\"math inline\">\\(\\phi_{\\hat P}\\)</span>. The leading term of the expansion then becomes\n<span class=\"math display\">\\[\n(P_n – P_0)\\phi_{\\hat P}\n=\n(P_n – P_0)\\phi_{P_0}\n+\n(P_n – P_0)(\\phi_{\\hat P} – \\phi_{P_0}).\n\\]</span>\nThe second term reflects nuisance estimation error. Because <span class=\"math inline\">\\((P_n – P_0)\\)</span> fluctuates at order <span class=\"math inline\">\\(1/\\sqrt{n}\\)</span>, this term can dominate the asymptotic behavior unless <span class=\"math inline\">\\(\\phi_{\\hat P}\\)</span> is constructed to be orthogonal to nuisance directions. Orthogonality ensures that this term is absorbed into the remainder and asymptotically negligible.</p>\n<p>Mathematically, this protection often appears as a <em>conditional</em> mean-zero property such as\n<span class=\"math display\">\\[\nE_{P_0}[\\phi_{P_0}(Z \\mid X)] = 0.\n\\]</span>\nThis condition implies that the influence function is deliberately constructed to be insensitive to errors that arise from estimating functions of the covariates <span class=\"math inline\">\\(X\\)</span>. Since nuisance components like outcome regressions and propensity scores depend on <span class=\"math inline\">\\(X\\)</span>, small errors in estimating these quantities tend to vary across values of <span class=\"math inline\">\\(X\\)</span>. The conditional mean-zero property ensures that when these errors interact with the influence function, their contributions cancel out when averaged across the sample. In effect, nuisance estimation errors add noise but do not systematically push the estimator away from the truth.</p>\n<p>This orthogonality is a key mechanism behind the robustness of TMLE. It allows flexible, slower-converging nuisance estimators to be used without contaminating the primary asymptotic behavior of the target parameter. More on how that influence function is constructed will hopefully follow in the next post that looks more closely at TMLE.</p>\n</div>\n</div>\n<div class=\"section level3\" id=\"next-steps\">\n<h3>Next steps</h3>\n<p>In the next post, I’ll take the same approach to walk through the main elements of the TMLE algorithm, focusing on how they operationalize the ideas developed here.</p>\n<p>\n</p><p><small><font color=\"darkkhaki\">\nReferences:</font></small></p>\n<p>Van der Laan, Mark J., and Sherri Rose. Targeted learning: causal inference for observational and experimental data. Vol. 4. New York: Springer, 2011.</p>\n<p>Hampel, Frank R. “The influence curve and its role in robust estimation.” Journal of the american statistical association 69, no. 346 (1974): 383-393.</p>\n</div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.rdatagen.net/post/2026-02-05-getting-to-the-bottom-of-tmle-1/\"> ouR data generation</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 2800,
    "reading_time_min": 14.0,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/our-data-generation/",
        "text": "ouR data generation"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://www.rdatagen.net/post/2026-02-05-getting-to-the-bottom-of-tmle-1/",
        "text": "ouR data generation"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://www.google.com/books/edition/Targeted_Learning/RGnSX5aCAgQC?hl=en",
        "text": "book"
      },
      {
        "href": "https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10482962",
        "text": "paper"
      },
      {
        "href": "https://www.rdatagen.net/post/2026-02-05-getting-to-the-bottom-of-tmle-1/",
        "text": "ouR data generation"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [],
    "lang": "en-US",
    "crawled_at_utc": "2026-02-06T23:17:28Z"
  }
}