{
  "id": "fcaa73ecc02451b8799f68ecf2fcbe77143c60b2",
  "url": "https://www.r-bloggers.com/2026/02/smooth-v4-4-0/",
  "created_at_utc": "2026-02-09T17:43:34Z",
  "crawled_at_utc": "2026-02-09T17:43:35Z",
  "html_title": "smooth v4.4.0 | R-bloggers",
  "meta_description": "Great news, everyone! smooth package for R version 4.4.0 is now on CRAN. Why is this a great news? Let me explain! On this page: What‚Äôs new? Evaluation Setup Results What‚Äôs next? Here is what‚Äôs new since 4.3.0: First, I have worked on tuning the initialisation in adam() in case of backcasting, and improved the [‚Ä¶] Message smooth v4.4.0 first appeared on Open Forecasting.",
  "data": {
    "url": "https://www.r-bloggers.com/2026/02/smooth-v4-4-0/",
    "canonical_url": "https://www.r-bloggers.com/2026/02/smooth-v4-4-0/",
    "html_title": "smooth v4.4.0 | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "Great news, everyone! smooth package for R version 4.4.0 is now on CRAN. Why is this a great news? Let me explain! On this page: What‚Äôs new? Evaluation Setup Results What‚Äôs next? Here is what‚Äôs new since 4.3.0: First, I have worked on tuning the initialisation in adam() in case of backcasting, and improved the [‚Ä¶] Message smooth v4.4.0 first appeared on Open Forecasting.",
    "meta_keywords": null,
    "og_title": "smooth v4.4.0 | R-bloggers",
    "og_description": "Great news, everyone! smooth package for R version 4.4.0 is now on CRAN. Why is this a great news? Let me explain! On this page: What‚Äôs new? Evaluation Setup Results What‚Äôs next? Here is what‚Äôs new since 4.3.0: First, I have worked on tuning the initialisation in adam() in case of backcasting, and improved the [‚Ä¶] Message smooth v4.4.0 first appeared on Open Forecasting.",
    "og_image": "https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-300x180.png&nocache=1",
    "twitter_title": "smooth v4.4.0 | R-bloggers",
    "twitter_description": "Great news, everyone! smooth package for R version 4.4.0 is now on CRAN. Why is this a great news? Let me explain! On this page: What‚Äôs new? Evaluation Setup Results What‚Äôs next? Here is what‚Äôs new since 4.3.0: First, I have worked on tuning the initialisation in adam() in case of backcasting, and improved the [‚Ä¶] Message smooth v4.4.0 first appeared on Open Forecasting.",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "smooth v4.4.0\nPosted on\nFebruary 9, 2026\nby\nIvan Svetunkov\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nArchives R - Open Forecasting\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nGreat news, everyone! smooth package for R version 4.4.0 is now on CRAN. Why is this a great news? Let me explain!\nOn this page:\nWhat‚Äôs new?\nEvaluation\nSetup\nResults\nWhat‚Äôs next?\nHere is what‚Äôs new since 4.3.0:\nFirst, I have worked on tuning the initialisation in\nadam()\nin case of backcasting, and improved the\nmsdecompose()\nfunction a bit to get more robust results. This was necessary to make sure that when the smoothing parameters are close to zero, initial values would still make sense. This is already in\nadam\n(use\nsmoother=\"global\"\nto test), but will become the default behaviour in the next version of the package, when we iron everything out. This is all a part of a larger work with Kandrika Pritularga on a paper about the initialisation of dynamic models.\nSecond, I have fixed a long standing issue of the eigenvalues calculation inside the dynamic models, which is applicable only in case of\nbounds=\"admissible\"\nand might impact ARIMA, CES and GUM. The parameter restriction are now done consistently across all functions, guaranteeing that they will not fail and will produce stable/invertible estimates of parameters.\nThird, I have added the Sparse ARMA function, which constructs ARMA(p,q) of the specific orders, dropping all the elements from 1 to those. e.g. SpARMA(2,3) would have the following form:\n\\begin{equation*}\ny_t = \\phi_2 y_{t-2} + \\theta_3 \\epsilon_{t-3} + \\epsilon_{t}\n\\end{equation*}\nThis weird model is needed for a project I am working on together with Devon Barrow, Nikos Kourentzes and Yves Sagaert. I‚Äôll explain more when we get the final draft of the paper.\nAnd something very important, which you will not notice: I refactored the C++ code in the package so that it is available not only for R, but also for Python‚Ä¶ Why? I‚Äôll explain in the next post :). But this also means that the old functions that relied on the previous generation of the C++ code are now discontinued, and all the smooth functions use the new core. This applies to\nes()\n,\nssarima()\n,\nmsarima()\n,\nces()\n,\ngum()\nand\nsma()\n. You will not notice any change, except that some of them should become a bit faster and probably more robust. And this also means that all of them will now be able to use methods for the\nadam()\nfunction. For example, the\nsummary()\nwill produce the proper output with standard errors and confidence intervals for all estimated parameters.\nEvaluation\nDISCLAIMER\n: The previous evaluation was for smooth v4.3.0, you can find it\nhere\n. I have changed one of error measures (sCE to SAME), but the rest is the same, so the results are widely comparable between the versions.\nThe setup\nAs usual, in situations like this, I have run the evaluation on the M1, M3 and Tourism competition data. This time, I have added more flavours of the ETS model selection so that you can see how the models pool impacts the forecasting accuracy. Short description:\n1. XXX ‚Äì select between pure additive ETS models only;\n2. ZZZ ‚Äì select from the pool of all 30 models, but use branch-and-bound to kick out the less suitable models;\n3. ZXZ ‚Äì same as (2), but without the multiplicative trend models. This is used in the\nsmooth\nfunctions\nby default\n;\n4. FFF ‚Äì select from the pool of all 30 models (exhaustive search);\n5. SXS ‚Äì the pool of models that is used by default in\nets()\nfrom the\nforecast\npackage in R.\nI also tested three types of the ETS initialisation:\n1. Back ‚Äì\ninitial=\"backcasting\"\n2. Opt ‚Äì\ninitial=\"optimal\"\n3. Two ‚Äì\ninitial=\"two-stage\"\nBackcasting is now the default method of initialisation, and does well in many cases, but I found that optimal initials (if done correctly) help in some difficult situations, as long a you have enough of computational time.\nI used two error measures and computational time to check how functions work. The first error measure is called RMSSE (Root Mean Squared Scaled Error) from\nAthanasopoulos & Kourentzes (2023)\n:\n\\begin{equation*}\n\\mathrm{RMSSE} = \\frac{1}{\\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T-1} \\Delta_t^2}} \\mathrm{RMSE},\n\\end{equation*}\nwhere \\(\\mathrm{RMSE} = \\sqrt{\\frac{1}{h} \\sum_{j=1}^h e^2_{t+j}}\\) is the Root Mean Squared Error of the point forecasts, and \\(\\Delta_t\\) is the first differences of the in-sample actual values.\nThe second measure does not have a standard name in the literature, but the idea of it is to the measure the bias of forecasts and to get rid of the sign to make sure that positively biased forecasts on some time series are not cancelled out by the negative ones on the other ones. I call this measure ‚ÄúScaled Absolute Mean Error‚Äù (SAME):\n\\begin{equation*}\n\\mathrm{SAME} = \\frac{1}{\\frac{1}{T-1} \\sum_{t=1}^{T-1} |\\Delta_t|} \\mathrm{AME},\n\\end{equation*}\nwhere \\(\\mathrm{AME}= \\left| \\frac{1}{h} \\sum_{j=1}^h e_{t+j} \\right|\\).\nFor both of these measures, the lower value is better than the higher one. As for the computational time, I have measured it for each model and each series, and this time I provided distribution of times to better see how methods perform.\nBoring code in R\nlibrary(Mcomp)\nlibrary(Tcomp)\nlibrary(forecast)\nlibrary(smooth)\n\nlibrary(doMC)\nregisterDoMC(detectCores())\n\n# Create a small but neat function that will return a vector of error measures\nerrorMeasuresFunction <- function(object, holdout, insample){\n        holdout <- as.vector(holdout);\n        insample <- as.vector(insample);\n\t# RMSSE and SAME are defined in greybox v2.0.7\n        return(c(RMSSE(holdout, object$mean, mean(diff(insample^2)),\n                 SAME(holdout, object$mean, mean(abs(diff(insample)))),\n                 object$timeElapsed))\n}\n\ndatasets <- c(M1,M3,tourism)\ndatasetLength <- length(datasets)\n\n# Method configuration list\n# Each method specifies: fn (function name), pkg (package), model, initial,\nmethodsConfig <- list(\n\t# ETS and Auto ARIMA from the forecast package in R\n\t\"ETS\" = list(fn = \"ets\", pkg = \"forecast\", use_x_only = TRUE),\n\t\"Auto ARIMA\" = list(fn = \"auto.arima\", pkg = \"forecast\", use_x_only = TRUE),\n\t# ADAM with different initialisation schemes\n\t\"ADAM ETS Back\" = list(fn = \"adam\", pkg = \"smooth\", model = \"ZXZ\", initial = \"back\"),\n\t\"ADAM ETS Opt\" = list(fn = \"adam\", pkg = \"smooth\", model = \"ZXZ\", initial = \"opt\"),\n\t\"ADAM ETS Two\" = list(fn = \"adam\", pkg = \"smooth\", model = \"ZXZ\", initial = \"two\"),\n\t# ES, which is a wrapper of ADAM. Should give very similar results to ADAM on regular data\n\t\"ES Back\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZXZ\", initial = \"back\"),\n\t\"ES Opt\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZXZ\", initial = \"opt\"),\n\t\"ES Two\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZXZ\", initial = \"two\"),\n\t# Several flavours for model selection in ES\n\t\"ES XXX\" = list(fn = \"es\", pkg = \"smooth\", model = \"XXX\", initial = \"back\"),\n\t\"ES ZZZ\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZZZ\", initial = \"back\"),\n\t\"ES FFF\" = list(fn = \"es\", pkg = \"smooth\", model = \"FFF\", initial = \"back\"),\n\t\"ES SXS\" = list(fn = \"es\", pkg = \"smooth\", model = \"SXS\", initial = \"back\"),\n\t# ARIMA implementations in smooth\n\t\"MSARIMA\" = list(fn = \"auto.msarima\", pkg = \"smooth\", initial = \"back\"),\n\t\"SSARIMA\" = list(fn = \"auto.ssarima\", pkg = \"smooth\", initial = \"back\"),\n\t# Complex Exponential Smoothing\n\t\"CES\" = list(fn = \"auto.ces\", pkg = \"smooth\", initial = \"back\"),\n\t# Generalised Univeriate Model (experimental)\n\t\"GUM\" = list(fn = \"auto.gum\", pkg = \"smooth\", initial = \"back\")\n)\n\nmethodsNames <- names(methodsConfig)\nmethodsNumber <- length(methodsNames)\n\nmeasuresNames <- c(\"RMSSE\",\"SAME\",\"Time\")\nmeasuresNumber <- length(measuresNames)\n\ntestResults <- array(NA, c(methodsNumber, datasetLength, measuresNumber),\n                     dimnames = list(methodsNames, NULL, measuresNames))\n\n# Unified loop over all methods\nfor(j in seq_along(methodsConfig)){\n\tcfg <- methodsConfig[[j]]\n\tcat(\"Running method:\", methodsNames[j], \"\\n\")\n\n\tresult <- foreach(i = 1:datasetLength, .combine = \"cbind\",\n\t                  .packages = c(\"smooth\", \"forecast\")) %dopar% {\n\t\tstartTime <- Sys.time()\n\n\t\t# Build model call based on method type\n\t\tif(isTRUE(cfg$use_x_only)){\n\t\t\t# forecast package methods: ets, auto.arima\n\t\t\ttest <- do.call(cfg$fn, list(datasets[[i]]$x))\n\t\t}else if(cfg$fn %in% c(\"adam\", \"es\")) {\n\t\t\t# adam and es take dataset and model\n\t\t\ttest <- do.call(cfg$fn, list(datasets[[i]], model=cfg$model, initial = cfg$initial))\n\t\t}else{\n\t\t\t# auto.msarima, auto.ssarima, auto.ces, auto.gum\n\t\t\ttest <- do.call(cfg$fn, list(datasets[[i]], initial = cfg$initial))\n\t\t}\n\n\t\t# Build forecast call\n\t\tforecast_args <- list(test, h = datasets[[i]]$h)\n\t\ttestForecast <- do.call(forecast, forecast_args)\n\t\ttestForecast$timeElapsed <- Sys.time() - startTime\n\n\t\treturn(errorMeasuresFunction(testForecast, datasets[[i]]$xx, datasets[[i]]$x))\n\t}\n\ttestResults[j,,] <- t(result)\n}\nResults\nAnd here are the results for the smooth functions in v4.4.0 for R. First, we summarise the RMSSEs. I produce quartiles of distribution of RMSSE together with the mean.\ncbind(t(apply(testResults[,,\"RMSSE\"],1,quantile, na.rm=T)),\n      mean=apply(testResults[,,\"RMSSE\"],1,mean)) |> round(4)\n                  0%    25%    50%    75%      100%   mean\nETS           0.0245 0.6772 1.1806 2.3765   51.6160 1.9697\nAuto ARIMA    0.0246 0.6802 1.1790 2.3583   51.6160 1.9864\nADAM ETS Back 0.0183 0.6647 1.1620 2.3023   50.2585 1.9283\nADAM ETS Opt  0.0242 0.6714 1.1868 2.3623   51.6160 1.9432\nADAM ETS Two  0.0246 0.6690 1.1875 2.3374   51.6160 1.9480\nES Back       0.0183 0.6674 1.1647 2.3164   50.2585 1.9292\nES Opt        0.0242 0.6740 1.1858 2.3644   51.6160 1.9469\nES Two        0.0245 0.6717 1.1874 2.3463   51.6160 1.9538\nES XXX        0.0183 0.6777 1.1708 2.3062   50.2585 1.9613\nES ZZZ        0.0108 0.6682 1.1816 2.3611  201.4959 2.0841\nES FFF        0.0145 0.6795 1.2170 2.4575 5946.1858 3.3033\nES SXS        0.0183 0.6754 1.1709 2.3539   50.2585 1.9448\nMSARIMA       0.0278 0.6988 1.1898 2.4208   51.6160 2.0750\nSSARIMA       0.0277 0.7371 1.2544 2.4425   51.6160 2.0625\nCES Back      0.0450 0.6761 1.1741 2.3205   51.0571 1.9650\nGUM Back      0.0333 0.7077 1.2073 2.4533   51.6184 2.0461\nThe worst performing models are the ETS with the multiplicative trend (ES ZZZ and ES FFF). This is because there are outliers in some time series, and the multiplicative trend reacts to them by amending the trend value to something large (e.g. 2, i.e. twice increase in level for each step), and then can never return to a reasonable level (see explanation of this phenomenon in\nSection 6.6 of ADAM book\n). As expected, ADAM ETS does very similar to the ES, and we can see that the default initialisation (backcasting) is pretty good in terms of RMSSE values. To be fair, if the models are tested on a different dataset, it might be the case that the optimal initialisation would do better.\nHere is a table with the SAME results:\ncbind(t(apply(testResults[,,\"SAME\"],1,quantile, na.rm=T)),\n      mean=apply(testResults[,,\"SAME\"],1,mean)) |> round(4)\n                 0%    25%    50%    75%      100%   mean\nETS           8e-04 0.3757 1.0203 2.5097   54.6872 1.9983\nAuto ARIMA    0e+00 0.3992 1.0429 2.4565   53.2710 2.0446\nADAM ETS Back 1e-04 0.3752 0.9965 2.4047   52.3418 1.9518\nADAM ETS Opt  5e-04 0.3733 1.0212 2.4848   55.1018 1.9618\nADAM ETS Two  8e-04 0.3780 1.0316 2.4511   55.1019 1.9712\nES Back       0e+00 0.3733 0.9945 2.4122   53.4504 1.9485\nES Opt        2e-04 0.3727 1.0255 2.4756   54.6860 1.9673\nES Two        1e-04 0.3855 1.0323 2.4535   54.6856 1.9799\nES XXX        1e-04 0.3733 1.0050 2.4257   53.1697 1.9927\nES ZZZ        3e-04 0.3824 1.0135 2.4885  229.7626 2.1376\nES FFF        3e-04 0.3972 1.0489 2.6042 3748.4268 2.9501\nES SXS        6e-04 0.3750 1.0125 2.4627   53.4504 1.9725\nMSARIMA       1e-04 0.3960 1.0094 2.5409   54.7916 2.1227\nSSARIMA       1e-04 0.4401 1.1222 2.5673   52.5023 2.1248\nCES Back      6e-04 0.3767 1.0079 2.4085   54.9026 2.0052\nGUM Back      0e+00 0.3803 1.0575 2.6259   63.0637 2.0858\nIn terms of bias, smooth implementations of ETS are doing well again, and we can see the same issue with the multiplicative trend here as before. Another thing to note is that MSARIMA and SSARIMA are not as good as the Auto ARIMA from the forecast package on these datasets in terms of RMSSE and SAME (at least, in terms of mean error measures). And actually, GUM and CES are now better than those in terms of both error measures.\nFinally, here is a table with the computational time:\ncbind(t(apply(testResults[,,\"Time\"],1,quantile, na.rm=T)),\n      mean=apply(testResults[,,\"Time\"],1,mean)) |> round(4)\n                  0%    25%    50%     75%    100%   mean\nETS           0.0032 0.0117 0.1660  0.6728  1.6400 0.3631\nAuto ARIMA    0.0100 0.1184 0.3618  1.0548 54.3652 1.4760\nADAM ETS Back 0.0162 0.1062 0.1854  0.4022  2.5109 0.2950\nADAM ETS Opt  0.0319 0.1920 0.3103  0.6792  3.8933 0.5368\nADAM ETS Two  0.0427 0.2548 0.4035  0.8567  3.7178 0.6331\nES Back       0.0153 0.0896 0.1521  0.3335  2.1128 0.2476\nES Opt        0.0303 0.1667 0.2565  0.5910  3.5887 0.4522\nES Two        0.0483 0.2561 0.4016  0.8626  3.5892 0.6309\nMSARIMA Back  0.0614 0.3418 0.6947  0.9868  3.9677 0.7534\nSSARIMA Back  0.0292 0.2963 0.8988  2.1729 13.7635 1.6581\nCES Back      0.0146 0.0400 0.1834  0.2298  1.2099 0.1713\nGUM Back      0.0165 0.2101 1.5221  3.0543  9.5380 1.9506\n\n# Separate table for special pools of ETS.\n# The time is proportional to the number of models here\n=========================================================\n                  0%    25%    50%     75%    100%   mean\nES XXX        0.0114 0.0539 0.0782  0.1110  0.8163 0.0859\nES ZZZ        0.0147 0.1371 0.2690  0.4947  2.2049 0.3780\nES FFF        0.0529 0.2775 1.1539  1.5926  3.8552 1.1231\nES SXS        0.0323 0.1303 0.4491  0.6013  2.2170 0.4581\nI have manually moved the specific ES model pools flavours below because there is no point in comparing their computational time with the time of the others (they have different pools of models and thus are not really comparable with the rest).\nWhat we can see from this, is that the ES with backcasting is faster in comparison with the other models in this setting (in terms of mean and median computational time). CES is very fast in terms of mean computational time, which is probably because of the very short pool of models to choose from (only four). SSARIMA is pretty slow, which is due to the nature of its order selection algorithm (I don't plan to update it any time soon, but if someone wants to contribute - let me know). But the interesting thing is that Auto ARIMA, while being relatively fine in terms of median time, has the highest maximum one, meaning that for some time series, it failed for some unknown reason. The series that caused the biggest issue for Auto ARIMA is N389 from the M1 competition. I'm not sure what the issue was, and I don't have time to investigate this.\nMean computational time vs mean RMSSE\nComparing the mean computational time with mean RMSSE value (image above), it looks like the overall tendency in the\nsmooth\n+\nforecast\nfunctions for the M1, M3 and Tourism datasets is that additional computational time does not improve the accuracy. But it also looks like a simpler pool of pure additive models (ETS(X,X,X)) harms the accuracy in comparison with the branch-and-bound based one of the default\nmodel=\"ZXZ\"\n. There seems to be a sweet spot in terms of the pool of models to choose from (no multiplicative trend, allow mixed models). This aligns well with the papers of\nPetropoulos et al. (2025)\n, who investigated the accuracy of arbitrary short pools of models and\nKourentzes et al. (2019)\n, who showed how pooling (if done correctly) can improve the accuracy on average.\nWhat's next?\nFor R, the main task now is to rewrite the\noes()\nfunction and substitute it with the\nom()\none - \"Occurrence Model\". This should be equivalent to\nadam()\nin functionality, allowing to introduce ETS, ARIMA and explanatory variables for the occurrence part of the model. This is a huge work, which I hope to progress slowly throughout the 2026 and finish by the end of the year. Doing that will also allow me removing the last bits of the old C++ code and switch to the ADAM core completely, introducing more functionality for capturing patterns on intermittent demand. The minor task, is to test the\nsmoother=\"global\"\nmore for the ETS initialisation and roll it out as the default in the next release for both R and Python.\nFor Python,... What Python? Ah! You'll see soon üôÇ\nMessage\nsmooth v4.4.0\nfirst appeared on\nOpen Forecasting\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nArchives R - Open Forecasting\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-398838 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">smooth v4.4.0</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 9, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/ivan-svetunkov/\">Ivan Svetunkov</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/\"> Archives R - Open Forecasting</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>Great news, everyone! smooth package for R version 4.4.0 is now on CRAN. Why is this a great news? Let me explain!</p>\n<p>On this page:</p>\n<ul>\n<li><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/#whatsNew\" rel=\"nofollow\" target=\"_blank\">What‚Äôs new?</a></li>\n<li><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/#evaluation\" rel=\"nofollow\" target=\"_blank\">Evaluation</a></li>\n<ul>\n<li><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/#evaluationSetup\" rel=\"nofollow\" target=\"_blank\">Setup</a></li>\n<li><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/#evaluationResults\" rel=\"nofollow\" target=\"_blank\">Results</a></li>\n</ul>\n<li><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/#whatsNext\" rel=\"nofollow\" target=\"_blank\">What‚Äôs next?</a></li>\n</ul>\n<h3 id=\"whatsNew\">Here is what‚Äôs new since 4.3.0:</h3>\n<p>First, I have worked on tuning the initialisation in <code>adam()</code> in case of backcasting, and improved the <code>msdecompose()</code> function a bit to get more robust results. This was necessary to make sure that when the smoothing parameters are close to zero, initial values would still make sense. This is already in <code>adam</code> (use <code>smoother=\"global\"</code> to test), but will become the default behaviour in the next version of the package, when we iron everything out. This is all a part of a larger work with Kandrika Pritularga on a paper about the initialisation of dynamic models.</p>\n<p>Second, I have fixed a long standing issue of the eigenvalues calculation inside the dynamic models, which is applicable only in case of <code>bounds=\"admissible\"</code> and might impact ARIMA, CES and GUM. The parameter restriction are now done consistently across all functions, guaranteeing that they will not fail and will produce stable/invertible estimates of parameters.</p>\n<p>Third, I have added the Sparse ARMA function, which constructs ARMA(p,q) of the specific orders, dropping all the elements from 1 to those. e.g. SpARMA(2,3) would have the following form:<br/>\n\\begin{equation*}<br/>\ny_t = \\phi_2 y_{t-2} + \\theta_3 \\epsilon_{t-3} + \\epsilon_{t}<br/>\n\\end{equation*}<br/>\nThis weird model is needed for a project I am working on together with Devon Barrow, Nikos Kourentzes and Yves Sagaert. I‚Äôll explain more when we get the final draft of the paper.</p>\n<p>And something very important, which you will not notice: I refactored the C++ code in the package so that it is available not only for R, but also for Python‚Ä¶ Why? I‚Äôll explain in the next post :). But this also means that the old functions that relied on the previous generation of the C++ code are now discontinued, and all the smooth functions use the new core. This applies to <code>es()</code>, <code>ssarima()</code>, <code>msarima()</code>, <code>ces()</code>, <code>gum()</code> and <code>sma()</code>. You will not notice any change, except that some of them should become a bit faster and probably more robust. And this also means that all of them will now be able to use methods for the <code>adam()</code> function. For example, the <code>summary()</code> will produce the proper output with standard errors and confidence intervals for all estimated parameters.</p>\n<h2 id=\"evaluation\">Evaluation</h2>\n<p><strong>DISCLAIMER</strong>: The previous evaluation was for smooth v4.3.0, you can find it <a href=\"https://openforecast.org/2025/07/04/smooth-v4-3-0-in-r-what-s-new-and-what-s-next/\" rel=\"nofollow\" target=\"_blank\">here</a>. I have changed one of error measures (sCE to SAME), but the rest is the same, so the results are widely comparable between the versions.</p>\n<h3 id=\"evaluationSetup\">The setup</h3>\n<p>As usual, in situations like this, I have run the evaluation on the M1, M3 and Tourism competition data. This time, I have added more flavours of the ETS model selection so that you can see how the models pool impacts the forecasting accuracy. Short description:</p>\n<p>1. XXX ‚Äì select between pure additive ETS models only;<br/>\n2. ZZZ ‚Äì select from the pool of all 30 models, but use branch-and-bound to kick out the less suitable models;<br/>\n3. ZXZ ‚Äì same as (2), but without the multiplicative trend models. This is used in the <code>smooth</code> functions <strong>by default</strong>;<br/>\n4. FFF ‚Äì select from the pool of all 30 models (exhaustive search);<br/>\n5. SXS ‚Äì the pool of models that is used by default in <code>ets()</code> from the <code>forecast</code> package in R.</p>\n<p>I also tested three types of the ETS initialisation:<br/>\n1. Back ‚Äì <code>initial=\"backcasting\"</code><br/>\n2. Opt ‚Äì <code>initial=\"optimal\"</code><br/>\n3. Two ‚Äì <code>initial=\"two-stage\"</code></p>\n<p>Backcasting is now the default method of initialisation, and does well in many cases, but I found that optimal initials (if done correctly) help in some difficult situations, as long a you have enough of computational time.</p>\n<p>I used two error measures and computational time to check how functions work. The first error measure is called RMSSE (Root Mean Squared Scaled Error) from <a href=\"https://dx.doi.org/10.1016/j.ijforecast.2021.11.013%3EM5%20competition%3C/a%3E,%20motivated%20by%20%3Ca%20href=\" rel=\"nofollow\" target=\"_blank\">Athanasopoulos &amp; Kourentzes (2023)</a>:</p>\n<p>\\begin{equation*}<br/>\n\\mathrm{RMSSE} = \\frac{1}{\\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T-1} \\Delta_t^2}} \\mathrm{RMSE},<br/>\n\\end{equation*}<br/>\nwhere \\(\\mathrm{RMSE} = \\sqrt{\\frac{1}{h} \\sum_{j=1}^h e^2_{t+j}}\\) is the Root Mean Squared Error of the point forecasts, and \\(\\Delta_t\\) is the first differences of the in-sample actual values.</p>\n<p>The second measure does not have a standard name in the literature, but the idea of it is to the measure the bias of forecasts and to get rid of the sign to make sure that positively biased forecasts on some time series are not cancelled out by the negative ones on the other ones. I call this measure ‚ÄúScaled Absolute Mean Error‚Äù (SAME):</p>\n<p>\\begin{equation*}<br/>\n\\mathrm{SAME} = \\frac{1}{\\frac{1}{T-1} \\sum_{t=1}^{T-1} |\\Delta_t|} \\mathrm{AME},<br/>\n\\end{equation*}<br/>\nwhere \\(\\mathrm{AME}= \\left| \\frac{1}{h} \\sum_{j=1}^h e_{t+j} \\right|\\).</p>\n<p>For both of these measures, the lower value is better than the higher one. As for the computational time, I have measured it for each model and each series, and this time I provided distribution of times to better see how methods perform.</p>\n<div class=\"su-spoiler su-spoiler-style-fancy su-spoiler-icon-plus su-spoiler-closed\" data-anchor-in-url=\"no\" data-scroll-offset=\"0\"><div class=\"su-spoiler-title\" role=\"button\" tabindex=\"0\"><span class=\"su-spoiler-icon\"></span>Boring code in R</div><div class=\"su-spoiler-content su-u-clearfix su-u-trim\">\n<pre>library(Mcomp)\nlibrary(Tcomp)\nlibrary(forecast)\nlibrary(smooth)\n\nlibrary(doMC)\nregisterDoMC(detectCores())\n\n# Create a small but neat function that will return a vector of error measures\nerrorMeasuresFunction &lt;- function(object, holdout, insample){\n        holdout &lt;- as.vector(holdout);\n        insample &lt;- as.vector(insample);\n\t# RMSSE and SAME are defined in greybox v2.0.7\n        return(c(RMSSE(holdout, object$mean, mean(diff(insample^2)),\n                 SAME(holdout, object$mean, mean(abs(diff(insample)))),\n                 object$timeElapsed))\n}\n\ndatasets &lt;- c(M1,M3,tourism)\ndatasetLength &lt;- length(datasets)\n\n# Method configuration list\n# Each method specifies: fn (function name), pkg (package), model, initial,\nmethodsConfig &lt;- list(\n\t# ETS and Auto ARIMA from the forecast package in R\n\t\"ETS\" = list(fn = \"ets\", pkg = \"forecast\", use_x_only = TRUE),\n\t\"Auto ARIMA\" = list(fn = \"auto.arima\", pkg = \"forecast\", use_x_only = TRUE),\n\t# ADAM with different initialisation schemes\n\t\"ADAM ETS Back\" = list(fn = \"adam\", pkg = \"smooth\", model = \"ZXZ\", initial = \"back\"),\n\t\"ADAM ETS Opt\" = list(fn = \"adam\", pkg = \"smooth\", model = \"ZXZ\", initial = \"opt\"),\n\t\"ADAM ETS Two\" = list(fn = \"adam\", pkg = \"smooth\", model = \"ZXZ\", initial = \"two\"),\n\t# ES, which is a wrapper of ADAM. Should give very similar results to ADAM on regular data\n\t\"ES Back\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZXZ\", initial = \"back\"),\n\t\"ES Opt\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZXZ\", initial = \"opt\"),\n\t\"ES Two\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZXZ\", initial = \"two\"),\n\t# Several flavours for model selection in ES\n\t\"ES XXX\" = list(fn = \"es\", pkg = \"smooth\", model = \"XXX\", initial = \"back\"),\n\t\"ES ZZZ\" = list(fn = \"es\", pkg = \"smooth\", model = \"ZZZ\", initial = \"back\"),\n\t\"ES FFF\" = list(fn = \"es\", pkg = \"smooth\", model = \"FFF\", initial = \"back\"),\n\t\"ES SXS\" = list(fn = \"es\", pkg = \"smooth\", model = \"SXS\", initial = \"back\"),\n\t# ARIMA implementations in smooth\n\t\"MSARIMA\" = list(fn = \"auto.msarima\", pkg = \"smooth\", initial = \"back\"),\n\t\"SSARIMA\" = list(fn = \"auto.ssarima\", pkg = \"smooth\", initial = \"back\"),\n\t# Complex Exponential Smoothing\n\t\"CES\" = list(fn = \"auto.ces\", pkg = \"smooth\", initial = \"back\"),\n\t# Generalised Univeriate Model (experimental)\n\t\"GUM\" = list(fn = \"auto.gum\", pkg = \"smooth\", initial = \"back\")\n)\n\nmethodsNames &lt;- names(methodsConfig)\nmethodsNumber &lt;- length(methodsNames)\n\nmeasuresNames &lt;- c(\"RMSSE\",\"SAME\",\"Time\")\nmeasuresNumber &lt;- length(measuresNames)\n\ntestResults &lt;- array(NA, c(methodsNumber, datasetLength, measuresNumber),\n                     dimnames = list(methodsNames, NULL, measuresNames))\n\n# Unified loop over all methods\nfor(j in seq_along(methodsConfig)){\n\tcfg &lt;- methodsConfig[[j]]\n\tcat(\"Running method:\", methodsNames[j], \"\\n\")\n\n\tresult &lt;- foreach(i = 1:datasetLength, .combine = \"cbind\",\n\t                  .packages = c(\"smooth\", \"forecast\")) %dopar% {\n\t\tstartTime &lt;- Sys.time()\n\n\t\t# Build model call based on method type\n\t\tif(isTRUE(cfg$use_x_only)){\n\t\t\t# forecast package methods: ets, auto.arima\n\t\t\ttest &lt;- do.call(cfg$fn, list(datasets[[i]]$x))\n\t\t}else if(cfg$fn %in% c(\"adam\", \"es\")) {\n\t\t\t# adam and es take dataset and model\n\t\t\ttest &lt;- do.call(cfg$fn, list(datasets[[i]], model=cfg$model, initial = cfg$initial))\n\t\t}else{\n\t\t\t# auto.msarima, auto.ssarima, auto.ces, auto.gum\n\t\t\ttest &lt;- do.call(cfg$fn, list(datasets[[i]], initial = cfg$initial))\n\t\t}\n\n\t\t# Build forecast call\n\t\tforecast_args &lt;- list(test, h = datasets[[i]]$h)\n\t\ttestForecast &lt;- do.call(forecast, forecast_args)\n\t\ttestForecast$timeElapsed &lt;- Sys.time() - startTime\n\n\t\treturn(errorMeasuresFunction(testForecast, datasets[[i]]$xx, datasets[[i]]$x))\n\t}\n\ttestResults[j,,] &lt;- t(result)\n}\n\n</pre>\n</div></div>\n<h3 id=\"evaluationResults\">Results</h3>\n<p>And here are the results for the smooth functions in v4.4.0 for R. First, we summarise the RMSSEs. I produce quartiles of distribution of RMSSE together with the mean.</p>\n<pre>cbind(t(apply(testResults[,,\"RMSSE\"],1,quantile, na.rm=T)),\n      mean=apply(testResults[,,\"RMSSE\"],1,mean)) |&gt; round(4)\r\n                  0%    25%    50%    75%      100%   mean\nETS           0.0245 0.6772 1.1806 2.3765   51.6160 1.9697\nAuto ARIMA    0.0246 0.6802 1.1790 2.3583   51.6160 1.9864\nADAM ETS Back 0.0183 0.6647 1.1620 2.3023   50.2585 1.9283\nADAM ETS Opt  0.0242 0.6714 1.1868 2.3623   51.6160 1.9432\nADAM ETS Two  0.0246 0.6690 1.1875 2.3374   51.6160 1.9480\nES Back       0.0183 0.6674 1.1647 2.3164   50.2585 1.9292\nES Opt        0.0242 0.6740 1.1858 2.3644   51.6160 1.9469\nES Two        0.0245 0.6717 1.1874 2.3463   51.6160 1.9538\nES XXX        0.0183 0.6777 1.1708 2.3062   50.2585 1.9613\nES ZZZ        0.0108 0.6682 1.1816 2.3611  201.4959 2.0841\nES FFF        0.0145 0.6795 1.2170 2.4575 5946.1858 3.3033\nES SXS        0.0183 0.6754 1.1709 2.3539   50.2585 1.9448\nMSARIMA       0.0278 0.6988 1.1898 2.4208   51.6160 2.0750\nSSARIMA       0.0277 0.7371 1.2544 2.4425   51.6160 2.0625\nCES Back      0.0450 0.6761 1.1741 2.3205   51.0571 1.9650\nGUM Back      0.0333 0.7077 1.2073 2.4533   51.6184 2.0461\n</pre>\n<p>The worst performing models are the ETS with the multiplicative trend (ES ZZZ and ES FFF). This is because there are outliers in some time series, and the multiplicative trend reacts to them by amending the trend value to something large (e.g. 2, i.e. twice increase in level for each step), and then can never return to a reasonable level (see explanation of this phenomenon in <a href=\"https://openforecast.org/adam/ADAMETSMultiplicativeAlternative.html\" rel=\"nofollow\" target=\"_blank\">Section 6.6 of ADAM book</a>). As expected, ADAM ETS does very similar to the ES, and we can see that the default initialisation (backcasting) is pretty good in terms of RMSSE values. To be fair, if the models are tested on a different dataset, it might be the case that the optimal initialisation would do better.</p>\n<p>Here is a table with the SAME results:</p>\n<pre>cbind(t(apply(testResults[,,\"SAME\"],1,quantile, na.rm=T)),\n      mean=apply(testResults[,,\"SAME\"],1,mean)) |&gt; round(4)\r\n                 0%    25%    50%    75%      100%   mean\nETS           8e-04 0.3757 1.0203 2.5097   54.6872 1.9983\nAuto ARIMA    0e+00 0.3992 1.0429 2.4565   53.2710 2.0446\nADAM ETS Back 1e-04 0.3752 0.9965 2.4047   52.3418 1.9518\nADAM ETS Opt  5e-04 0.3733 1.0212 2.4848   55.1018 1.9618\nADAM ETS Two  8e-04 0.3780 1.0316 2.4511   55.1019 1.9712\nES Back       0e+00 0.3733 0.9945 2.4122   53.4504 1.9485\nES Opt        2e-04 0.3727 1.0255 2.4756   54.6860 1.9673\nES Two        1e-04 0.3855 1.0323 2.4535   54.6856 1.9799\nES XXX        1e-04 0.3733 1.0050 2.4257   53.1697 1.9927\nES ZZZ        3e-04 0.3824 1.0135 2.4885  229.7626 2.1376\nES FFF        3e-04 0.3972 1.0489 2.6042 3748.4268 2.9501\nES SXS        6e-04 0.3750 1.0125 2.4627   53.4504 1.9725\nMSARIMA       1e-04 0.3960 1.0094 2.5409   54.7916 2.1227\nSSARIMA       1e-04 0.4401 1.1222 2.5673   52.5023 2.1248\nCES Back      6e-04 0.3767 1.0079 2.4085   54.9026 2.0052\nGUM Back      0e+00 0.3803 1.0575 2.6259   63.0637 2.0858\n</pre>\n<p>In terms of bias, smooth implementations of ETS are doing well again, and we can see the same issue with the multiplicative trend here as before. Another thing to note is that MSARIMA and SSARIMA are not as good as the Auto ARIMA from the forecast package on these datasets in terms of RMSSE and SAME (at least, in terms of mean error measures). And actually, GUM and CES are now better than those in terms of both error measures.</p>\n<p>Finally, here is a table with the computational time:</p>\n<pre>cbind(t(apply(testResults[,,\"Time\"],1,quantile, na.rm=T)),\n      mean=apply(testResults[,,\"Time\"],1,mean)) |&gt; round(4)\r\n                  0%    25%    50%     75%    100%   mean\nETS           0.0032 0.0117 0.1660  0.6728  1.6400 0.3631\nAuto ARIMA    0.0100 0.1184 0.3618  1.0548 54.3652 1.4760\nADAM ETS Back 0.0162 0.1062 0.1854  0.4022  2.5109 0.2950\nADAM ETS Opt  0.0319 0.1920 0.3103  0.6792  3.8933 0.5368\nADAM ETS Two  0.0427 0.2548 0.4035  0.8567  3.7178 0.6331\nES Back       0.0153 0.0896 0.1521  0.3335  2.1128 0.2476\nES Opt        0.0303 0.1667 0.2565  0.5910  3.5887 0.4522\nES Two        0.0483 0.2561 0.4016  0.8626  3.5892 0.6309\nMSARIMA Back  0.0614 0.3418 0.6947  0.9868  3.9677 0.7534\nSSARIMA Back  0.0292 0.2963 0.8988  2.1729 13.7635 1.6581\nCES Back      0.0146 0.0400 0.1834  0.2298  1.2099 0.1713\nGUM Back      0.0165 0.2101 1.5221  3.0543  9.5380 1.9506\n\n# Separate table for special pools of ETS.\n# The time is proportional to the number of models here\n=========================================================\n                  0%    25%    50%     75%    100%   mean\nES XXX        0.0114 0.0539 0.0782  0.1110  0.8163 0.0859\nES ZZZ        0.0147 0.1371 0.2690  0.4947  2.2049 0.3780\nES FFF        0.0529 0.2775 1.1539  1.5926  3.8552 1.1231\nES SXS        0.0323 0.1303 0.4491  0.6013  2.2170 0.4581\n</pre>\n<p><em><br/>\nI have manually moved the specific ES model pools flavours below because there is no point in comparing their computational time with the time of the others (they have different pools of models and thus are not really comparable with the rest).</em></p>\n<p>What we can see from this, is that the ES with backcasting is faster in comparison with the other models in this setting (in terms of mean and median computational time). CES is very fast in terms of mean computational time, which is probably because of the very short pool of models to choose from (only four). SSARIMA is pretty slow, which is due to the nature of its order selection algorithm (I don't plan to update it any time soon, but if someone wants to contribute - let me know). But the interesting thing is that Auto ARIMA, while being relatively fine in terms of median time, has the highest maximum one, meaning that for some time series, it failed for some unknown reason. The series that caused the biggest issue for Auto ARIMA is N389 from the M1 competition. I'm not sure what the issue was, and I don't have time to investigate this.</p>\n<div class=\"wp-caption aligncenter\" id=\"attachment_4002\" style=\"width: 310px\"><a href=\"https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE.png&amp;nocache=1\" rel=\"nofollow\" target=\"_blank\"><img alt=\"Mean computational time vs mean RMSSE\" aria-describedby=\"caption-attachment-4002\" class=\"size-medium wp-image-4002\" data-lazy-sizes=\"(max-width: 300px) 100vw, 300px\" data-lazy-src=\"https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-300x180.png&amp;nocache=1\" decoding=\"async\" fetchpriority=\"high\" height=\"180\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-300x180.png&amp;nocache=1 300w, https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-768x461.png&amp;nocache=1 768w, https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE.png&amp;nocache=1 1000w\" width=\"300\"/><noscript><img alt=\"Mean computational time vs mean RMSSE\" aria-describedby=\"caption-attachment-4002\" class=\"size-medium wp-image-4002\" decoding=\"async\" fetchpriority=\"high\" height=\"180\" loading=\"lazy\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-300x180.png&amp;nocache=1\" srcset_temp=\"https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-300x180.png&amp;nocache=1 300w, https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-768x461.png&amp;nocache=1 768w, https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE.png&amp;nocache=1 1000w\" width=\"300\"/></noscript></a><p class=\"wp-caption-text\" id=\"caption-attachment-4002\">Mean computational time vs mean RMSSE</p></div>\n<p>Comparing the mean computational time with mean RMSSE value (image above), it looks like the overall tendency in the <code>smooth</code> + <code>forecast</code> functions for the M1, M3 and Tourism datasets is that additional computational time does not improve the accuracy. But it also looks like a simpler pool of pure additive models (ETS(X,X,X)) harms the accuracy in comparison with the branch-and-bound based one of the default <code>model=\"ZXZ\"</code>. There seems to be a sweet spot in terms of the pool of models to choose from (no multiplicative trend, allow mixed models). This aligns well with the papers of <a href=\"https://doi.org/10.1080/01605682.2024.2421339\" rel=\"nofollow\" target=\"_blank\">Petropoulos et al. (2025)</a>, who investigated the accuracy of arbitrary short pools of models and <a href=\"https://doi.org/10.1016/j.ijpe.2018.05.019\" rel=\"nofollow\" target=\"_blank\">Kourentzes et al. (2019)</a>, who showed how pooling (if done correctly) can improve the accuracy on average.</p>\n<h3 id=\"whatsNext\">What's next?</h3>\n<p>For R, the main task now is to rewrite the <code>oes()</code> function and substitute it with the <code>om()</code> one - \"Occurrence Model\". This should be equivalent to <code>adam()</code> in functionality, allowing to introduce ETS, ARIMA and explanatory variables for the occurrence part of the model. This is a huge work, which I hope to progress slowly throughout the 2026 and finish by the end of the year. Doing that will also allow me removing the last bits of the old C++ code and switch to the ADAM core completely, introducing more functionality for capturing patterns on intermittent demand. The minor task, is to test the <code>smoother=\"global\"</code> more for the ETS initialisation and roll it out as the default in the next release for both R and Python.</p>\n<p>For Python,... What Python? Ah! You'll see soon üôÇ</p>\n<p>Message <a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/\" rel=\"nofollow\" target=\"_blank\">smooth v4.4.0</a> first appeared on <a href=\"https://openforecast.org/\" rel=\"nofollow\" target=\"_blank\">Open Forecasting</a>.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://openforecast.org/2026/02/09/smooth-v4-4-0/\"> Archives R - Open Forecasting</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 3012,
    "reading_time_min": 15.1,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/ivan-svetunkov/",
        "text": "Ivan Svetunkov"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/",
        "text": "Archives R - Open Forecasting"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/#whatsNew",
        "text": "What‚Äôs new?"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/#evaluation",
        "text": "Evaluation"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/#evaluationSetup",
        "text": "Setup"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/#evaluationResults",
        "text": "Results"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/#whatsNext",
        "text": "What‚Äôs next?"
      },
      {
        "href": "https://openforecast.org/2025/07/04/smooth-v4-3-0-in-r-what-s-new-and-what-s-next/",
        "text": "here"
      },
      {
        "href": "https://dx.doi.org/10.1016/j.ijforecast.2021.11.013%3EM5%20competition%3C/a%3E,%20motivated%20by%20%3Ca%20href=",
        "text": "Athanasopoulos & Kourentzes (2023)"
      },
      {
        "href": "https://openforecast.org/adam/ADAMETSMultiplicativeAlternative.html",
        "text": "Section 6.6 of ADAM book"
      },
      {
        "href": "https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE.png&nocache=1",
        "text": null
      },
      {
        "href": "https://doi.org/10.1080/01605682.2024.2421339",
        "text": "Petropoulos et al. (2025)"
      },
      {
        "href": "https://doi.org/10.1016/j.ijpe.2018.05.019",
        "text": "Kourentzes et al. (2019)"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/",
        "text": "smooth v4.4.0"
      },
      {
        "href": "https://openforecast.org/",
        "text": "Open Forecasting"
      },
      {
        "href": "https://openforecast.org/2026/02/09/smooth-v4-4-0/",
        "text": "Archives R - Open Forecasting"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [
      {
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif",
        "alt": "Mean computational time vs mean RMSSE",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"
      },
      {
        "src": "https://openforecast.org/wp-content/webpc-passthru.php?src=https://openforecast.org/wp-content/uploads/2026/02/smoot-4-4-0-time-vs-RMSSE-300x180.png&nocache=1",
        "alt": "Mean computational time vs mean RMSSE",
        "base64": "data:image/jpeg;base64,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"
      }
    ],
    "lang": "en-US",
    "crawled_at_utc": "2026-02-09T17:43:35Z"
  }
}