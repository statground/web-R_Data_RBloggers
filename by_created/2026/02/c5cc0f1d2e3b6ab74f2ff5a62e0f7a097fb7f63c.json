{
  "id": "c5cc0f1d2e3b6ab74f2ff5a62e0f7a097fb7f63c",
  "url": "https://www.r-bloggers.com/2026/02/quantitative-horse-racing-with-r-calibration-backtesting-and-deployment/",
  "created_at_utc": "2026-02-20T01:16:26Z",
  "crawled_at_utc": "2026-02-20T01:16:26Z",
  "html_title": "Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment | R-bloggers",
  "meta_description": "R DuckDB Parquet Calibration Ranking Bayesian Odds TS Backtesting Racing analytics as an inference-and-decision system Thoroughbred flat racing is not a binary classification problem. It is a multi-competitor outcome process with hierarchy (horse / trainer / jockey / track), time dependence (form cycles, market moves), and decision layers (how you act on probabilities). This macro […] The post Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment appeared first on R Programming Books.",
  "data": {
    "url": "https://www.r-bloggers.com/2026/02/quantitative-horse-racing-with-r-calibration-backtesting-and-deployment/",
    "canonical_url": "https://www.r-bloggers.com/2026/02/quantitative-horse-racing-with-r-calibration-backtesting-and-deployment/",
    "html_title": "Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "R DuckDB Parquet Calibration Ranking Bayesian Odds TS Backtesting Racing analytics as an inference-and-decision system Thoroughbred flat racing is not a binary classification problem. It is a multi-competitor outcome process with hierarchy (horse / trainer / jockey / track), time dependence (form cycles, market moves), and decision layers (how you act on probabilities). This macro […] The post Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment appeared first on R Programming Books.",
    "meta_keywords": null,
    "og_title": "Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment | R-bloggers",
    "og_description": "R DuckDB Parquet Calibration Ranking Bayesian Odds TS Backtesting Racing analytics as an inference-and-decision system Thoroughbred flat racing is not a binary classification problem. It is a multi-competitor outcome process with hierarchy (horse / trainer / jockey / track), time dependence (form cycles, market moves), and decision layers (how you act on probabilities). This macro […] The post Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment appeared first on R Programming Books.",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "twitter_title": "Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment | R-bloggers",
    "twitter_description": "R DuckDB Parquet Calibration Ranking Bayesian Odds TS Backtesting Racing analytics as an inference-and-decision system Thoroughbred flat racing is not a binary classification problem. It is a multi-competitor outcome process with hierarchy (horse / trainer / jockey / track), time dependence (form cycles, market moves), and decision layers (how you act on probabilities). This macro […] The post Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment appeared first on R Programming Books.",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment\nPosted on\nFebruary 19, 2026\nby\nrprogrammingbooks\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nBlog - R Programming Books\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nR\nDuckDB\nParquet\nCalibration\nRanking\nBayesian\nOdds TS\nBacktesting\nRacing analytics as an inference-and-decision system\nThoroughbred flat racing is not a binary classification problem. It is a\nmulti-competitor\noutcome process with\nhierarchy\n(horse / trainer / jockey / track),\ntime dependence\n(form cycles, market moves),\n            and\ndecision layers\n(how you act on probabilities).\nThis macro post is deliberately code-heavy. The goal is to show a canonical, reproducible R\n            workflow you can adapt: from a data contract to modeling to evaluation to deployment.\nDesign principle:\ntreat predictive skill and wagering decisions as separate layers.\nModel layer: calibrated probabilities and uncertainty.\nDecision layer: explicit assumptions, risk controls, stress tests.\nIf you do only one thing, do this: never judge models by ROI alone. Use proper scoring rules\n            and calibration first.\n1) Canonical data contract: race → runner → odds snapshots\nThe fastest way to avoid analytics chaos is to standardize your storage model.\n        A minimal schema lets every chapter, notebook, or experiment start from the same tables.\nDuckDB schema (SQL)\n-- sql/schema.sql\nCREATE TABLE IF NOT EXISTS races (\n  race_id     VARCHAR PRIMARY KEY,\n  race_date   DATE,\n  region      VARCHAR,\n  track       VARCHAR,\n  surface     VARCHAR,\n  going       VARCHAR,\n  distance_m  INTEGER,\n  race_class  VARCHAR,\n  purse       DOUBLE,\n  field_size  INTEGER\n);\n\nCREATE TABLE IF NOT EXISTS runners (\n  race_id        VARCHAR,\n  runner_id      VARCHAR,\n  horse_id       VARCHAR,\n  jockey_id      VARCHAR,\n  trainer_id     VARCHAR,\n  draw           INTEGER,\n  weight_kg      DOUBLE,\n  age            INTEGER,\n  sex            VARCHAR,\n  odds_decimal   DOUBLE,\n  finish_pos     INTEGER,\n  finish_time_s  DOUBLE,\n  win            INTEGER,\n  PRIMARY KEY (race_id, runner_id)\n);\n\nCREATE TABLE IF NOT EXISTS odds_snapshots (\n  race_id      VARCHAR,\n  runner_id    VARCHAR,\n  ts           TIMESTAMP,\n  odds_decimal DOUBLE,\n  traded_vol   DOUBLE,\n  PRIMARY KEY (race_id, runner_id, ts)\n);\nWhy DuckDB?\nIt queries Parquet locally with high performance (predicate pushdown, parallel scans)\n            and avoids running a server database for most use-cases.\nWhy Parquet?\nColumnar storage + compression + fast scans. Perfect for large runner-level tables and\n            time-series snapshots.\n2) Generate a reproducible simulated dataset\nHigh-quality racing data is often license-restricted. A robust approach is to make every lab runnable\n        on a simulator that matches your canonical schema, then provide adapters to ingest real data only\n        when access is lawful.\nRace simulator (R)\n# R/simulate.R\nlibrary(dplyr)\nlibrary(tidyr)\n\nsimulate_races <- function(\n  n_races = 200, n_horses = 600, n_jockeys = 200, n_trainers = 150,\n  min_field = 6, max_field = 14, seed = 1\n) {\n  set.seed(seed)\n\n  horses  <- tibble(horse_id  = sprintf(\"H%04d\", 1:n_horses),\n                    ability   = rnorm(n_horses, 0, 1))\n  jockeys <- tibble(jockey_id = sprintf(\"J%03d\", 1:n_jockeys),\n                    skill     = rnorm(n_jockeys, 0, 0.4))\n  trainers<- tibble(trainer_id= sprintf(\"T%03d\", 1:n_trainers),\n                    skill     = rnorm(n_trainers, 0, 0.5))\n\n  surfaces <- c(\"Turf\",\"Dirt\",\"AllWeather\")\n  goings   <- c(\"Firm\",\"Good\",\"Gd-Fm\",\"Gd-Sft\",\"Soft\",\"Heavy\",\"Standard\",\"Fast\",\"Sloppy\")\n\n  races <- tibble(\n    race_id    = sprintf(\"R%05d\", 1:n_races),\n    race_date  = as.Date(\"2025-01-01\") + sample(0:365, n_races, TRUE),\n    region     = sample(c(\"GB\",\"IE\",\"US\",\"AU\",\"FR\"), n_races, TRUE),\n    track      = sample(c(\"TRACK_A\",\"TRACK_B\",\"TRACK_C\"), n_races, TRUE),\n    surface    = sample(surfaces, n_races, TRUE, prob = c(0.45,0.35,0.20)),\n    going      = sample(goings, n_races, TRUE),\n    distance_m = sample(c(1000,1200,1400,1600,1800,2000,2400,2800), n_races, TRUE),\n    race_class = sample(c(\"G1\",\"G2\",\"G3\",\"HCP\",\"ALW\",\"CLM\"), n_races, TRUE),\n    purse      = round(exp(rnorm(n_races, log(25000), 0.7))),\n    field_size = sample(min_field:max_field, n_races, TRUE)\n  )\n\n  runners <- races |>\n    rowwise() |>\n    do({\n      r <- .\n      k <- r$field_size\n\n      df <- tibble(\n        race_id    = r$race_id,\n        runner_id  = paste0(r$race_id, \"_\", seq_len(k)),\n        horse_id   = sample(horses$horse_id, k, FALSE),\n        jockey_id  = sample(jockeys$jockey_id, k, TRUE),\n        trainer_id = sample(trainers$trainer_id, k, TRUE),\n        draw       = sample(1:k, k, FALSE),\n        weight_kg  = pmax(45, rnorm(k, 55, 3)),\n        age        = sample(2:7, k, TRUE),\n        sex        = sample(c(\"C\",\"F\",\"G\",\"H\"), k, TRUE, prob = c(0.2,0.35,0.35,0.1))\n      ) |>\n        left_join(horses, by=\"horse_id\") |>\n        left_join(jockeys, by=\"jockey_id\", suffix=c(\"\",\"_j\")) |>\n        left_join(trainers, by=\"trainer_id\", suffix=c(\"\",\"_t\")) |>\n        mutate(\n          draw_effect    = ifelse(k >= 12, (k + 1 - draw)/k, 0),\n          surface_effect = case_when(r$surface==\"Turf\" ~ 0.15,\n                                     r$surface==\"AllWeather\" ~ 0.05,\n                                     TRUE ~ 0),\n          weight_penalty = -0.03*(weight_kg - 55),\n          utility        = ability + skill + skill_t + 0.12*draw_effect +\n                           surface_effect + weight_penalty + rnorm(k, 0, 0.25)\n        )\n\n      # Sequential proportional-to-exp(utility) finish ordering\n      remaining <- seq_len(k)\n      w <- exp(df$utility - max(df$utility))\n      order_idx <- integer(0)\n      for (pos in seq_len(k)) {\n        probs <- w[remaining]/sum(w[remaining])\n        pick <- sample(remaining, 1, prob = probs)\n        order_idx <- c(order_idx, pick)\n        remaining <- setdiff(remaining, pick)\n      }\n      df$finish_pos <- match(seq_len(k), order_idx)\n      df$win <- as.integer(df$finish_pos == 1)\n\n      base_time <- r$distance_m/16\n      df$finish_time_s <- base_time - 0.8*df$ability + rnorm(k, 0, 1.2)\n\n      # Market-like odds with an overround\n      p <- exp(df$utility - max(df$utility)); p <- p/sum(p)\n      overround <- 1.18\n      df$odds_decimal <- pmin(200, pmax(1.01, 1/(overround*p) * exp(rnorm(k, 0, 0.06))))\n\n      df |>\n        select(race_id, runner_id, horse_id, jockey_id, trainer_id,\n               draw, weight_kg, age, sex, odds_decimal,\n               finish_pos, finish_time_s, win)\n    }) |>\n    ungroup()\n\n  list(races = races, runners = runners)\n}\n\nsim <- simulate_races(seed = 42)\nQuick EDA sanity checks\nlibrary(ggplot2)\n\nggplot(sim$races, aes(field_size)) +\n  geom_histogram(bins = 12) +\n  labs(x = \"Runners per race\", y = \"Count\")\n\nggplot(sim$runners, aes(odds_decimal)) +\n  geom_histogram(bins = 50) +\n  scale_x_log10() +\n  labs(x = \"Decimal odds (log scale)\", y = \"Count\")\n3) Persist to Parquet and query with DuckDB\nA clean pattern is: write Parquet (Arrow) → create tables in DuckDB from\nread_parquet()\n.\n        This yields a fast, local warehouse without heavyweight infrastructure.\nlibrary(arrow)\nlibrary(DBI)\nlibrary(duckdb)\n\ndir.create(\"data/derived\", recursive = TRUE, showWarnings = FALSE)\nwrite_parquet(sim$races,   \"data/derived/races.parquet\")\nwrite_parquet(sim$runners, \"data/derived/runners.parquet\")\n\ncon <- dbConnect(duckdb(), \"data/warehouse/racing.duckdb\")\ndbExecute(con, \"INSTALL parquet; LOAD parquet;\")\n\ndbExecute(con, \"\n  CREATE OR REPLACE TABLE races AS\n  SELECT * FROM read_parquet('data/derived/races.parquet');\n\")\n\ndbExecute(con, \"\n  CREATE OR REPLACE TABLE runners AS\n  SELECT * FROM read_parquet('data/derived/runners.parquet');\n\")\n\ndbGetQuery(con, \"SELECT COUNT(*) AS n_races FROM races;\")\ndbGetQuery(con, \"SELECT COUNT(*) AS n_runners FROM runners;\")\n\ndbDisconnect(con, shutdown = TRUE)\nQuery patterns you will use constantly\n-- Average odds and win rate by surface\nSELECT\n  surface,\n  AVG(odds_decimal) AS avg_odds,\n  AVG(win)          AS win_rate,\n  COUNT(*)          AS n\nFROM runners r\nJOIN races   x USING (race_id)\nGROUP BY surface\nORDER BY n DESC;\n\n-- Within-race normalization of implied probability\nWITH base AS (\n  SELECT\n    race_id,\n    runner_id,\n    odds_decimal,\n    1.0 / odds_decimal AS implied_raw\n  FROM runners\n)\nSELECT\n  race_id,\n  runner_id,\n  implied_raw / SUM(implied_raw) OVER (PARTITION BY race_id) AS implied_p_norm\nFROM base;\n4) Baselines: probabilistic win modeling + calibration\nIn racing analytics, your model output is typically a probability. That means evaluation should\n        prioritize\nproper scoring rules\nand\ncalibration\n, not just AUC.\nFeature store (leakage-safe)\nlibrary(dplyr)\n\nfeatures <- sim$runners |>\n  left_join(sim$races, by = \"race_id\") |>\n  transmute(\n    race_id, runner_id,\n    win,\n    # pre-race covariates only\n    draw, weight_kg, age, sex,\n    surface, going, distance_m, race_class, purse,\n    odds_decimal,\n    log_odds = log(odds_decimal),\n    implied_p_raw = 1/odds_decimal\n  )\n\nstopifnot(all(is.finite(features$log_odds)))\nstopifnot(all(features$odds_decimal >= 1.01))\ntidymodels GLM baseline + log loss / Brier / AUC\nlibrary(tidymodels)\n\nset.seed(123)\n\ndf <- features |>\n  group_by(race_id) |>\n  mutate(implied_p = implied_p_raw / sum(implied_p_raw)) |>\n  ungroup() |>\n  mutate(win = factor(win, levels = c(0,1), labels = c(\"no\",\"yes\")))\n\nspl  <- initial_split(df, strata = win)\ntrain<- training(spl)\ntest <- testing(spl)\n\nrec <- recipe(win ~ draw + weight_kg + age + sex + surface + going +\n                distance_m + odds_decimal, data = train) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_zv(all_predictors()) |>\n  step_normalize(all_numeric_predictors())\n\nmod <- logistic_reg() |> set_engine(\"glm\")\nwf  <- workflow() |> add_recipe(rec) |> add_model(mod)\n\nfit  <- fit(wf, train)\npred <- predict(fit, test, type = \"prob\") |>\n  bind_cols(test |> select(win))\n\nmetric_set(mn_log_loss, brier_class, roc_auc)(pred, truth = win, .pred_yes)\nCalibration plot\nlibrary(probably)\n\n# Binned calibration plot\ncal_plot_breaks(pred, truth = win, estimate = .pred_yes, num_breaks = 10)\nCalibration mindset:\na model can be “accurate” in ranking yet systematically\n        overconfident. In betting or pricing contexts, miscalibration is usually more dangerous than\n        a small drop in AUC.\n5) Multi-runner outcome modeling: rankings, not just winners\nBinary win models throw away a lot of structure. Racing naturally produces a ranking:\n        finish positions across multiple runners. Ranking models let you target that structure directly.\nPlackett–Luce (direct ranking likelihood)\nlibrary(PlackettLuce)\n\nset.seed(99)\nitems  <- paste0(\"H\", 1:12)\nn_races <- 40\nability <- rnorm(length(items)); names(ability) <- items\n\nRmat <- matrix(0, nrow = n_races, ncol = length(items))\ncolnames(Rmat) <- items\n\nfor (r in 1:n_races) {\n  field <- sample(items, size = sample(6:10, 1), replace = FALSE)\n  util  <- ability[field] + rnorm(length(field), 0, 0.3)\n  ord   <- field[order(util, decreasing = TRUE)]\n  Rmat[r, ord] <- seq_along(ord) # 1 = best; 0 = absent\n}\n\nrankings <- as.rankings(Rmat)\npl <- PlackettLuce(rankings)\n\nsummary(pl)\ncoef(pl)  # worth parameters\nBradley–Terry via pairwise decomposition\nlibrary(dplyr)\nlibrary(BradleyTerry2)\n\npairwise <- tibble()\nfor (r in 1:n_races) {\n  in_race <- names(which(Rmat[r, ] > 0))\n  ranks   <- Rmat[r, in_race]\n  for (i in 1:(length(in_race) - 1)) {\n    for (j in (i + 1):length(in_race)) {\n      h1 <- in_race[i]; h2 <- in_race[j]\n      win1 <- as.integer(ranks[h1] < ranks[h2])\n      pairwise <- bind_rows(pairwise, tibble(h1=h1, h2=h2, win1=win1, win2=1-win1))\n    }\n  }\n}\n\nagg <- pairwise |>\n  group_by(h1, h2) |>\n  summarise(win1 = sum(win1), win2 = sum(win2), .groups = \"drop\")\n\nbt <- BTm(cbind(win1, win2), h1, h2, data = agg, id = \"horse\")\nBTabilities(bt)\nInterpretation\nBoth families estimate “ability-like” latent parameters, but they do it with different data representations:\n            full rankings vs aggregated pairwise outcomes.\nWhen rankings matter\nIf you care about place markets, exactas/quinellas, or modeling finish times/positions beyond “win,”\n            ranking-aware methods often align better with the objective.\n6) Hierarchy: GLMMs and Bayesian multilevel models\nRacing is fundamentally hierarchical: repeated horses, trainers, jockeys, tracks, regions.\n        Fixed effects can overfit; multilevel models provide shrinkage and better uncertainty accounting.\nGLMM with trainer/jockey random effects (lme4)\nlibrary(lme4)\nlibrary(dplyr)\n\nsim2 <- simulate_races(n_races=500, n_horses=800, n_jockeys=120, n_trainers=100, seed=202)\ndf2  <- sim2$runners |> left_join(sim2$races, by=\"race_id\")\n\nm_glmm <- glmer(\n  win ~ scale(weight_kg) + scale(draw) + surface + going +\n    (1 | trainer_id) + (1 | jockey_id),\n  data   = df2,\n  family = binomial()\n)\n\nsummary(m_glmm)\nBayesian multilevel model (brms)\nlibrary(brms)\n\npriors <- c(\n  prior(normal(0, 1), class = \"b\"),\n  prior(exponential(1), class = \"sd\")\n)\n\nfit_bayes <- brm(\n  win ~ scale(weight_kg) + scale(draw) + surface + going +\n    (1 | trainer_id) + (1 | jockey_id),\n  data   = df2,\n  family = bernoulli(),\n  prior  = priors,\n  chains = 2, iter = 1000, cores = 2, seed = 202\n)\n\nsummary(fit_bayes)\npp_check(fit_bayes)\nWhy Bayesian here?\nYou get a principled way to encode priors (e.g., smaller effects are more likely than huge ones),\n        propagate uncertainty through derived probabilities, and do posterior predictive checks.\n7) Time-to-event: survival and frailty modeling\nSome questions in racing are naturally time-to-event: time to first win, time to peak performance,\n        time to retirement, time between races. Survival analysis gives you the right likelihood and handles censoring.\nCox proportional hazards\nlibrary(dplyr)\nlibrary(survival)\n\nset.seed(1)\nn <- 1000\n\nhorses <- tibble(\n  horse_id  = sprintf(\"H%04d\", 1:n),\n  talent    = rnorm(n),\n  trainer_id= sample(sprintf(\"T%03d\", 1:80), n, TRUE),\n  debut_age = pmax(2, rnorm(n, 3.0, 0.6))\n)\n\nbase_rate <- 0.002\nlinpred   <- 0.65 * horses$talent - 0.25 * (horses$debut_age - 3)\nrate      <- base_rate * exp(linpred)\n\ntime_days   <- rexp(n, rate = rate)\ncensor_days <- runif(n, 60, 700)\n\nstatus   <- as.integer(time_days  mutate(time_days = time_obs, status = status)\n\ncox <- coxph(Surv(time_days, status) ~ scale(talent) + scale(debut_age), data = df_surv)\nsummary(cox)\ncox.zph(cox)\nFrailty term (trainer-level heterogeneity)\ncox_f <- coxph(\n  Surv(time_days, status) ~ scale(talent) + scale(debut_age) + frailty(trainer_id),\n  data = df_surv\n)\n\nsummary(cox_f)\n8) Modern ML: XGBoost in tidymodels and mlr3\nGradient boosting can capture non-linearities and interactions without manual feature engineering.\n        But if you don’t evaluate probabilistically (log loss, calibration), you can accidentally ship a model\n        that “ranks well” yet prices poorly.\ntidymodels: tuned XGBoost\nlibrary(tidymodels)\nlibrary(dplyr)\n\nset.seed(314)\nsim3 <- simulate_races(n_races = 500, n_horses = 1200, seed = 314)\n\ndf3 <- sim3$runners |>\n  left_join(sim3$races, by=\"race_id\") |>\n  mutate(win = factor(win, levels=c(0,1), labels=c(\"no\",\"yes\")))\n\nspl   <- initial_split(df3, strata = win)\ntrain <- training(spl)\ntest  <- testing(spl)\n\nrec <- recipe(win ~ draw + weight_kg + age + sex + surface + going +\n               distance_m + odds_decimal, data=train) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_zv(all_predictors())\n\nfolds <- vfold_cv(train, v = 5, strata = win)\n\nxgb_spec <- boost_tree(\n  trees = 800,\n  tree_depth     = tune(),\n  learn_rate     = tune(),\n  loss_reduction = tune(),\n  sample_size    = tune(),\n  mtry           = tune()\n) |>\n  set_engine(\"xgboost\") |>\n  set_mode(\"classification\")\n\nwf <- workflow() |> add_recipe(rec) |> add_model(xgb_spec)\n\ngrid <- grid_space_filling(\n  tree_depth(),\n  learn_rate(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  mtry(range = c(5, 60)),\n  size = 20\n)\n\nres <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(mn_log_loss, roc_auc)\n)\n\nbest <- select_best(res, \"mn_log_loss\")\nfinal_fit <- finalize_workflow(wf, best) |> fit(train)\n\npred <- predict(final_fit, test, type=\"prob\") |> bind_cols(test |> select(win))\nmetric_set(mn_log_loss, brier_class, roc_auc)(pred, truth = win, .pred_yes)\nmlr3: comparable workflow\nlibrary(data.table)\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nset.seed(99)\nsim4 <- simulate_races(n_races = 400, n_horses = 900, seed = 99)\n\ndf4 <- as.data.table(merge(sim4$runners, sim4$races, by = \"race_id\"))\ndf4[, win := factor(ifelse(win == 1, \"yes\", \"no\"))]\n\ntask <- as_task_classif(df4, target = \"win\", id = \"win_task\")\ntask$set_col_roles(\n  c(\"race_id\",\"runner_id\",\"horse_id\",\"jockey_id\",\"trainer_id\"),\n  roles = \"name\"\n)\n\nrsmp <- rsmp(\"cv\", folds = 5)\n\nlrn_glm <- lrn(\"classif.log_reg\", predict_type = \"prob\")\nrr_glm  <- resample(task, lrn_glm, rsmp)\nrr_glm$aggregate(msr(\"classif.logloss\"))\nrr_glm$aggregate(msr(\"classif.auc\"))\n\nlrn_xgb <- lrn(\"classif.xgboost\", predict_type = \"prob\",\n               nrounds = 300, eta = 0.05, max_depth = 4)\nrr_xgb <- resample(task, lrn_xgb, rsmp)\nrr_xgb$aggregate(msr(\"classif.logloss\"))\nrr_xgb$aggregate(msr(\"classif.auc\"))\n9) Odds time series: from snapshots to features\nExchange markets produce time-stamped odds and volumes. Treat them as time series and engineer features\n        like drift, volatility, late steam, and liquidity. Keep it honest: market microstructure is noisy,\n        and backtests require strong assumptions.\nSimulated snapshots as a tsibble + feature extraction\nlibrary(dplyr)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(ggplot2)\n\nset.seed(1)\nsnap <- tibble(\n  runner_id = \"R00001_01\",\n  ts = as.POSIXct(\"2025-06-01 14:00:00\", tz = \"UTC\") + seq(0, 60*60, by = 60),\n  odds_decimal = pmax(1.01, 6 + cumsum(rnorm(61, 0, 0.05))),\n  traded_vol = cumsum(pmax(0, rnorm(61, 10, 3)))\n)\n\ntsbl <- snap |> as_tsibble(index = ts, key = runner_id)\n\nfeat <- tsbl |>\n  features(odds_decimal, list(\n    mean  = ~ mean(.),\n    sd    = ~ sd(.),\n    min   = ~ min(.),\n    max   = ~ max(.),\n    slope = ~ coef(lm(. ~ seq_along(.)))[2]\n  ))\n\nfeat\n\nggplot(tsbl, aes(ts, odds_decimal)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Decimal odds\")\nSchema normalization: snapshots table\n# Example transformation skeleton for real exchange data:\n# 1) parse raw feed messages\n# 2) map to normalized columns: race_id, runner_id, ts, odds_decimal, traded_vol\n# 3) write Parquet and load into DuckDB\n\n# write_parquet(odds_snapshots, \"data/derived/odds_snapshots.parquet\")\n# dbExecute(con, \"CREATE OR REPLACE TABLE odds_snapshots AS\n#                 SELECT * FROM read_parquet('data/derived/odds_snapshots.parquet');\")\n10) Backtesting as a decision layer (fractional Kelly + guardrails)\nOnce you have calibrated probabilities, you can add a decision policy.\n        One common framework is fractional Kelly sizing, but it must be capped and stress tested.\n        The same probabilities can produce very different outcomes depending on execution constraints,\n        market impact, limits, and selection filters.\nSimulated fractional Kelly example with risk caps\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsim5 <- simulate_races(n_races = 180, n_horses = 500, seed = 9)\nrunners <- sim5$runners |>\n  group_by(race_id) |>\n  mutate(\n    p_market = (1/odds_decimal) / sum(1/odds_decimal),\n    # toy \"model\": market + noise (for demonstration)\n    p_model  = pmin(0.99, pmax(0.001, p_market + rnorm(n(), 0, 0.02))),\n    edge     = p_model - p_market\n  ) |>\n  ungroup()\n\nfractional_kelly <- function(p, odds_dec, fraction = 0.25, cap = 0.05) {\n  b <- odds_dec - 1\n  f_star <- (b*p - (1 - p)) / b\n  f <- pmax(0, f_star) * fraction\n  pmin(f, cap)\n}\n\nbankroll <- 1\nmax_drawdown <- 0.25\npeak <- bankroll\nhist <- tibble()\n\nfor (rid in unique(runners$race_id)) {\n  if (bankroll < (1 - max_drawdown) * peak) break  # guardrail: stop\n\n  race <- runners |> filter(race_id == rid)\n  pick <- race |> arrange(desc(edge)) |> slice(1)\n\n  if (pick$edge > 0.01 && pick$odds_decimal  mutate(drawdown = 1 - bankroll/peak)\n\nggplot(hist, aes(seq_along(bankroll), bankroll)) + geom_line() +\n  labs(x = \"Race index\", y = \"Bankroll\")\n\nggplot(hist, aes(seq_along(drawdown), drawdown)) + geom_line() +\n  labs(x = \"Race index\", y = \"Drawdown\")\nPractical warning:\nthis example is intentionally simplified. Real backtests require:\n        commissions, slippage, bet limits, late price changes, liquidity constraints, and selection bias checks.\n11) Deployment: versioned models + HTTP prediction API\nIf you want your work to be reusable, build a deployable artifact. A clean R-native pattern is:\npins\nfor versioning +\nvetiver\nfor model packaging +\nplumber\nfor the API server.\nlibrary(tidymodels)\nlibrary(pins)\nlibrary(vetiver)\nlibrary(plumber)\nlibrary(dplyr)\n\nsim6 <- simulate_races(n_races = 250, n_horses = 700, seed = 101)\n\ndf6 <- sim6$runners |>\n  left_join(sim6$races, by=\"race_id\") |>\n  mutate(win = factor(win, levels=c(0,1), labels=c(\"no\",\"yes\")))\n\nrec <- recipe(win ~ draw + weight_kg + age + sex + surface + going +\n                distance_m + odds_decimal, data = df6) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_zv(all_predictors())\n\nfit <- workflow() |>\n  add_recipe(rec) |>\n  add_model(logistic_reg() |> set_engine(\"glm\")) |>\n  fit(df6)\n\nboard <- board_folder(\"pins\", versioned = TRUE)\nv <- vetiver_model(fit, model_name = \"win_prob_glm\")\n\nvetiver_pin_write(board, v)\n\napi <- pr() |> vetiver_api(v)\n\n# Run locally:\n# api |> pr_run(host = \"0.0.0.0\", port = 8080)\n\napi\nMinimal request payload shape (JSON)\n{\n  \"draw\": 7,\n  \"weight_kg\": 55.5,\n  \"age\": 4,\n  \"sex\": \"G\",\n  \"surface\": \"Turf\",\n  \"going\": \"Good\",\n  \"distance_m\": 1600,\n  \"odds_decimal\": 6.2\n}\n12) Reproducibility mechanics: renv + deterministic builds\nReproducibility is not vibes. Lock dependencies, record versions, and standardize builds.\n        This is especially important when your analytics becomes a book, a report, or a product artifact.\nrenv workflow\n# Initialize (once)\n# renv::init()\n\n# Snapshot current library into renv.lock\nrenv::snapshot()\n\n# Restore exact package versions from renv.lock\nrenv::restore()\nDockerfile for deterministic PDF builds (Quarto)\nFROM rocker/verse:4.4.2\nWORKDIR /book\n\nCOPY renv.lock renv.lock\nCOPY renv/ renv/\nCOPY .Rprofile .Rprofile\n\nRUN R -q -e \"install.packages('renv'); renv::restore(prompt = FALSE)\"\n\nCOPY . .\nCMD [\"quarto\", \"render\", \"book\", \"--to\", \"pdf\"]\nCI skeleton (GitHub Actions)\nname: Render PDF\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  pdf:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: r-lib/actions/setup-r@v2\n      - uses: r-lib/actions/setup-renv@v2\n      - name: Render Quarto book\n        run: quarto render book --to pdf\n13) An “engineering checklist” you can actually follow\nData contract\nCanonical schema\nParquet + DuckDB\nAdapters, not redistribution\nModeling\nGLM/GLMM baselines\nRanking likelihoods\nBayesian uncertainty\nEvaluation\nLog loss / Brier\nCalibration curves\nStress-tested backtests\nTime dependence\nOdds snapshots\nDrift/volatility features\nLiquidity constraints\nDecision layer\nExplicit assumptions\nFractional Kelly caps\nDrawdown guardrails\nDeployment\npins versioning\nvetiver packaging\nplumber API\nOne more code snippet: within-race probability normalization\nMany racing tables store “odds” per runner, but for inference you often want a normalized probability\n        within a field. Here’s a standard pattern that also highlights overround issues.\nlibrary(dplyr)\n\nnormalize_field_probs <- function(df, odds_col = odds_decimal) {\n  df |>\n    group_by(race_id) |>\n    mutate(\n      implied_raw = 1 / {{ odds_col }},\n      overround   = sum(implied_raw),\n      p_market    = implied_raw / overround\n    ) |>\n    ungroup()\n}\n\ndf_norm <- normalize_field_probs(sim$runners)\n\ndf_norm |>\n  summarise(\n    avg_overround = mean(overround),\n    pmin_overround= min(overround),\n    pmax_overround= max(overround)\n  )\nInterpretation:\noverround > 1 implies the market’s implied probabilities sum above 1\n        (built-in margin). Separating “market implied probability” from “model probability” is essential if\n        you want honest evaluation.\nIf you’re building a full, reproducible workflow (data contract → modeling → evaluation → deployment),\n        you may want a more structured, end-to-end blueprint to keep everything consistent.\nYou can find a concise reference here:\nracing-analytics-r\n.\nTip: keep your repository “clean-room” by shipping simulated data + adapters, and never bundling restricted datasets.\n        That one decision makes your work easier to share, review, and publish.\nThe post\nQuantitative Horse Racing with R: Calibration, Backtesting, and Deployment\nappeared first on\nR Programming Books\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nBlog - R Programming Books\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-399152 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 19, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/rprogrammingbooks/\">rprogrammingbooks</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://rprogrammingbooks.com/quantitative-horse-racing-r-calibration-backtesting-deployment/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=quantitative-horse-racing-r-calibration-backtesting-deployment\"> Blog - R Programming Books</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<!DOCTYPE html>\n\n<meta charset=\"utf-8\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<meta content=\"A deep, code-heavy macro post on racing analytics in R: data contracts, DuckDB+Parquet, probabilistic prediction, calibration, ranking models, Bayesian multilevel modeling, survival/frailty, odds time series, backtesting, and deployment.\" name=\"description\"/>\n<meta content=\"index,follow\" name=\"robots\"/>\n\n<main class=\"wrap\">\n<section class=\"card\">\n<div class=\"grid cols2\">\n<div>\n<div class=\"pill\">R</div>\n<div class=\"pill\">DuckDB</div>\n<div class=\"pill\">Parquet</div>\n<div class=\"pill\">Calibration</div>\n<div class=\"pill\">Ranking</div>\n<div class=\"pill\">Bayesian</div>\n<div class=\"pill\">Odds TS</div>\n<div class=\"pill\">Backtesting</div>\n<h2>Racing analytics as an inference-and-decision system</h2>\n<p class=\"muted\">\n            Thoroughbred flat racing is not a binary classification problem. It is a\n            <strong>multi-competitor</strong> outcome process with\n            <strong>hierarchy</strong> (horse / trainer / jockey / track),\n            <strong>time dependence</strong> (form cycles, market moves),\n            and <strong>decision layers</strong> (how you act on probabilities).\n          </p>\n<p class=\"muted\">\n            This macro post is deliberately code-heavy. The goal is to show a canonical, reproducible R\n            workflow you can adapt: from a data contract to modeling to evaluation to deployment.\n          </p>\n</div>\n<div class=\"note\">\n<strong>Design principle:</strong> treat predictive skill and wagering decisions as separate layers.\n          <ul>\n<li>Model layer: calibrated probabilities and uncertainty.</li>\n<li>Decision layer: explicit assumptions, risk controls, stress tests.</li>\n</ul>\n<p class=\"small muted\">\n            If you do only one thing, do this: never judge models by ROI alone. Use proper scoring rules\n            and calibration first.\n          </p>\n</div>\n</div>\n</section>\n<section class=\"card\">\n<h3>1) Canonical data contract: race → runner → odds snapshots</h3>\n<p class=\"muted\">\n        The fastest way to avoid analytics chaos is to standardize your storage model.\n        A minimal schema lets every chapter, notebook, or experiment start from the same tables.\n      </p>\n<h4>DuckDB schema (SQL)</h4>\n<pre>-- sql/schema.sql\nCREATE TABLE IF NOT EXISTS races (\n  race_id     VARCHAR PRIMARY KEY,\n  race_date   DATE,\n  region      VARCHAR,\n  track       VARCHAR,\n  surface     VARCHAR,\n  going       VARCHAR,\n  distance_m  INTEGER,\n  race_class  VARCHAR,\n  purse       DOUBLE,\n  field_size  INTEGER\n);\n\nCREATE TABLE IF NOT EXISTS runners (\n  race_id        VARCHAR,\n  runner_id      VARCHAR,\n  horse_id       VARCHAR,\n  jockey_id      VARCHAR,\n  trainer_id     VARCHAR,\n  draw           INTEGER,\n  weight_kg      DOUBLE,\n  age            INTEGER,\n  sex            VARCHAR,\n  odds_decimal   DOUBLE,\n  finish_pos     INTEGER,\n  finish_time_s  DOUBLE,\n  win            INTEGER,\n  PRIMARY KEY (race_id, runner_id)\n);\n\nCREATE TABLE IF NOT EXISTS odds_snapshots (\n  race_id      VARCHAR,\n  runner_id    VARCHAR,\n  ts           TIMESTAMP,\n  odds_decimal DOUBLE,\n  traded_vol   DOUBLE,\n  PRIMARY KEY (race_id, runner_id, ts)\n);</pre>\n<div class=\"grid cols2\">\n<div class=\"kpi\">\n<strong>Why DuckDB?</strong>\n<div class=\"muted\">\n            It queries Parquet locally with high performance (predicate pushdown, parallel scans)\n            and avoids running a server database for most use-cases.\n          </div>\n</div>\n<div class=\"kpi\">\n<strong>Why Parquet?</strong>\n<div class=\"muted\">\n            Columnar storage + compression + fast scans. Perfect for large runner-level tables and\n            time-series snapshots.\n          </div>\n</div>\n</div>\n</section>\n<section class=\"card\">\n<h3>2) Generate a reproducible simulated dataset</h3>\n<p class=\"muted\">\n        High-quality racing data is often license-restricted. A robust approach is to make every lab runnable\n        on a simulator that matches your canonical schema, then provide adapters to ingest real data only\n        when access is lawful.\n      </p>\n<h4>Race simulator (R)</h4>\n<pre># R/simulate.R\nlibrary(dplyr)\nlibrary(tidyr)\n\nsimulate_races &lt;- function(\n  n_races = 200, n_horses = 600, n_jockeys = 200, n_trainers = 150,\n  min_field = 6, max_field = 14, seed = 1\n) {\n  set.seed(seed)\n\n  horses  &lt;- tibble(horse_id  = sprintf(\"H%04d\", 1:n_horses),\n                    ability   = rnorm(n_horses, 0, 1))\n  jockeys &lt;- tibble(jockey_id = sprintf(\"J%03d\", 1:n_jockeys),\n                    skill     = rnorm(n_jockeys, 0, 0.4))\n  trainers&lt;- tibble(trainer_id= sprintf(\"T%03d\", 1:n_trainers),\n                    skill     = rnorm(n_trainers, 0, 0.5))\n\n  surfaces &lt;- c(\"Turf\",\"Dirt\",\"AllWeather\")\n  goings   &lt;- c(\"Firm\",\"Good\",\"Gd-Fm\",\"Gd-Sft\",\"Soft\",\"Heavy\",\"Standard\",\"Fast\",\"Sloppy\")\n\n  races &lt;- tibble(\n    race_id    = sprintf(\"R%05d\", 1:n_races),\n    race_date  = as.Date(\"2025-01-01\") + sample(0:365, n_races, TRUE),\n    region     = sample(c(\"GB\",\"IE\",\"US\",\"AU\",\"FR\"), n_races, TRUE),\n    track      = sample(c(\"TRACK_A\",\"TRACK_B\",\"TRACK_C\"), n_races, TRUE),\n    surface    = sample(surfaces, n_races, TRUE, prob = c(0.45,0.35,0.20)),\n    going      = sample(goings, n_races, TRUE),\n    distance_m = sample(c(1000,1200,1400,1600,1800,2000,2400,2800), n_races, TRUE),\n    race_class = sample(c(\"G1\",\"G2\",\"G3\",\"HCP\",\"ALW\",\"CLM\"), n_races, TRUE),\n    purse      = round(exp(rnorm(n_races, log(25000), 0.7))),\n    field_size = sample(min_field:max_field, n_races, TRUE)\n  )\n\n  runners &lt;- races |&gt;\n    rowwise() |&gt;\n    do({\n      r &lt;- .\n      k &lt;- r$field_size\n\n      df &lt;- tibble(\n        race_id    = r$race_id,\n        runner_id  = paste0(r$race_id, \"_\", seq_len(k)),\n        horse_id   = sample(horses$horse_id, k, FALSE),\n        jockey_id  = sample(jockeys$jockey_id, k, TRUE),\n        trainer_id = sample(trainers$trainer_id, k, TRUE),\n        draw       = sample(1:k, k, FALSE),\n        weight_kg  = pmax(45, rnorm(k, 55, 3)),\n        age        = sample(2:7, k, TRUE),\n        sex        = sample(c(\"C\",\"F\",\"G\",\"H\"), k, TRUE, prob = c(0.2,0.35,0.35,0.1))\n      ) |&gt;\n        left_join(horses, by=\"horse_id\") |&gt;\n        left_join(jockeys, by=\"jockey_id\", suffix=c(\"\",\"_j\")) |&gt;\n        left_join(trainers, by=\"trainer_id\", suffix=c(\"\",\"_t\")) |&gt;\n        mutate(\n          draw_effect    = ifelse(k &gt;= 12, (k + 1 - draw)/k, 0),\n          surface_effect = case_when(r$surface==\"Turf\" ~ 0.15,\n                                     r$surface==\"AllWeather\" ~ 0.05,\n                                     TRUE ~ 0),\n          weight_penalty = -0.03*(weight_kg - 55),\n          utility        = ability + skill + skill_t + 0.12*draw_effect +\n                           surface_effect + weight_penalty + rnorm(k, 0, 0.25)\n        )\n\n      # Sequential proportional-to-exp(utility) finish ordering\n      remaining &lt;- seq_len(k)\n      w &lt;- exp(df$utility - max(df$utility))\n      order_idx &lt;- integer(0)\n      for (pos in seq_len(k)) {\n        probs &lt;- w[remaining]/sum(w[remaining])\n        pick &lt;- sample(remaining, 1, prob = probs)\n        order_idx &lt;- c(order_idx, pick)\n        remaining &lt;- setdiff(remaining, pick)\n      }\n      df$finish_pos &lt;- match(seq_len(k), order_idx)\n      df$win &lt;- as.integer(df$finish_pos == 1)\n\n      base_time &lt;- r$distance_m/16\n      df$finish_time_s &lt;- base_time - 0.8*df$ability + rnorm(k, 0, 1.2)\n\n      # Market-like odds with an overround\n      p &lt;- exp(df$utility - max(df$utility)); p &lt;- p/sum(p)\n      overround &lt;- 1.18\n      df$odds_decimal &lt;- pmin(200, pmax(1.01, 1/(overround*p) * exp(rnorm(k, 0, 0.06))))\n\n      df |&gt;\n        select(race_id, runner_id, horse_id, jockey_id, trainer_id,\n               draw, weight_kg, age, sex, odds_decimal,\n               finish_pos, finish_time_s, win)\n    }) |&gt;\n    ungroup()\n\n  list(races = races, runners = runners)\n}\n\nsim &lt;- simulate_races(seed = 42)</pre>\n<h4>Quick EDA sanity checks</h4>\n<pre>library(ggplot2)\n\nggplot(sim$races, aes(field_size)) +\n  geom_histogram(bins = 12) +\n  labs(x = \"Runners per race\", y = \"Count\")\n\nggplot(sim$runners, aes(odds_decimal)) +\n  geom_histogram(bins = 50) +\n  scale_x_log10() +\n  labs(x = \"Decimal odds (log scale)\", y = \"Count\")</pre>\n</section>\n<section class=\"card\">\n<h3>3) Persist to Parquet and query with DuckDB</h3>\n<p class=\"muted\">\n        A clean pattern is: write Parquet (Arrow) → create tables in DuckDB from <code>read_parquet()</code>.\n        This yields a fast, local warehouse without heavyweight infrastructure.\n      </p>\n<pre>library(arrow)\nlibrary(DBI)\nlibrary(duckdb)\n\ndir.create(\"data/derived\", recursive = TRUE, showWarnings = FALSE)\nwrite_parquet(sim$races,   \"data/derived/races.parquet\")\nwrite_parquet(sim$runners, \"data/derived/runners.parquet\")\n\ncon &lt;- dbConnect(duckdb(), \"data/warehouse/racing.duckdb\")\ndbExecute(con, \"INSTALL parquet; LOAD parquet;\")\n\ndbExecute(con, \"\n  CREATE OR REPLACE TABLE races AS\n  SELECT * FROM read_parquet('data/derived/races.parquet');\n\")\n\ndbExecute(con, \"\n  CREATE OR REPLACE TABLE runners AS\n  SELECT * FROM read_parquet('data/derived/runners.parquet');\n\")\n\ndbGetQuery(con, \"SELECT COUNT(*) AS n_races FROM races;\")\ndbGetQuery(con, \"SELECT COUNT(*) AS n_runners FROM runners;\")\n\ndbDisconnect(con, shutdown = TRUE)</pre>\n<h4>Query patterns you will use constantly</h4>\n<pre>-- Average odds and win rate by surface\nSELECT\n  surface,\n  AVG(odds_decimal) AS avg_odds,\n  AVG(win)          AS win_rate,\n  COUNT(*)          AS n\nFROM runners r\nJOIN races   x USING (race_id)\nGROUP BY surface\nORDER BY n DESC;\n\n-- Within-race normalization of implied probability\nWITH base AS (\n  SELECT\n    race_id,\n    runner_id,\n    odds_decimal,\n    1.0 / odds_decimal AS implied_raw\n  FROM runners\n)\nSELECT\n  race_id,\n  runner_id,\n  implied_raw / SUM(implied_raw) OVER (PARTITION BY race_id) AS implied_p_norm\nFROM base;</pre>\n</section>\n<section class=\"card\">\n<h3>4) Baselines: probabilistic win modeling + calibration</h3>\n<p class=\"muted\">\n        In racing analytics, your model output is typically a probability. That means evaluation should\n        prioritize <strong>proper scoring rules</strong> and <strong>calibration</strong>, not just AUC.\n      </p>\n<h4>Feature store (leakage-safe)</h4>\n<pre>library(dplyr)\n\nfeatures &lt;- sim$runners |&gt;\n  left_join(sim$races, by = \"race_id\") |&gt;\n  transmute(\n    race_id, runner_id,\n    win,\n    # pre-race covariates only\n    draw, weight_kg, age, sex,\n    surface, going, distance_m, race_class, purse,\n    odds_decimal,\n    log_odds = log(odds_decimal),\n    implied_p_raw = 1/odds_decimal\n  )\n\nstopifnot(all(is.finite(features$log_odds)))\nstopifnot(all(features$odds_decimal &gt;= 1.01))</pre>\n<h4>tidymodels GLM baseline + log loss / Brier / AUC</h4>\n<pre>library(tidymodels)\n\nset.seed(123)\n\ndf &lt;- features |&gt;\n  group_by(race_id) |&gt;\n  mutate(implied_p = implied_p_raw / sum(implied_p_raw)) |&gt;\n  ungroup() |&gt;\n  mutate(win = factor(win, levels = c(0,1), labels = c(\"no\",\"yes\")))\n\nspl  &lt;- initial_split(df, strata = win)\ntrain&lt;- training(spl)\ntest &lt;- testing(spl)\n\nrec &lt;- recipe(win ~ draw + weight_kg + age + sex + surface + going +\n                distance_m + odds_decimal, data = train) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors())\n\nmod &lt;- logistic_reg() |&gt; set_engine(\"glm\")\nwf  &lt;- workflow() |&gt; add_recipe(rec) |&gt; add_model(mod)\n\nfit  &lt;- fit(wf, train)\npred &lt;- predict(fit, test, type = \"prob\") |&gt;\n  bind_cols(test |&gt; select(win))\n\nmetric_set(mn_log_loss, brier_class, roc_auc)(pred, truth = win, .pred_yes)</pre>\n<h4>Calibration plot</h4>\n<pre>library(probably)\n\n# Binned calibration plot\ncal_plot_breaks(pred, truth = win, estimate = .pred_yes, num_breaks = 10)</pre>\n<div class=\"note\">\n<strong>Calibration mindset:</strong> a model can be “accurate” in ranking yet systematically\n        overconfident. In betting or pricing contexts, miscalibration is usually more dangerous than\n        a small drop in AUC.\n      </div>\n</section>\n<section class=\"card\">\n<h3>5) Multi-runner outcome modeling: rankings, not just winners</h3>\n<p class=\"muted\">\n        Binary win models throw away a lot of structure. Racing naturally produces a ranking:\n        finish positions across multiple runners. Ranking models let you target that structure directly.\n      </p>\n<h4>Plackett–Luce (direct ranking likelihood)</h4>\n<pre>library(PlackettLuce)\n\nset.seed(99)\nitems  &lt;- paste0(\"H\", 1:12)\nn_races &lt;- 40\nability &lt;- rnorm(length(items)); names(ability) &lt;- items\n\nRmat &lt;- matrix(0, nrow = n_races, ncol = length(items))\ncolnames(Rmat) &lt;- items\n\nfor (r in 1:n_races) {\n  field &lt;- sample(items, size = sample(6:10, 1), replace = FALSE)\n  util  &lt;- ability[field] + rnorm(length(field), 0, 0.3)\n  ord   &lt;- field[order(util, decreasing = TRUE)]\n  Rmat[r, ord] &lt;- seq_along(ord) # 1 = best; 0 = absent\n}\n\nrankings &lt;- as.rankings(Rmat)\npl &lt;- PlackettLuce(rankings)\n\nsummary(pl)\ncoef(pl)  # worth parameters</pre>\n<h4>Bradley–Terry via pairwise decomposition</h4>\n<pre>library(dplyr)\nlibrary(BradleyTerry2)\n\npairwise &lt;- tibble()\nfor (r in 1:n_races) {\n  in_race &lt;- names(which(Rmat[r, ] &gt; 0))\n  ranks   &lt;- Rmat[r, in_race]\n  for (i in 1:(length(in_race) - 1)) {\n    for (j in (i + 1):length(in_race)) {\n      h1 &lt;- in_race[i]; h2 &lt;- in_race[j]\n      win1 &lt;- as.integer(ranks[h1] &lt; ranks[h2])\n      pairwise &lt;- bind_rows(pairwise, tibble(h1=h1, h2=h2, win1=win1, win2=1-win1))\n    }\n  }\n}\n\nagg &lt;- pairwise |&gt;\n  group_by(h1, h2) |&gt;\n  summarise(win1 = sum(win1), win2 = sum(win2), .groups = \"drop\")\n\nbt &lt;- BTm(cbind(win1, win2), h1, h2, data = agg, id = \"horse\")\nBTabilities(bt)</pre>\n<div class=\"grid cols2\">\n<div class=\"kpi\">\n<strong>Interpretation</strong>\n<div class=\"muted\">\n            Both families estimate “ability-like” latent parameters, but they do it with different data representations:\n            full rankings vs aggregated pairwise outcomes.\n          </div>\n</div>\n<div class=\"kpi\">\n<strong>When rankings matter</strong>\n<div class=\"muted\">\n            If you care about place markets, exactas/quinellas, or modeling finish times/positions beyond “win,”\n            ranking-aware methods often align better with the objective.\n          </div>\n</div>\n</div>\n</section>\n<section class=\"card\">\n<h3>6) Hierarchy: GLMMs and Bayesian multilevel models</h3>\n<p class=\"muted\">\n        Racing is fundamentally hierarchical: repeated horses, trainers, jockeys, tracks, regions.\n        Fixed effects can overfit; multilevel models provide shrinkage and better uncertainty accounting.\n      </p>\n<h4>GLMM with trainer/jockey random effects (lme4)</h4>\n<pre>library(lme4)\nlibrary(dplyr)\n\nsim2 &lt;- simulate_races(n_races=500, n_horses=800, n_jockeys=120, n_trainers=100, seed=202)\ndf2  &lt;- sim2$runners |&gt; left_join(sim2$races, by=\"race_id\")\n\nm_glmm &lt;- glmer(\n  win ~ scale(weight_kg) + scale(draw) + surface + going +\n    (1 | trainer_id) + (1 | jockey_id),\n  data   = df2,\n  family = binomial()\n)\n\nsummary(m_glmm)</pre>\n<h4>Bayesian multilevel model (brms)</h4>\n<pre>library(brms)\n\npriors &lt;- c(\n  prior(normal(0, 1), class = \"b\"),\n  prior(exponential(1), class = \"sd\")\n)\n\nfit_bayes &lt;- brm(\n  win ~ scale(weight_kg) + scale(draw) + surface + going +\n    (1 | trainer_id) + (1 | jockey_id),\n  data   = df2,\n  family = bernoulli(),\n  prior  = priors,\n  chains = 2, iter = 1000, cores = 2, seed = 202\n)\n\nsummary(fit_bayes)\npp_check(fit_bayes)</pre>\n<div class=\"note\">\n<strong>Why Bayesian here?</strong>\n        You get a principled way to encode priors (e.g., smaller effects are more likely than huge ones),\n        propagate uncertainty through derived probabilities, and do posterior predictive checks.\n      </div>\n</section>\n<section class=\"card\">\n<h3>7) Time-to-event: survival and frailty modeling</h3>\n<p class=\"muted\">\n        Some questions in racing are naturally time-to-event: time to first win, time to peak performance,\n        time to retirement, time between races. Survival analysis gives you the right likelihood and handles censoring.\n      </p>\n<h4>Cox proportional hazards</h4>\n<pre>library(dplyr)\nlibrary(survival)\n\nset.seed(1)\nn &lt;- 1000\n\nhorses &lt;- tibble(\n  horse_id  = sprintf(\"H%04d\", 1:n),\n  talent    = rnorm(n),\n  trainer_id= sample(sprintf(\"T%03d\", 1:80), n, TRUE),\n  debut_age = pmax(2, rnorm(n, 3.0, 0.6))\n)\n\nbase_rate &lt;- 0.002\nlinpred   &lt;- 0.65 * horses$talent - 0.25 * (horses$debut_age - 3)\nrate      &lt;- base_rate * exp(linpred)\n\ntime_days   &lt;- rexp(n, rate = rate)\ncensor_days &lt;- runif(n, 60, 700)\n\nstatus   &lt;- as.integer(time_days  mutate(time_days = time_obs, status = status)\n\ncox &lt;- coxph(Surv(time_days, status) ~ scale(talent) + scale(debut_age), data = df_surv)\nsummary(cox)\ncox.zph(cox)</pre>\n<h4>Frailty term (trainer-level heterogeneity)</h4>\n<pre>cox_f &lt;- coxph(\n  Surv(time_days, status) ~ scale(talent) + scale(debut_age) + frailty(trainer_id),\n  data = df_surv\n)\n\nsummary(cox_f)</pre>\n</section>\n<section class=\"card\">\n<h3>8) Modern ML: XGBoost in tidymodels and mlr3</h3>\n<p class=\"muted\">\n        Gradient boosting can capture non-linearities and interactions without manual feature engineering.\n        But if you don’t evaluate probabilistically (log loss, calibration), you can accidentally ship a model\n        that “ranks well” yet prices poorly.\n      </p>\n<h4>tidymodels: tuned XGBoost</h4>\n<pre>library(tidymodels)\nlibrary(dplyr)\n\nset.seed(314)\nsim3 &lt;- simulate_races(n_races = 500, n_horses = 1200, seed = 314)\n\ndf3 &lt;- sim3$runners |&gt;\n  left_join(sim3$races, by=\"race_id\") |&gt;\n  mutate(win = factor(win, levels=c(0,1), labels=c(\"no\",\"yes\")))\n\nspl   &lt;- initial_split(df3, strata = win)\ntrain &lt;- training(spl)\ntest  &lt;- testing(spl)\n\nrec &lt;- recipe(win ~ draw + weight_kg + age + sex + surface + going +\n               distance_m + odds_decimal, data=train) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\nfolds &lt;- vfold_cv(train, v = 5, strata = win)\n\nxgb_spec &lt;- boost_tree(\n  trees = 800,\n  tree_depth     = tune(),\n  learn_rate     = tune(),\n  loss_reduction = tune(),\n  sample_size    = tune(),\n  mtry           = tune()\n) |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\nwf &lt;- workflow() |&gt; add_recipe(rec) |&gt; add_model(xgb_spec)\n\ngrid &lt;- grid_space_filling(\n  tree_depth(),\n  learn_rate(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  mtry(range = c(5, 60)),\n  size = 20\n)\n\nres &lt;- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(mn_log_loss, roc_auc)\n)\n\nbest &lt;- select_best(res, \"mn_log_loss\")\nfinal_fit &lt;- finalize_workflow(wf, best) |&gt; fit(train)\n\npred &lt;- predict(final_fit, test, type=\"prob\") |&gt; bind_cols(test |&gt; select(win))\nmetric_set(mn_log_loss, brier_class, roc_auc)(pred, truth = win, .pred_yes)</pre>\n<h4>mlr3: comparable workflow</h4>\n<pre>library(data.table)\nlibrary(mlr3)\nlibrary(mlr3learners)\n\nset.seed(99)\nsim4 &lt;- simulate_races(n_races = 400, n_horses = 900, seed = 99)\n\ndf4 &lt;- as.data.table(merge(sim4$runners, sim4$races, by = \"race_id\"))\ndf4[, win := factor(ifelse(win == 1, \"yes\", \"no\"))]\n\ntask &lt;- as_task_classif(df4, target = \"win\", id = \"win_task\")\ntask$set_col_roles(\n  c(\"race_id\",\"runner_id\",\"horse_id\",\"jockey_id\",\"trainer_id\"),\n  roles = \"name\"\n)\n\nrsmp &lt;- rsmp(\"cv\", folds = 5)\n\nlrn_glm &lt;- lrn(\"classif.log_reg\", predict_type = \"prob\")\nrr_glm  &lt;- resample(task, lrn_glm, rsmp)\nrr_glm$aggregate(msr(\"classif.logloss\"))\nrr_glm$aggregate(msr(\"classif.auc\"))\n\nlrn_xgb &lt;- lrn(\"classif.xgboost\", predict_type = \"prob\",\n               nrounds = 300, eta = 0.05, max_depth = 4)\nrr_xgb &lt;- resample(task, lrn_xgb, rsmp)\nrr_xgb$aggregate(msr(\"classif.logloss\"))\nrr_xgb$aggregate(msr(\"classif.auc\"))</pre>\n</section>\n<section class=\"card\">\n<h3>9) Odds time series: from snapshots to features</h3>\n<p class=\"muted\">\n        Exchange markets produce time-stamped odds and volumes. Treat them as time series and engineer features\n        like drift, volatility, late steam, and liquidity. Keep it honest: market microstructure is noisy,\n        and backtests require strong assumptions.\n      </p>\n<h4>Simulated snapshots as a tsibble + feature extraction</h4>\n<pre>library(dplyr)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(ggplot2)\n\nset.seed(1)\nsnap &lt;- tibble(\n  runner_id = \"R00001_01\",\n  ts = as.POSIXct(\"2025-06-01 14:00:00\", tz = \"UTC\") + seq(0, 60*60, by = 60),\n  odds_decimal = pmax(1.01, 6 + cumsum(rnorm(61, 0, 0.05))),\n  traded_vol = cumsum(pmax(0, rnorm(61, 10, 3)))\n)\n\ntsbl &lt;- snap |&gt; as_tsibble(index = ts, key = runner_id)\n\nfeat &lt;- tsbl |&gt;\n  features(odds_decimal, list(\n    mean  = ~ mean(.),\n    sd    = ~ sd(.),\n    min   = ~ min(.),\n    max   = ~ max(.),\n    slope = ~ coef(lm(. ~ seq_along(.)))[2]\n  ))\n\nfeat\n\nggplot(tsbl, aes(ts, odds_decimal)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Decimal odds\")</pre>\n<h4>Schema normalization: snapshots table</h4>\n<pre># Example transformation skeleton for real exchange data:\n# 1) parse raw feed messages\n# 2) map to normalized columns: race_id, runner_id, ts, odds_decimal, traded_vol\n# 3) write Parquet and load into DuckDB\n\n# write_parquet(odds_snapshots, \"data/derived/odds_snapshots.parquet\")\n# dbExecute(con, \"CREATE OR REPLACE TABLE odds_snapshots AS\n#                 SELECT * FROM read_parquet('data/derived/odds_snapshots.parquet');\")</pre>\n</section>\n<section class=\"card\">\n<h3>10) Backtesting as a decision layer (fractional Kelly + guardrails)</h3>\n<p class=\"muted\">\n        Once you have calibrated probabilities, you can add a decision policy.\n        One common framework is fractional Kelly sizing, but it must be capped and stress tested.\n        The same probabilities can produce very different outcomes depending on execution constraints,\n        market impact, limits, and selection filters.\n      </p>\n<h4>Simulated fractional Kelly example with risk caps</h4>\n<pre>library(dplyr)\nlibrary(ggplot2)\n\nsim5 &lt;- simulate_races(n_races = 180, n_horses = 500, seed = 9)\nrunners &lt;- sim5$runners |&gt;\n  group_by(race_id) |&gt;\n  mutate(\n    p_market = (1/odds_decimal) / sum(1/odds_decimal),\n    # toy \"model\": market + noise (for demonstration)\n    p_model  = pmin(0.99, pmax(0.001, p_market + rnorm(n(), 0, 0.02))),\n    edge     = p_model - p_market\n  ) |&gt;\n  ungroup()\n\nfractional_kelly &lt;- function(p, odds_dec, fraction = 0.25, cap = 0.05) {\n  b &lt;- odds_dec - 1\n  f_star &lt;- (b*p - (1 - p)) / b\n  f &lt;- pmax(0, f_star) * fraction\n  pmin(f, cap)\n}\n\nbankroll &lt;- 1\nmax_drawdown &lt;- 0.25\npeak &lt;- bankroll\nhist &lt;- tibble()\n\nfor (rid in unique(runners$race_id)) {\n  if (bankroll &lt; (1 - max_drawdown) * peak) break  # guardrail: stop\n\n  race &lt;- runners |&gt; filter(race_id == rid)\n  pick &lt;- race |&gt; arrange(desc(edge)) |&gt; slice(1)\n\n  if (pick$edge &gt; 0.01 &amp;&amp; pick$odds_decimal  mutate(drawdown = 1 - bankroll/peak)\n\nggplot(hist, aes(seq_along(bankroll), bankroll)) + geom_line() +\n  labs(x = \"Race index\", y = \"Bankroll\")\n\nggplot(hist, aes(seq_along(drawdown), drawdown)) + geom_line() +\n  labs(x = \"Race index\", y = \"Drawdown\")</pre>\n<div class=\"note\">\n<strong>Practical warning:</strong> this example is intentionally simplified. Real backtests require:\n        commissions, slippage, bet limits, late price changes, liquidity constraints, and selection bias checks.\n      </div>\n</section>\n<section class=\"card\">\n<h3>11) Deployment: versioned models + HTTP prediction API</h3>\n<p class=\"muted\">\n        If you want your work to be reusable, build a deployable artifact. A clean R-native pattern is:\n        <code>pins</code> for versioning + <code>vetiver</code> for model packaging +\n        <code>plumber</code> for the API server.\n      </p>\n<pre>library(tidymodels)\nlibrary(pins)\nlibrary(vetiver)\nlibrary(plumber)\nlibrary(dplyr)\n\nsim6 &lt;- simulate_races(n_races = 250, n_horses = 700, seed = 101)\n\ndf6 &lt;- sim6$runners |&gt;\n  left_join(sim6$races, by=\"race_id\") |&gt;\n  mutate(win = factor(win, levels=c(0,1), labels=c(\"no\",\"yes\")))\n\nrec &lt;- recipe(win ~ draw + weight_kg + age + sex + surface + going +\n                distance_m + odds_decimal, data = df6) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\nfit &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(logistic_reg() |&gt; set_engine(\"glm\")) |&gt;\n  fit(df6)\n\nboard &lt;- board_folder(\"pins\", versioned = TRUE)\nv &lt;- vetiver_model(fit, model_name = \"win_prob_glm\")\n\nvetiver_pin_write(board, v)\n\napi &lt;- pr() |&gt; vetiver_api(v)\n\n# Run locally:\n# api |&gt; pr_run(host = \"0.0.0.0\", port = 8080)\n\napi</pre>\n<h4>Minimal request payload shape (JSON)</h4>\n<pre>{\n  \"draw\": 7,\n  \"weight_kg\": 55.5,\n  \"age\": 4,\n  \"sex\": \"G\",\n  \"surface\": \"Turf\",\n  \"going\": \"Good\",\n  \"distance_m\": 1600,\n  \"odds_decimal\": 6.2\n}</pre>\n</section>\n<section class=\"card\">\n<h3>12) Reproducibility mechanics: renv + deterministic builds</h3>\n<p class=\"muted\">\n        Reproducibility is not vibes. Lock dependencies, record versions, and standardize builds.\n        This is especially important when your analytics becomes a book, a report, or a product artifact.\n      </p>\n<h4>renv workflow</h4>\n<pre># Initialize (once)\n# renv::init()\n\n# Snapshot current library into renv.lock\nrenv::snapshot()\n\n# Restore exact package versions from renv.lock\nrenv::restore()</pre>\n<h4>Dockerfile for deterministic PDF builds (Quarto)</h4>\n<pre>FROM rocker/verse:4.4.2\nWORKDIR /book\n\nCOPY renv.lock renv.lock\nCOPY renv/ renv/\nCOPY .Rprofile .Rprofile\n\nRUN R -q -e \"install.packages('renv'); renv::restore(prompt = FALSE)\"\n\nCOPY . .\nCMD [\"quarto\", \"render\", \"book\", \"--to\", \"pdf\"]</pre>\n<h4>CI skeleton (GitHub Actions)</h4>\n<pre>name: Render PDF\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  pdf:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: r-lib/actions/setup-r@v2\n      - uses: r-lib/actions/setup-renv@v2\n      - name: Render Quarto book\n        run: quarto render book --to pdf</pre>\n</section>\n<section class=\"card\">\n<h3>13) An “engineering checklist” you can actually follow</h3>\n<div class=\"grid cols3\">\n<div class=\"kpi\">\n<strong>Data contract</strong>\n<ul class=\"muted\">\n<li>Canonical schema</li>\n<li>Parquet + DuckDB</li>\n<li>Adapters, not redistribution</li>\n</ul>\n</div>\n<div class=\"kpi\">\n<strong>Modeling</strong>\n<ul class=\"muted\">\n<li>GLM/GLMM baselines</li>\n<li>Ranking likelihoods</li>\n<li>Bayesian uncertainty</li>\n</ul>\n</div>\n<div class=\"kpi\">\n<strong>Evaluation</strong>\n<ul class=\"muted\">\n<li>Log loss / Brier</li>\n<li>Calibration curves</li>\n<li>Stress-tested backtests</li>\n</ul>\n</div>\n<div class=\"kpi\">\n<strong>Time dependence</strong>\n<ul class=\"muted\">\n<li>Odds snapshots</li>\n<li>Drift/volatility features</li>\n<li>Liquidity constraints</li>\n</ul>\n</div>\n<div class=\"kpi\">\n<strong>Decision layer</strong>\n<ul class=\"muted\">\n<li>Explicit assumptions</li>\n<li>Fractional Kelly caps</li>\n<li>Drawdown guardrails</li>\n</ul>\n</div>\n<div class=\"kpi\">\n<strong>Deployment</strong>\n<ul class=\"muted\">\n<li>pins versioning</li>\n<li>vetiver packaging</li>\n<li>plumber API</li>\n</ul>\n</div>\n</div>\n<div class=\"hr\"></div>\n<h4>One more code snippet: within-race probability normalization</h4>\n<p class=\"muted\">\n        Many racing tables store “odds” per runner, but for inference you often want a normalized probability\n        within a field. Here’s a standard pattern that also highlights overround issues.\n      </p>\n<pre>library(dplyr)\n\nnormalize_field_probs &lt;- function(df, odds_col = odds_decimal) {\n  df |&gt;\n    group_by(race_id) |&gt;\n    mutate(\n      implied_raw = 1 / {{ odds_col }},\n      overround   = sum(implied_raw),\n      p_market    = implied_raw / overround\n    ) |&gt;\n    ungroup()\n}\n\ndf_norm &lt;- normalize_field_probs(sim$runners)\n\ndf_norm |&gt;\n  summarise(\n    avg_overround = mean(overround),\n    pmin_overround= min(overround),\n    pmax_overround= max(overround)\n  )</pre>\n<div class=\"note\">\n<strong>Interpretation:</strong> overround &gt; 1 implies the market’s implied probabilities sum above 1\n        (built-in margin). Separating “market implied probability” from “model probability” is essential if\n        you want honest evaluation.\n      </div>\n</section>\n<section class=\"footer\">\n<p>\n        If you’re building a full, reproducible workflow (data contract → modeling → evaluation → deployment),\n        you may want a more structured, end-to-end blueprint to keep everything consistent.\n        <span class=\"small\">\n          You can find a concise reference here:\n          <a href=\"https://rprogrammingbooks.com/product/racing-analytics-r/\" rel=\"nofollow\" target=\"_blank\">racing-analytics-r</a>.\n        </span>\n</p>\n<p class=\"small muted\">\n        Tip: keep your repository “clean-room” by shipping simulated data + adapters, and never bundling restricted datasets.\n        That one decision makes your work easier to share, review, and publish.\n      </p>\n</section>\n</main>\n\n<p>The post <a href=\"https://rprogrammingbooks.com/quantitative-horse-racing-r-calibration-backtesting-deployment/\" rel=\"nofollow\" target=\"_blank\">Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment</a> appeared first on <a href=\"https://rprogrammingbooks.com/\" rel=\"nofollow\" target=\"_blank\">R Programming Books</a>.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://rprogrammingbooks.com/quantitative-horse-racing-r-calibration-backtesting-deployment/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=quantitative-horse-racing-r-calibration-backtesting-deployment\"> Blog - R Programming Books</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 3238,
    "reading_time_min": 16.2,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/rprogrammingbooks/",
        "text": "rprogrammingbooks"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://rprogrammingbooks.com/quantitative-horse-racing-r-calibration-backtesting-deployment/?utm_source=rss&utm_medium=rss&utm_campaign=quantitative-horse-racing-r-calibration-backtesting-deployment",
        "text": "Blog - R Programming Books"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://rprogrammingbooks.com/product/racing-analytics-r/",
        "text": "racing-analytics-r"
      },
      {
        "href": "https://rprogrammingbooks.com/quantitative-horse-racing-r-calibration-backtesting-deployment/",
        "text": "Quantitative Horse Racing with R: Calibration, Backtesting, and Deployment"
      },
      {
        "href": "https://rprogrammingbooks.com/",
        "text": "R Programming Books"
      },
      {
        "href": "https://rprogrammingbooks.com/quantitative-horse-racing-r-calibration-backtesting-deployment/?utm_source=rss&utm_medium=rss&utm_campaign=quantitative-horse-racing-r-calibration-backtesting-deployment",
        "text": "Blog - R Programming Books"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [],
    "lang": "en-US",
    "crawled_at_utc": "2026-02-20T01:16:26Z"
  }
}