{
  "id": "d4f85ff293e36f2b4fb61eb7ebe24b31bf028d84",
  "url": "https://www.r-bloggers.com/2026/01/when-adtech-misrepresent-their-own-a-b-tests/",
  "created_at_utc": "2026-01-27T12:22:30Z",
  "crawled_at_utc": "2026-01-27T12:22:37Z",
  "html_title": "When AdTech Misrepresent Their Own A/B Tests | R-bloggers",
  "meta_description": "Preamble: I had a roughly 8 year break from blogging and focussed on kids plus related duties. This year, I will try to start again. My goal is to write about (advertising-related) research and how practitioners can apply the insights. Todays paper i...",
  "data": {
    "url": "https://www.r-bloggers.com/2026/01/when-adtech-misrepresent-their-own-a-b-tests/",
    "canonical_url": "https://www.r-bloggers.com/2026/01/when-adtech-misrepresent-their-own-a-b-tests/",
    "html_title": "When AdTech Misrepresent Their Own A/B Tests | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "Preamble: I had a roughly 8 year break from blogging and focussed on kids plus related duties. This year, I will try to start again. My goal is to write about (advertising-related) research and how practitioners can apply the insights. Todays paper i...",
    "meta_keywords": null,
    "og_title": "When AdTech Misrepresent Their Own A/B Tests | R-bloggers",
    "og_description": "Preamble: I had a roughly 8 year break from blogging and focussed on kids plus related duties. This year, I will try to start again. My goal is to write about (advertising-related) research and how practitioners can apply the insights. Todays paper i...",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "twitter_title": "When AdTech Misrepresent Their Own A/B Tests | R-bloggers",
    "twitter_description": "Preamble: I had a roughly 8 year break from blogging and focussed on kids plus related duties. This year, I will try to start again. My goal is to write about (advertising-related) research and how practitioners can apply the insights. Todays paper i...",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "When AdTech Misrepresent Their Own A/B Tests\nPosted on\nJanuary 24, 2026\nby\nFlorian Teschner\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nFlorian Teschner\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nPreamble: I had a roughly 8 year break from blogging and focussed on kids plus related duties. This year, I will try to start again.\nMy goal is to write about (advertising-related) research and how practitioners can apply the insights.\nTodays paper is from the “International Journal of Research in Marketing” and decribing how big Ad Tech companies (Google / Meta) are miscommunicating their methods and potentially inflating the results of advertising A/B-tests.\nTakeaways for advertising practitioners?\nTreat platform “A/B tests” with skepticism\n: When Google or Meta say they’ll run an A/B test, assume it’s observational analysis unless they specifically confirm proper randomization. Don’t rely on these results for high-stakes decisions.\nUnderstand the limitations\n: Observational analysis can still be valuable, but you need to understand its limitations. It can show correlations, not causation. Use it for insights, not definitive conclusions.\nPush for better options\n: If you’re a large advertiser, push your platform representatives for truly experimental testing options. The technology exists – it’s just not being offered.\nConsider third-party solutions\n: For valid A/B testing, you may need to use third-party tools or build your own testing infrastructure outside the platform’s built-in tools.\nLong Version:\nI recently came across a paper by Bögershausen, Oertzen, and Bock (2025) titled “On the persistent mischaracterization of Google and Facebook A/B tests” that exposes something important about how major advertising platforms represent their testing tools. The paper reveals that Google and Meta have been systematically misrepresenting the nature of their A/B testing capabilities to researchers and advertisers.\nHere’s the core issue: Google and Meta present their A/B testing tools as conducting clean, randomized controlled experiments. They position these as proper experimental designs where users are randomly assigned to different ad variations, allowing advertisers to compare performance in a scientifically valid way. But the reality is quite different.\nThe authors analyzed what these platforms actually do, and found that their “A/B tests” aren’t true experiments at all. Instead of random assignment, these tools typically use non-ex observational approaches that don’t establish proper control groups. Users aren’t randomly assigned to treatment conditions – they’re just shown different ads based on whatever targeting the platform’s algorithm decides.\nThis matters because true A/B testing requires randomization to establish causal inference. Without proper random assignment, you can’t attribute differences in performance to the ad variations themselves – there could be systematic differences in who sees which ad, confounding factors, or other biases. The platforms are essentially selling observational analysis dressed up as experimental design.\nWhy would the platforms do this? The paper suggests it’s partly a communication problem – the tools are marketed as “A/B tests” because that’s a familiar, reassuring term to advertisers, even though the actual methodology doesn’t match what that term means in experimental design. It’s also probably easier to sell “we’ll run an A/B test” than “we’ll show different ads to different people and analyze the results.”\nThe problem is serious for several reasons:\nFirst, it misleads advertisers who think they’re running scientifically valid experiments. When an advertiser sees Google or Meta say “run an A/B test to compare your ads,” they expect proper randomization and causal inference. But they’re not getting that – they’re getting observational analysis with all the limitations that entails.\nSecond, it creates false confidence in marketing decisions. Advertisers make budget allocation and creative decisions based on these “A/B test” results, believing they have experimental evidence when they don’t. This can lead to suboptimal marketing strategies because the fundamental validity of the results is compromised.\nThird, it creates a credibility problem for the platforms. When researchers dig into how these tools actually work and discover they’re not true experiments, it undermines trust in the platforms more broadly. The paper notes that this misrepresentation has persisted across both companies for years, despite researchers pointing out the issues.\nThe authors call (and I totally agree) for Google and Meta to do two things:\nCommunicate more accurately\nabout what their A/B testing tools actually do. Be clear that these are observational analyses, not randomized controlled experiments. Stop using terminology that implies proper experimental design when that’s not what’s happening.\nProvide truly experimental options\n. Ideally, the platforms should offer advertisers and researchers a way to conduct actual randomized A/B tests where users are properly assigned to different ad variations in a truly random manner. This would give the option for valid causal inference when it’s needed.\nThis is particularly interesting because it’s not about technical complexity – the platforms have the infrastructure to run proper experiments (they do it internally all the time). The issue is about transparency and offering the right tools to advertisers.\nThe paper is an important reminder that we need to be critical of what platforms tell us about their tools, even – or especially – when the terminology sounds familiar and reassuring. “A/B test” has a specific meaning in experimental design, and marketing platforms shouldn’t be allowed to co-opt that term for something fundamentally different.\nReference:\nBögershausen, A., Oertzen, T. v., & Bock, K. (2025). On the persistent mischaracterization of Google and Facebook A/B tests.\nNote: AI-supported content.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nFlorian Teschner\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-398567 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">When AdTech Misrepresent Their Own A/B Tests</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 24, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/florian-teschner/\">Florian Teschner</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"http://flovv.github.io/facebook-ab-tests/\"> Florian Teschner</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>Preamble: I had a roughly 8 year break from blogging and focussed on kids plus related duties. This year, I will try to start again. <br/>\nMy goal is to write about (advertising-related) research and how practitioners can apply the insights.</p>\n<p>Todays paper is from the “International Journal of Research in Marketing” and decribing how big Ad Tech companies (Google / Meta) are miscommunicating their methods and potentially inflating the results of advertising A/B-tests.</p>\n<h2 id=\"takeaways-for-advertising-practitioners\">Takeaways for advertising practitioners?</h2>\n<ol>\n<li>\n<p><strong>Treat platform “A/B tests” with skepticism</strong>: When Google or Meta say they’ll run an A/B test, assume it’s observational analysis unless they specifically confirm proper randomization. Don’t rely on these results for high-stakes decisions.</p>\n</li>\n<li>\n<p><strong>Understand the limitations</strong>: Observational analysis can still be valuable, but you need to understand its limitations. It can show correlations, not causation. Use it for insights, not definitive conclusions.</p>\n</li>\n<li>\n<p><strong>Push for better options</strong>: If you’re a large advertiser, push your platform representatives for truly experimental testing options. The technology exists – it’s just not being offered.</p>\n</li>\n<li>\n<p><strong>Consider third-party solutions</strong>: For valid A/B testing, you may need to use third-party tools or build your own testing infrastructure outside the platform’s built-in tools.</p>\n</li>\n</ol>\n<h2 id=\"long-version\">Long Version:</h2>\n<p>I recently came across a paper by Bögershausen, Oertzen, and Bock (2025) titled “On the persistent mischaracterization of Google and Facebook A/B tests” that exposes something important about how major advertising platforms represent their testing tools. The paper reveals that Google and Meta have been systematically misrepresenting the nature of their A/B testing capabilities to researchers and advertisers.</p>\n<p>Here’s the core issue: Google and Meta present their A/B testing tools as conducting clean, randomized controlled experiments. They position these as proper experimental designs where users are randomly assigned to different ad variations, allowing advertisers to compare performance in a scientifically valid way. But the reality is quite different.</p>\n<p>The authors analyzed what these platforms actually do, and found that their “A/B tests” aren’t true experiments at all. Instead of random assignment, these tools typically use non-ex observational approaches that don’t establish proper control groups. Users aren’t randomly assigned to treatment conditions – they’re just shown different ads based on whatever targeting the platform’s algorithm decides.</p>\n<p>This matters because true A/B testing requires randomization to establish causal inference. Without proper random assignment, you can’t attribute differences in performance to the ad variations themselves – there could be systematic differences in who sees which ad, confounding factors, or other biases. The platforms are essentially selling observational analysis dressed up as experimental design.</p>\n<p>Why would the platforms do this? The paper suggests it’s partly a communication problem – the tools are marketed as “A/B tests” because that’s a familiar, reassuring term to advertisers, even though the actual methodology doesn’t match what that term means in experimental design. It’s also probably easier to sell “we’ll run an A/B test” than “we’ll show different ads to different people and analyze the results.”</p>\n<p>The problem is serious for several reasons:</p>\n<p>First, it misleads advertisers who think they’re running scientifically valid experiments. When an advertiser sees Google or Meta say “run an A/B test to compare your ads,” they expect proper randomization and causal inference. But they’re not getting that – they’re getting observational analysis with all the limitations that entails.</p>\n<p>Second, it creates false confidence in marketing decisions. Advertisers make budget allocation and creative decisions based on these “A/B test” results, believing they have experimental evidence when they don’t. This can lead to suboptimal marketing strategies because the fundamental validity of the results is compromised.</p>\n<p>Third, it creates a credibility problem for the platforms. When researchers dig into how these tools actually work and discover they’re not true experiments, it undermines trust in the platforms more broadly. The paper notes that this misrepresentation has persisted across both companies for years, despite researchers pointing out the issues.</p>\n<p>The authors call (and I totally agree) for Google and Meta to do two things:</p>\n<ol>\n<li>\n<p><strong>Communicate more accurately</strong> about what their A/B testing tools actually do. Be clear that these are observational analyses, not randomized controlled experiments. Stop using terminology that implies proper experimental design when that’s not what’s happening.</p>\n</li>\n<li>\n<p><strong>Provide truly experimental options</strong>. Ideally, the platforms should offer advertisers and researchers a way to conduct actual randomized A/B tests where users are properly assigned to different ad variations in a truly random manner. This would give the option for valid causal inference when it’s needed.</p>\n</li>\n</ol>\n<p>This is particularly interesting because it’s not about technical complexity – the platforms have the infrastructure to run proper experiments (they do it internally all the time). The issue is about transparency and offering the right tools to advertisers.</p>\n<p>The paper is an important reminder that we need to be critical of what platforms tell us about their tools, even – or especially – when the terminology sounds familiar and reassuring. “A/B test” has a specific meaning in experimental design, and marketing platforms shouldn’t be allowed to co-opt that term for something fundamentally different.</p>\n<p>Reference:\nBögershausen, A., Oertzen, T. v., &amp; Bock, K. (2025). On the persistent mischaracterization of Google and Facebook A/B tests.</p>\n<p>Note: AI-supported content.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"http://flovv.github.io/facebook-ab-tests/\"> Florian Teschner</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 1034,
    "reading_time_min": 5.2,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/florian-teschner/",
        "text": "Florian Teschner"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "http://flovv.github.io/facebook-ab-tests/",
        "text": "Florian Teschner"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "http://flovv.github.io/facebook-ab-tests/",
        "text": "Florian Teschner"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [],
    "lang": "en-US",
    "crawled_at_utc": "2026-01-27T12:22:37Z"
  }
}