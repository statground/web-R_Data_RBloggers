{
  "id": "36992d7da74734f6bb0e90ba38e0d1b76a924495",
  "url": "https://www.r-bloggers.com/2025/10/of-course-someone-has-to-write-imperative-code-to-build-reproducible-data-science-pipelines-it-doesnt-have-to-be-you/",
  "created_at_utc": "2026-01-04T05:35:43Z",
  "crawled_at_utc": "2026-01-04T05:36:12Z",
  "html_title": "Of course, someone has to write imperative code to build reproducible data science pipelines. It doesn’t have to be you. | R-bloggers",
  "meta_description": "Last time I quickly introduced my latest package, {rixpress}, but I think that to really understand what {rixpress} brings to the table, one needs to solve the same problem without it. And incidentally, I think that this exercise also show...",
  "data": {
    "url": "https://www.r-bloggers.com/2025/10/of-course-someone-has-to-write-imperative-code-to-build-reproducible-data-science-pipelines-it-doesnt-have-to-be-you/",
    "canonical_url": "https://www.r-bloggers.com/2025/10/of-course-someone-has-to-write-imperative-code-to-build-reproducible-data-science-pipelines-it-doesnt-have-to-be-you/",
    "html_title": "Of course, someone has to write imperative code to build reproducible data science pipelines. It doesn’t have to be you. | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "Last time I quickly introduced my latest package, {rixpress}, but I think that to really understand what {rixpress} brings to the table, one needs to solve the same problem without it. And incidentally, I think that this exercise also show...",
    "meta_keywords": null,
    "og_title": "Of course, someone has to write imperative code to build reproducible data science pipelines. It doesn’t have to be you. | R-bloggers",
    "og_description": "Last time I quickly introduced my latest package, {rixpress}, but I think that to really understand what {rixpress} brings to the table, one needs to solve the same problem without it. And incidentally, I think that this exercise also show...",
    "og_image": "https://b-rodrigues.github.io/assets/img/xkcd-nix.png",
    "twitter_title": "Of course, someone has to write imperative code to build reproducible data science pipelines. It doesn’t have to be you. | R-bloggers",
    "twitter_description": "Last time I quickly introduced my latest package, {rixpress}, but I think that to really understand what {rixpress} brings to the table, one needs to solve the same problem without it. And incidentally, I think that this exercise also show...",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "Of course, someone has to write imperative code to build reproducible data science pipelines. It doesn’t have to be you.\nPosted on\nOctober 28, 2025\nby\nEconometrics and Free Software\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nEconometrics and Free Software\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nLast time\nI quickly introduced my latest package,\n{rixpress}\n, but I think that to really understand what\n{rixpress}\nbrings to the table, one needs to solve the same problem without it. And incidentally, I think that this exercise also show what makes Nix actually so good.\nThe goal is to build a data science pipeline. The example here is purely illustrative, and compare a Nix-based approach to a non Nix-based approach. So, I built the same polyglot Real Business Cycle model pipeline twice. First, I did it without\n{rixpress}\n(nor\n{rix}\n), using a combination of Docker, Make, and a bunch of wrapper scripts. Then, I did it with\n{rix}\nand\n{rixpress}\n.\nBoth pipelines produce the exact same result. But the way to get there is fundamentally different.\nJuggling imperative tools\nWithout Nix, you have to use language-specific package managers and tooling to first set up the environment. So for Python I’ve used\nuv\n(which is fantastic to be honest), then to install the right version of R I’ve used\nrig\nand a Posit CRAN snapshot for packages and for Julia I’ve simply downloaded a pre-compiled package of the version I needed, and used its built-in package manager to install specific versions of packages as well.\nAlso, to deal with system level dependencies, I’ve bundled everything inside a Docker image. This is a sketch of the\nDockerfile\n:\n# Add R repository and install specific version\nRUN apt-get update && apt-get install -y software-properties-common\nRUN add-apt-repository ppa:...\nRUN curl -L https://rig.r-pkg.org/... | sh\nRUN rig add 4.5.1\n\n# Install Python with uv\nRUN curl -LsSf https://astral.sh/uv/install.sh | sh\nRUN uv python install 3.13\n\n# Download and extract Julia\nRUN curl -fsSL https://julialang-s3.julialang.org/... -o julia.tar.gz\nRUN tar -xzf julia.tar.gz -C /opt/\n\n# Install packages for each language separately\nRUN echo 'options(repos = c(CRAN = ...))' > /root/.Rprofile\nRUN Rscript -e 'install.packages(...)'\n\n# Install Python packages using uv with specific versions for reproducibility.\nRUN echo \"pandas==2.3.3\" > /tmp/requirements.txt && \\\n    echo \"scikit-learn==1.7.2\" >> /tmp/requirements.txt && \\\n    # ... more packages ...\n\nRUN uv pip install --no-cache -r /tmp/requirements.txt && \\\n    rm /tmp/requirements.txt\n\n# Install specific versions of Julia packages for reproducibility\nRUN julia -e 'using Pkg; \\\n    Pkg.add(name=\"Arrow\", version=\"2.8.0\"); \\\nThis\ntraditional\napproach feels like you’re a sysadmin first and a data scientist second. The\nDockerfile\nis a long, step-by-step, imperative script of shell commands. You have to write\nhow\nstuff needs to be installed, and this of course varies for each language. Each language needs its own special treatment, its own package installation command, and its own set of dependencies. For example, for Python, I actually even needed more configuration than what I’ve shown above:\n# Ensure the installed binary is on the `PATH`\nENV PATH=\"/root/.local/bin/:$PATH\"\n\n# Install the specified Python version using uv.\nRUN uv python install ${PYTHON_VERSION}\n\n# Setup default virtual env\nRUN uv venv /opt/venv\n# Use the virtual environment automatically\nENV VIRTUAL_ENV=/opt/venv\n# Place entry points in the environment at the front of the path\nENV PATH=\"/opt/venv/bin:$PATH\"\nThis is because I needed to set the virtual environment installed by\nuv\nas the one to be used by default. This is ok inside Docker, but that’s not something you’d likely want to do on a real machine. The final\nDockerfile\nfor our “simple” example was over 100 lines long (including comments).\nNow that the environment is said, we actually need to orchestrate the workflow. I’ve used\nMake\nfor this, which means writing a\nMakefile\n. Honestly, nowadays, thanks to LLMs that’s not so much of an issue. But before LLMs, it would be quite annoying, because you need to manually define which file depends on which other file. Here’s what it looks like:\n# ==============================================================================\n# Makefile for the Polyglot RBC Model Pipeline\n# ==============================================================================\n\n# Define the interpreters for each language.\nJULIA := julia\nPYTHON := python\nRSCRIPT := Rscript\nQUARTO := quarto\n\n# Define directory variables for better organization.\nDATA_DIR := data\nPLOTS_DIR := plots\nREPORT_DIR := report\nFUNCTIONS_DIR := functions\n\n# Define the final and intermediate data files.\nSIMULATED_DATA := $(DATA_DIR)/simulated_rbc_data.arrow\nPREDICTIONS := $(DATA_DIR)/predictions.arrow\nFINAL_PLOT := $(PLOTS_DIR)/output_plot.png\nFINAL_REPORT := $(REPORT_DIR)/readme.html\n\n# --- Main Rules ---\n\n# The default 'all' rule now points to the final compiled HTML report.\nall: $(FINAL_REPORT)\n\n# Rule to render the final Quarto report.\n# Depends on the Quarto source file and the plot from the R step.\n$(FINAL_REPORT): readme.qmd $(FINAL_PLOT) | $(REPORT_DIR)\n    @echo \"--- [Quarto] Compiling final report ---\"\n    $(QUARTO) render $< --to html --output-dir $(REPORT_DIR)\n\n... and so on ...\nThat’s another 65 lines for the orchestration.\nFinally, and probably worst of all, is that you end up writing tons of “glue code.” Because\nmake\njust runs scripts, every step of your analysis (the Julia simulation, the Python training) needs to be wrapped in a script that does nothing but parse command-line arguments, read an input file, call your\nactual\nanalysis function, and write an output file. That’s a lot of code just to get things talking to each other.\nThe final tally for the traditional, imperative, approach?\nNine separate files\njust to manage the environment and run the pipeline. It’s a fragile, complicated house of cards, but it takes only 3 minutes to run on a standard Ubuntu GitHub Actions runner.\nNix: Declarative, Simple, and Clean\nNix makes this whole process so much easier, it’s actually not even fair. Instead of telling the computer\nhow\nto do everything, you just declare\nwhat\nyou want. You describe your requirements, and Nix figures the rest out. But because Nix is not that easy to get into, I wrotk the\n{rix}\nand\n{rixpress}\npackages as high-level interfaces to Nix’s power.\nFor example, to set up the environment, you just list the R, Python, and Julia packages you need, and\n{rix}\nhandles everything else. It figures out how to install them, resolves all the system-level dependencies, and generates the complex Nix expression for you. You don’t need to be a sysadmin; you just need to know what packages your analysis requires. This is because all the\nsysadminy\nwork was handled upstream by Nix package maintainers (real MVPs); Nix maintainers encode the build recipes, dependency graphs, and patches needed for each package, so you don’t have to. (Reminds me of this quote from\nJenny Bryan\n:\nOf course, someone has to write for loops. It doesn’t have to be you\n, but here it’s unglamorous Nix code to make packages work well instead of loops.)\nHere’s what the\ngen-env.R\nscript looks like:\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"ggplot2\", \"dplyr\", \"arrow\"),\n  jl_conf = list(jl_version = \"lts\", ...),\n  py_conf = list(py_version = \"3.13\", ...),\n  ...\n)\nThen, for the pipeline, it’s the same story. You just write what you need, not how it’s done. Nix can handle this. Here’s what the\ngen-pipeline.R\nscript looks like:\n// gen-pipeline.R - a small part\nlist(\n  rxp_jl(name = simulated_rbc_data, expr = \"simulate_rbc_model(...)\"),\n  rxp_py(name = predictions, expr = \"train_model(simulated_rbc_data)\"),\n  rxp_r(name = output_plot, expr = \"plot_predictions(predictions)\"),\n  ...\n)\nDependencies are inferred automatically.\n{rixpress}\nsees that\npredictions\nuses the\nsimulated_rbc_data\nobject and knows to run the Julia step first. It handles all the I/O for you as well. Objects get serialised and unserialised transparently for you.\nYour scientific code now lives in pure functions, free of any command-line parsing or file I/O. You can focus entirely on the analysis.\nThe final tally for the Nix-based approach?\nSix files\n, and four of them (\ngen-env.R\n,\ngen-pipeline.R\n, and the two\nfunctions\nfiles) are simple, clean declarations of what you need and what you want to do. The whole set up of the environment and execution of the pipeline takes 5 minutes on a standard GitHub Actions runner. That’s 2 minutes longer that the imperative approach, but I think it’s a small price to pay. Plus, you’re not setting up the environment from scratch each time you execute the pipeline, so subsequent executions will only take seconds.\nThe biggest difference isn’t just the simplicity; it’s the guarantee. The Docker approach gives you reproducibility\ntoday\n. But a year from now, if you rebuild the\nDockerfile\n, mutable base images and shifting package dependencies mean you might get a subtly different environment. The underlying base Docker image will change, and in some years, will completely stop functioning (Ubuntu 24.04, which is quite often used as the base image, will reach of end of life in 2029).\nThe Nix approach, by pinning everything to a specific date, gives you\ntemporal reproducibility\n. Your environment will build the exact same way today, next year, or five years from now, for as long as the\nnixpkgs\nGitHub repository will stay online (we can hope for a 1000 years if Microsoft doesn’t fuck it up). It’s a level of long-term stability that the traditional stack simply can’t match without a heroic amount of manual effort. But also, it’s just so much\nsimpler\n!\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nEconometrics and Free Software\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-396392 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Of course, someone has to write imperative code to build reproducible data science pipelines. It doesn’t have to be you.</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">October 28, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/econometrics-and-free-software/\">Econometrics and Free Software</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://b-rodrigues.github.io/posts/2025-10-29-imperative-vs-function.html\"> Econometrics and Free Software</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"text-align: center;\">\n<p>\n<a> <img data-lazy-src=\"https://i1.wp.com/b-rodrigues.github.io/assets/img/xkcd-nix.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" style=\"width: 100%; height: auto;\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/b-rodrigues.github.io/assets/img/xkcd-nix.png?w=578&amp;ssl=1\" style=\"width: 100%; height: auto;\"/></noscript> </a>\n</p>\n</div>\n<p><a href=\"https://brodrigues.co/posts/2025-10-23-rixpress_cran.html\" rel=\"nofollow\" target=\"_blank\">Last time</a> I quickly introduced my latest package, <code>{rixpress}</code>, but I think that to really understand what <code>{rixpress}</code> brings to the table, one needs to solve the same problem without it. And incidentally, I think that this exercise also show what makes Nix actually so good.</p>\n<p>The goal is to build a data science pipeline. The example here is purely illustrative, and compare a Nix-based approach to a non Nix-based approach. So, I built the same polyglot Real Business Cycle model pipeline twice. First, I did it without <code>{rixpress}</code> (nor <code>{rix}</code>), using a combination of Docker, Make, and a bunch of wrapper scripts. Then, I did it with <code>{rix}</code> and <code>{rixpress}</code>.</p>\n<p>Both pipelines produce the exact same result. But the way to get there is fundamentally different.</p>\n<section class=\"level2\" id=\"juggling-imperative-tools\">\n<h2 class=\"anchored\" data-anchor-id=\"juggling-imperative-tools\">Juggling imperative tools</h2>\n<p>Without Nix, you have to use language-specific package managers and tooling to first set up the environment. So for Python I’ve used <code>uv</code> (which is fantastic to be honest), then to install the right version of R I’ve used <code>rig</code> and a Posit CRAN snapshot for packages and for Julia I’ve simply downloaded a pre-compiled package of the version I needed, and used its built-in package manager to install specific versions of packages as well.</p>\n<p>Also, to deal with system level dependencies, I’ve bundled everything inside a Docker image. This is a sketch of the <code>Dockerfile</code>:</p>\n<pre># Add R repository and install specific version\nRUN apt-get update &amp;&amp; apt-get install -y software-properties-common\nRUN add-apt-repository ppa:...\nRUN curl -L https://rig.r-pkg.org/... | sh\nRUN rig add 4.5.1\n\n# Install Python with uv\nRUN curl -LsSf https://astral.sh/uv/install.sh | sh\nRUN uv python install 3.13\n\n# Download and extract Julia\nRUN curl -fsSL https://julialang-s3.julialang.org/... -o julia.tar.gz\nRUN tar -xzf julia.tar.gz -C /opt/\n\n# Install packages for each language separately\nRUN echo 'options(repos = c(CRAN = ...))' &gt; /root/.Rprofile\nRUN Rscript -e 'install.packages(...)'\n\n# Install Python packages using uv with specific versions for reproducibility.\nRUN echo \"pandas==2.3.3\" &gt; /tmp/requirements.txt &amp;&amp; \\\n    echo \"scikit-learn==1.7.2\" &gt;&gt; /tmp/requirements.txt &amp;&amp; \\\n    # ... more packages ...\n\nRUN uv pip install --no-cache -r /tmp/requirements.txt &amp;&amp; \\\n    rm /tmp/requirements.txt\n\n# Install specific versions of Julia packages for reproducibility\nRUN julia -e 'using Pkg; \\\n    Pkg.add(name=\"Arrow\", version=\"2.8.0\"); \\</pre>\n<p>This <em>traditional</em> approach feels like you’re a sysadmin first and a data scientist second. The <code>Dockerfile</code> is a long, step-by-step, imperative script of shell commands. You have to write <em>how</em> stuff needs to be installed, and this of course varies for each language. Each language needs its own special treatment, its own package installation command, and its own set of dependencies. For example, for Python, I actually even needed more configuration than what I’ve shown above:</p>\n<pre># Ensure the installed binary is on the `PATH`\nENV PATH=\"/root/.local/bin/:$PATH\"\n\n# Install the specified Python version using uv.\nRUN uv python install ${PYTHON_VERSION}\n\n# Setup default virtual env\nRUN uv venv /opt/venv\n# Use the virtual environment automatically\nENV VIRTUAL_ENV=/opt/venv\n# Place entry points in the environment at the front of the path\nENV PATH=\"/opt/venv/bin:$PATH\"</pre>\n<p>This is because I needed to set the virtual environment installed by <code>uv</code> as the one to be used by default. This is ok inside Docker, but that’s not something you’d likely want to do on a real machine. The final <code>Dockerfile</code> for our “simple” example was over 100 lines long (including comments).</p>\n<p>Now that the environment is said, we actually need to orchestrate the workflow. I’ve used <code>Make</code> for this, which means writing a <code>Makefile</code>. Honestly, nowadays, thanks to LLMs that’s not so much of an issue. But before LLMs, it would be quite annoying, because you need to manually define which file depends on which other file. Here’s what it looks like:</p>\n<pre># ==============================================================================\n# Makefile for the Polyglot RBC Model Pipeline\n# ==============================================================================\n\n# Define the interpreters for each language.\nJULIA := julia\nPYTHON := python\nRSCRIPT := Rscript\nQUARTO := quarto\n\n# Define directory variables for better organization.\nDATA_DIR := data\nPLOTS_DIR := plots\nREPORT_DIR := report\nFUNCTIONS_DIR := functions\n\n# Define the final and intermediate data files.\nSIMULATED_DATA := $(DATA_DIR)/simulated_rbc_data.arrow\nPREDICTIONS := $(DATA_DIR)/predictions.arrow\nFINAL_PLOT := $(PLOTS_DIR)/output_plot.png\nFINAL_REPORT := $(REPORT_DIR)/readme.html\n\n# --- Main Rules ---\n\n# The default 'all' rule now points to the final compiled HTML report.\nall: $(FINAL_REPORT)\n\n# Rule to render the final Quarto report.\n# Depends on the Quarto source file and the plot from the R step.\n$(FINAL_REPORT): readme.qmd $(FINAL_PLOT) | $(REPORT_DIR)\n    @echo \"--- [Quarto] Compiling final report ---\"\n    $(QUARTO) render $&lt; --to html --output-dir $(REPORT_DIR)\n\n... and so on ...</pre>\n<p>That’s another 65 lines for the orchestration.</p>\n<p>Finally, and probably worst of all, is that you end up writing tons of “glue code.” Because <code>make</code> just runs scripts, every step of your analysis (the Julia simulation, the Python training) needs to be wrapped in a script that does nothing but parse command-line arguments, read an input file, call your <em>actual</em> analysis function, and write an output file. That’s a lot of code just to get things talking to each other.</p>\n<p>The final tally for the traditional, imperative, approach? <strong>Nine separate files</strong> just to manage the environment and run the pipeline. It’s a fragile, complicated house of cards, but it takes only 3 minutes to run on a standard Ubuntu GitHub Actions runner.</p>\n</section>\n<section class=\"level2\" id=\"nix-declarative-simple-and-clean\">\n<h2 class=\"anchored\" data-anchor-id=\"nix-declarative-simple-and-clean\">Nix: Declarative, Simple, and Clean</h2>\n<p>Nix makes this whole process so much easier, it’s actually not even fair. Instead of telling the computer <em>how</em> to do everything, you just declare <em>what</em> you want. You describe your requirements, and Nix figures the rest out. But because Nix is not that easy to get into, I wrotk the <code>{rix}</code> and <code>{rixpress}</code> packages as high-level interfaces to Nix’s power.</p>\n<p>For example, to set up the environment, you just list the R, Python, and Julia packages you need, and <code>{rix}</code> handles everything else. It figures out how to install them, resolves all the system-level dependencies, and generates the complex Nix expression for you. You don’t need to be a sysadmin; you just need to know what packages your analysis requires. This is because all the <em>sysadminy</em> work was handled upstream by Nix package maintainers (real MVPs); Nix maintainers encode the build recipes, dependency graphs, and patches needed for each package, so you don’t have to. (Reminds me of this quote from <a href=\"https://speakerdeck.com/jennybc/row-oriented-workflows-in-r-with-the-tidyverse?slide=16\" rel=\"nofollow\" target=\"_blank\">Jenny Bryan</a>: <em>Of course, someone has to write for loops. It doesn’t have to be you</em>, but here it’s unglamorous Nix code to make packages work well instead of loops.)</p>\n<p>Here’s what the <code>gen-env.R</code> script looks like:</p>\n<pre>rix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"ggplot2\", \"dplyr\", \"arrow\"),\n  jl_conf = list(jl_version = \"lts\", ...),\n  py_conf = list(py_version = \"3.13\", ...),\n  ...\n)</pre>\n<p>Then, for the pipeline, it’s the same story. You just write what you need, not how it’s done. Nix can handle this. Here’s what the <code>gen-pipeline.R</code> script looks like:</p>\n<pre>// gen-pipeline.R - a small part\nlist(\n  rxp_jl(name = simulated_rbc_data, expr = \"simulate_rbc_model(...)\"),\n  rxp_py(name = predictions, expr = \"train_model(simulated_rbc_data)\"),\n  rxp_r(name = output_plot, expr = \"plot_predictions(predictions)\"),\n  ...\n)</pre>\n<p>Dependencies are inferred automatically. <code>{rixpress}</code> sees that <code>predictions</code> uses the <code>simulated_rbc_data</code> object and knows to run the Julia step first. It handles all the I/O for you as well. Objects get serialised and unserialised transparently for you.</p>\n<p>Your scientific code now lives in pure functions, free of any command-line parsing or file I/O. You can focus entirely on the analysis.</p>\n<p>The final tally for the Nix-based approach? <strong>Six files</strong>, and four of them (<code>gen-env.R</code>, <code>gen-pipeline.R</code>, and the two <code>functions</code> files) are simple, clean declarations of what you need and what you want to do. The whole set up of the environment and execution of the pipeline takes 5 minutes on a standard GitHub Actions runner. That’s 2 minutes longer that the imperative approach, but I think it’s a small price to pay. Plus, you’re not setting up the environment from scratch each time you execute the pipeline, so subsequent executions will only take seconds.</p>\n<p>The biggest difference isn’t just the simplicity; it’s the guarantee. The Docker approach gives you reproducibility <em>today</em>. But a year from now, if you rebuild the <code>Dockerfile</code>, mutable base images and shifting package dependencies mean you might get a subtly different environment. The underlying base Docker image will change, and in some years, will completely stop functioning (Ubuntu 24.04, which is quite often used as the base image, will reach of end of life in 2029).</p>\n<p>The Nix approach, by pinning everything to a specific date, gives you <strong>temporal reproducibility</strong>. Your environment will build the exact same way today, next year, or five years from now, for as long as the <code>nixpkgs</code> GitHub repository will stay online (we can hope for a 1000 years if Microsoft doesn’t fuck it up). It’s a level of long-term stability that the traditional stack simply can’t match without a heroic amount of manual effort. But also, it’s just so much <em>simpler</em>!</p>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://b-rodrigues.github.io/posts/2025-10-29-imperative-vs-function.html\"> Econometrics and Free Software</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 1685,
    "reading_time_min": 8.4,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/econometrics-and-free-software/",
        "text": "Econometrics and Free Software"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://b-rodrigues.github.io/posts/2025-10-29-imperative-vs-function.html",
        "text": "Econometrics and Free Software"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://brodrigues.co/posts/2025-10-23-rixpress_cran.html",
        "text": "Last time"
      },
      {
        "href": "https://speakerdeck.com/jennybc/row-oriented-workflows-in-r-with-the-tidyverse?slide=16",
        "text": "Jenny Bryan"
      },
      {
        "href": "https://b-rodrigues.github.io/posts/2025-10-29-imperative-vs-function.html",
        "text": "Econometrics and Free Software"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [
      {
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif",
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"
      },
      {
        "src": "https://i1.wp.com/b-rodrigues.github.io/assets/img/xkcd-nix.png?w=578&ssl=1",
        "alt": null,
        "base64": null
      }
    ],
    "lang": "en-US",
    "crawled_at_utc": "2026-01-04T05:36:12Z"
  }
}