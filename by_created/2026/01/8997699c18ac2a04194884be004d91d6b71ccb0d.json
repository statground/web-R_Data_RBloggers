{
  "id": "8997699c18ac2a04194884be004d91d6b71ccb0d",
  "url": "https://www.r-bloggers.com/2026/01/rtopy-an-r-to-python-bridge-novelties/",
  "created_at_utc": "2026-01-08T18:26:41Z",
  "crawled_at_utc": "2026-01-08T18:26:41Z",
  "html_title": "rtopy: an R to Python bridge — novelties | R-bloggers",
  "meta_description": "rtopy: an R to Python bridge -- novelties",
  "data": {
    "url": "https://www.r-bloggers.com/2026/01/rtopy-an-r-to-python-bridge-novelties/",
    "canonical_url": "https://www.r-bloggers.com/2026/01/rtopy-an-r-to-python-bridge-novelties/",
    "html_title": "rtopy: an R to Python bridge — novelties | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "rtopy: an R to Python bridge -- novelties",
    "meta_keywords": null,
    "og_title": "rtopy: an R to Python bridge — novelties | R-bloggers",
    "og_description": "rtopy: an R to Python bridge -- novelties",
    "og_image": "https://thierrymoudiki.github.io/images/2026-01-08/2026-01-08-rtopy_4_0.png",
    "twitter_title": "rtopy: an R to Python bridge — novelties | R-bloggers",
    "twitter_description": "rtopy: an R to Python bridge -- novelties",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "rtopy: an R to Python bridge — novelties\nPosted on\nJanuary 7, 2026\nby\nT. Moudiki\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nT. Moudiki's Webpage - R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIn this post, I present the novelties of python package\nrtopy\n; a package allowing (whose ultimate objective is to) translate R to Python without much hassle. The intro is still available in available in\nhttps://thierrymoudiki.github.io/blog/2024/03/04/python/r/rtopyintro\n.\nThe novelties mainly concern the\nRBridge\nclass and the\ncall_r\nfunction. The\nRBridge\nclass is more about persistency, while the\ncall_r\nfunction is more about ease of use.\nSee for yourself in the following – hopefully comprehensive – examples (classification, regression, time series, hypothesis testing).\ncontents\nInstallation\nRBridge class\ncall_r function\nAdvanced RBridge Usage Examples\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n%%R\n\ninstall.packages(\"pak\")\npak::pak(c(\"e1071\", \"forecast\", \"randomForest\"))\n\nlibrary(jsonlite)\n\n!pip install rtopy\n\n\"\"\"\nAdvanced RBridge Usage Examples\n================================\n\nDemonstrates using R packages, statistical modeling, and data processing\nthrough the Python-R bridge `rtopy`.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom rtopy import RBridge, call_r\n\n# ============================================================================\n# Example 1: Support Vector Machine with e1071\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Example 1: SVM Classification with e1071\")\nprint(\"=\" * 70)\n\n# Generate training data\nnp.random.seed(42)\nn_samples = 100\n\n# Class 0: centered at (-1, -1)\nX0 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([-1, -1])\n# Class 1: centered at (1, 1)\nX1 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([1, 1])\n\nX_train = np.vstack([X0, X1])\ny_train = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n\n# Create R code for SVM training and prediction\nsvm_code = '''\nlibrary(e1071)\n\ntrain_svm <- function(X, y, kernel_type = \"radial\") {\n    # Convert to data frame\n    df <- data.frame(\n        x1 = X[, 1],\n        x2 = X[, 2],\n        y = as.factor(y)\n    )\n\n    # Train SVM\n    model <- e1071::svm(y ~ x1 + x2, data = df, kernel = kernel_type, cost = 1)\n\n    # Make predictions on training data\n    predictions <- predict(model, df)\n\n    # Calculate accuracy\n    accuracy <- mean(predictions == df$y)\n\n    # Return results\n    list(\n        predictions = as.numeric(as.character(predictions)),\n        accuracy = accuracy,\n        n_support = model$tot.nSV\n    )\n}\n'''\n\nrb = RBridge(verbose=True)\nresult = rb.call(\n    svm_code,\n    \"train_svm\",\n    return_type=\"dict\",\n    X=X_train,\n    y=y_train,\n    kernel_type=\"radial\"\n)\n\nprint(f\"Training Accuracy: {result['accuracy']:.2%}\")\nprint(f\"Number of Support Vectors: {result['n_support']}\")\nprint(f\"Sample Predictions: {result['predictions'][:10]}\")\n\n# ============================================================================\n# Example 2: Time Series Analysis with forecast package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 2: Time Series Forecasting with forecast\")\nprint(\"=\" * 70)\n\n# Generate time series data\ntime_series = np.sin(np.linspace(0, 4*np.pi, 50)) + np.random.randn(50) * 0.1\n\nts_code = '''\nlibrary(forecast)\n\nforecast_ts <- function(x, h = 10) {\n    # Convert to time series object\n    ts_data <- ts(x, frequency = 12)\n\n    # Fit ARIMA model\n    fit <- auto.arima(ts_data, seasonal = FALSE)\n\n    # Generate forecast\n    fc <- forecast(fit, h = h)\n\n    # Return results\n    list(\n        forecast_mean = as.numeric(fc$mean),\n        forecast_lower = as.numeric(fc$lower[, 2]),  # 95% CI\n        forecast_upper = as.numeric(fc$upper[, 2]),\n        model_aic = fit$aic,\n        model_order = paste0(\"ARIMA(\",\n                            paste(arimaorder(fit), collapse = \",\"),\n                            \")\")\n    )\n}\n'''\n\nresult = rb.call(\n    ts_code,\n    \"forecast_ts\",\n    return_type=\"dict\",\n    x=time_series.tolist(),\n    h=10\n)\n\nprint(f\"Model: {result['model_order']}\")\nprint(f\"AIC: {result['model_aic']:.2f}\")\nprint(f\"5-step forecast: {np.array(result['forecast_mean'])[:5]}...\")\n\n# ============================================================================\n# Example 3: Random Forest with randomForest package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 3: Random Forest Regression\")\nprint(\"=\" * 70)\n\n# Generate regression data\nnp.random.seed(123)\nX = np.random.rand(200, 3) * 10\ny = 2*X[:, 0] + 3*X[:, 1] - X[:, 2] + np.random.randn(200) * 2\n\nrf_code = '''\nlibrary(randomForest)\n\ntrain_rf <- function(X, y, ntree = 500) {\n    # Create data frame\n    df <- data.frame(\n        x1 = X[, 1],\n        x2 = X[, 2],\n        x3 = X[, 3],\n        y = y\n    )\n\n    # Train random forest\n    rf_model <- randomForest(y ~ ., data = df, ntree = ntree, importance = TRUE)\n\n    # Get predictions\n    predictions <- predict(rf_model, df)\n\n    # Calculate R-squared\n    r_squared <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)\n\n    # Get feature importance\n    importance_scores <- importance(rf_model)[, 1]  # %IncMSE\n\n    list(\n        r_squared = r_squared,\n        mse = rf_model$mse[ntree],\n        predictions = predictions,\n        importance = importance_scores\n    )\n}\n'''\n\nresult = rb.call(\n    rf_code,\n    \"train_rf\",\n    return_type=\"dict\",\n    X=X,\n    y=y.tolist(),\n    ntree=500\n)\n\nprint(f\"R-squared: {result['r_squared']:.3f}\")\nprint(f\"MSE: {result['mse']:.3f}\")\nprint(f\"Feature Importance: {result['importance']}\")\n\n# ============================================================================\n# Example 4: Statistical Tests with stats package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 4: Statistical Hypothesis Testing\")\nprint(\"=\" * 70)\n\n# Generate two samples\ngroup1 = np.random.normal(5, 2, 50)\ngroup2 = np.random.normal(6, 2, 50)\n\nstats_code = '''\nperform_tests <- function(group1, group2) {\n    # T-test\n    t_result <- t.test(group1, group2)\n\n    # Wilcoxon test (non-parametric alternative)\n    w_result <- wilcox.test(group1, group2)\n\n    # Kolmogorov-Smirnov test\n    ks_result <- ks.test(group1, group2)\n\n    list(\n        t_test = list(\n            statistic = t_result$statistic,\n            p_value = t_result$p.value,\n            conf_int = t_result$conf.int\n        ),\n        wilcox_test = list(\n            statistic = w_result$statistic,\n            p_value = w_result$p.value\n        ),\n        ks_test = list(\n            statistic = ks_result$statistic,\n            p_value = ks_result$p.value\n        ),\n        summary_stats = list(\n            group1_mean = mean(group1),\n            group2_mean = mean(group2),\n            group1_sd = sd(group1),\n            group2_sd = sd(group2)\n        )\n    )\n}\n'''\n\nresult = rb.call(\n    stats_code,\n    \"perform_tests\",\n    return_type=\"dict\",\n    group1=group1.tolist(),\n    group2=group2.tolist()\n)\n\nprint(f\"Group 1 Mean: {result['summary_stats']['group1_mean']:.2f} ± {result['summary_stats']['group1_sd']:.2f}\")\nprint(f\"Group 2 Mean: {result['summary_stats']['group2_mean']:.2f} ± {result['summary_stats']['group2_sd']:.2f}\")\nprint(f\"\\nT-test p-value: {result['t_test']['p_value']:.4f}\")\nprint(f\"Wilcoxon p-value: {result['wilcox_test']['p_value']:.4f}\")\n\n# ============================================================================\n# Example 5: Data Transformation with dplyr\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 5: Data Wrangling with dplyr\")\nprint(\"=\" * 70)\n\n# Create sample dataset\ndata = pd.DataFrame({\n    'id': range(1, 101),\n    'group': np.random.choice(['A', 'B', 'C'], 100),\n    'value': np.random.randn(100) * 10 + 50,\n    'score': np.random.randint(1, 101, 100)\n})\n\ndplyr_code = '''\nlibrary(dplyr)\n\nprocess_data <- function(df) {\n    # Convert list columns to data frame\n    data <- as.data.frame(df)\n\n    # Perform dplyr operations\n    result <- data %>%\n        filter(score > 50) %>%\n        group_by(group) %>%\n        summarise(\n            n = n(),\n            mean_value = mean(value),\n            median_score = median(score),\n            sd_value = sd(value)\n        ) %>%\n        arrange(desc(mean_value))\n\n    # Convert back to list format for JSON\n    as.list(result)\n}\n'''\n\nresult = rb.call(\n    dplyr_code,\n    \"process_data\",\n    return_type=\"pandas\",\n    df=data\n)\n\nprint(\"\\nGrouped Summary Statistics:\")\nprint(result)\n\n# ============================================================================\n# Example 6: Clustering with cluster package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 6: K-means and Hierarchical Clustering\")\nprint(\"=\" * 70)\n\n# Generate clustered data\nnp.random.seed(42)\ncluster_data = np.vstack([\n    np.random.randn(30, 2) * 0.5 + np.array([0, 0]),\n    np.random.randn(30, 2) * 0.5 + np.array([3, 3]),\n    np.random.randn(30, 2) * 0.5 + np.array([0, 3])\n])\n\ncluster_code = '''\nlibrary(cluster)\n\nperform_clustering <- function(X, k = 3) {\n    # Convert to matrix\n    data_matrix <- as.matrix(X)\n\n    # K-means clustering\n    kmeans_result <- kmeans(data_matrix, centers = k, nstart = 25)\n\n    # Hierarchical clustering\n    dist_matrix <- dist(data_matrix)\n    hc <- hclust(dist_matrix, method = \"ward.D2\")\n    hc_clusters <- cutree(hc, k = k)\n\n    # Silhouette analysis for k-means\n    sil <- silhouette(kmeans_result$cluster, dist_matrix)\n    avg_silhouette <- mean(sil[, 3])\n\n    list(\n        kmeans_clusters = kmeans_result$cluster,\n        kmeans_centers = kmeans_result$centers,\n        kmeans_withinss = kmeans_result$tot.withinss,\n        hc_clusters = hc_clusters,\n        silhouette_score = avg_silhouette\n    )\n}\n'''\n\nresult = rb.call(\n    cluster_code,\n    \"perform_clustering\",\n    return_type=\"dict\",\n    X=cluster_data,\n    k=3\n)\n\nprint(f\"K-means Within-cluster SS: {result['kmeans_withinss']:.2f}\")\nprint(f\"Average Silhouette Score: {result['silhouette_score']:.3f}\")\nprint(f\"\\nCluster Centers:\\n{np.array(result['kmeans_centers'])}\")\nprint(f\"\\nCluster sizes: {np.bincount(result['kmeans_clusters'])}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"All examples completed successfully!\")\nprint(\"=\" * 70)\n\n======================================================================\nExample 1: SVM Classification with e1071\n======================================================================\nTraining Accuracy: 100.00%\nNumber of Support Vectors: 9\nSample Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n======================================================================\nExample 2: Time Series Forecasting with forecast\n======================================================================\nModel: ARIMA(3,1,0)\nAIC: -10.21\n5-step forecast: [0.29557391 0.4948255  0.64553023 0.80823028 0.93656539]...\n\n======================================================================\nExample 3: Random Forest Regression\n======================================================================\nR-squared: 0.972\nMSE: 11.996\nFeature Importance: [62.57255479535195, 86.55470841243113, 21.4933655703039]\n\n======================================================================\nExample 4: Statistical Hypothesis Testing\n======================================================================\nGroup 1 Mean: 5.33 ± 2.06\nGroup 2 Mean: 5.37 ± 2.28\n\nT-test p-value: 0.9381\nWilcoxon p-value: 0.8876\n\n======================================================================\nExample 5: Data Wrangling with dplyr\n======================================================================\n\nGrouped Summary Statistics:\n  group   n  mean_value  median_score   sd_value\n0     C  23   49.711861            76  11.367167\n1     A  14   49.219788            74   9.744709\n2     B  23   47.459312            80  10.126835\n\n======================================================================\nExample 6: K-means and Hierarchical Clustering\n======================================================================\nK-means Within-cluster SS: 39.38\nAverage Silhouette Score: 0.713\n\nCluster Centers:\n[[-0.03545142  3.12736567]\n [ 2.9470395   3.04927708]\n [-0.07207628 -0.0825784 ]]\n\nCluster sizes: [ 0 30 30 30]\n\n======================================================================\nAll examples completed successfully!\n======================================================================\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a style for better aesthetics\nsns.set_style(\"whitegrid\")\n\n# Create a scatter plot of the clustered data\nplt.figure(figsize=(10, 7))\nsns.scatterplot(\n    x=cluster_data[:, 0],\n    y=cluster_data[:, 1],\n    hue=result['kmeans_clusters'],\n    palette='viridis',\n    s=100, # size of points\n    alpha=0.8, # transparency\n    legend='full'\n)\n\n# Plot the cluster centers\ncenters = np.array(result['kmeans_centers'])\nplt.scatter(\n    centers[:, 0],\n    centers[:, 1],\n    marker='X',\n    s=200, # size of centers\n    color='red',\n    edgecolors='black',\n    label='Cluster Centers'\n)\n\nplt.title('K-means Clustering of Generated Data')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True)\nplt.show()\nfrom rtopy import RBridge, call_r\n\n# ============================================================================\n# Optional: SVM Classification (High vs Low Price)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Optional: SVM Classification on Boston\")\nprint(\"=\" * 70)\n\nsvm_boston_class_code = '''\nlibrary(MASS)\nlibrary(e1071)\n\ntrain_boston_svm_class <- function(kernel_type = \"radial\", cost = 1) {\n\n    data(Boston)\n\n    # Binary target: expensive vs cheap housing\n    Boston$high_medv <- as.factor(ifelse(Boston$medv >\n                                         median(Boston$medv), 1, 0))\n\n    model <- svm(\n        high_medv ~ . - medv,\n        data = Boston,\n        kernel = kernel_type,\n        cost = cost,\n        scale = TRUE\n    )\n\n    preds <- predict(model, Boston)\n\n    accuracy <- mean(preds == Boston$high_medv)\n\n    list(\n        accuracy = accuracy,\n        n_support = model$tot.nSV,\n        confusion = table(\n            predicted = preds,\n            actual = Boston$high_medv\n        )\n    )\n}\n'''\n\nresult = rb.call(\n    svm_boston_class_code,\n    \"train_boston_svm_class\",\n    return_type=\"dict\",\n    kernel_type=\"radial\",\n    cost=1\n)\n\nprint(f\"Classification Accuracy: {result['accuracy']:.2%}\")\nprint(f\"Number of Support Vectors: {result['n_support']}\")\nprint(\"Confusion Matrix:\")\nprint(result[\"confusion\"])\n\n======================================================================\nOptional: SVM Classification on Boston\n======================================================================\nClassification Accuracy: 90.51%\nNumber of Support Vectors: 209\nConfusion Matrix:\n[[237, 29], [19, 221]]\n\n# ============================================================================\n# Optional: SVM Classification (High vs Low Price)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Optional: SVM Classification on Boston\")\nprint(\"=\" * 70)\n\nsvm_boston_class_code = '''\nlibrary(MASS)\nlibrary(e1071)\n\ntrain_boston_svm_class <- function(kernel_type = \"radial\", cost = 1) {\n\n    data(Boston)\n\n    # Binary target: expensive vs cheap housing\n    Boston$high_medv <- as.factor(ifelse(Boston$medv >\n                                         median(Boston$medv), 1, 0))\n\n    model <- svm(\n        high_medv ~ . - medv,\n        data = Boston,\n        kernel = kernel_type,\n        cost = cost,\n        scale = TRUE\n    )\n\n    preds <- predict(model, Boston)\n\n    accuracy <- mean(preds == Boston$high_medv)\n\n    list(\n        accuracy = accuracy,\n        n_support = model$tot.nSV,\n        confusion = table(\n            predicted = preds,\n            actual = Boston$high_medv\n        )\n    )\n}\n'''\n\nresult = rb.call(\n    svm_boston_class_code,\n    \"train_boston_svm_class\",\n    return_type=\"dict\",\n    kernel_type=\"radial\",\n    cost=1\n)\n\nprint(f\"Classification Accuracy: {result['accuracy']:.2%}\")\nprint(f\"Number of Support Vectors: {result['n_support']}\")\nprint(\"Confusion Matrix:\")\nprint(result[\"confusion\"])\n\n======================================================================\nOptional: SVM Classification on Boston\n======================================================================\nClassification Accuracy: 90.51%\nNumber of Support Vectors: 209\nConfusion Matrix:\n[[237, 29], [19, 221]]\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nT. Moudiki's Webpage - R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-398159 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">rtopy: an R to Python bridge — novelties</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 7, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/t-moudiki/\">T. Moudiki</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://thierrymoudiki.github.io//blog/2026/01/08/r/python/rtopy\"> T. Moudiki's Webpage - R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>In this post, I present the novelties of python package <code>rtopy</code>; a package allowing (whose ultimate objective is to) translate R to Python without much hassle. The intro is still available in available in <a href=\"https://thierrymoudiki.github.io/blog/2024/03/04/python/r/rtopyintro\" rel=\"nofollow\" target=\"_blank\">https://thierrymoudiki.github.io/blog/2024/03/04/python/r/rtopyintro</a>.</p>\n<p>The novelties mainly concern the <code>RBridge</code> class and the <code>call_r</code> function. The <code>RBridge</code> class is more about persistency, while the <code>call_r</code> function is more about ease of use.</p>\n<p>See for yourself in the following – hopefully comprehensive – examples (classification, regression, time series, hypothesis testing).</p>\n<h2 id=\"contents\">contents</h2>\n<ol>\n<li>Installation</li>\n<li>RBridge class</li>\n<li>call_r function</li>\n<li>Advanced RBridge Usage Examples</li>\n</ol>\n<pre> %load_ext rpy2.ipython\n\r\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\r\n%%R\n\ninstall.packages(\"pak\")\npak::pak(c(\"e1071\", \"forecast\", \"randomForest\"))\n\nlibrary(jsonlite)\n\r\n!pip install rtopy\n\r\n\"\"\"\nAdvanced RBridge Usage Examples\n================================\n\nDemonstrates using R packages, statistical modeling, and data processing\nthrough the Python-R bridge `rtopy`.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom rtopy import RBridge, call_r\n\n\n# ============================================================================\n# Example 1: Support Vector Machine with e1071\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Example 1: SVM Classification with e1071\")\nprint(\"=\" * 70)\n\n# Generate training data\nnp.random.seed(42)\nn_samples = 100\n\n# Class 0: centered at (-1, -1)\nX0 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([-1, -1])\n# Class 1: centered at (1, 1)\nX1 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([1, 1])\n\nX_train = np.vstack([X0, X1])\ny_train = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n\n# Create R code for SVM training and prediction\nsvm_code = '''\nlibrary(e1071)\n\ntrain_svm &lt;- function(X, y, kernel_type = \"radial\") {\n    # Convert to data frame\n    df &lt;- data.frame(\n        x1 = X[, 1],\n        x2 = X[, 2],\n        y = as.factor(y)\n    )\n\n    # Train SVM\n    model &lt;- e1071::svm(y ~ x1 + x2, data = df, kernel = kernel_type, cost = 1)\n\n    # Make predictions on training data\n    predictions &lt;- predict(model, df)\n\n    # Calculate accuracy\n    accuracy &lt;- mean(predictions == df$y)\n\n    # Return results\n    list(\n        predictions = as.numeric(as.character(predictions)),\n        accuracy = accuracy,\n        n_support = model$tot.nSV\n    )\n}\n'''\n\nrb = RBridge(verbose=True)\nresult = rb.call(\n    svm_code,\n    \"train_svm\",\n    return_type=\"dict\",\n    X=X_train,\n    y=y_train,\n    kernel_type=\"radial\"\n)\n\nprint(f\"Training Accuracy: {result['accuracy']:.2%}\")\nprint(f\"Number of Support Vectors: {result['n_support']}\")\nprint(f\"Sample Predictions: {result['predictions'][:10]}\")\n\n\n# ============================================================================\n# Example 2: Time Series Analysis with forecast package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 2: Time Series Forecasting with forecast\")\nprint(\"=\" * 70)\n\n# Generate time series data\ntime_series = np.sin(np.linspace(0, 4*np.pi, 50)) + np.random.randn(50) * 0.1\n\nts_code = '''\nlibrary(forecast)\n\nforecast_ts &lt;- function(x, h = 10) {\n    # Convert to time series object\n    ts_data &lt;- ts(x, frequency = 12)\n\n    # Fit ARIMA model\n    fit &lt;- auto.arima(ts_data, seasonal = FALSE)\n\n    # Generate forecast\n    fc &lt;- forecast(fit, h = h)\n\n    # Return results\n    list(\n        forecast_mean = as.numeric(fc$mean),\n        forecast_lower = as.numeric(fc$lower[, 2]),  # 95% CI\n        forecast_upper = as.numeric(fc$upper[, 2]),\n        model_aic = fit$aic,\n        model_order = paste0(\"ARIMA(\",\n                            paste(arimaorder(fit), collapse = \",\"),\n                            \")\")\n    )\n}\n'''\n\nresult = rb.call(\n    ts_code,\n    \"forecast_ts\",\n    return_type=\"dict\",\n    x=time_series.tolist(),\n    h=10\n)\n\nprint(f\"Model: {result['model_order']}\")\nprint(f\"AIC: {result['model_aic']:.2f}\")\nprint(f\"5-step forecast: {np.array(result['forecast_mean'])[:5]}...\")\n\n\n# ============================================================================\n# Example 3: Random Forest with randomForest package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 3: Random Forest Regression\")\nprint(\"=\" * 70)\n\n# Generate regression data\nnp.random.seed(123)\nX = np.random.rand(200, 3) * 10\ny = 2*X[:, 0] + 3*X[:, 1] - X[:, 2] + np.random.randn(200) * 2\n\nrf_code = '''\nlibrary(randomForest)\n\ntrain_rf &lt;- function(X, y, ntree = 500) {\n    # Create data frame\n    df &lt;- data.frame(\n        x1 = X[, 1],\n        x2 = X[, 2],\n        x3 = X[, 3],\n        y = y\n    )\n\n    # Train random forest\n    rf_model &lt;- randomForest(y ~ ., data = df, ntree = ntree, importance = TRUE)\n\n    # Get predictions\n    predictions &lt;- predict(rf_model, df)\n\n    # Calculate R-squared\n    r_squared &lt;- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)\n\n    # Get feature importance\n    importance_scores &lt;- importance(rf_model)[, 1]  # %IncMSE\n\n    list(\n        r_squared = r_squared,\n        mse = rf_model$mse[ntree],\n        predictions = predictions,\n        importance = importance_scores\n    )\n}\n'''\n\nresult = rb.call(\n    rf_code,\n    \"train_rf\",\n    return_type=\"dict\",\n    X=X,\n    y=y.tolist(),\n    ntree=500\n)\n\nprint(f\"R-squared: {result['r_squared']:.3f}\")\nprint(f\"MSE: {result['mse']:.3f}\")\nprint(f\"Feature Importance: {result['importance']}\")\n\n\n# ============================================================================\n# Example 4: Statistical Tests with stats package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 4: Statistical Hypothesis Testing\")\nprint(\"=\" * 70)\n\n# Generate two samples\ngroup1 = np.random.normal(5, 2, 50)\ngroup2 = np.random.normal(6, 2, 50)\n\nstats_code = '''\nperform_tests &lt;- function(group1, group2) {\n    # T-test\n    t_result &lt;- t.test(group1, group2)\n\n    # Wilcoxon test (non-parametric alternative)\n    w_result &lt;- wilcox.test(group1, group2)\n\n    # Kolmogorov-Smirnov test\n    ks_result &lt;- ks.test(group1, group2)\n\n    list(\n        t_test = list(\n            statistic = t_result$statistic,\n            p_value = t_result$p.value,\n            conf_int = t_result$conf.int\n        ),\n        wilcox_test = list(\n            statistic = w_result$statistic,\n            p_value = w_result$p.value\n        ),\n        ks_test = list(\n            statistic = ks_result$statistic,\n            p_value = ks_result$p.value\n        ),\n        summary_stats = list(\n            group1_mean = mean(group1),\n            group2_mean = mean(group2),\n            group1_sd = sd(group1),\n            group2_sd = sd(group2)\n        )\n    )\n}\n'''\n\nresult = rb.call(\n    stats_code,\n    \"perform_tests\",\n    return_type=\"dict\",\n    group1=group1.tolist(),\n    group2=group2.tolist()\n)\n\nprint(f\"Group 1 Mean: {result['summary_stats']['group1_mean']:.2f} ± {result['summary_stats']['group1_sd']:.2f}\")\nprint(f\"Group 2 Mean: {result['summary_stats']['group2_mean']:.2f} ± {result['summary_stats']['group2_sd']:.2f}\")\nprint(f\"\\nT-test p-value: {result['t_test']['p_value']:.4f}\")\nprint(f\"Wilcoxon p-value: {result['wilcox_test']['p_value']:.4f}\")\n\n\n# ============================================================================\n# Example 5: Data Transformation with dplyr\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 5: Data Wrangling with dplyr\")\nprint(\"=\" * 70)\n\n# Create sample dataset\ndata = pd.DataFrame({\n    'id': range(1, 101),\n    'group': np.random.choice(['A', 'B', 'C'], 100),\n    'value': np.random.randn(100) * 10 + 50,\n    'score': np.random.randint(1, 101, 100)\n})\n\ndplyr_code = '''\nlibrary(dplyr)\n\nprocess_data &lt;- function(df) {\n    # Convert list columns to data frame\n    data &lt;- as.data.frame(df)\n\n    # Perform dplyr operations\n    result &lt;- data %&gt;%\n        filter(score &gt; 50) %&gt;%\n        group_by(group) %&gt;%\n        summarise(\n            n = n(),\n            mean_value = mean(value),\n            median_score = median(score),\n            sd_value = sd(value)\n        ) %&gt;%\n        arrange(desc(mean_value))\n\n    # Convert back to list format for JSON\n    as.list(result)\n}\n'''\n\nresult = rb.call(\n    dplyr_code,\n    \"process_data\",\n    return_type=\"pandas\",\n    df=data\n)\n\nprint(\"\\nGrouped Summary Statistics:\")\nprint(result)\n\n\n# ============================================================================\n# Example 6: Clustering with cluster package\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Example 6: K-means and Hierarchical Clustering\")\nprint(\"=\" * 70)\n\n# Generate clustered data\nnp.random.seed(42)\ncluster_data = np.vstack([\n    np.random.randn(30, 2) * 0.5 + np.array([0, 0]),\n    np.random.randn(30, 2) * 0.5 + np.array([3, 3]),\n    np.random.randn(30, 2) * 0.5 + np.array([0, 3])\n])\n\ncluster_code = '''\nlibrary(cluster)\n\nperform_clustering &lt;- function(X, k = 3) {\n    # Convert to matrix\n    data_matrix &lt;- as.matrix(X)\n\n    # K-means clustering\n    kmeans_result &lt;- kmeans(data_matrix, centers = k, nstart = 25)\n\n    # Hierarchical clustering\n    dist_matrix &lt;- dist(data_matrix)\n    hc &lt;- hclust(dist_matrix, method = \"ward.D2\")\n    hc_clusters &lt;- cutree(hc, k = k)\n\n    # Silhouette analysis for k-means\n    sil &lt;- silhouette(kmeans_result$cluster, dist_matrix)\n    avg_silhouette &lt;- mean(sil[, 3])\n\n    list(\n        kmeans_clusters = kmeans_result$cluster,\n        kmeans_centers = kmeans_result$centers,\n        kmeans_withinss = kmeans_result$tot.withinss,\n        hc_clusters = hc_clusters,\n        silhouette_score = avg_silhouette\n    )\n}\n'''\n\nresult = rb.call(\n    cluster_code,\n    \"perform_clustering\",\n    return_type=\"dict\",\n    X=cluster_data,\n    k=3\n)\n\nprint(f\"K-means Within-cluster SS: {result['kmeans_withinss']:.2f}\")\nprint(f\"Average Silhouette Score: {result['silhouette_score']:.3f}\")\nprint(f\"\\nCluster Centers:\\n{np.array(result['kmeans_centers'])}\")\nprint(f\"\\nCluster sizes: {np.bincount(result['kmeans_clusters'])}\")\n\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"All examples completed successfully!\")\nprint(\"=\" * 70)\n\r\n======================================================================\nExample 1: SVM Classification with e1071\n======================================================================\nTraining Accuracy: 100.00%\nNumber of Support Vectors: 9\nSample Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n======================================================================\nExample 2: Time Series Forecasting with forecast\n======================================================================\nModel: ARIMA(3,1,0)\nAIC: -10.21\n5-step forecast: [0.29557391 0.4948255  0.64553023 0.80823028 0.93656539]...\n\n======================================================================\nExample 3: Random Forest Regression\n======================================================================\nR-squared: 0.972\nMSE: 11.996\nFeature Importance: [62.57255479535195, 86.55470841243113, 21.4933655703039]\n\n======================================================================\nExample 4: Statistical Hypothesis Testing\n======================================================================\nGroup 1 Mean: 5.33 ± 2.06\nGroup 2 Mean: 5.37 ± 2.28\n\nT-test p-value: 0.9381\nWilcoxon p-value: 0.8876\n\n======================================================================\nExample 5: Data Wrangling with dplyr\n======================================================================\n\nGrouped Summary Statistics:\n  group   n  mean_value  median_score   sd_value\n0     C  23   49.711861            76  11.367167\n1     A  14   49.219788            74   9.744709\n2     B  23   47.459312            80  10.126835\n\n======================================================================\nExample 6: K-means and Hierarchical Clustering\n======================================================================\nK-means Within-cluster SS: 39.38\nAverage Silhouette Score: 0.713\n\nCluster Centers:\n[[-0.03545142  3.12736567]\n [ 2.9470395   3.04927708]\n [-0.07207628 -0.0825784 ]]\n\nCluster sizes: [ 0 30 30 30]\n\n======================================================================\nAll examples completed successfully!\n======================================================================\n\r\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a style for better aesthetics\nsns.set_style(\"whitegrid\")\n\n# Create a scatter plot of the clustered data\nplt.figure(figsize=(10, 7))\nsns.scatterplot(\n    x=cluster_data[:, 0],\n    y=cluster_data[:, 1],\n    hue=result['kmeans_clusters'],\n    palette='viridis',\n    s=100, # size of points\n    alpha=0.8, # transparency\n    legend='full'\n)\n\n# Plot the cluster centers\ncenters = np.array(result['kmeans_centers'])\nplt.scatter(\n    centers[:, 0],\n    centers[:, 1],\n    marker='X',\n    s=200, # size of centers\n    color='red',\n    edgecolors='black',\n    label='Cluster Centers'\n)\n\nplt.title('K-means Clustering of Generated Data')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre>\n<p><img alt=\"image-title-here\" class=\"img-responsive\" data-lazy-src=\"https://i0.wp.com/thierrymoudiki.github.io/images/2026-01-08/2026-01-08-rtopy_4_0.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"image-title-here\" class=\"img-responsive\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/thierrymoudiki.github.io/images/2026-01-08/2026-01-08-rtopy_4_0.png?w=578&amp;ssl=1\"/></noscript></p>\n<pre>from rtopy import RBridge, call_r\n\n# ============================================================================\n# Optional: SVM Classification (High vs Low Price)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Optional: SVM Classification on Boston\")\nprint(\"=\" * 70)\n\nsvm_boston_class_code = '''\nlibrary(MASS)\nlibrary(e1071)\n\ntrain_boston_svm_class &lt;- function(kernel_type = \"radial\", cost = 1) {\n\n    data(Boston)\n\n    # Binary target: expensive vs cheap housing\n    Boston$high_medv &lt;- as.factor(ifelse(Boston$medv &gt;\n                                         median(Boston$medv), 1, 0))\n\n    model &lt;- svm(\n        high_medv ~ . - medv,\n        data = Boston,\n        kernel = kernel_type,\n        cost = cost,\n        scale = TRUE\n    )\n\n    preds &lt;- predict(model, Boston)\n\n    accuracy &lt;- mean(preds == Boston$high_medv)\n\n    list(\n        accuracy = accuracy,\n        n_support = model$tot.nSV,\n        confusion = table(\n            predicted = preds,\n            actual = Boston$high_medv\n        )\n    )\n}\n'''\n\nresult = rb.call(\n    svm_boston_class_code,\n    \"train_boston_svm_class\",\n    return_type=\"dict\",\n    kernel_type=\"radial\",\n    cost=1\n)\n\nprint(f\"Classification Accuracy: {result['accuracy']:.2%}\")\nprint(f\"Number of Support Vectors: {result['n_support']}\")\nprint(\"Confusion Matrix:\")\nprint(result[\"confusion\"])\n\n\r\n======================================================================\nOptional: SVM Classification on Boston\n======================================================================\nClassification Accuracy: 90.51%\nNumber of Support Vectors: 209\nConfusion Matrix:\n[[237, 29], [19, 221]]\n\r\n# ============================================================================\n# Optional: SVM Classification (High vs Low Price)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Optional: SVM Classification on Boston\")\nprint(\"=\" * 70)\n\nsvm_boston_class_code = '''\nlibrary(MASS)\nlibrary(e1071)\n\ntrain_boston_svm_class &lt;- function(kernel_type = \"radial\", cost = 1) {\n\n    data(Boston)\n\n    # Binary target: expensive vs cheap housing\n    Boston$high_medv &lt;- as.factor(ifelse(Boston$medv &gt;\n                                         median(Boston$medv), 1, 0))\n\n    model &lt;- svm(\n        high_medv ~ . - medv,\n        data = Boston,\n        kernel = kernel_type,\n        cost = cost,\n        scale = TRUE\n    )\n\n    preds &lt;- predict(model, Boston)\n\n    accuracy &lt;- mean(preds == Boston$high_medv)\n\n    list(\n        accuracy = accuracy,\n        n_support = model$tot.nSV,\n        confusion = table(\n            predicted = preds,\n            actual = Boston$high_medv\n        )\n    )\n}\n'''\n\nresult = rb.call(\n    svm_boston_class_code,\n    \"train_boston_svm_class\",\n    return_type=\"dict\",\n    kernel_type=\"radial\",\n    cost=1\n)\n\nprint(f\"Classification Accuracy: {result['accuracy']:.2%}\")\nprint(f\"Number of Support Vectors: {result['n_support']}\")\nprint(\"Confusion Matrix:\")\nprint(result[\"confusion\"])\n\n\r\n======================================================================\nOptional: SVM Classification on Boston\n======================================================================\nClassification Accuracy: 90.51%\nNumber of Support Vectors: 209\nConfusion Matrix:\n[[237, 29], [19, 221]]\n</pre>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://thierrymoudiki.github.io//blog/2026/01/08/r/python/rtopy\"> T. Moudiki's Webpage - R</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 1846,
    "reading_time_min": 9.2,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/t-moudiki/",
        "text": "T. Moudiki"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://thierrymoudiki.github.io//blog/2026/01/08/r/python/rtopy",
        "text": "T. Moudiki's Webpage - R"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://thierrymoudiki.github.io/blog/2024/03/04/python/r/rtopyintro",
        "text": "https://thierrymoudiki.github.io/blog/2024/03/04/python/r/rtopyintro"
      },
      {
        "href": "https://thierrymoudiki.github.io//blog/2026/01/08/r/python/rtopy",
        "text": "T. Moudiki's Webpage - R"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [
      {
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif",
        "alt": "image-title-here",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"
      },
      {
        "src": "https://i0.wp.com/thierrymoudiki.github.io/images/2026-01-08/2026-01-08-rtopy_4_0.png?w=578&ssl=1",
        "alt": "image-title-here",
        "base64": "data:image/jpeg;base64,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"
      }
    ],
    "lang": "en-US",
    "crawled_at_utc": "2026-01-08T18:26:41Z"
  }
}