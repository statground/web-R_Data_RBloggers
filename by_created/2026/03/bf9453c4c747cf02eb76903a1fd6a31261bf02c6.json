{
  "id": "bf9453c4c747cf02eb76903a1fd6a31261bf02c6",
  "url": "https://www.r-bloggers.com/2026/03/how-to-fit-hierarchical-bayesian-models-in-r-with-brms-partial-pooling-explained/",
  "created_at_utc": "2026-03-01T22:15:09Z",
  "crawled_at_utc": "2026-03-01T22:15:09Z",
  "html_title": "How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained | R-bloggers",
  "meta_description": "Hierarchical Bayesian modeling (also called multilevel modeling) is one of the most reliable ways to build predictive and inferential models when your data has natural grouping—teams, players, seasons, leagues, referees, venues, or even game states. In sports analytics, that grouping is unavoidable. In R, hierarchical Bayesian models are commonly implemented via brms (Stan), rstanarm, or […] The post How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained appeared first on R Programming Books.",
  "data": {
    "url": "https://www.r-bloggers.com/2026/03/how-to-fit-hierarchical-bayesian-models-in-r-with-brms-partial-pooling-explained/",
    "canonical_url": "https://www.r-bloggers.com/2026/03/how-to-fit-hierarchical-bayesian-models-in-r-with-brms-partial-pooling-explained/",
    "html_title": "How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained | R-bloggers",
    "h1_title": "R-bloggers",
    "meta_description": "Hierarchical Bayesian modeling (also called multilevel modeling) is one of the most reliable ways to build predictive and inferential models when your data has natural grouping—teams, players, seasons, leagues, referees, venues, or even game states. In sports analytics, that grouping is unavoidable. In R, hierarchical Bayesian models are commonly implemented via brms (Stan), rstanarm, or […] The post How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained appeared first on R Programming Books.",
    "meta_keywords": null,
    "og_title": "How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained | R-bloggers",
    "og_description": "Hierarchical Bayesian modeling (also called multilevel modeling) is one of the most reliable ways to build predictive and inferential models when your data has natural grouping—teams, players, seasons, leagues, referees, venues, or even game states. In sports analytics, that grouping is unavoidable. In R, hierarchical Bayesian models are commonly implemented via brms (Stan), rstanarm, or […] The post How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained appeared first on R Programming Books.",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "twitter_title": "How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained | R-bloggers",
    "twitter_description": "Hierarchical Bayesian modeling (also called multilevel modeling) is one of the most reliable ways to build predictive and inferential models when your data has natural grouping—teams, players, seasons, leagues, referees, venues, or even game states. In sports analytics, that grouping is unavoidable. In R, hierarchical Bayesian models are commonly implemented via brms (Stan), rstanarm, or […] The post How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained appeared first on R Programming Books.",
    "raw_jsonld_article": null,
    "article_headline": null,
    "article_section": null,
    "article_tags": null,
    "article_author": null,
    "article_published": null,
    "article_modified": null,
    "main_text": "How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained\nPosted on\nMarch 1, 2026\nby\nrprogrammingbooks\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nBlog - R Programming Books\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nHierarchical Bayesian modeling (also called\nmultilevel modeling\n) is one of the most reliable ways to build\n    predictive and inferential models when your data has natural grouping—teams, players, seasons, leagues, referees,\n    venues, or even game states. In sports analytics, that grouping is unavoidable. In R, hierarchical Bayesian models\n    are commonly implemented via\nbrms\n(Stan),\nrstanarm\n, or\ncmdstanr\n.\nThis tutorial focuses on\npartial pooling\n(a.k.a. shrinkage) and why it’s the default choice for academic,\n    production-grade modeling: it reduces overfitting, improves out-of-sample performance, and produces honest uncertainty\n    quantification. We will use a sports dataset as a concrete example, but the modeling principles generalize to\n    many domains (education, marketing, medicine, A/B testing, and more).\nCore keywords covered:\nhierarchical Bayesian models in R, partial pooling, multilevel modeling, Bayesian shrinkage,\n      Bayesian inference in Stan, brms tutorial, posterior predictive checks, priors, model calibration, sports analytics in R.\nPooling Strategies: No Pooling, Complete Pooling, Partial Pooling\nSuppose we want to estimate team strength. There are three classic approaches:\nNo pooling\n: estimate a separate parameter for each team using only that team’s data.\n      This can be high-variance (overfits small samples).\nComplete pooling\n: ignore teams and estimate one global parameter.\n      This is low-variance but high-bias (misses real differences).\nPartial pooling\n: estimate team parameters while sharing information via a population-level distribution.\n      This is the hierarchical Bayesian compromise that tends to dominate in practice.\nIn partial pooling, each group effect is “pulled” toward the global mean in proportion to how much information that group has.\n    Teams with few matches shrink more; teams with many matches shrink less. This is not a trick—it’s what the posterior implies\n    under a reasonable hierarchical prior structure.\nR Setup\nWe will fit models using\nbrms\n, which compiles Bayesian models to Stan and provides a high-level formula interface.\n    The workflow below emphasizes reproducibility and good Bayesian practice: priors, diagnostics, posterior predictive checks,\n    and principled comparison against simpler baselines.\n# Core modeling stack\ninstall.packages(c(\"brms\", \"tidyverse\", \"posterior\", \"bayesplot\", \"tidybayes\"))\n# Optional but recommended for speed (CmdStan backend)\ninstall.packages(\"cmdstanr\")\n\nlibrary(brms)\nlibrary(tidyverse)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# If using cmdstanr (recommended), set backend once:\n# cmdstanr::install_cmdstan()\noptions(brms.backend = \"cmdstanr\")\nExample Data: Match Outcomes with Team Effects\nTo keep this tutorial self-contained, we’ll simulate a dataset that resembles soccer-style goal scoring.\n    The same structure appears in hockey, handball, futsal, and other low-to-moderate scoring sports.\n    The key is that each match has two teams and outcomes depend on\nlatent team strength\n.\nset.seed(123)\n\nn_teams <- 20\nteams   <- paste0(\"Team_\", seq_len(n_teams))\n\n# True latent parameters (unknown in real life)\ntrue_attack  <- rnorm(n_teams, 0, 0.35)\ntrue_defense <- rnorm(n_teams, 0, 0.35)\ntrue_home_adv <- 0.20\n\n# Schedule: random pairings\nn_matches <- 600\nhome_id <- sample(seq_len(n_teams), n_matches, replace = TRUE)\naway_id <- sample(seq_len(n_teams), n_matches, replace = TRUE)\n# Avoid self-matches\nsame <- home_id == away_id\nwhile (any(same)) {\n  away_id[same] <- sample(seq_len(n_teams), sum(same), replace = TRUE)\n  same <- home_id == away_id\n}\n\nhome_team <- teams[home_id]\naway_team <- teams[away_id]\n\n# Poisson rates for goals\nlog_lambda_home <- true_home_adv + true_attack[home_id] - true_defense[away_id]\nlog_lambda_away <-               true_attack[away_id] - true_defense[home_id]\n\nlambda_home <- exp(log_lambda_home)\nlambda_away <- exp(log_lambda_away)\n\nhome_goals <- rpois(n_matches, lambda_home)\naway_goals <- rpois(n_matches, lambda_away)\n\nmatches <- tibble(\n  match_id   = seq_len(n_matches),\n  home_team  = factor(home_team),\n  away_team  = factor(away_team),\n  home_goals = home_goals,\n  away_goals = away_goals\n)\n\nmatches %>% glimpse()\nIn a real sports analytics pipeline, you would replace simulation with data ingestion (CSV/APIs),\n    feature engineering (rest days, injuries, xG proxies, travel, Elo, etc.), and train/test splits. The hierarchical core remains:\n    group-level effects with partial pooling.\nBaseline: Complete Pooling (No Team Effects)\nAs a baseline, model home goals using only a global intercept and home advantage indicator.\n    This is intentionally too simple: it cannot learn that some teams are stronger than others.\nm_pool <- brm(\n  home_goals ~ 1,\n  data   = matches,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\")\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1\n)\n\nsummary(m_pool)\nComplete pooling often underfits: it produces well-behaved uncertainty but misses systematic structure.\n    Next we move to\nno pooling\nand then\npartial pooling\n.\nNo Pooling: Separate Team Parameters (High Variance)\nNo pooling estimates a fixed effect for each team without sharing strength across teams.\n    This can be unstable when some teams have fewer observations or unbalanced schedules.\nm_nopool <- brm(\n  home_goals ~ 1 + home_team + away_team,\n  data   = matches,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\"),\n    prior(normal(0, 0.5), class = \"b\")  # regularization, but still no pooling\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 2\n)\n\nsummary(m_nopool)\nEven with regularizing priors, no pooling is typically noisier than hierarchical partial pooling, especially in small samples.\n    The multilevel model handles this in a more principled way.\nPartial Pooling: Hierarchical (Multilevel) Team Effects\nA standard approach is to model goals with\nrandom effects\nfor home and away team.\n    In brms syntax,\n(1 | home_team)\nmeans each home team gets its own intercept drawn from a common distribution.\n    The distribution’s standard deviation is learned from data and controls the amount of shrinkage.\nBelow, we fit a model for home goals with hierarchical intercepts for both home and away teams. This captures:\nhome attacking propensity\n(home team effect) and\naway defensive vulnerability\n(away team effect),\n    albeit in a simplified way.\npriors_hier <- c(\n  prior(normal(0, 1.0), class = \"Intercept\"),\n  # SD priors control how much team-to-team variation is plausible\n  prior(exponential(1.0), class = \"sd\")\n)\n\nm_hier <- brm(\n  home_goals ~ 1 + (1 | home_team) + (1 | away_team),\n  data   = matches,\n  family = poisson(),\n  prior  = priors_hier,\n  chains = 4, cores = 4, iter = 2500, seed = 3\n)\n\nsummary(m_hier)\nInterpretation: the model estimates a global goal rate (intercept) plus deviations for each team, but those deviations are\n    partially pooled. This is the practical meaning of partial pooling:\ngroup-level parameters borrow statistical strength\n.\nModeling Both Scores Jointly\nA more realistic sports modeling setup estimates both home and away goals and separates attack/defense structure.\n    brms supports multivariate models. Here’s a simple bivariate Poisson-like approach by fitting two Poisson responses with\n    correlated random effects (conceptually similar to attack/defense components).\nbf_home <- bf(home_goals ~ 1 + (1 | home_team) + (1 | away_team))\nbf_away <- bf(away_goals ~ 1 + (1 | away_team) + (1 | home_team))\n\nm_mv <- brm(\n  bf_home + bf_away + set_rescor(FALSE),\n  data   = matches,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\", resp = \"homegoals\"),\n    prior(normal(0, 1.0), class = \"Intercept\", resp = \"awaygoals\"),\n    prior(exponential(1.0), class = \"sd\")\n  ),\n  chains = 4, cores = 4, iter = 3000, seed = 4\n)\n\nsummary(m_mv)\nThis multivariate model is already a meaningful step toward academic-grade sports inference: it produces\n    posterior distributions for team effects with coherent uncertainty, and it avoids overreacting to noisy short-term form.\nDiagnostics and Posterior Predictive Checks (Academic Essentials)\nA Bayesian model is only as credible as its diagnostics. At minimum, check:\nR-hat\n(convergence),\neffective sample size\n, and posterior predictive fit.\n# Convergence diagnostics\nm_hier %>% summary()\n\n# Posterior predictive checks: do simulated goals resemble observed?\npp_check(m_hier, type = \"hist\", ndraws = 100)\n\n# Another useful view: check distribution by team (subset example)\nsome_teams <- levels(matches$home_team)[1:6]\npp_check(m_hier, type = \"hist\", ndraws = 50) +\n  ggplot2::facet_wrap(~ home_team, ncol = 3)\nPosterior predictive checks help detect mis-specification: too many zeros, heavy tails, or systematic under/over-dispersion.\n    If your sport has extra dispersion, consider a\nnegative binomial\nlikelihood:\nfamily = negbinomial()\n.\nm_hier_nb <- brm(\n  home_goals ~ 1 + (1 | home_team) + (1 | away_team),\n  data   = matches,\n  family = negbinomial(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\"),\n    prior(exponential(1.0), class = \"sd\"),\n    prior(exponential(1.0), class = \"shape\")  # NB dispersion\n  ),\n  chains = 4, cores = 4, iter = 2500, seed = 5\n)\n\npp_check(m_hier_nb, type = \"hist\", ndraws = 100)\nVisualizing Partial Pooling (Shrinkage) in R\nThe cleanest way to “see” partial pooling is to compare group estimates under:\n    (a) no pooling and (b) hierarchical pooling. Teams with limited data will shrink toward the global mean in the hierarchical model.\n# Extract team effects from both models\nre_hier <- ranef(m_hier)$home_team[, , \"Intercept\"] %>%\n  as_tibble(.name_repair = \"minimal\") %>%\n  setNames(c(\"estimate\", \"est_error\", \"q2.5\", \"q97.5\")) %>%\n  mutate(team = rownames(ranef(m_hier)$home_team[, , \"Intercept\"]))\n\n# No pooling fixed effects: home_team coefficients (approx comparison)\nfix_nopool <- fixef(m_nopool) %>% as.data.frame() %>% rownames_to_column(\"term\") %>%\n  filter(str_starts(term, \"home_team\")) %>%\n  mutate(team = str_remove(term, \"home_team\")) %>%\n  transmute(team, estimate = Estimate, q2.5 = Q2.5, q97.5 = Q97.5)\n\n# Join and compare\ncomp <- re_hier %>%\n  left_join(fix_nopool, by = \"team\", suffix = c(\"_hier\", \"_nopool\"))\n\ncomp %>%\n  ggplot(aes(x = estimate_nopool, y = estimate_hier)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  labs(\n    x = \"No pooling estimate (fixed effects)\",\n    y = \"Partial pooling estimate (hierarchical)\",\n    title = \"Shrinkage: hierarchical estimates pull extreme values toward the mean\"\n  )\nIn applied work, this shrinkage is a feature, not a bug: it protects you from chasing noise, especially early in a season\n    or when some teams have faced unusually strong/weak opponents.\nPosterior Predictions: From Parameters to Predictive Distributions\nHierarchical Bayesian models shine when you need uncertainty-aware predictions. Instead of a single number,\n    you get a full posterior predictive distribution—useful for forecast intervals, simulations, and decision analysis.\n# Create a small set of future fixtures (example)\nnew_matches <- tibble(\n  home_team = factor(c(\"Team_1\", \"Team_2\", \"Team_3\"), levels = levels(matches$home_team)),\n  away_team = factor(c(\"Team_4\", \"Team_5\", \"Team_6\"), levels = levels(matches$away_team))\n)\n\n# Posterior expected goals (lambda) for home_goals model\nepred <- posterior_epred(m_hier, newdata = new_matches, ndraws = 2000)\n# epred is draws x rows. Summarize mean and interval per match:\npred_summary <- apply(epred, 2, function(x) {\n  c(mean = mean(x), q10 = quantile(x, 0.10), q90 = quantile(x, 0.90))\n}) %>% t() %>% as_tibble()\n\nbind_cols(new_matches, pred_summary)\nIf you need simulated goal counts (not just expected value), use\nposterior_predict()\n.\nyrep <- posterior_predict(m_hier, newdata = new_matches, ndraws = 2000)\n\nsim_summary <- apply(yrep, 2, function(x) {\n  c(mean = mean(x), q10 = quantile(x, 0.10), q90 = quantile(x, 0.90))\n}) %>% t() %>% as_tibble()\n\nbind_cols(new_matches, sim_summary)\nModel Comparison: Why Hierarchical Often Wins\nA common academic question: does partial pooling actually improve predictive accuracy?\n    One principled approach is approximate leave-one-out cross-validation (LOO) using\nloo()\n.\n# Requires: install.packages(\"loo\")\nlibrary(loo)\n\nloo_pool   <- loo(m_pool)\nloo_nopool <- loo(m_nopool)\nloo_hier   <- loo(m_hier)\n\nloo_compare(loo_pool, loo_nopool, loo_hier)\nIn many real datasets, hierarchical models dominate no-pooling models because they reduce variance without imposing\n    the high bias of complete pooling. When the sample is large and balanced, no-pooling can catch up, but it remains\n    more fragile under distribution shift (new season, roster changes, schedule imbalance).\nPriors for Multilevel Sports Models: Practical Guidance\nPriors are not “optional decoration”—they formalize regularization and encode plausible scales.\n    For sports scoring, consider:\nIntercept prior\n: reflects typical goal rate (log scale for Poisson).\nGroup SD priors\n: control how much teams can vary from the league mean.\nLikelihood choice\n: Poisson vs negative binomial for overdispersion.\n# Example: informative-ish intercept prior based on typical goals per match\n# If average home goals ~ 1.4, then log(1.4) ~ 0.336\npriors_sporty <- c(\n  prior(normal(log(1.4), 0.5), class = \"Intercept\"),\n  prior(exponential(1.0), class = \"sd\")\n)\n\nm_hier2 <- brm(\n  home_goals ~ 1 + (1 | home_team) + (1 | away_team),\n  data   = matches,\n  family = poisson(),\n  prior  = priors_sporty,\n  chains = 4, cores = 4, iter = 2500, seed = 6\n)\nIf you publish academic-style modeling content, a short prior sensitivity check is a strong credibility signal:\n    refit with slightly wider SD priors and confirm conclusions are stable.\nPractical Notes for Real Sports Data\nUnbalanced schedules\n: hierarchical structure helps stabilize estimates when teams face different opponent quality.\nSmall samples\n: early season or new leagues are where partial pooling is most valuable.\nCovariates\n: add rest days, travel, injuries, Elo, or rolling form as fixed effects, but keep team effects hierarchical.\nTime dynamics\n: for long seasons, consider random walks or season-by-season hierarchical layers.\n# Add a covariate example (simulated here):\nmatches2 <- matches %>%\n  mutate(rest_diff = rnorm(n(), 0, 1))  # placeholder for real engineered feature\n\nm_cov <- brm(\n  home_goals ~ 1 + rest_diff + (1 | home_team) + (1 | away_team),\n  data   = matches2,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\"),\n    prior(normal(0, 0.3), class = \"b\"),   # effect size prior for covariate\n    prior(exponential(1.0), class = \"sd\")\n  ),\n  chains = 4, cores = 4, iter = 2500, seed = 7\n)\n\nsummary(m_cov)\nFurther Reading and Deeper Projects (Optional)\nIf you want to extend this tutorial into a full sports analytics workflow—data acquisition, feature engineering,\n    predictive evaluation, and model deployment—two longer-form references can be useful as project companions:\nBayesian Sports Analytics with R: Predictive Modeling for Betting & Performance\n— focused on Bayesian modeling patterns and sports prediction pipelines in R.\nBayesian Sports Betting with R: Probability, Kelly Criterion and Betting Strategies\n— useful if you’re specifically interested in decision-making under uncertainty and probabilistic outputs.\nYou can treat those as optional deep-dives; the core multilevel concepts in this post stand on their own and transfer\n    cleanly to non-sports hierarchical modeling problems.\nConclusion\nPartial pooling is the practical heart of hierarchical Bayesian modeling. In R, multilevel models with brms/Stan give you:\n    (1) stable group estimates, (2) principled regularization, and (3) posterior predictive uncertainty that supports\n    simulation, forecasting, and evaluation. If your data has groups—and most real-world data does—hierarchical Bayes is often\n    the most defensible baseline you can set.\nThe post\nHow to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained\nappeared first on\nR Programming Books\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nBlog - R Programming Books\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "main_html": "<article class=\"post-399441 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">March 1, 2026</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/rprogrammingbooks/\">rprogrammingbooks</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://rprogrammingbooks.com/hierarchical-bayesian-models-in-r-brms-partial-pooling/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=hierarchical-bayesian-models-in-r-brms-partial-pooling\"> Blog - R Programming Books</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<!-- Intro / Lead -->\n<div class=\"post-content\">\n<p>\n    Hierarchical Bayesian modeling (also called <em>multilevel modeling</em>) is one of the most reliable ways to build\n    predictive and inferential models when your data has natural grouping—teams, players, seasons, leagues, referees,\n    venues, or even game states. In sports analytics, that grouping is unavoidable. In R, hierarchical Bayesian models\n    are commonly implemented via <strong>brms</strong> (Stan), <strong>rstanarm</strong>, or <strong>cmdstanr</strong>.\n  </p>\n<p>\n    This tutorial focuses on <strong>partial pooling</strong> (a.k.a. shrinkage) and why it’s the default choice for academic,\n    production-grade modeling: it reduces overfitting, improves out-of-sample performance, and produces honest uncertainty\n    quantification. We will use a sports dataset as a concrete example, but the modeling principles generalize to\n    many domains (education, marketing, medicine, A/B testing, and more).\n  </p>\n<div class=\"callout\">\n<p><strong>Core keywords covered:</strong> hierarchical Bayesian models in R, partial pooling, multilevel modeling, Bayesian shrinkage,\n      Bayesian inference in Stan, brms tutorial, posterior predictive checks, priors, model calibration, sports analytics in R.</p>\n</div>\n<hr/>\n<!-- Conceptual: pooling -->\n<h2>Pooling Strategies: No Pooling, Complete Pooling, Partial Pooling</h2>\n<p>\n    Suppose we want to estimate team strength. There are three classic approaches:\n  </p>\n<ul>\n<li>\n<strong>No pooling</strong>: estimate a separate parameter for each team using only that team’s data.\n      This can be high-variance (overfits small samples).\n    </li>\n<li>\n<strong>Complete pooling</strong>: ignore teams and estimate one global parameter.\n      This is low-variance but high-bias (misses real differences).\n    </li>\n<li>\n<strong>Partial pooling</strong>: estimate team parameters while sharing information via a population-level distribution.\n      This is the hierarchical Bayesian compromise that tends to dominate in practice.\n    </li>\n</ul>\n<p>\n    In partial pooling, each group effect is “pulled” toward the global mean in proportion to how much information that group has.\n    Teams with few matches shrink more; teams with many matches shrink less. This is not a trick—it’s what the posterior implies\n    under a reasonable hierarchical prior structure.\n  </p>\n<hr/>\n<!-- Setup -->\n<h2>R Setup</h2>\n<p>\n    We will fit models using <strong>brms</strong>, which compiles Bayesian models to Stan and provides a high-level formula interface.\n    The workflow below emphasizes reproducibility and good Bayesian practice: priors, diagnostics, posterior predictive checks,\n    and principled comparison against simpler baselines.\n  </p>\n<pre># Core modeling stack\ninstall.packages(c(\"brms\", \"tidyverse\", \"posterior\", \"bayesplot\", \"tidybayes\"))\n# Optional but recommended for speed (CmdStan backend)\ninstall.packages(\"cmdstanr\")\n\nlibrary(brms)\nlibrary(tidyverse)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# If using cmdstanr (recommended), set backend once:\n# cmdstanr::install_cmdstan()\noptions(brms.backend = \"cmdstanr\")</pre>\n<hr/>\n<!-- Data -->\n<h2>Example Data: Match Outcomes with Team Effects</h2>\n<p>\n    To keep this tutorial self-contained, we’ll simulate a dataset that resembles soccer-style goal scoring.\n    The same structure appears in hockey, handball, futsal, and other low-to-moderate scoring sports.\n    The key is that each match has two teams and outcomes depend on <em>latent team strength</em>.\n  </p>\n<pre>set.seed(123)\n\nn_teams &lt;- 20\nteams   &lt;- paste0(\"Team_\", seq_len(n_teams))\n\n# True latent parameters (unknown in real life)\ntrue_attack  &lt;- rnorm(n_teams, 0, 0.35)\ntrue_defense &lt;- rnorm(n_teams, 0, 0.35)\ntrue_home_adv &lt;- 0.20\n\n# Schedule: random pairings\nn_matches &lt;- 600\nhome_id &lt;- sample(seq_len(n_teams), n_matches, replace = TRUE)\naway_id &lt;- sample(seq_len(n_teams), n_matches, replace = TRUE)\n# Avoid self-matches\nsame &lt;- home_id == away_id\nwhile (any(same)) {\n  away_id[same] &lt;- sample(seq_len(n_teams), sum(same), replace = TRUE)\n  same &lt;- home_id == away_id\n}\n\nhome_team &lt;- teams[home_id]\naway_team &lt;- teams[away_id]\n\n# Poisson rates for goals\nlog_lambda_home &lt;- true_home_adv + true_attack[home_id] - true_defense[away_id]\nlog_lambda_away &lt;-               true_attack[away_id] - true_defense[home_id]\n\nlambda_home &lt;- exp(log_lambda_home)\nlambda_away &lt;- exp(log_lambda_away)\n\nhome_goals &lt;- rpois(n_matches, lambda_home)\naway_goals &lt;- rpois(n_matches, lambda_away)\n\nmatches &lt;- tibble(\n  match_id   = seq_len(n_matches),\n  home_team  = factor(home_team),\n  away_team  = factor(away_team),\n  home_goals = home_goals,\n  away_goals = away_goals\n)\n\nmatches %&gt;% glimpse()</pre>\n<p>\n    In a real sports analytics pipeline, you would replace simulation with data ingestion (CSV/APIs),\n    feature engineering (rest days, injuries, xG proxies, travel, Elo, etc.), and train/test splits. The hierarchical core remains:\n    group-level effects with partial pooling.\n  </p>\n<hr/>\n<!-- Baseline models -->\n<h2>Baseline: Complete Pooling (No Team Effects)</h2>\n<p>\n    As a baseline, model home goals using only a global intercept and home advantage indicator.\n    This is intentionally too simple: it cannot learn that some teams are stronger than others.\n  </p>\n<pre>m_pool &lt;- brm(\n  home_goals ~ 1,\n  data   = matches,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\")\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1\n)\n\nsummary(m_pool)</pre>\n<p>\n    Complete pooling often underfits: it produces well-behaved uncertainty but misses systematic structure.\n    Next we move to <strong>no pooling</strong> and then <strong>partial pooling</strong>.\n  </p>\n<hr/>\n<h2>No Pooling: Separate Team Parameters (High Variance)</h2>\n<p>\n    No pooling estimates a fixed effect for each team without sharing strength across teams.\n    This can be unstable when some teams have fewer observations or unbalanced schedules.\n  </p>\n<pre>m_nopool &lt;- brm(\n  home_goals ~ 1 + home_team + away_team,\n  data   = matches,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\"),\n    prior(normal(0, 0.5), class = \"b\")  # regularization, but still no pooling\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 2\n)\n\nsummary(m_nopool)</pre>\n<p>\n    Even with regularizing priors, no pooling is typically noisier than hierarchical partial pooling, especially in small samples.\n    The multilevel model handles this in a more principled way.\n  </p>\n<hr/>\n<!-- Hierarchical model -->\n<h2>Partial Pooling: Hierarchical (Multilevel) Team Effects</h2>\n<p>\n    A standard approach is to model goals with <strong>random effects</strong> for home and away team.\n    In brms syntax, <code>(1 | home_team)</code> means each home team gets its own intercept drawn from a common distribution.\n    The distribution’s standard deviation is learned from data and controls the amount of shrinkage.\n  </p>\n<p>\n    Below, we fit a model for home goals with hierarchical intercepts for both home and away teams. This captures:\n    <strong>home attacking propensity</strong> (home team effect) and <strong>away defensive vulnerability</strong> (away team effect),\n    albeit in a simplified way.\n  </p>\n<pre>priors_hier &lt;- c(\n  prior(normal(0, 1.0), class = \"Intercept\"),\n  # SD priors control how much team-to-team variation is plausible\n  prior(exponential(1.0), class = \"sd\")\n)\n\nm_hier &lt;- brm(\n  home_goals ~ 1 + (1 | home_team) + (1 | away_team),\n  data   = matches,\n  family = poisson(),\n  prior  = priors_hier,\n  chains = 4, cores = 4, iter = 2500, seed = 3\n)\n\nsummary(m_hier)</pre>\n<p>\n    Interpretation: the model estimates a global goal rate (intercept) plus deviations for each team, but those deviations are\n    partially pooled. This is the practical meaning of partial pooling: <em>group-level parameters borrow statistical strength</em>.\n  </p>\n<hr/>\n<!-- Two-response model -->\n<h2>Modeling Both Scores Jointly</h2>\n<p>\n    A more realistic sports modeling setup estimates both home and away goals and separates attack/defense structure.\n    brms supports multivariate models. Here’s a simple bivariate Poisson-like approach by fitting two Poisson responses with\n    correlated random effects (conceptually similar to attack/defense components).\n  </p>\n<pre>bf_home &lt;- bf(home_goals ~ 1 + (1 | home_team) + (1 | away_team))\nbf_away &lt;- bf(away_goals ~ 1 + (1 | away_team) + (1 | home_team))\n\nm_mv &lt;- brm(\n  bf_home + bf_away + set_rescor(FALSE),\n  data   = matches,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\", resp = \"homegoals\"),\n    prior(normal(0, 1.0), class = \"Intercept\", resp = \"awaygoals\"),\n    prior(exponential(1.0), class = \"sd\")\n  ),\n  chains = 4, cores = 4, iter = 3000, seed = 4\n)\n\nsummary(m_mv)</pre>\n<p>\n    This multivariate model is already a meaningful step toward academic-grade sports inference: it produces\n    posterior distributions for team effects with coherent uncertainty, and it avoids overreacting to noisy short-term form.\n  </p>\n<hr/>\n<!-- Diagnostics -->\n<h2>Diagnostics and Posterior Predictive Checks (Academic Essentials)</h2>\n<p>\n    A Bayesian model is only as credible as its diagnostics. At minimum, check:\n    <strong>R-hat</strong> (convergence), <strong>effective sample size</strong>, and posterior predictive fit.\n  </p>\n<pre># Convergence diagnostics\nm_hier %&gt;% summary()\n\n# Posterior predictive checks: do simulated goals resemble observed?\npp_check(m_hier, type = \"hist\", ndraws = 100)\n\n# Another useful view: check distribution by team (subset example)\nsome_teams &lt;- levels(matches$home_team)[1:6]\npp_check(m_hier, type = \"hist\", ndraws = 50) +\n  ggplot2::facet_wrap(~ home_team, ncol = 3)</pre>\n<p>\n    Posterior predictive checks help detect mis-specification: too many zeros, heavy tails, or systematic under/over-dispersion.\n    If your sport has extra dispersion, consider a <strong>negative binomial</strong> likelihood:\n    <code>family = negbinomial()</code>.\n  </p>\n<pre>m_hier_nb &lt;- brm(\n  home_goals ~ 1 + (1 | home_team) + (1 | away_team),\n  data   = matches,\n  family = negbinomial(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\"),\n    prior(exponential(1.0), class = \"sd\"),\n    prior(exponential(1.0), class = \"shape\")  # NB dispersion\n  ),\n  chains = 4, cores = 4, iter = 2500, seed = 5\n)\n\npp_check(m_hier_nb, type = \"hist\", ndraws = 100)</pre>\n<hr/>\n<!-- Partial pooling demonstration -->\n<h2>Visualizing Partial Pooling (Shrinkage) in R</h2>\n<p>\n    The cleanest way to “see” partial pooling is to compare group estimates under:\n    (a) no pooling and (b) hierarchical pooling. Teams with limited data will shrink toward the global mean in the hierarchical model.\n  </p>\n<pre># Extract team effects from both models\nre_hier &lt;- ranef(m_hier)$home_team[, , \"Intercept\"] %&gt;%\n  as_tibble(.name_repair = \"minimal\") %&gt;%\n  setNames(c(\"estimate\", \"est_error\", \"q2.5\", \"q97.5\")) %&gt;%\n  mutate(team = rownames(ranef(m_hier)$home_team[, , \"Intercept\"]))\n\n# No pooling fixed effects: home_team coefficients (approx comparison)\nfix_nopool &lt;- fixef(m_nopool) %&gt;% as.data.frame() %&gt;% rownames_to_column(\"term\") %&gt;%\n  filter(str_starts(term, \"home_team\")) %&gt;%\n  mutate(team = str_remove(term, \"home_team\")) %&gt;%\n  transmute(team, estimate = Estimate, q2.5 = Q2.5, q97.5 = Q97.5)\n\n# Join and compare\ncomp &lt;- re_hier %&gt;%\n  left_join(fix_nopool, by = \"team\", suffix = c(\"_hier\", \"_nopool\"))\n\ncomp %&gt;%\n  ggplot(aes(x = estimate_nopool, y = estimate_hier)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  labs(\n    x = \"No pooling estimate (fixed effects)\",\n    y = \"Partial pooling estimate (hierarchical)\",\n    title = \"Shrinkage: hierarchical estimates pull extreme values toward the mean\"\n  )</pre>\n<p>\n    In applied work, this shrinkage is a feature, not a bug: it protects you from chasing noise, especially early in a season\n    or when some teams have faced unusually strong/weak opponents.\n  </p>\n<hr/>\n<!-- Prediction -->\n<h2>Posterior Predictions: From Parameters to Predictive Distributions</h2>\n<p>\n    Hierarchical Bayesian models shine when you need uncertainty-aware predictions. Instead of a single number,\n    you get a full posterior predictive distribution—useful for forecast intervals, simulations, and decision analysis.\n  </p>\n<pre># Create a small set of future fixtures (example)\nnew_matches &lt;- tibble(\n  home_team = factor(c(\"Team_1\", \"Team_2\", \"Team_3\"), levels = levels(matches$home_team)),\n  away_team = factor(c(\"Team_4\", \"Team_5\", \"Team_6\"), levels = levels(matches$away_team))\n)\n\n# Posterior expected goals (lambda) for home_goals model\nepred &lt;- posterior_epred(m_hier, newdata = new_matches, ndraws = 2000)\n# epred is draws x rows. Summarize mean and interval per match:\npred_summary &lt;- apply(epred, 2, function(x) {\n  c(mean = mean(x), q10 = quantile(x, 0.10), q90 = quantile(x, 0.90))\n}) %&gt;% t() %&gt;% as_tibble()\n\nbind_cols(new_matches, pred_summary)</pre>\n<p>\n    If you need simulated goal counts (not just expected value), use <code>posterior_predict()</code>.\n  </p>\n<pre>yrep &lt;- posterior_predict(m_hier, newdata = new_matches, ndraws = 2000)\n\nsim_summary &lt;- apply(yrep, 2, function(x) {\n  c(mean = mean(x), q10 = quantile(x, 0.10), q90 = quantile(x, 0.90))\n}) %&gt;% t() %&gt;% as_tibble()\n\nbind_cols(new_matches, sim_summary)</pre>\n<hr/>\n<!-- Model comparison -->\n<h2>Model Comparison: Why Hierarchical Often Wins</h2>\n<p>\n    A common academic question: does partial pooling actually improve predictive accuracy?\n    One principled approach is approximate leave-one-out cross-validation (LOO) using <code>loo()</code>.\n  </p>\n<pre># Requires: install.packages(\"loo\")\nlibrary(loo)\n\nloo_pool   &lt;- loo(m_pool)\nloo_nopool &lt;- loo(m_nopool)\nloo_hier   &lt;- loo(m_hier)\n\nloo_compare(loo_pool, loo_nopool, loo_hier)</pre>\n<p>\n    In many real datasets, hierarchical models dominate no-pooling models because they reduce variance without imposing\n    the high bias of complete pooling. When the sample is large and balanced, no-pooling can catch up, but it remains\n    more fragile under distribution shift (new season, roster changes, schedule imbalance).\n  </p>\n<hr/>\n<!-- Priors section -->\n<h2>Priors for Multilevel Sports Models: Practical Guidance</h2>\n<p>\n    Priors are not “optional decoration”—they formalize regularization and encode plausible scales.\n    For sports scoring, consider:\n  </p>\n<ul>\n<li><strong>Intercept prior</strong>: reflects typical goal rate (log scale for Poisson).</li>\n<li><strong>Group SD priors</strong>: control how much teams can vary from the league mean.</li>\n<li><strong>Likelihood choice</strong>: Poisson vs negative binomial for overdispersion.</li>\n</ul>\n<pre># Example: informative-ish intercept prior based on typical goals per match\n# If average home goals ~ 1.4, then log(1.4) ~ 0.336\npriors_sporty &lt;- c(\n  prior(normal(log(1.4), 0.5), class = \"Intercept\"),\n  prior(exponential(1.0), class = \"sd\")\n)\n\nm_hier2 &lt;- brm(\n  home_goals ~ 1 + (1 | home_team) + (1 | away_team),\n  data   = matches,\n  family = poisson(),\n  prior  = priors_sporty,\n  chains = 4, cores = 4, iter = 2500, seed = 6\n)</pre>\n<p>\n    If you publish academic-style modeling content, a short prior sensitivity check is a strong credibility signal:\n    refit with slightly wider SD priors and confirm conclusions are stable.\n  </p>\n<hr/>\n<!-- Practical notes -->\n<h2>Practical Notes for Real Sports Data</h2>\n<ul>\n<li>\n<strong>Unbalanced schedules</strong>: hierarchical structure helps stabilize estimates when teams face different opponent quality.\n    </li>\n<li>\n<strong>Small samples</strong>: early season or new leagues are where partial pooling is most valuable.\n    </li>\n<li>\n<strong>Covariates</strong>: add rest days, travel, injuries, Elo, or rolling form as fixed effects, but keep team effects hierarchical.\n    </li>\n<li>\n<strong>Time dynamics</strong>: for long seasons, consider random walks or season-by-season hierarchical layers.\n    </li>\n</ul>\n<pre># Add a covariate example (simulated here):\nmatches2 &lt;- matches %&gt;%\n  mutate(rest_diff = rnorm(n(), 0, 1))  # placeholder for real engineered feature\n\nm_cov &lt;- brm(\n  home_goals ~ 1 + rest_diff + (1 | home_team) + (1 | away_team),\n  data   = matches2,\n  family = poisson(),\n  prior  = c(\n    prior(normal(0, 1.0), class = \"Intercept\"),\n    prior(normal(0, 0.3), class = \"b\"),   # effect size prior for covariate\n    prior(exponential(1.0), class = \"sd\")\n  ),\n  chains = 4, cores = 4, iter = 2500, seed = 7\n)\n\nsummary(m_cov)</pre>\n<hr/>\n<!-- Subtle references to books -->\n<h2>Further Reading and Deeper Projects (Optional)</h2>\n<p>\n    If you want to extend this tutorial into a full sports analytics workflow—data acquisition, feature engineering,\n    predictive evaluation, and model deployment—two longer-form references can be useful as project companions:\n  </p>\n<ul>\n<li>\n<a href=\"https://rprogrammingbooks.com/product/bayesian-sports-analytics-r-predictive-modeling-betting-performance/\" rel=\"nofollow\" target=\"_blank\">\n        Bayesian Sports Analytics with R: Predictive Modeling for Betting &amp; Performance\n      </a>\n<span> — focused on Bayesian modeling patterns and sports prediction pipelines in R.</span>\n</li>\n<li>\n<a href=\"https://rprogrammingbooks.com/product/bayesian-sports-betting-with-r/\" rel=\"nofollow\" target=\"_blank\">\n        Bayesian Sports Betting with R: Probability, Kelly Criterion and Betting Strategies\n      </a>\n<span> — useful if you’re specifically interested in decision-making under uncertainty and probabilistic outputs.</span>\n</li>\n</ul>\n<p>\n    You can treat those as optional deep-dives; the core multilevel concepts in this post stand on their own and transfer\n    cleanly to non-sports hierarchical modeling problems.\n  </p>\n<hr/>\n<!-- Conclusion -->\n<h2>Conclusion</h2>\n<p>\n    Partial pooling is the practical heart of hierarchical Bayesian modeling. In R, multilevel models with brms/Stan give you:\n    (1) stable group estimates, (2) principled regularization, and (3) posterior predictive uncertainty that supports\n    simulation, forecasting, and evaluation. If your data has groups—and most real-world data does—hierarchical Bayes is often\n    the most defensible baseline you can set.\n  </p>\n</div>\n\n<p>The post <a href=\"https://rprogrammingbooks.com/hierarchical-bayesian-models-in-r-brms-partial-pooling/\" rel=\"nofollow\" target=\"_blank\">How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained</a> appeared first on <a href=\"https://rprogrammingbooks.com/\" rel=\"nofollow\" target=\"_blank\">R Programming Books</a>.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://rprogrammingbooks.com/hierarchical-bayesian-models-in-r-brms-partial-pooling/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=hierarchical-bayesian-models-in-r-brms-partial-pooling\"> Blog - R Programming Books</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
    "word_count": 2311,
    "reading_time_min": 11.6,
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/rprogrammingbooks/",
        "text": "rprogrammingbooks"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "external_links": [
      {
        "href": "https://rprogrammingbooks.com/hierarchical-bayesian-models-in-r-brms-partial-pooling/?utm_source=rss&utm_medium=rss&utm_campaign=hierarchical-bayesian-models-in-r-brms-partial-pooling",
        "text": "Blog - R Programming Books"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://rprogrammingbooks.com/product/bayesian-sports-analytics-r-predictive-modeling-betting-performance/",
        "text": "Bayesian Sports Analytics with R: Predictive Modeling for Betting & Performance"
      },
      {
        "href": "https://rprogrammingbooks.com/product/bayesian-sports-betting-with-r/",
        "text": "Bayesian Sports Betting with R: Probability, Kelly Criterion and Betting Strategies"
      },
      {
        "href": "https://rprogrammingbooks.com/hierarchical-bayesian-models-in-r-brms-partial-pooling/",
        "text": "How to Fit Hierarchical Bayesian Models in R with brms: Partial Pooling Explained"
      },
      {
        "href": "https://rprogrammingbooks.com/",
        "text": "R Programming Books"
      },
      {
        "href": "https://rprogrammingbooks.com/hierarchical-bayesian-models-in-r-brms-partial-pooling/?utm_source=rss&utm_medium=rss&utm_campaign=hierarchical-bayesian-models-in-r-brms-partial-pooling",
        "text": "Blog - R Programming Books"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "images": [],
    "lang": "en-US",
    "crawled_at_utc": "2026-03-01T22:15:09Z"
  }
}