{
  "id": "6937ab3b5a8abaea837d58a38af31c81088af2c0",
  "url": "https://www.r-bloggers.com/2025/12/my-messy-notes-on-building-a-super-learner-peeking-under-the-hood-of-nnls/",
  "created_at_utc": "2025-12-22T23:52:38Z",
  "data": null,
  "raw_original": {
    "uuid": "04f2d973-b107-406d-9461-59e00329d0f3",
    "created_at": "2025-12-22 23:52:38",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/12/my-messy-notes-on-building-a-super-learner-peeking-under-the-hood-of-nnls/",
      "crawled_at": "2025-12-22T14:52:01.294733",
      "external_links": [
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#motivations",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/tmle/",
          "text": "TMLE"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#objectives",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#what",
          "text": "What is Super Learner?"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#engine",
          "text": "What is the algorithm behind Super Learner?"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#nnls",
          "text": "Non-negative Least Square"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#lha",
          "text": "Lawson-Hanson algorithm"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#code",
          "text": "Let‚Äôs Put Them All Together"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#super",
          "text": "Let‚Äôs Super Learn this thing"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#opportunity",
          "text": "Opportunities for improvement"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#lessons",
          "text": "Lessons learnt"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#what",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#engine",
          "text": null
        },
        {
          "href": "https://vanderlaan-lab.org/2019/05/11/adaptive-algorithm-selection-via-the-super-learner/",
          "text": "oracle inequality"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Non-negative_least_squares",
          "text": "Non-Negative Least Squares (NNLS)"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#nnls",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#lha",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#step-0-initialize-your-sets",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#step-1-find-gradient",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#step-2-check-optimality--find-next-variable-to-add-to-p",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#step-3-solve-the-unconstrained-least-squares-problem-for-active-set-p",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#step-4-check-for-negative-weights",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#code",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#simulate-data",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#super",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#lets-compare-rmse-of-solo-models-vs-super-learner-models",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#comparing-our-nnls-to-nnls-package",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#opportunity",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/#lessons",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/",
          "text": "comment or visit my other blogs"
        },
        {
          "href": "https://bsky.app/profile/kenkoonwong.bsky.social",
          "text": "BlueSky"
        },
        {
          "href": "https://twitter.com/kenkoonwong/",
          "text": "twitter"
        },
        {
          "href": "https://github.com/kenkoonwong/",
          "text": "GitHub"
        },
        {
          "href": "https://med-mastodon.com/@kenkoonwong",
          "text": "Mastodon"
        },
        {
          "href": "https://www.kenkoonwong.com/contact/",
          "text": "contact me"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/superlearner/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "My Messy Notes on Building a Super Learner: Peeking Under The Hood of NNLS | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/ensemble.jpg?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-12-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-14-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-16-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-18-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-20-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-397826 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">My Messy Notes on Building a Super Learner: Peeking Under The Hood of NNLS</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">December 20, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/\">r on Everyday Is A School Day</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.kenkoonwong.com/blog/superlearner/\"> r on Everyday Is A School Day</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><blockquote>\n<p>üìö Tried building Super Learner from scratch to understand what‚Äôs happening under the hood. Walked through the NNLS algorithm step-by-step‚Äîturns out ensembling models may beat solo models! Our homegrown version? Surprisingly close to nnls package results ‚ù§Ô∏è But, does it really work in real life? ü§∑‚Äç‚ôÇÔ∏è</p>\n</blockquote>\n<p><img alt=\"\" data-lazy-src=\"https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/ensemble.jpg?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/ensemble.jpg?w=578&amp;ssl=1\"/></noscript></p>\n<h2 id=\"motivations\">Motivations\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#motivations\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Previously we have learnt the workflow of \n<a href=\"https://www.kenkoonwong.com/blog/tmle/\" rel=\"nofollow\" target=\"_blank\">TMLE</a> and most people would say to use it with Super Learner. But what is Super Learner? The name sounds fancy and cool. Let‚Äôs take a look under the hood of how super is this Super Learner. In this blog, we will see what non-negative least square is and what is the algorithm that is behind this method that fuels Super Learner. We‚Äôll take a look at the mathematical procedures and then code from scratch and see if we can reproduce the result. Let‚Äôs do this!</p>\n<h2 id=\"objectives\">Objectives:\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#objectives\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#what\" rel=\"nofollow\" target=\"_blank\">What is Super Learner?</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#engine\" rel=\"nofollow\" target=\"_blank\">What is the algorithm behind Super Learner?</a>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#nnls\" rel=\"nofollow\" target=\"_blank\">Non-negative Least Square</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#lha\" rel=\"nofollow\" target=\"_blank\">Lawson-Hanson algorithm</a></li>\n</ul>\n</li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#code\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs Put Them All Together</a>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#super\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs Super Learn this thing</a></li>\n</ul>\n</li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#opportunity\" rel=\"nofollow\" target=\"_blank\">Opportunities for improvement</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/superlearner/#lessons\" rel=\"nofollow\" target=\"_blank\">Lessons learnt</a></li>\n</ul>\n<h2 id=\"what\">What is Super Learner?\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#what\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Super Learner is an ensemble machine learning algorithm that optimally combines predictions from multiple candidate algorithms to create a single ensembled model. Rather than selecting a single ‚Äúbest‚Äù model through traditional model selection methods, Super Learner leverages the strengths of various algorithms by creating a weighted average of their predictions. The fundamental insight is elegant: why choose between a random forest, generalized linear model, or gradient boosting machine when you can let the data determine the optimal combination of all three? This approach was introduced by Mark van der Laan and colleagues and has become particularly popular in causal inference and epidemiology, often paired with Targeted Maximum Likelihood Estimation (TMLE) to obtain robust, efficient estimates of causal effects.</p>\n<h2 id=\"engine\">What is the algorithm behind Super Learner?\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#engine\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>The beauty of Super Learner lies in its theoretical guarantee: it will perform at least as well as the best single algorithm in your library of candidate learners, and often performs substantially better. This property, known as the \n<a href=\"https://vanderlaan-lab.org/2019/05/11/adaptive-algorithm-selection-via-the-super-learner/\" rel=\"nofollow\" target=\"_blank\">oracle inequality</a>, means that Super Learner asymptotically achieves the lowest possible prediction error among the combinations of the candidate algorithms. ü§î To be transparent, I don‚Äôt really understand all these. But, let‚Äôs move on. The engine behind this is \n<a href=\"https://en.wikipedia.org/wiki/Non-negative_least_squares\" rel=\"nofollow\" target=\"_blank\">Non-Negative Least Squares (NNLS)</a>, an elegant constrained optimization method that finds the optimal weights for combining your candidate algorithms.</p>\n<h3 id=\"nnls\">NNLS\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#nnls\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>At its core, NNLS solves a seemingly simple problem: given a matrix <code>X</code> of predictions from your candidate algorithms and an outcome vector <code>y</code>, find <code>weights Œ≤</code> that minimize the squared prediction error <code>||y - XŒ≤||¬≤</code> subject to two crucial constraints:</p>\n<ol>\n<li>all weights must be non-negative (Œ≤ ‚â• 0)</li>\n<li>the weights must sum to one (ensuring a proper convex combination).</li>\n</ol>\n<h3 id=\"lha\">Lawson-Hanson Algorithm\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#lha\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>The most commonly used algorithm for solving NNLS is the active set method developed by Lawson and Hanson in 1974. This iterative algorithm is remarkably intuitive: it maintains two sets of variables‚Äîan ‚Äúactive set‚Äù of variables currently in the model with positive weights, and a ‚Äúpassive set‚Äù of variables currently excluded (with zero weights). The algorithm begins with all variables in the passive set, then iteratively identifies which passive variable, if added to the active set, would most improve the fit. Once a variable enters the active set, the algorithm solves an unconstrained least squares problem using only the active variables. If any weights become negative during this step, the algorithm removes the most negative variable from the active set and repeats the process. This addition-and-removal dance continues until no passive variables would improve the fit and all active variables have positive weights‚Äîat which point we‚Äôve found our optimal solution.</p>\n<p>OK, too many words above. Not a fan. üòµ‚Äçüí´ Lots of procedures above, let‚Äôs break it down to steps and write a simple example with code to go through the process. Let‚Äôs create a simple example.</p>\n<details>\n<summary>code</summary>\n<pre>X &lt;- rbind(\n  c(1.5,3,4),\n  c(0.5,2,3),\n  c(4.5,6,6)\n)\n\ny &lt;- c(2,1,5)\n</pre></details>\n<p>$$\n\\begin{gather}\n\\text{X} =\n\\begin{bmatrix}\n1.5 &amp; 3 &amp; 4 \\\\\n0.5 &amp; 2 &amp; 3 \\\\\n4.5 &amp; 6 &amp; 6\n\\end{bmatrix}\n;\n\\text{y} =\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix}\n\\end{gather}\n$$</p>\n<p>Let‚Äôs take a quick look at the matrices above. Just glancing at it you would think the weights for each models (columns) should be within column <code>1</code> and column <code>2</code>. Let‚Äôs go through Lawson-hanson algorithm procedure</p>\n<h4 id=\"step-0-initialize-your-sets\">Step 0: Initialize Your Sets\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#step-0-initialize-your-sets\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>Start with all variables in the passive set (R) and none in the active set (P). Like so:</p>\n<p>$$\n\\text{P} = \\emptyset\n$$</p>\n<p>$$\nR = \\{1, 2, 3\\}\n$$\n$$\n\\beta =\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n$$</p>\n<ul>\n<li><code>P</code> : Active Set (Take note, I used P as active, not passive; also these are indexes)</li>\n<li><code>R</code> : Passive Set (take note, these are indexes)</li>\n<li><code>Œ≤</code> : weights</li>\n</ul>\n<p>We will go throught the iterative procedure below, move Passive set (R) one by one to Active set (P) until we no longer have any passive sets available.</p>\n<h4 id=\"step-1-find-gradient\">Step 1 Find Gradient\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#step-1-find-gradient\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<details>\n<summary>code</summary>\n<pre># step 1: find gradient \ngradient &lt;- t(X) %*% (y-X %*% beta)\nif (sum(gradient&lt;=0)==dim(X)[2]) { stop(\"all gradients are zero or negative, we have achieved optimality\") }\nif (length(R)==0) { stop(\"R is empty\")}\n</pre></details>\n<p>$$\n\\begin{gather}\n\\text{Gradient} = \\text{X}^{\\text{T}} \\cdot (\\text{y} - \\text{X}\\beta)\n\\end{gather}\n$$\nThe above is the procedure to find gradient for <code>||y - XŒ≤||¬≤</code>. Let‚Äôs put in the numbers and calculate</p>\n<p>$$\n\\begin{gather}\n\\text{Gradient} = \\text{X}^{\\text{T}} \\cdot (\\text{y} - \\text{X}\\beta) \\\\\n= \\begin{bmatrix}\n1.5 &amp; 3 &amp; 4 \\\\\n0.5 &amp; 2 &amp; 3 \\\\\n4.5 &amp; 6 &amp; 6\n\\end{bmatrix}^\\text{T} \\cdot (\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix} -\n\\begin{bmatrix}\n1.5 &amp; 3 &amp; 4 \\\\\n0.5 &amp; 2 &amp; 3 \\\\\n4.5 &amp; 6 &amp; 6\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n) \\\\\n= \\begin{bmatrix}\n1.5 &amp; 0.5 &amp; 4.5 \\\\\n3 &amp; 2 &amp; 6 \\\\\n4 &amp; 3 &amp; 6\n\\end{bmatrix} \\cdot (\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix} -\n\\begin{bmatrix}\n1.5 &amp; 3 &amp; 4 \\\\\n0.5 &amp; 2 &amp; 3 \\\\\n4.5 &amp; 6 &amp; 6\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n) \\\\\n= \\begin{bmatrix}\n1.5 &amp; 0.5 &amp; 4.5 \\\\\n3 &amp; 2 &amp; 6 \\\\\n4 &amp; 3 &amp; 6\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n26.5 \\\\ 44 \\\\ 41\n\\end{bmatrix}\n\\end{gather}\n$$</p>\n<h4 id=\"step-2-check-optimality--find-next-variable-to-add-to-p\">Step 2 Check Optimality &amp; Find Next Variable to Add To P\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#step-2-check-optimality--find-next-variable-to-add-to-p\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>If all gradients are non-positive, we have achieved optimality. If not, proceed to the next step. Find the index of R of the maximum gradient. In this case, max of <code>26.5, 44, 41</code> is <code>44</code>, which is the second column of <code>X</code>.</p>\n<p>$$\n\\begin{gather}\n\\text{Next Variable} = \\text{argmax}_{j \\in R} \\text{Gradient}_j \\\\\n= 2\n\\end{gather}\n$$\nWe then move <code>2</code> from <code>R</code> (passive set) to <code>P</code> (active set) like so:</p>\n<p>$$\n\\text{P} = \\{2\\}\n$$\n$$\nR = \\{1, 3\\}\n$$</p>\n<h4 id=\"step-3-solve-the-unconstrained-least-squares-problem-for-active-set-p\">Step 3 Solve the Unconstrained Least Squares Problem for Active Set P.\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#step-3-solve-the-unconstrained-least-squares-problem-for-active-set-p\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\begin{gather}\n\\beta_P = (\\text{X}_P^{\\text{T}} \\cdot \\text{X}_P)^{-1} \\cdot \\text{X}_P^{\\text{T}} \\cdot \\text{y} \\\\\n= (\\begin{bmatrix}\n3 \\\\\n2 \\\\\n6\n\\end{bmatrix}^{\\text{T}} \\cdot\n\\begin{bmatrix}\n3 \\\\\n2 \\\\\n6\n\\end{bmatrix})^{-1} \\cdot\n\\begin{bmatrix}\n3 \\\\\n2 \\\\\n6\n\\end{bmatrix}^{\\text{T}} \\cdot\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n5\n\\end{bmatrix} \\\\\n= (9 + 4 + 36)^{-1} \\cdot\n\\begin{bmatrix}\n3 &amp; 2 &amp; 6\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n5\n\\end{bmatrix} \\\\\n= 49^{-1} \\cdot\n\\begin{bmatrix}\n3 \\cdot 2 + 2 \\cdot 1 + 6 \\cdot 5\n\\end{bmatrix} \\\\\n= 49^{-1} \\cdot\n\\begin{bmatrix}\n44\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n0.8979592\n\\end{bmatrix}\n\\end{gather}\n$$</p>\n<p>Where <code>X_P</code> is the sub-matrix of <code>X</code> containing only the columns in the active set <code>P</code>. In our case, <code>P = {2}</code>, so <code>Œ≤</code> is:</p>\n<p>$$\n\\begin{gather}\n\\beta =\n\\begin{bmatrix}\n0 \\\\ 0.8979592 \\\\ 0\n\\end{bmatrix}\n\\end{gather}\n$$\nStill with me? We went from initializing zero weights (beta) for all 3 to now with the second model having weight of <code>0.89796</code></p>\n<h4 id=\"step-4-check-for-negative-weights\">Step 4 Check For Negative Weights\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#step-4-check-for-negative-weights\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\begin{gather}\n\\text{If any } \\beta_P \\leq 0, \\text{ calculate } \\alpha \\text{ else go back to step 1}\\\\\n\\alpha = \\min_{\\beta_P \\leq 0} \\frac{\\beta_{old}}{\\beta_{old} - \\beta_P} \\\\\n\\text{If } \\alpha &lt; 1, \\text{ update } \\beta = \\beta_{old} + \\alpha (\\beta_P - \\beta_{old}) \\\\\n\\text{Remove any variables from P where } \\beta \\leq 0 \\text{ and return them to R}\n\\end{gather}\n$$\nSince our weights <code>$\\beta$</code> cannot be negative, and if we hit a negative value, we want to shift all <code>\\(\\beta\\)</code> by <code>\\(\\alpha\\)</code> proportion of the difference and make the calculated negative weight <code>0</code> and adjust the other weights equally.</p>\n<p>After the above, we go iterate until <code>R</code> set is empty. You get the point, instead of latex the entire calculation, let‚Äôs use code to get to our answers.</p>\n<pre>P &lt;- c()\nR &lt;- c(1:dim(X)[2])\nbeta &lt;- rep(0, dim(X)[2])\n\nwhile (T) {\n# step 1: find gradient \ngradient &lt;- t(X) %*% (y-X %*% beta)\nif (sum(gradient&lt;=0)==dim(X)[2]) { print(\"all gradients are zero or negative, we have achieved optimality\") ; break }\nif (length(R)==0) { print(\"R is empty\") ; break }\n\n# step 2: check optimality\ngradient_not_active &lt;- gradient\ngradient_not_active[P] &lt;- -Inf\nP_x &lt;- which(gradient_not_active==max(gradient_not_active))\nP &lt;- c(P,P_x) |&gt; unique() |&gt; sort()\nR &lt;- setdiff(R, P_x)\n\n# solve P\nbeta_i &lt;- beta\nbeta_i[P] &lt;- solve(t(X[,P]) %*% X[,P]) %*% t(X[,P])%*%y\nif (any(beta_i&lt;0)) { \n  print(paste0(\"negative weights: \",paste(beta_i, collapse = \" \")))  \n  idx &lt;- which(beta_i&lt;0)\n  beta_old &lt;- beta[idx]\n  beta_new &lt;- beta_i[idx]\n  alpha &lt;- beta_old/-(beta_new-beta_old)\n  beta_i_new &lt;- beta - alpha*(beta-beta_i) \n  beta &lt;- beta_i_new |&gt; round(digits = 4)\n  print(paste0(\"new weights after setting negative weight as zero: \", paste(beta,collapse = \" \")))\n  } else {  beta &lt;- beta_i ; print(beta) }\n}\n\n## [1] 0.0000000 0.0000000 0.6721311\n## [1] 0.8683544 0.0000000 0.1810127\n## [1] \"negative weights: 0.666666666666675 0.333333333333364 -7.105427357601e-15\"\n## [1] \"new weights after setting negative weight as zero: 0.6667 0.3333 0\"\n## [1] \"R is empty\"\n\nbeta\n\n## [1] 0.6667 0.3333 0.0000\n\nX %*% beta\n\n##         [,1]\n## [1,] 1.99995\n## [2,] 0.99995\n## [3,] 4.99995\n</pre><p>Wow, it worked! Look at our weights (beta) and our final results! As suspected, column 1 and 2 will have the weights (more on column 1) and when combined our final numbers are quite close to our <code>y</code>, which is 2, 1, 5 . Awesome! Now, let‚Äôs simulate more data and see if our code works and compare it with <code>nnls</code> package!</p>\n<h2 id=\"code\">Let‚Äôs Put Them All Together\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#code\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<h4 id=\"simulate-data\">Simulate Data\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#simulate-data\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<pre># labels/outcome/y\nnum_labels &lt;- 1000\nlabel_range &lt;- 1:5\ny &lt;- sample(label_range, num_labels, replace=T)\n\n# X matrix\nnum_models &lt;- 5\nX &lt;- matrix(nrow = num_labels, ncol = num_models)\nfor (i in 1:num_models) {\n  sd &lt;- sample(c(0.01,1,10))\n  for (j in 1:num_labels) {\n  X[j, i] &lt;- rnorm(1, mean = y[j], sd = sd)\n}\n}\n</pre><p>Alright, what we did above is basically simulated <code>y</code> and <code>X</code></p>\n<pre>P &lt;- c()\nR &lt;- c(1:dim(X)[2])\nbeta &lt;- rep(0, dim(X)[2])\n\nwhile (T) {\n# step 1: find gradient \ngradient &lt;- t(X) %*% (y-X %*% beta)\nif (sum(gradient&lt;=0)==dim(X)[2]) { print(\"all gradients are zero or negative, we have achieved optimality\") ; break }\nif (length(R)==0) { print(\"R is empty\") ; break }\n\n# step 2: check optimality\ngradient_not_active &lt;- gradient\ngradient_not_active[P] &lt;- -Inf\nP_x &lt;- which(gradient_not_active==max(gradient_not_active))\nP &lt;- c(P,P_x) |&gt; unique() |&gt; sort()\nR &lt;- setdiff(R, P_x)\n\n# solve P\nbeta_i &lt;- beta\nbeta_i[P] &lt;- solve(t(X[,P]) %*% X[,P]) %*% t(X[,P])%*%y\nif (any(beta_i&lt;0)) { \n  print(paste0(\"negative weights: \",paste(beta_i, collapse = \" \")))  \n  idx &lt;- which(beta_i&lt;0)\n  beta_old &lt;- beta[idx]\n  beta_new &lt;- beta_i[idx]\n  alpha &lt;- beta_old/-(beta_new-beta_old)\n  beta_i_new &lt;- beta - alpha*(beta-beta_i) \n  beta &lt;- beta_i_new |&gt; round(digits = 4)\n  print(paste0(\"new weights after setting negative weight as zero: \", paste(beta,collapse = \" \")))\n  } else {  beta &lt;- beta_i ; print(beta) }\n}\n\n## [1] 0.9998029 0.0000000 0.0000000 0.0000000 0.0000000\n## [1] 0.5145682 0.0000000 0.0000000 0.4853314 0.0000000\n## [1] 0.3242697 0.3571370 0.0000000 0.3184922 0.0000000\n## [1] 0.2346782 0.2762402 0.0000000 0.2480602 0.2409658\n## [1] 0.1884031 0.2342831 0.1677427 0.2059978 0.2035208\n## [1] \"R is empty\"\n</pre><p>Let‚Äôs look at our weights and RMSE</p>\n<pre>beta\n\n## [1] 0.1884031 0.2342831 0.1677427 0.2059978 0.2035208\n\nsqrt(mean((y - X %*% beta)^2))\n\n## [1] 0.004614622\n</pre><p>Let‚Äôs look at <code>nnls</code> package and see if we can the same result</p>\n<pre>model &lt;- nnls::nnls(A=X,b=y)\nmodel\n\n## Nonnegative least squares model\n## x estimates: 0.1884031 0.2342831 0.1677427 0.2059978 0.2035208 \n## residual sum-of-squares: 0.02129\n## reason terminated: The solution has been computed sucessfully.\n\nsqrt(mean((y - X %*% model$x)^2))\n\n## [1] 0.004614622\n</pre><p>wow! Awesome!!! Looks the same or at least very similar. Alright, now we‚Äôre at least able to reproduce the nnls portion from scratch. Let‚Äôs see if we can simulate a non-linear data and train with different models and see how our end result is!</p>\n<h3 id=\"super\">Let‚Äôs Super Learn this thing\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#super\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>Click below at <code>code</code> to expand for the entire procedures. We basically ran 3 different models in tidymodels, linear regression, xgboost, and random forest with recipe (y ~ .), not specifying any interaction/polynomial relationships for a simulated data below.</p>\n<pre>n &lt;- 1000\nx &lt;- rnorm(n)\nw &lt;- rnorm(n, 0.5*x)\ny &lt;- 0.2*x + 0.5*w + 0.2*x*w + 0.05*x^2\n</pre><p>Made sure to set seed for reproducibility, create 5 fold for cross validation. Then extract all the prediction for validation sets from each models and stack them into <code>X</code> matrix. Then extract the RMSE from each models and stack them into <code>metrics</code> matrix. Finally, we run our nnls code above to get the weights and RMSE for super learner. We repeat this for 1000 iterations and log the results.</p>\n<p>This cross-validation step is the defining feature of the Super Learner. By fitting each base learner on training folds and generating out-of-fold predictions, we obtain an unbiased prediction matrix that is then used to estimate optimal ensemble weights via NNLS.</p>\n<details>\n<summary>code</summary>\n<pre>library(tidymodels)\nlibrary(future)\nlibrary(furrr)\n\n# Set up parallel processing\nplan(multisession, workers = availableCores() - 2)\n\n# Define the function to run for each iteration\nrun_iteration &lt;- function(i) {\n  set.seed(i)\n  n &lt;- 1000\n  x &lt;- rnorm(n)\n  w &lt;- rnorm(n, 0.5*x)\n  y &lt;- 0.2*x + 0.5*w + 0.2*x*w + 0.05*x^2\n  \n  df &lt;- tibble(x,w,y)\n  split &lt;- initial_split(df)\n  train &lt;- training(split)\n  test &lt;- testing(split)\n  # preprocess\n  rec &lt;- recipe(y ~ ., data=train) \n  \n  # linear regression\n  lr_spec &lt;- linear_reg()\n  wf &lt;- workflow() |&gt;\n    add_recipe(rec) |&gt;\n    add_model(lr_spec)\n  folds &lt;- vfold_cv(train, 5)\n  cv_results &lt;- wf |&gt;\n    fit_resamples(folds, control = control_resamples(save_pred = TRUE))\n  cv_metrics &lt;- collect_metrics(cv_results) |&gt; filter(.metric == \"rmse\") |&gt; pull(mean)\n  cv_preds &lt;- collect_predictions(cv_results) \n  \n  #xgboost\n  xgb_spec &lt;- boost_tree(engine = \"xgboost\", mode = \"regression\")\n  wf &lt;- workflow() |&gt;\n    add_recipe(rec) |&gt;\n    add_model(xgb_spec)\n  cv_results &lt;- wf |&gt;\n    fit_resamples(folds, control = control_resamples(save_pred = T))\n  cv_metrics2 &lt;- collect_metrics(cv_results) |&gt; filter(.metric == \"rmse\") |&gt; pull(mean)\n  cv_preds2 &lt;- collect_predictions(cv_results) \n  \n  # random forest\n  rf_spec &lt;- rand_forest(mode = \"regression\")\n  wf &lt;- workflow() |&gt;\n    add_recipe(rec) |&gt;\n    add_model(rf_spec)\n  cv_results &lt;- wf |&gt;\n    fit_resamples(folds, control = control_resamples(save_pred = T))\n  cv_metrics3 &lt;- collect_metrics(cv_results) |&gt; filter(.metric == \"rmse\") |&gt; pull(mean)\n  cv_preds3 &lt;- collect_predictions(cv_results) |&gt;\n    mutate(model = \"rf\")\n  X &lt;- cbind(cv_preds |&gt; select(X1=.pred),cv_preds2 |&gt; select(X2=.pred), cv_preds3 |&gt; select(X3=.pred)) |&gt; as.matrix()\n  y &lt;- cv_preds |&gt; select(y) |&gt; as.matrix()\n  metrics &lt;- cbind(cv_metrics,cv_metrics2,cv_metrics3)\n  \n  # nnls\n  P &lt;- c()\n  R &lt;- c(1:dim(X)[2])\n  beta &lt;- rep(0, dim(X)[2])\n  \n  while (T) {\n    # step 1: find gradient \n    gradient &lt;- t(X) %*% (y-X %*% beta)\n    if (sum(gradient&lt;=0)==dim(X)[2]) { print(\"all gradients are zero or negative, we have achieved optimality\") ; break }\n    if (length(R)==0) { print(\"R is empty\") ; break }\n    # step 2: check optimality\n    gradient_not_active &lt;- gradient\n    gradient_not_active[P] &lt;- -Inf\n    P_x &lt;- which(gradient_not_active==max(gradient_not_active))\n    P &lt;- c(P,P_x) |&gt; unique() |&gt; sort()\n    R &lt;- setdiff(R, P_x)\n    # solve P\n    beta_i &lt;- beta\n    beta_i[P] &lt;- solve(t(X[,P]) %*% X[,P]) %*% t(X[,P])%*%y\n    if (any(beta_i&lt;0)) { \n      print(paste0(\"negative weights: \",paste(beta_i, collapse = \" \")))  \n      idx &lt;- which(beta_i&lt;0)\n      beta_old &lt;- beta[idx]\n      beta_new &lt;- beta_i[idx]\n      alpha &lt;- beta_old/-(beta_new-beta_old)\n      beta_i_new &lt;- beta - alpha*(beta-beta_i) \n      beta &lt;- beta_i_new |&gt; round(digits = 4)\n      print(paste0(\"new weights after setting negative weight as zero: \", paste(beta,collapse = \" \")))\n    } else {  beta &lt;- beta_i ; print(beta) }\n  }\n\n  rmse_superlearner &lt;- sqrt(mean((y - X %*% beta)^2))\n  rmse_result &lt;- if (sum(metrics &lt; rmse_superlearner) &gt;= 1) { \"solo_better\" } else { \"superlearner_better\" }\n  \n  model &lt;- nnls::nnls(A=X,b=y)\n  rmse_ours_nnls &lt;- c(rmse_superlearner, sqrt(mean((y - X %*% model$x)^2)))\n  same_weights_result &lt;- if (sum(round(beta, 4) == round(model$x, 4)) == 3) { \"same\" } else { \"not_same\" }\n  weights_log &lt;- c(model$x, beta)\n  \n  return(list(rmse_log = rmse_result, same_weights_log = same_weights_result, weights_log=weights_log))\n}\n\n# Run with future_map\nresults &lt;- future_map(1:1000, run_iteration, .options = furrr_options(seed = TRUE), .progress = TRUE)\n</pre></details>\n<h4 id=\"lets-compare-rmse-of-solo-models-vs-super-learner-models\">Let‚Äôs Compare RMSE of Solo models vs Super Learner models\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#lets-compare-rmse-of-solo-models-vs-super-learner-models\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<details>\n<summary>code</summary>\n<pre>library(tidyverse)\n\n# Extract results\nrmse_log &lt;- map_chr(results, \"rmse_log\")\nsame_weights_log &lt;- map_chr(results, \"same_weights_log\")\nweight_logs &lt;- matrix(NA, ncol = 6, nrow = 1000)\nfor (i in 1:1000) {\n  weight_logs[i, 1:6] &lt;- results[[i]]$weights_log\n}\n\nplotrmse_log &lt;- tibble(rmse=rmse_log) |&gt;\n  ggplot(aes(x=rmse)) +\n  geom_bar() +\n  theme_bw()\n</pre></details>\n<img data-lazy-src=\"https://i1.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Wow, look at that! superlearner/ensembled model does appear to have better RMSE compared to solo models! Let‚Äôs take a look and see if our noob nnls from scratch is comparable with <code>nnls</code> package.</p>\n<h4 id=\"comparing-our-nnls-to-nnls-package\">Comparing Our NNLS to <code>nnls</code> package\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#comparing-our-nnls-to-nnls-package\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<details>\n<summary>code</summary>\n<pre>plotsameweights &lt;- tibble(same_weights=same_weights_log) |&gt;\n  ggplot(aes(x=same_weights)) +\n  geom_bar() +\n  theme_bw()\n</pre></details>\n<img data-lazy-src=\"https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-14-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-14-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Wow, most of the weights are the same if we round up to 4 digits! Let‚Äôs check on the ones with difference, is it REALLY that different?</p>\n<details>\n<summary>code</summary>\n<pre>plotdiff123 &lt;- weight_logs |&gt;\n  as_tibble() |&gt;\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |&gt;\n  filter(sum_diff != 0) |&gt;\n  pivot_longer(cols = c(diff1,diff2,diff3), names_to = \"diff\", values_to = \"values\") |&gt;\n  ggplot(aes(x=values,fill=diff)) +\n  geom_histogram(position = \"dodge2\") +\n  theme_bw()\n</pre></details>\n<img data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-16-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-16-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>This makes sense, most of the differences are between <code>xgboost</code> (diff2) and <code>random forest</code> (diff3), as our linear regression (diff1) model without correct specification probably won‚Äôt have a whole of contributions, hence if there is a difference between our algorithm and <code>nnls</code>, it would be minimal (center in red). It also make sense that if there is a difference in xgboost or random forest model, we would see different weight on the other model contribution. Now the question is, with these weight differences, does it make a huge difference in RMSE? I suspect not so much.</p>\n<details>\n<summary>code</summary>\n<pre>rmse_compare &lt;- matrix(NA, ncol = 2, nrow = 1000)\nfor (i in 1:1000) {\n  rmse_compare[i,1:2] &lt;- results[[i]]$rmse_ours_nnls \n}\n\nplotcompare &lt;- weight_logs |&gt;\n  as_tibble() |&gt;\n  mutate(row = row_number()) |&gt;\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |&gt;\n  filter(sum_diff != 0) |&gt;\n  left_join(as_tibble(rmse_compare) |&gt;\n              mutate(row = row_number()), by = \"row\") |&gt;\n  mutate(V1.y = round(V1.y, 4),\n         V2.y = round(V2.y, 4)) |&gt;\n  mutate(check = case_when(\n    V1.y == V2.y ~ \"same\",\n    V1.y &lt; V2.y ~ \"our_nnls_better\",\n    V1.y &gt; V2.y ~ \"nnls_package_better\",\n    TRUE ~ NA_character_\n  )) |&gt;\n  ggplot(aes(x=check)) +\n  geom_bar() +\n  theme_bw()\n</pre></details>\n<img data-lazy-src=\"https://i1.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-18-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-18-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>lol, <code>nnls</code> package clearly is better than our home-grown algorithm! But by how much?</p>\n<details>\n<summary>code</summary>\n<pre>plotdiffrmse &lt;- weight_logs |&gt;\n  as_tibble() |&gt;\n  mutate(row = row_number()) |&gt;\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |&gt;\n  filter(sum_diff != 0) |&gt;\n  left_join(as_tibble(rmse_compare) |&gt;\n              mutate(row = row_number()), by = \"row\") |&gt;\n  mutate(V1.y = round(V1.y, 4),\n         V2.y = round(V2.y, 4)) |&gt;\n  mutate(diff_rmse = V1.y - V2.y) |&gt;\n  ggplot(aes(diff_rmse)) +\n  geom_histogram() +\n  theme_bw() \n</pre></details>\n<img data-lazy-src=\"https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-20-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.kenkoonwong.com/blog/superlearner/index_files/figure-html/unnamed-chunk-20-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>üòµ‚Äçüí´ It‚Äôs really not that much different! Let‚Äôs find the max.</p>\n<details>\n<summary>code</summary>\n<pre>weight_logs |&gt;\n  as_tibble() |&gt;\n  mutate(row = row_number()) |&gt;\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |&gt;\n  filter(sum_diff != 0) |&gt;\n  left_join(as_tibble(rmse_compare) |&gt;\n              mutate(row = row_number()), by = \"row\") |&gt;\n  mutate(V1.y = round(V1.y, 4),\n         V2.y = round(V2.y, 4)) |&gt;\n  mutate(diff_rmse = V1.y - V2.y) |&gt;\n  pull(diff_rmse) |&gt; max()\n\n## [1] 9e-04\n</pre></details>\n<pre>## [1] 9e-04\n</pre><p>ü•π Does that mean our home-grown algorithm works just as well? You be the judge. Let me know if this is due to pure luck!</p>\n<h2 id=\"opportunity\">Opportunities for improvement\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#opportunity\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>will try multicore sometime in the future, is it really faster than multisession?</li>\n<li>need to learn/figure out FAST nnls algorithm which I believe <code>nnls</code> package uses</li>\n<li>need to venture more in parallel computing</li>\n<li>compare with the actual <code>SuperLearner</code> package</li>\n</ul>\n<h2 id=\"lessons\">Lessons learnt\n  <a href=\"https://www.kenkoonwong.com/blog/superlearner/#lessons\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>learnt to build Super Learner using non-negative least square model</li>\n<li>learnt Lawson-Hanson algorithm and how it‚Äôs implemented, compared with <code>nnls</code> and results not too shabby!</li>\n<li>learnt some basics of parallel computing</li>\n</ul>\n<p>If you like this article:</p>\n<ul>\n<li>please feel free to send me a \n<a href=\"https://www.kenkoonwong.com/blog/\" rel=\"nofollow\" target=\"_blank\">comment or visit my other blogs</a></li>\n<li>please feel free to follow me on \n<a href=\"https://bsky.app/profile/kenkoonwong.bsky.social\" rel=\"nofollow\" target=\"_blank\">BlueSky</a>, \n<a href=\"https://twitter.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">twitter</a>, \n<a href=\"https://github.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">GitHub</a> or \n<a href=\"https://med-mastodon.com/@kenkoonwong\" rel=\"nofollow\" target=\"_blank\">Mastodon</a></li>\n<li>if you would like collaborate please feel free to \n<a href=\"https://www.kenkoonwong.com/contact/\" rel=\"nofollow\" target=\"_blank\">contact me</a></li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.kenkoonwong.com/blog/superlearner/\"> r on Everyday Is A School Day</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "My Messy Notes on Building a Super Learner: Peeking Under The Hood of NNLS\nPosted on\nDecember 20, 2025\nby\nr on Everyday Is A School Day\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nr on Everyday Is A School Day\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nüìö Tried building Super Learner from scratch to understand what‚Äôs happening under the hood. Walked through the NNLS algorithm step-by-step‚Äîturns out ensembling models may beat solo models! Our homegrown version? Surprisingly close to nnls package results ‚ù§Ô∏è But, does it really work in real life? ü§∑‚Äç‚ôÇÔ∏è\nMotivations\nPreviously we have learnt the workflow of\nTMLE\nand most people would say to use it with Super Learner. But what is Super Learner? The name sounds fancy and cool. Let‚Äôs take a look under the hood of how super is this Super Learner. In this blog, we will see what non-negative least square is and what is the algorithm that is behind this method that fuels Super Learner. We‚Äôll take a look at the mathematical procedures and then code from scratch and see if we can reproduce the result. Let‚Äôs do this!\nObjectives:\nWhat is Super Learner?\nWhat is the algorithm behind Super Learner?\nNon-negative Least Square\nLawson-Hanson algorithm\nLet‚Äôs Put Them All Together\nLet‚Äôs Super Learn this thing\nOpportunities for improvement\nLessons learnt\nWhat is Super Learner?\nSuper Learner is an ensemble machine learning algorithm that optimally combines predictions from multiple candidate algorithms to create a single ensembled model. Rather than selecting a single ‚Äúbest‚Äù model through traditional model selection methods, Super Learner leverages the strengths of various algorithms by creating a weighted average of their predictions. The fundamental insight is elegant: why choose between a random forest, generalized linear model, or gradient boosting machine when you can let the data determine the optimal combination of all three? This approach was introduced by Mark van der Laan and colleagues and has become particularly popular in causal inference and epidemiology, often paired with Targeted Maximum Likelihood Estimation (TMLE) to obtain robust, efficient estimates of causal effects.\nWhat is the algorithm behind Super Learner?\nThe beauty of Super Learner lies in its theoretical guarantee: it will perform at least as well as the best single algorithm in your library of candidate learners, and often performs substantially better. This property, known as the\noracle inequality\n, means that Super Learner asymptotically achieves the lowest possible prediction error among the combinations of the candidate algorithms. ü§î To be transparent, I don‚Äôt really understand all these. But, let‚Äôs move on. The engine behind this is\nNon-Negative Least Squares (NNLS)\n, an elegant constrained optimization method that finds the optimal weights for combining your candidate algorithms.\nNNLS\nAt its core, NNLS solves a seemingly simple problem: given a matrix\nX\nof predictions from your candidate algorithms and an outcome vector\ny\n, find\nweights Œ≤\nthat minimize the squared prediction error\n||y - XŒ≤||¬≤\nsubject to two crucial constraints:\nall weights must be non-negative (Œ≤ ‚â• 0)\nthe weights must sum to one (ensuring a proper convex combination).\nLawson-Hanson Algorithm\nThe most commonly used algorithm for solving NNLS is the active set method developed by Lawson and Hanson in 1974. This iterative algorithm is remarkably intuitive: it maintains two sets of variables‚Äîan ‚Äúactive set‚Äù of variables currently in the model with positive weights, and a ‚Äúpassive set‚Äù of variables currently excluded (with zero weights). The algorithm begins with all variables in the passive set, then iteratively identifies which passive variable, if added to the active set, would most improve the fit. Once a variable enters the active set, the algorithm solves an unconstrained least squares problem using only the active variables. If any weights become negative during this step, the algorithm removes the most negative variable from the active set and repeats the process. This addition-and-removal dance continues until no passive variables would improve the fit and all active variables have positive weights‚Äîat which point we‚Äôve found our optimal solution.\nOK, too many words above. Not a fan. üòµ‚Äçüí´ Lots of procedures above, let‚Äôs break it down to steps and write a simple example with code to go through the process. Let‚Äôs create a simple example.\ncode\nX <- rbind(\n  c(1.5,3,4),\n  c(0.5,2,3),\n  c(4.5,6,6)\n)\n\ny <- c(2,1,5)\n$$\n\\begin{gather}\n\\text{X} =\n\\begin{bmatrix}\n1.5 & 3 & 4 \\\\\n0.5 & 2 & 3 \\\\\n4.5 & 6 & 6\n\\end{bmatrix}\n;\n\\text{y} =\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix}\n\\end{gather}\n$$\nLet‚Äôs take a quick look at the matrices above. Just glancing at it you would think the weights for each models (columns) should be within column\n1\nand column\n2\n. Let‚Äôs go through Lawson-hanson algorithm procedure\nStep 0: Initialize Your Sets\nStart with all variables in the passive set (R) and none in the active set (P). Like so:\n$$\n\\text{P} = \\emptyset\n$$\n$$\nR = \\{1, 2, 3\\}\n$$\n$$\n\\beta =\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n$$\nP\n: Active Set (Take note, I used P as active, not passive; also these are indexes)\nR\n: Passive Set (take note, these are indexes)\nŒ≤\n: weights\nWe will go throught the iterative procedure below, move Passive set (R) one by one to Active set (P) until we no longer have any passive sets available.\nStep 1 Find Gradient\ncode\n# step 1: find gradient \ngradient <- t(X) %*% (y-X %*% beta)\nif (sum(gradient<=0)==dim(X)[2]) { stop(\"all gradients are zero or negative, we have achieved optimality\") }\nif (length(R)==0) { stop(\"R is empty\")}\n$$\n\\begin{gather}\n\\text{Gradient} = \\text{X}^{\\text{T}} \\cdot (\\text{y} - \\text{X}\\beta)\n\\end{gather}\n$$\nThe above is the procedure to find gradient for\n||y - XŒ≤||¬≤\n. Let‚Äôs put in the numbers and calculate\n$$\n\\begin{gather}\n\\text{Gradient} = \\text{X}^{\\text{T}} \\cdot (\\text{y} - \\text{X}\\beta) \\\\\n= \\begin{bmatrix}\n1.5 & 3 & 4 \\\\\n0.5 & 2 & 3 \\\\\n4.5 & 6 & 6\n\\end{bmatrix}^\\text{T} \\cdot (\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix} -\n\\begin{bmatrix}\n1.5 & 3 & 4 \\\\\n0.5 & 2 & 3 \\\\\n4.5 & 6 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n) \\\\\n= \\begin{bmatrix}\n1.5 & 0.5 & 4.5 \\\\\n3 & 2 & 6 \\\\\n4 & 3 & 6\n\\end{bmatrix} \\cdot (\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix} -\n\\begin{bmatrix}\n1.5 & 3 & 4 \\\\\n0.5 & 2 & 3 \\\\\n4.5 & 6 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n) \\\\\n= \\begin{bmatrix}\n1.5 & 0.5 & 4.5 \\\\\n3 & 2 & 6 \\\\\n4 & 3 & 6\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n2 \\\\ 1 \\\\ 5\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n26.5 \\\\ 44 \\\\ 41\n\\end{bmatrix}\n\\end{gather}\n$$\nStep 2 Check Optimality & Find Next Variable to Add To P\nIf all gradients are non-positive, we have achieved optimality. If not, proceed to the next step. Find the index of R of the maximum gradient. In this case, max of\n26.5, 44, 41\nis\n44\n, which is the second column of\nX\n.\n$$\n\\begin{gather}\n\\text{Next Variable} = \\text{argmax}_{j \\in R} \\text{Gradient}_j \\\\\n= 2\n\\end{gather}\n$$\nWe then move\n2\nfrom\nR\n(passive set) to\nP\n(active set) like so:\n$$\n\\text{P} = \\{2\\}\n$$\n$$\nR = \\{1, 3\\}\n$$\nStep 3 Solve the Unconstrained Least Squares Problem for Active Set P.\n$$\n\\begin{gather}\n\\beta_P = (\\text{X}_P^{\\text{T}} \\cdot \\text{X}_P)^{-1} \\cdot \\text{X}_P^{\\text{T}} \\cdot \\text{y} \\\\\n= (\\begin{bmatrix}\n3 \\\\\n2 \\\\\n6\n\\end{bmatrix}^{\\text{T}} \\cdot\n\\begin{bmatrix}\n3 \\\\\n2 \\\\\n6\n\\end{bmatrix})^{-1} \\cdot\n\\begin{bmatrix}\n3 \\\\\n2 \\\\\n6\n\\end{bmatrix}^{\\text{T}} \\cdot\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n5\n\\end{bmatrix} \\\\\n= (9 + 4 + 36)^{-1} \\cdot\n\\begin{bmatrix}\n3 & 2 & 6\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n5\n\\end{bmatrix} \\\\\n= 49^{-1} \\cdot\n\\begin{bmatrix}\n3 \\cdot 2 + 2 \\cdot 1 + 6 \\cdot 5\n\\end{bmatrix} \\\\\n= 49^{-1} \\cdot\n\\begin{bmatrix}\n44\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n0.8979592\n\\end{bmatrix}\n\\end{gather}\n$$\nWhere\nX_P\nis the sub-matrix of\nX\ncontaining only the columns in the active set\nP\n. In our case,\nP = {2}\n, so\nŒ≤\nis:\n$$\n\\begin{gather}\n\\beta =\n\\begin{bmatrix}\n0 \\\\ 0.8979592 \\\\ 0\n\\end{bmatrix}\n\\end{gather}\n$$\nStill with me? We went from initializing zero weights (beta) for all 3 to now with the second model having weight of\n0.89796\nStep 4 Check For Negative Weights\n$$\n\\begin{gather}\n\\text{If any } \\beta_P \\leq 0, \\text{ calculate } \\alpha \\text{ else go back to step 1}\\\\\n\\alpha = \\min_{\\beta_P \\leq 0} \\frac{\\beta_{old}}{\\beta_{old} - \\beta_P} \\\\\n\\text{If } \\alpha < 1, \\text{ update } \\beta = \\beta_{old} + \\alpha (\\beta_P - \\beta_{old}) \\\\\n\\text{Remove any variables from P where } \\beta \\leq 0 \\text{ and return them to R}\n\\end{gather}\n$$\nSince our weights\n$\\beta$\ncannot be negative, and if we hit a negative value, we want to shift all\n\\(\\beta\\)\nby\n\\(\\alpha\\)\nproportion of the difference and make the calculated negative weight\n0\nand adjust the other weights equally.\nAfter the above, we go iterate until\nR\nset is empty. You get the point, instead of latex the entire calculation, let‚Äôs use code to get to our answers.\nP <- c()\nR <- c(1:dim(X)[2])\nbeta <- rep(0, dim(X)[2])\n\nwhile (T) {\n# step 1: find gradient \ngradient <- t(X) %*% (y-X %*% beta)\nif (sum(gradient<=0)==dim(X)[2]) { print(\"all gradients are zero or negative, we have achieved optimality\") ; break }\nif (length(R)==0) { print(\"R is empty\") ; break }\n\n# step 2: check optimality\ngradient_not_active <- gradient\ngradient_not_active[P] <- -Inf\nP_x <- which(gradient_not_active==max(gradient_not_active))\nP <- c(P,P_x) |> unique() |> sort()\nR <- setdiff(R, P_x)\n\n# solve P\nbeta_i <- beta\nbeta_i[P] <- solve(t(X[,P]) %*% X[,P]) %*% t(X[,P])%*%y\nif (any(beta_i<0)) { \n  print(paste0(\"negative weights: \",paste(beta_i, collapse = \" \")))  \n  idx <- which(beta_i<0)\n  beta_old <- beta[idx]\n  beta_new <- beta_i[idx]\n  alpha <- beta_old/-(beta_new-beta_old)\n  beta_i_new <- beta - alpha*(beta-beta_i) \n  beta <- beta_i_new |> round(digits = 4)\n  print(paste0(\"new weights after setting negative weight as zero: \", paste(beta,collapse = \" \")))\n  } else {  beta <- beta_i ; print(beta) }\n}\n\n## [1] 0.0000000 0.0000000 0.6721311\n## [1] 0.8683544 0.0000000 0.1810127\n## [1] \"negative weights: 0.666666666666675 0.333333333333364 -7.105427357601e-15\"\n## [1] \"new weights after setting negative weight as zero: 0.6667 0.3333 0\"\n## [1] \"R is empty\"\n\nbeta\n\n## [1] 0.6667 0.3333 0.0000\n\nX %*% beta\n\n##         [,1]\n## [1,] 1.99995\n## [2,] 0.99995\n## [3,] 4.99995\nWow, it worked! Look at our weights (beta) and our final results! As suspected, column 1 and 2 will have the weights (more on column 1) and when combined our final numbers are quite close to our\ny\n, which is 2, 1, 5 . Awesome! Now, let‚Äôs simulate more data and see if our code works and compare it with\nnnls\npackage!\nLet‚Äôs Put Them All Together\nSimulate Data\n# labels/outcome/y\nnum_labels <- 1000\nlabel_range <- 1:5\ny <- sample(label_range, num_labels, replace=T)\n\n# X matrix\nnum_models <- 5\nX <- matrix(nrow = num_labels, ncol = num_models)\nfor (i in 1:num_models) {\n  sd <- sample(c(0.01,1,10))\n  for (j in 1:num_labels) {\n  X[j, i] <- rnorm(1, mean = y[j], sd = sd)\n}\n}\nAlright, what we did above is basically simulated\ny\nand\nX\nP <- c()\nR <- c(1:dim(X)[2])\nbeta <- rep(0, dim(X)[2])\n\nwhile (T) {\n# step 1: find gradient \ngradient <- t(X) %*% (y-X %*% beta)\nif (sum(gradient<=0)==dim(X)[2]) { print(\"all gradients are zero or negative, we have achieved optimality\") ; break }\nif (length(R)==0) { print(\"R is empty\") ; break }\n\n# step 2: check optimality\ngradient_not_active <- gradient\ngradient_not_active[P] <- -Inf\nP_x <- which(gradient_not_active==max(gradient_not_active))\nP <- c(P,P_x) |> unique() |> sort()\nR <- setdiff(R, P_x)\n\n# solve P\nbeta_i <- beta\nbeta_i[P] <- solve(t(X[,P]) %*% X[,P]) %*% t(X[,P])%*%y\nif (any(beta_i<0)) { \n  print(paste0(\"negative weights: \",paste(beta_i, collapse = \" \")))  \n  idx <- which(beta_i<0)\n  beta_old <- beta[idx]\n  beta_new <- beta_i[idx]\n  alpha <- beta_old/-(beta_new-beta_old)\n  beta_i_new <- beta - alpha*(beta-beta_i) \n  beta <- beta_i_new |> round(digits = 4)\n  print(paste0(\"new weights after setting negative weight as zero: \", paste(beta,collapse = \" \")))\n  } else {  beta <- beta_i ; print(beta) }\n}\n\n## [1] 0.9998029 0.0000000 0.0000000 0.0000000 0.0000000\n## [1] 0.5145682 0.0000000 0.0000000 0.4853314 0.0000000\n## [1] 0.3242697 0.3571370 0.0000000 0.3184922 0.0000000\n## [1] 0.2346782 0.2762402 0.0000000 0.2480602 0.2409658\n## [1] 0.1884031 0.2342831 0.1677427 0.2059978 0.2035208\n## [1] \"R is empty\"\nLet‚Äôs look at our weights and RMSE\nbeta\n\n## [1] 0.1884031 0.2342831 0.1677427 0.2059978 0.2035208\n\nsqrt(mean((y - X %*% beta)^2))\n\n## [1] 0.004614622\nLet‚Äôs look at\nnnls\npackage and see if we can the same result\nmodel <- nnls::nnls(A=X,b=y)\nmodel\n\n## Nonnegative least squares model\n## x estimates: 0.1884031 0.2342831 0.1677427 0.2059978 0.2035208 \n## residual sum-of-squares: 0.02129\n## reason terminated: The solution has been computed sucessfully.\n\nsqrt(mean((y - X %*% model$x)^2))\n\n## [1] 0.004614622\nwow! Awesome!!! Looks the same or at least very similar. Alright, now we‚Äôre at least able to reproduce the nnls portion from scratch. Let‚Äôs see if we can simulate a non-linear data and train with different models and see how our end result is!\nLet‚Äôs Super Learn this thing\nClick below at\ncode\nto expand for the entire procedures. We basically ran 3 different models in tidymodels, linear regression, xgboost, and random forest with recipe (y ~ .), not specifying any interaction/polynomial relationships for a simulated data below.\nn <- 1000\nx <- rnorm(n)\nw <- rnorm(n, 0.5*x)\ny <- 0.2*x + 0.5*w + 0.2*x*w + 0.05*x^2\nMade sure to set seed for reproducibility, create 5 fold for cross validation. Then extract all the prediction for validation sets from each models and stack them into\nX\nmatrix. Then extract the RMSE from each models and stack them into\nmetrics\nmatrix. Finally, we run our nnls code above to get the weights and RMSE for super learner. We repeat this for 1000 iterations and log the results.\nThis cross-validation step is the defining feature of the Super Learner. By fitting each base learner on training folds and generating out-of-fold predictions, we obtain an unbiased prediction matrix that is then used to estimate optimal ensemble weights via NNLS.\ncode\nlibrary(tidymodels)\nlibrary(future)\nlibrary(furrr)\n\n# Set up parallel processing\nplan(multisession, workers = availableCores() - 2)\n\n# Define the function to run for each iteration\nrun_iteration <- function(i) {\n  set.seed(i)\n  n <- 1000\n  x <- rnorm(n)\n  w <- rnorm(n, 0.5*x)\n  y <- 0.2*x + 0.5*w + 0.2*x*w + 0.05*x^2\n  \n  df <- tibble(x,w,y)\n  split <- initial_split(df)\n  train <- training(split)\n  test <- testing(split)\n  # preprocess\n  rec <- recipe(y ~ ., data=train) \n  \n  # linear regression\n  lr_spec <- linear_reg()\n  wf <- workflow() |>\n    add_recipe(rec) |>\n    add_model(lr_spec)\n  folds <- vfold_cv(train, 5)\n  cv_results <- wf |>\n    fit_resamples(folds, control = control_resamples(save_pred = TRUE))\n  cv_metrics <- collect_metrics(cv_results) |> filter(.metric == \"rmse\") |> pull(mean)\n  cv_preds <- collect_predictions(cv_results) \n  \n  #xgboost\n  xgb_spec <- boost_tree(engine = \"xgboost\", mode = \"regression\")\n  wf <- workflow() |>\n    add_recipe(rec) |>\n    add_model(xgb_spec)\n  cv_results <- wf |>\n    fit_resamples(folds, control = control_resamples(save_pred = T))\n  cv_metrics2 <- collect_metrics(cv_results) |> filter(.metric == \"rmse\") |> pull(mean)\n  cv_preds2 <- collect_predictions(cv_results) \n  \n  # random forest\n  rf_spec <- rand_forest(mode = \"regression\")\n  wf <- workflow() |>\n    add_recipe(rec) |>\n    add_model(rf_spec)\n  cv_results <- wf |>\n    fit_resamples(folds, control = control_resamples(save_pred = T))\n  cv_metrics3 <- collect_metrics(cv_results) |> filter(.metric == \"rmse\") |> pull(mean)\n  cv_preds3 <- collect_predictions(cv_results) |>\n    mutate(model = \"rf\")\n  X <- cbind(cv_preds |> select(X1=.pred),cv_preds2 |> select(X2=.pred), cv_preds3 |> select(X3=.pred)) |> as.matrix()\n  y <- cv_preds |> select(y) |> as.matrix()\n  metrics <- cbind(cv_metrics,cv_metrics2,cv_metrics3)\n  \n  # nnls\n  P <- c()\n  R <- c(1:dim(X)[2])\n  beta <- rep(0, dim(X)[2])\n  \n  while (T) {\n    # step 1: find gradient \n    gradient <- t(X) %*% (y-X %*% beta)\n    if (sum(gradient<=0)==dim(X)[2]) { print(\"all gradients are zero or negative, we have achieved optimality\") ; break }\n    if (length(R)==0) { print(\"R is empty\") ; break }\n    # step 2: check optimality\n    gradient_not_active <- gradient\n    gradient_not_active[P] <- -Inf\n    P_x <- which(gradient_not_active==max(gradient_not_active))\n    P <- c(P,P_x) |> unique() |> sort()\n    R <- setdiff(R, P_x)\n    # solve P\n    beta_i <- beta\n    beta_i[P] <- solve(t(X[,P]) %*% X[,P]) %*% t(X[,P])%*%y\n    if (any(beta_i<0)) { \n      print(paste0(\"negative weights: \",paste(beta_i, collapse = \" \")))  \n      idx <- which(beta_i<0)\n      beta_old <- beta[idx]\n      beta_new <- beta_i[idx]\n      alpha <- beta_old/-(beta_new-beta_old)\n      beta_i_new <- beta - alpha*(beta-beta_i) \n      beta <- beta_i_new |> round(digits = 4)\n      print(paste0(\"new weights after setting negative weight as zero: \", paste(beta,collapse = \" \")))\n    } else {  beta <- beta_i ; print(beta) }\n  }\n\n  rmse_superlearner <- sqrt(mean((y - X %*% beta)^2))\n  rmse_result <- if (sum(metrics < rmse_superlearner) >= 1) { \"solo_better\" } else { \"superlearner_better\" }\n  \n  model <- nnls::nnls(A=X,b=y)\n  rmse_ours_nnls <- c(rmse_superlearner, sqrt(mean((y - X %*% model$x)^2)))\n  same_weights_result <- if (sum(round(beta, 4) == round(model$x, 4)) == 3) { \"same\" } else { \"not_same\" }\n  weights_log <- c(model$x, beta)\n  \n  return(list(rmse_log = rmse_result, same_weights_log = same_weights_result, weights_log=weights_log))\n}\n\n# Run with future_map\nresults <- future_map(1:1000, run_iteration, .options = furrr_options(seed = TRUE), .progress = TRUE)\nLet‚Äôs Compare RMSE of Solo models vs Super Learner models\ncode\nlibrary(tidyverse)\n\n# Extract results\nrmse_log <- map_chr(results, \"rmse_log\")\nsame_weights_log <- map_chr(results, \"same_weights_log\")\nweight_logs <- matrix(NA, ncol = 6, nrow = 1000)\nfor (i in 1:1000) {\n  weight_logs[i, 1:6] <- results[[i]]$weights_log\n}\n\nplotrmse_log <- tibble(rmse=rmse_log) |>\n  ggplot(aes(x=rmse)) +\n  geom_bar() +\n  theme_bw()\nWow, look at that! superlearner/ensembled model does appear to have better RMSE compared to solo models! Let‚Äôs take a look and see if our noob nnls from scratch is comparable with\nnnls\npackage.\nComparing Our NNLS to\nnnls\npackage\ncode\nplotsameweights <- tibble(same_weights=same_weights_log) |>\n  ggplot(aes(x=same_weights)) +\n  geom_bar() +\n  theme_bw()\nWow, most of the weights are the same if we round up to 4 digits! Let‚Äôs check on the ones with difference, is it REALLY that different?\ncode\nplotdiff123 <- weight_logs |>\n  as_tibble() |>\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |>\n  filter(sum_diff != 0) |>\n  pivot_longer(cols = c(diff1,diff2,diff3), names_to = \"diff\", values_to = \"values\") |>\n  ggplot(aes(x=values,fill=diff)) +\n  geom_histogram(position = \"dodge2\") +\n  theme_bw()\nThis makes sense, most of the differences are between\nxgboost\n(diff2) and\nrandom forest\n(diff3), as our linear regression (diff1) model without correct specification probably won‚Äôt have a whole of contributions, hence if there is a difference between our algorithm and\nnnls\n, it would be minimal (center in red). It also make sense that if there is a difference in xgboost or random forest model, we would see different weight on the other model contribution. Now the question is, with these weight differences, does it make a huge difference in RMSE? I suspect not so much.\ncode\nrmse_compare <- matrix(NA, ncol = 2, nrow = 1000)\nfor (i in 1:1000) {\n  rmse_compare[i,1:2] <- results[[i]]$rmse_ours_nnls \n}\n\nplotcompare <- weight_logs |>\n  as_tibble() |>\n  mutate(row = row_number()) |>\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |>\n  filter(sum_diff != 0) |>\n  left_join(as_tibble(rmse_compare) |>\n              mutate(row = row_number()), by = \"row\") |>\n  mutate(V1.y = round(V1.y, 4),\n         V2.y = round(V2.y, 4)) |>\n  mutate(check = case_when(\n    V1.y == V2.y ~ \"same\",\n    V1.y < V2.y ~ \"our_nnls_better\",\n    V1.y > V2.y ~ \"nnls_package_better\",\n    TRUE ~ NA_character_\n  )) |>\n  ggplot(aes(x=check)) +\n  geom_bar() +\n  theme_bw()\nlol,\nnnls\npackage clearly is better than our home-grown algorithm! But by how much?\ncode\nplotdiffrmse <- weight_logs |>\n  as_tibble() |>\n  mutate(row = row_number()) |>\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |>\n  filter(sum_diff != 0) |>\n  left_join(as_tibble(rmse_compare) |>\n              mutate(row = row_number()), by = \"row\") |>\n  mutate(V1.y = round(V1.y, 4),\n         V2.y = round(V2.y, 4)) |>\n  mutate(diff_rmse = V1.y - V2.y) |>\n  ggplot(aes(diff_rmse)) +\n  geom_histogram() +\n  theme_bw()\nüòµ‚Äçüí´ It‚Äôs really not that much different! Let‚Äôs find the max.\ncode\nweight_logs |>\n  as_tibble() |>\n  mutate(row = row_number()) |>\n  mutate(diff1 = round(V4-V1, 4),\n         diff2 = round(V5-V2, 4),\n         diff3 = round(V6-V3, 4),\n         sum_diff = diff1+diff2+diff3) |>\n  filter(sum_diff != 0) |>\n  left_join(as_tibble(rmse_compare) |>\n              mutate(row = row_number()), by = \"row\") |>\n  mutate(V1.y = round(V1.y, 4),\n         V2.y = round(V2.y, 4)) |>\n  mutate(diff_rmse = V1.y - V2.y) |>\n  pull(diff_rmse) |> max()\n\n## [1] 9e-04\n## [1] 9e-04\nü•π Does that mean our home-grown algorithm works just as well? You be the judge. Let me know if this is due to pure luck!\nOpportunities for improvement\nwill try multicore sometime in the future, is it really faster than multisession?\nneed to learn/figure out FAST nnls algorithm which I believe\nnnls\npackage uses\nneed to venture more in parallel computing\ncompare with the actual\nSuperLearner\npackage\nLessons learnt\nlearnt to build Super Learner using non-negative least square model\nlearnt Lawson-Hanson algorithm and how it‚Äôs implemented, compared with\nnnls\nand results not too shabby!\nlearnt some basics of parallel computing\nIf you like this article:\nplease feel free to send me a\ncomment or visit my other blogs\nplease feel free to follow me on\nBlueSky\n,\ntwitter\n,\nGitHub\nor\nMastodon\nif you would like collaborate please feel free to\ncontact me\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nr on Everyday Is A School Day\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "üìö Tried building Super Learner from scratch to understand what‚Äôs happening under the hood. Walked through the NNLS algorithm step-by-step‚Äîturns out ensembling models may beat solo models! Our homegrown version? Surprisingly close to nnls package...",
      "meta_keywords": null,
      "og_description": "üìö Tried building Super Learner from scratch to understand what‚Äôs happening under the hood. Walked through the NNLS algorithm step-by-step‚Äîturns out ensembling models may beat solo models! Our homegrown version? Surprisingly close to nnls package...",
      "og_image": "https://www.kenkoonwong.com/blog/superlearner/ensemble.jpg",
      "og_title": "My Messy Notes on Building a Super Learner: Peeking Under The Hood of NNLS | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 18,
      "sitemap_lastmod": null,
      "twitter_description": "üìö Tried building Super Learner from scratch to understand what‚Äôs happening under the hood. Walked through the NNLS algorithm step-by-step‚Äîturns out ensembling models may beat solo models! Our homegrown version? Surprisingly close to nnls package...",
      "twitter_title": "My Messy Notes on Building a Super Learner: Peeking Under The Hood of NNLS | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/12/my-messy-notes-on-building-a-super-learner-peeking-under-the-hood-of-nnls/",
      "word_count": 3607
    }
  }
}