{
  "uuid": "0eaf6e19-bede-4c93-a748-d792bd0ca8d5",
  "created_at": "2025-11-22 19:59:13",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/02/burn-notice/",
    "crawled_at": "2025-11-22T10:53:57.820794",
    "external_links": [
      {
        "href": "https://kieranhealy.org/blog/archives/2025/02/16/burn-notice/",
        "text": "R on kieranhealy.org"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://theordinalsociety.com/",
        "text": "a book all about the general issues for society that this raises"
      },
      {
        "href": "https://theordinalsociety.com/",
        "text": "read a good book on this topic"
      },
      {
        "href": "https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/health-download.png?ssl=1",
        "text": null
      },
      {
        "href": "https://en.wikipedia.org/wiki/GPS_Exchange_Format",
        "text": "GPX"
      },
      {
        "href": "https://en.wikipedia.org/wiki/XML",
        "text": "XML"
      },
      {
        "href": "https://en.wikipedia.org/wiki/Clinical_Document_Architecture",
        "text": "Clinical Document Architecture"
      },
      {
        "href": "https://duckdb.org/",
        "text": "DuckDB"
      },
      {
        "href": "https://duckplyr.tidyverse.org/",
        "text": "very nicely"
      },
      {
        "href": "https://dplyr.tidyverse.org/",
        "text": "dplyr"
      },
      {
        "href": "https://vroom.r-lib.org/",
        "text": "vroom"
      },
      {
        "href": "https://allpoetry.com/poem/8480975-A-Sonnet-by-James-Kenneth-Stephen",
        "text": "bleating articulate monotony"
      },
      {
        "href": "https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/burn-detail.png?ssl=1",
        "text": null
      },
      {
        "href": "https://kieranhealy.org/blog/archives/2025/02/16/burn-notice/",
        "text": "R on kieranhealy.org"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Burn Notice | R-bloggers",
    "images": [
      {
        "alt": "Inside the export folder.",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Inside the export folder.",
        "base64": null,
        "src": "https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/health-download.png?w=578&ssl=1"
      },
      {
        "alt": "A detail from the plot.",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "A detail from the plot.",
        "base64": null,
        "src": "https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/burn-detail.png?w=578&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/r-on-kieranhealy-org/",
        "text": "R on kieranhealy.org"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-390624 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Burn Notice</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 16, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-kieranhealy-org/\">R on kieranhealy.org</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://kieranhealy.org/blog/archives/2025/02/16/burn-notice/\"> R on kieranhealy.org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>Your Phone and Watch have a lot of data about you. I mean, like, a <em>lot</em>. Someone should really write <a href=\"https://theordinalsociety.com/\" rel=\"nofollow\" target=\"_blank\">a book all about the general issues for society that this raises</a>. Yesterday I decided I wanted to take a look specifically at the health data on my iPhone. I’m not a huge user of the iPhone’s or the Apple Watch’s health features. I don’t use or subscribe to Apple Fitness+, for example. I’m not in any studies. But I do have a bathroom scale that records data and I allow the Watch to keep an eye on my activity. This means that, like so many people, I have grown to heartily despise the blandly affirming Californian inside those devices who periodically encourages me to take a walk, or stand up, or be mindful, and so forth.</p>\n<p>I went to the Health app and selected my ID photo up in the top right corner, then scrolled down to “Export All Health Data”. When I did, it asked me if was I sure I wanted to do this, as it might take a while. Very stupidly, my first thought was “Eh, how much data can there be?” Again, I should make time to <a href=\"https://theordinalsociety.com/\" rel=\"nofollow\" target=\"_blank\">read a good book on this topic</a>. I think I vaguely had in mind a CSV with a few thousand rows of Withings Scale Meaurements. After a minute or two, what I got was a zip file that expanded to a folder with about four  gigabytes of stuff inside.</p>\n<figure><a href=\"https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/health-download.png?ssl=1\" rel=\"nofollow\" target=\"_blank\">\n<img alt=\"Inside the export folder.\" data-lazy-src=\"https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/health-download.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"Inside the export folder.\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/health-download.png?w=578&amp;ssl=1\"/></noscript> </a><figcaption>\n<p>Inside the export folder.</p>\n</figcaption>\n</figure>\n<p>As I say, I’m not a big direct user of Apple’s own health offerings. So there were a few files in the ECG folder, each about 120KB in size. And there were just a few <a href=\"https://en.wikipedia.org/wiki/GPS_Exchange_Format\" rel=\"nofollow\" target=\"_blank\">GPX</a> files in the workout-routes folder, each about half a megabyte in size. Everything else was in <code>export_cda.xml</code> and <code>export.xml</code>.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/XML\" rel=\"nofollow\" target=\"_blank\">XML</a> is, amongst other things, a way of specifying the contents of arbitrary data structures in a flat file while also providing that data in the file. It’s a real pain in the neck to deal with, too. The <code>export_cda.xml</code> file is a specific kind of XML spec, a <a href=\"https://en.wikipedia.org/wiki/Clinical_Document_Architecture\" rel=\"nofollow\" target=\"_blank\">Clinical Document Architecture</a> file that in principle allows for medical records to be written and transported in a standard format. This is what allows, for example, your doctor’s or dentist’s office to seamlessly accept medical records from other providers and smoothly integrate them into their own record system, just prior to asking you to fill it all out again on paper each time you visit them. Like your doctor’s office, I decided to keep away from this file because it is very complicated and no-one knows how it works.</p>\n<p>Instead I took a look at the larger file, <code>export.xml</code>. It is structurally simpler. It consists of some number of blobs of data, delimited by <code>Record</code> nodes. My goal was to get R to parse that file and work with the various pieces of data in there. Starting out I did not know how many distinct pieces of data were in there.</p>\n<p>Working with XML is, as I say, a pain. Professional programmers no doubt have more efficient tools for dealing with this stuff (such as, for example, better ways to extract the file’s schema and apply it directly to reconstruct the data). I just went ahead and had R ingest the whole thing, stacking all the <code>Record</code> pieces up in a single large table.</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n</pre></td>\n<td class=\"lntd\">\n<pre>library(tidyverse)\nlibrary(xml2)\nlibrary(here)\n\nraw &lt;- read_xml(here(\"raw\", \"export.xml\"))\napple_health_all &lt;- raw |&gt;\n  xml_find_all(\"//Record\") |&gt;\n  xml_attrs() |&gt;\n  bind_rows()</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>This took about three minutes, which is a long time. Unfortunately there’s no straightforward way to make it go faster, because the xml file is stored with pointers and you can’t parallelize the reading-in easily with those. I’d have had to split it up first, which is the thing I’m trying to do to begin with. As I say, there is definitely a better way to do this. However, I only had to do it once. What did we end up with?</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n</pre></td>\n<td class=\"lntd\">\n<pre>&gt; apple_health_all\n# A tibble: 7,808,311 × 9\n   type                                  sourceName sourceVersion device                                                                              unit  creationDate startDate \n   &lt;chr&gt;                                 &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;                                                                               &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;     \n 1 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 2 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 3 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 4 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 5 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 6 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 7 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 8 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 9 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n10 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       &lt;&lt;HKDevice: &gt;, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n# ℹ 7,808,301 more rows\n# ℹ Use `print(n = ...)` to see more rows</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>Nearly <em>eight million</em> rows of data! Entirely about myself! Now, in many ways I am quite a charming fellow but, let’s face it, I am not eight million rows of data interesting. At this point I saved the file out to disk:</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre>1\n</pre></td>\n<td class=\"lntd\">\n<pre>write_tsv(apple_health_all, here(\"data\", \"apple_health_all.tsv\"))</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>I wrote it out as a tab-delimited rather than a comma-delimited file just because that <code>device</code> column is full of comma-separated information (about my bathroom scale) and I didn’t want to make trouble for myself later.</p>\n<p>Originally what I wanted to do next was to read the whole TSV file back into memory via R’s interface with <a href=\"https://duckdb.org/\" rel=\"nofollow\" target=\"_blank\">DuckDB</a>. DuckDB is a very fast in-memory database with many attractive features for data analysis. Big databases out in the world are optimized for things like very rapidly adding and deleting records (i.e. rows), and are carefully designed to be indexed on just the required number of keys. But when we’re working with data, we’re generally never adding or subtracting rows to the main dataset. (Columns, yes. But not rows.) We just want to summarize it really fast and that is hard when it’s large with respect to memory. DuckDB reads in large amounts of data very quickly and by default indexes all the columns. It plays <a href=\"https://duckplyr.tidyverse.org/\" rel=\"nofollow\" target=\"_blank\">very nicely</a> with <a href=\"https://dplyr.tidyverse.org/\" rel=\"nofollow\" target=\"_blank\">dplyr</a> and other R data tabulation engines.</p>\n<p>Annoyingly, getting the data re-ingested via DuckDB was tricky because the <code>export.xml</code> file is essentially a bunch of different datasets stacked one on top of the other. They <em>nearly</em> all have the same structure but <em>not quite</em>. (The columns all have the same names. DuckDB complained that in the ingestion process it sniffed different types from what it was expecting.) I could have gone down the road of figuring out (via the XML) what these fields all were and how they differed across sections of the data. But I was impatient and instead I decided to just split the big data frame into its component parts and save them as separate files.</p>\n<p>First we read it back in, with <code>read_tsv()</code>. This is extremely fast, as behind the scenes it uses <a href=\"https://vroom.r-lib.org/\" rel=\"nofollow\" target=\"_blank\"><code>vroom</code></a> to hoover up everything.</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n</pre></td>\n<td class=\"lntd\">\n<pre>df &lt;- read_tsv(here(\"data\", \"apple_health_all.tsv\"),\n               col_types = cols(\n                 type = col_character(),\n                 sourceName = col_character(),\n                 sourceVersion = col_character(),\n                 device = col_character(),\n                 unit = col_character(),\n                 creationDate = col_character(),\n                 startDate = col_character(),\n                 endDate = col_character(),\n                 value = col_character()\n               )) |&gt;\n  rowid_to_column()</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>Here I specifically make every column of type <code>character</code> because we’re not going to be computing on this data frame. I also add an explicit row id; might be useful later on.</p>\n<p>Next, we extract the group names of the <code>type</code> column. We’re going to use this information in a moment to name the new data files we’ll be making.</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n</pre></td>\n<td class=\"lntd\">\n<pre>grp_keys &lt;- df |&gt;\n  group_by(type) |&gt;\n  group_keys()\n\ngrp_keys\n#&gt; # A tibble: 48 × 1\n#&gt;    type                                                      \n#&gt;    &lt;chr&gt;                                                     \n#&gt;  1 HKCategoryTypeIdentifierAppleStandHour                    \n#&gt;  2 HKCategoryTypeIdentifierAudioExposureEvent                \n#&gt;  3 HKCategoryTypeIdentifierLowHeartRateEvent                 \n#&gt;  4 HKCategoryTypeIdentifierMindfulSession                    \n#&gt;  5 HKCategoryTypeIdentifierSleepAnalysis                     \n#&gt;  6 HKDataTypeSleepDurationGoal                               \n#&gt;  7 HKQuantityTypeIdentifierActiveEnergyBurned                \n#&gt;  8 HKQuantityTypeIdentifierAppleExerciseTime                 \n#&gt;  9 HKQuantityTypeIdentifierAppleSleepingBreathingDisturbances\n#&gt; 10 HKQuantityTypeIdentifierAppleSleepingWristTemperature     \n#&gt; # ℹ 38 more rows</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>We have forty eight individual datasets. Let’s convert these to a vector of strings we can use as filenames:</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre>1\n2\n3\n4\n5\n6\n7\n</pre></td>\n<td class=\"lntd\">\n<pre>fnames &lt;- grp_keys |&gt;\n  pull(type) |&gt;\n  paste0(\".csv\")\n\nfnames[1:5]\n#&gt; [1] \"HKCategoryTypeIdentifierAppleStandHour.csv\"     \"HKCategoryTypeIdentifierAudioExposureEvent.csv\" \"HKCategoryTypeIdentifierLowHeartRateEvent.csv\" \n#&gt; [4] \"HKCategoryTypeIdentifierMindfulSession.csv\"     \"HKCategoryTypeIdentifierSleepAnalysis.csv\"</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>Next comes the elegant bit. We have a vector of names that’s the same length as the number of unique elements in our dataframe’s <code>type</code> column. So we split the big dataframe into a list of dataframes by <code>type</code> and name the list elements with the corresponding file name. Then we walk the list and write out each element of it as its own CSV file, using the name as the file name. The call to <code>iwalk()</code> lets us write an anonymous function that refers to the list element <code>x</code> and its name <code>idx</code> and applies <code>write_csv()</code> to each element in turn. Nice.</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre>1\n2\n3\n4\n</pre></td>\n<td class=\"lntd\">\n<pre>df |&gt;\n  group_split(type) |&gt;\n  set_names(fnames) |&gt;\n  iwalk(\\(x, idx) write_csv(x, here(\"data\", idx)))</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>Now we can look in the <code>data/</code> folder:</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n</pre></td>\n<td class=\"lntd\">\n<pre>fs::dir_ls(here(\"data\"))\n#&gt; data/HKCategoryTypeIdentifierAppleStandHour.csv\n#&gt; data/HKCategoryTypeIdentifierAudioExposureEvent.csv\n#&gt; data/HKCategoryTypeIdentifierLowHeartRateEvent.csv\n#&gt; data/HKCategoryTypeIdentifierMindfulSession.csv\n#&gt; data/HKCategoryTypeIdentifierSleepAnalysis.csv\n#&gt; data/HKDataTypeSleepDurationGoal.csv\n#&gt; data/HKQuantityTypeIdentifierActiveEnergyBurned.csv\n#&gt; data/HKQuantityTypeIdentifierAppleExerciseTime.csv\n#&gt; data/HKQuantityTypeIdentifierAppleSleepingBreathingDisturbances.csv\n#&gt; data/HKQuantityTypeIdentifierAppleSleepingWristTemperature.csv\n#&gt; data/HKQuantityTypeIdentifierAppleStandTime.csv\n#&gt; data/HKQuantityTypeIdentifierAppleWalkingSteadiness.csv\n#&gt; data/HKQuantityTypeIdentifierBasalEnergyBurned.csv\n#&gt; data/HKQuantityTypeIdentifierBodyFatPercentage.csv\n#&gt; data/HKQuantityTypeIdentifierBodyMass.csv\n#&gt; data/HKQuantityTypeIdentifierBodyMassIndex.csv\n#&gt; data/HKQuantityTypeIdentifierDistanceCycling.csv\n#&gt; data/HKQuantityTypeIdentifierDistanceRowing.csv\n#&gt; data/HKQuantityTypeIdentifierDistanceWalkingRunning.csv\n#&gt; data/HKQuantityTypeIdentifierEnvironmentalAudioExposure.csv\n#&gt; data/HKQuantityTypeIdentifierEnvironmentalSoundReduction.csv\n#&gt; data/HKQuantityTypeIdentifierFlightsClimbed.csv\n#&gt; data/HKQuantityTypeIdentifierHeadphoneAudioExposure.csv\n#&gt; data/HKQuantityTypeIdentifierHeartRate.csv\n#&gt; data/HKQuantityTypeIdentifierHeartRateRecoveryOneMinute.csv\n#&gt; data/HKQuantityTypeIdentifierHeartRateVariabilitySDNN.csv\n#&gt; data/HKQuantityTypeIdentifierHeight.csv\n#&gt; data/HKQuantityTypeIdentifierLeanBodyMass.csv\n#&gt; data/HKQuantityTypeIdentifierPhysicalEffort.csv\n#&gt; data/HKQuantityTypeIdentifierRespiratoryRate.csv\n#&gt; data/HKQuantityTypeIdentifierRestingHeartRate.csv\n#&gt; data/HKQuantityTypeIdentifierRowingSpeed.csv\n#&gt; data/HKQuantityTypeIdentifierRunningGroundContactTime.csv\n#&gt; data/HKQuantityTypeIdentifierRunningPower.csv\n#&gt; data/HKQuantityTypeIdentifierRunningSpeed.csv\n#&gt; data/HKQuantityTypeIdentifierRunningStrideLength.csv\n#&gt; data/HKQuantityTypeIdentifierRunningVerticalOscillation.csv\n#&gt; data/HKQuantityTypeIdentifierSixMinuteWalkTestDistance.csv\n#&gt; data/HKQuantityTypeIdentifierStairAscentSpeed.csv\n#&gt; data/HKQuantityTypeIdentifierStairDescentSpeed.csv\n#&gt; data/HKQuantityTypeIdentifierStepCount.csv\n#&gt; data/HKQuantityTypeIdentifierTimeInDaylight.csv\n#&gt; data/HKQuantityTypeIdentifierVO2Max.csv\n#&gt; data/HKQuantityTypeIdentifierWalkingAsymmetryPercentage.csv\n#&gt; data/HKQuantityTypeIdentifierWalkingDoubleSupportPercentage.csv\n#&gt; data/HKQuantityTypeIdentifierWalkingHeartRateAverage.csv\n#&gt; data/HKQuantityTypeIdentifierWalkingSpeed.csv\n#&gt; data/HKQuantityTypeIdentifierWalkingStepLength.csv\n#&gt; data/apple_health_all.tsv</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>You can see that some of these files are prefixed with <code>HKQuantityTypeIdentifier</code> and others with <code>HKCategoryTypeIdentifier</code>. One, a 221 byte file, is prefixed with <code>HKDataType</code> and is my <code>SleepDurationGoal</code>. It consists a single row with the frankly delusional value of “8”. Small differences between the Quantity and Category type files were, I think, responsible for DuckDB complaining. The named columns are the same across all the records but I think they vary just enough in terms of what the read engine wants that when it got to one of them it choked.</p>\n<p>The largest of these files is an 830MB file measuring <code>ActiveEnergyBurned</code>. Records for this quantity are generated when you start doing something long enough for either your iPhone or your Watch to notice. Here’s what it looks like when read in by itself:</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n</pre></td>\n<td class=\"lntd\">\n<pre>burn_df\n#&gt; # A tibble: 2,658,839 × 10\n#&gt;      rowid type                                  sourceName sourceVersion device unit  creationDate        startDate           endDate             value\n#&gt;      &lt;int&gt; &lt;chr&gt;                                 &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;\n#&gt;  1 3910445 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:43:01 2024-09-17 12:41:01 2024-09-17 12:42:02 0.389\n#&gt;  2 3910446 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:44:23 2024-09-17 12:42:02 2024-09-17 12:42:54 1.10 \n#&gt;  3 3910447 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:44:26 2024-09-17 12:42:54 2024-09-17 12:43:34 1.83 \n#&gt;  4 3910448 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:47:55 2024-09-17 12:43:55 2024-09-17 12:44:46 1.62 \n#&gt;  5 3910449 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:47:55 2024-09-17 12:44:56 2024-09-17 12:45:16 0.659\n#&gt;  6 3910450 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:47:55 2024-09-17 12:45:16 2024-09-17 12:46:07 1.15 \n#&gt;  7 3910451 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:47:57 2024-09-17 12:46:07 2024-09-17 12:47:08 0.432\n#&gt;  8 3910452 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:49:42 2024-09-17 12:47:08 2024-09-17 12:48:10 2.91 \n#&gt;  9 3910453 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:49:42 2024-09-17 12:48:10 2024-09-17 12:48:40 3.09 \n#&gt; 10 3910454 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        &lt;&lt;HKD… Cal   2024-09-17 12:50:03 2024-09-17 12:48:40 2024-09-17 12:49:21 0.74 \n#&gt; # ℹ 2,658,829 more rows</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>The <code>startDate</code> and <code>endDate</code> periods on each row were almost all between thirty and ninety seconds, though there was a long, sparse tail of outliers. I haven’t looked yet to see if these artifacts are errors or if they were created in some recognizable way. It’s not that a workout gets recorded as 60 minutes in a single row or anything—once you’re doing something, the phone or watch will create a new row every 30 to 90 seconds for as long as you continue doing it. While the first rows of the table here begin in September 2024, the rows are not ordered by date. I think it’s by date within device, or <code>sourceName</code>. My data go back to 2018. Again, if you were the sort of person who worked out once or twice a day, and took full advantage of Apple’s market offerings with health, and had owned an iPhone or Watch for a long time, you would end up with a <em>lot</em> of information about yourself.</p>\n<p>Is it useful information? It’s certainly extremely fine-grained. Whether all that monitoring amounts to anything particularly insightful is a harder question, particularly at the level of individuals. One of the many ironies of all broadly person-centric or social data over the past fifty years is that the people doing work on everything from social networks to epidemiology to personal health and beyond prayed for more and more fine-grained data … and then they got it. Now we are much better than we used to be at <a href=\"https://allpoetry.com/poem/8480975-A-Sonnet-by-James-Kenneth-Stephen\" rel=\"nofollow\" target=\"_blank\">bleating articulate monotony</a>, quantitatively, and indicating that two and one are three, that grass is green, lakes damp, and mountains steep.</p>\n<p>In the case of the ActiveEnergyBurned table, 2.5 million moment-to-moment estimates of the calories I’m burning might or might not be useful. We can aggregate it a bit by grouping the values by hour and summing them up for every hour, day, month, and year that we have. Like this:</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n</pre></td>\n<td class=\"lntd\">\n<pre>burn_df &lt;- read_csv(here(\"data\", \"HKQuantityTypeIdentifierActiveEnergyBurned.csv\"),\n                    col_types = colspec) |&gt;\n  arrange(startDate) |&gt;\n  select(rowid, sourceName, startDate:value) |&gt;\n  mutate(startDate = with_tz(startDate, tzone = \"US/Eastern\"),\n         endDate = with_tz(startDate, tzone = \"US/Eastern\")) |&gt;\n  mutate(\n    start_year = year(startDate),\n    start_month = month(startDate),\n    start_day = day(startDate),\n    start_hour = hour(startDate),\n    start_minute = minute(startDate),\n    end_year = year(endDate),\n    end_month = month(endDate),\n    end_day = day(endDate),\n    end_hour = hour(endDate),\n    end_minute = minute(endDate),\n    length = endDate - startDate)\n\nhourly_burn &lt;- burn_df |&gt;\n  group_by(start_year, start_month, start_day, start_hour) |&gt;\n  summarize(burned = sum(value),\n            time = first(startDate)) |&gt;\n  mutate(index = as.integer(as.factor(time)))\n\nhourly_burn\n#&gt; # A tibble: 36,338 × 7\n#&gt; # Groups:   start_year, start_month, start_day [2,333]\n#&gt;    start_year start_month start_day start_hour burned time                index\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;int&gt;      &lt;int&gt;  &lt;dbl&gt; &lt;dttm&gt;              &lt;int&gt;\n#&gt;  1       2018           7        14          9  73.2  2018-07-14 09:51:54     1\n#&gt;  2       2018           7        14         10 525.   2018-07-14 10:00:13     2\n#&gt;  3       2018           7        14         11   4.84 2018-07-14 11:37:34     3\n#&gt;  4       2018           7        14         12  27.4  2018-07-14 12:12:27     4\n#&gt;  5       2018           7        14         13  72.5  2018-07-14 13:00:03     5\n#&gt;  6       2018           7        14         14  93.1  2018-07-14 14:00:51     6\n#&gt;  7       2018           7        14         15  68.0  2018-07-14 15:00:55     7\n#&gt;  8       2018           7        14         16  18.9  2018-07-14 16:00:08     8\n#&gt;  9       2018           7        14         17  29.1  2018-07-14 17:00:23     9\n#&gt; 10       2018           7        14         18  30.4  2018-07-14 18:00:19    10\n#&gt; # ℹ 36,328 more rows</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>From there, we can make a kind of grid of every single hour of my life since July 2018, with each box of the grid showing—if there’s data—an estimate of calories burned in that hour. Notice above how I used <code>with_tz()</code> to adjust the start and end dates before going the other calculations. If I didn’t do that most of the data would be off by five hours (the difference between US/Eastern and UTC). This correction will still leave some data points off, as I do sometimes leave not only the house but even the time zone. But for now it’s good enough.</p>\n<p>To get a nice picture we should fold this data into a grid of all the days and hours since July 2018, taking care to drop dates that don’t exist, like June 31st or February 29th 2019.</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n</pre></td>\n<td class=\"lntd\">\n<pre>time_grid &lt;- expand_grid(\n  start_year = c(2018L:2025L),\n  start_month = c(1L:12L),\n  start_day = c(1L:31L),\n  start_hour = c(0:23)) |&gt;\n  filter(!(start_month == 2 &amp; start_day &gt; 29)) |&gt;\n  filter(!(start_month %in% c(4, 6, 9, 11) &amp; start_day &gt; 30)) |&gt;\n  filter(!(start_year %nin% c(2020, 2024) &amp; start_month == 2 &amp; start_day &gt; 28)) # careful here\n\n\nburned_grid &lt;- time_grid |&gt;\n  left_join(hourly_burn, by = c(\"start_year\", \"start_month\",\n                                \"start_day\", \"start_hour\")) </pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>Then we can draw a plot:</p>\n<div class=\"highlight-wrapper\">\n<div class=\"highlight-before\">r</div>\n<div class=\"highlight\"><div class=\"chroma\">\n<table class=\"lntable\"><tr><td class=\"lntd\">\n<pre> 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n</pre></td>\n<td class=\"lntd\">\n<pre>out &lt;- burned_grid |&gt;\n  ggplot(aes(x = factor(start_day),\n             y = factor(start_hour),\n             fill = burned)) +\n  geom_tile(color = \"gray10\", linewidth = 0.01) +\n  coord_fixed() +\n  facet_grid(start_year ~ month_rc) +\n  scale_fill_binned_sequential(palette = \"Inferno\",\n                               na.value = \"white\") +\n  scale_x_discrete(breaks = c(5, 15, 25), labels = c(5, 15, 25)) +\n  scale_y_discrete(breaks = c(6, 12, 18, 0), labels = c(\"6am\", \"Noon\", \"6pm\", \"Midnight\")) +\n  guides(fill = guide_legend(theme = theme(legend.title.position = \"top\",\n                                           legend.text.position = \"bottom\"))) +\n  labs(x = \"Day of the Month\",\n       y = \"Hour of the Day\",\n       fill = \"Energy (Cal)\",\n       title = \"iPhone/Watch Health Data Dump\",\n       subtitle = \"Hourly sums of all ‘Active Energy Burned’ measurements, by day and year\") +\n  theme(strip.text = element_text(size = rel(1.2)),\n        axis.text.y = element_text(size = rel(0.8)),\n        plot.title = element_text(size = rel(2)))</pre></td></tr></table>\n</div>\n</div>\n</div>\n<p>I’m not going to show you the whole thing—a large faceted grid of all eight years of hourly data—because that would make me, and indeed perhaps you, feel a little queasy. But here’s a typical recent month:</p>\n<figure><a href=\"https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/burn-detail.png?ssl=1\" rel=\"nofollow\" target=\"_blank\">\n<img alt=\"A detail from the plot.\" data-lazy-src=\"https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/burn-detail.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"A detail from the plot.\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/kieranhealy.org/blog/archives/2025/02/16/burn-notice/burn-detail.png?w=578&amp;ssl=1\"/></noscript> </a><figcaption>\n<p>A detail from the big graph.</p>\n</figcaption>\n</figure>\n<p>Days of the month are on the x-axis, hours of the day on the y-axis. Where a cell is filled in, there’s at least one activity measure for that hour. Where the grid cell is white, there’s no data. You can see two things from this detail. The first thing is that on most weekday mornings I go for a run. Now, to be clear, it’s not like I spend the rest of the day entirely comatose. But given that this is a detail from a graph covering eight years of hourly data, the only reliable signal that jumps out from the background is running for a while. The second thing is that, recently, I have been experimenting with wearing my Watch while sleeping, but more often than not I forget to put it back on after taking it off to charge in the evenings. I’m sure there are other things to be found in this data. And I haven’t even looked at some of the other files yet. (<code>StairAscentSpeed.csv</code>? <code>TimeInDaylight.csv</code>? <code>WalkingAsymmetryPercentage.csv</code>? So many options.) And, in fairness, some of the features of this sort of monitoring—like fall detection, and AFib triggering—are properly important and very useful. But I am still not entirely convinced that collecting this much data about me personally is going to be of much use to anyone.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://kieranhealy.org/blog/archives/2025/02/16/burn-notice/\"> R on kieranhealy.org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Burn Notice\nPosted on\nFebruary 16, 2025\nby\nR on kieranhealy.org\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR on kieranhealy.org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nYour Phone and Watch have a lot of data about you. I mean, like, a\nlot\n. Someone should really write\na book all about the general issues for society that this raises\n. Yesterday I decided I wanted to take a look specifically at the health data on my iPhone. I’m not a huge user of the iPhone’s or the Apple Watch’s health features. I don’t use or subscribe to Apple Fitness+, for example. I’m not in any studies. But I do have a bathroom scale that records data and I allow the Watch to keep an eye on my activity. This means that, like so many people, I have grown to heartily despise the blandly affirming Californian inside those devices who periodically encourages me to take a walk, or stand up, or be mindful, and so forth.\nI went to the Health app and selected my ID photo up in the top right corner, then scrolled down to “Export All Health Data”. When I did, it asked me if was I sure I wanted to do this, as it might take a while. Very stupidly, my first thought was “Eh, how much data can there be?” Again, I should make time to\nread a good book on this topic\n. I think I vaguely had in mind a CSV with a few thousand rows of Withings Scale Meaurements. After a minute or two, what I got was a zip file that expanded to a folder with about four  gigabytes of stuff inside.\nInside the export folder.\nAs I say, I’m not a big direct user of Apple’s own health offerings. So there were a few files in the ECG folder, each about 120KB in size. And there were just a few\nGPX\nfiles in the workout-routes folder, each about half a megabyte in size. Everything else was in\nexport_cda.xml\nand\nexport.xml\n.\nXML\nis, amongst other things, a way of specifying the contents of arbitrary data structures in a flat file while also providing that data in the file. It’s a real pain in the neck to deal with, too. The\nexport_cda.xml\nfile is a specific kind of XML spec, a\nClinical Document Architecture\nfile that in principle allows for medical records to be written and transported in a standard format. This is what allows, for example, your doctor’s or dentist’s office to seamlessly accept medical records from other providers and smoothly integrate them into their own record system, just prior to asking you to fill it all out again on paper each time you visit them. Like your doctor’s office, I decided to keep away from this file because it is very complicated and no-one knows how it works.\nInstead I took a look at the larger file,\nexport.xml\n. It is structurally simpler. It consists of some number of blobs of data, delimited by\nRecord\nnodes. My goal was to get R to parse that file and work with the various pieces of data in there. Starting out I did not know how many distinct pieces of data were in there.\nWorking with XML is, as I say, a pain. Professional programmers no doubt have more efficient tools for dealing with this stuff (such as, for example, better ways to extract the file’s schema and apply it directly to reconstruct the data). I just went ahead and had R ingest the whole thing, stacking all the\nRecord\npieces up in a single large table.\nr\n1\n2\n3\n4\n5\n6\n7\n8\n9\nlibrary(tidyverse)\nlibrary(xml2)\nlibrary(here)\n\nraw <- read_xml(here(\"raw\", \"export.xml\"))\napple_health_all <- raw |>\n  xml_find_all(\"//Record\") |>\n  xml_attrs() |>\n  bind_rows()\nThis took about three minutes, which is a long time. Unfortunately there’s no straightforward way to make it go faster, because the xml file is stored with pointers and you can’t parallelize the reading-in easily with those. I’d have had to split it up first, which is the thing I’m trying to do to begin with. As I say, there is definitely a better way to do this. However, I only had to do it once. What did we end up with?\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n> apple_health_all\n# A tibble: 7,808,311 × 9\n   type                                  sourceName sourceVersion device                                                                              unit  creationDate startDate \n   <chr>                                 <chr>      <chr>         <chr>                                                                               <chr> <chr>        <chr>     \n 1 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 2 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 3 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 4 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 5 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 6 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 7 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 8 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n 9 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n10 HKQuantityTypeIdentifierBodyMassIndex Withings   6050301       <<HKDevice: >, name:Body+, manufacturer:Withings, model:Withings Scale,… count 2024-08-31 … 2024-05-… \n# ℹ 7,808,301 more rows\n# ℹ Use `print(n = ...)` to see more rows\nNearly\neight million\nrows of data! Entirely about myself! Now, in many ways I am quite a charming fellow but, let’s face it, I am not eight million rows of data interesting. At this point I saved the file out to disk:\nr\n1\nwrite_tsv(apple_health_all, here(\"data\", \"apple_health_all.tsv\"))\nI wrote it out as a tab-delimited rather than a comma-delimited file just because that\ndevice\ncolumn is full of comma-separated information (about my bathroom scale) and I didn’t want to make trouble for myself later.\nOriginally what I wanted to do next was to read the whole TSV file back into memory via R’s interface with\nDuckDB\n. DuckDB is a very fast in-memory database with many attractive features for data analysis. Big databases out in the world are optimized for things like very rapidly adding and deleting records (i.e. rows), and are carefully designed to be indexed on just the required number of keys. But when we’re working with data, we’re generally never adding or subtracting rows to the main dataset. (Columns, yes. But not rows.) We just want to summarize it really fast and that is hard when it’s large with respect to memory. DuckDB reads in large amounts of data very quickly and by default indexes all the columns. It plays\nvery nicely\nwith\ndplyr\nand other R data tabulation engines.\nAnnoyingly, getting the data re-ingested via DuckDB was tricky because the\nexport.xml\nfile is essentially a bunch of different datasets stacked one on top of the other. They\nnearly\nall have the same structure but\nnot quite\n. (The columns all have the same names. DuckDB complained that in the ingestion process it sniffed different types from what it was expecting.) I could have gone down the road of figuring out (via the XML) what these fields all were and how they differed across sections of the data. But I was impatient and instead I decided to just split the big data frame into its component parts and save them as separate files.\nFirst we read it back in, with\nread_tsv()\n. This is extremely fast, as behind the scenes it uses\nvroom\nto hoover up everything.\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ndf <- read_tsv(here(\"data\", \"apple_health_all.tsv\"),\n               col_types = cols(\n                 type = col_character(),\n                 sourceName = col_character(),\n                 sourceVersion = col_character(),\n                 device = col_character(),\n                 unit = col_character(),\n                 creationDate = col_character(),\n                 startDate = col_character(),\n                 endDate = col_character(),\n                 value = col_character()\n               )) |>\n  rowid_to_column()\nHere I specifically make every column of type\ncharacter\nbecause we’re not going to be computing on this data frame. I also add an explicit row id; might be useful later on.\nNext, we extract the group names of the\ntype\ncolumn. We’re going to use this information in a moment to name the new data files we’ll be making.\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\ngrp_keys <- df |>\n  group_by(type) |>\n  group_keys()\n\ngrp_keys\n#> # A tibble: 48 × 1\n#>    type                                                      \n#>    <chr>                                                     \n#>  1 HKCategoryTypeIdentifierAppleStandHour                    \n#>  2 HKCategoryTypeIdentifierAudioExposureEvent                \n#>  3 HKCategoryTypeIdentifierLowHeartRateEvent                 \n#>  4 HKCategoryTypeIdentifierMindfulSession                    \n#>  5 HKCategoryTypeIdentifierSleepAnalysis                     \n#>  6 HKDataTypeSleepDurationGoal                               \n#>  7 HKQuantityTypeIdentifierActiveEnergyBurned                \n#>  8 HKQuantityTypeIdentifierAppleExerciseTime                 \n#>  9 HKQuantityTypeIdentifierAppleSleepingBreathingDisturbances\n#> 10 HKQuantityTypeIdentifierAppleSleepingWristTemperature     \n#> # ℹ 38 more rows\nWe have forty eight individual datasets. Let’s convert these to a vector of strings we can use as filenames:\nr\n1\n2\n3\n4\n5\n6\n7\nfnames <- grp_keys |>\n  pull(type) |>\n  paste0(\".csv\")\n\nfnames[1:5]\n#> [1] \"HKCategoryTypeIdentifierAppleStandHour.csv\"     \"HKCategoryTypeIdentifierAudioExposureEvent.csv\" \"HKCategoryTypeIdentifierLowHeartRateEvent.csv\" \n#> [4] \"HKCategoryTypeIdentifierMindfulSession.csv\"     \"HKCategoryTypeIdentifierSleepAnalysis.csv\"\nNext comes the elegant bit. We have a vector of names that’s the same length as the number of unique elements in our dataframe’s\ntype\ncolumn. So we split the big dataframe into a list of dataframes by\ntype\nand name the list elements with the corresponding file name. Then we walk the list and write out each element of it as its own CSV file, using the name as the file name. The call to\niwalk()\nlets us write an anonymous function that refers to the list element\nx\nand its name\nidx\nand applies\nwrite_csv()\nto each element in turn. Nice.\nr\n1\n2\n3\n4\ndf |>\n  group_split(type) |>\n  set_names(fnames) |>\n  iwalk(\\(x, idx) write_csv(x, here(\"data\", idx)))\nNow we can look in the\ndata/\nfolder:\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\nfs::dir_ls(here(\"data\"))\n#> data/HKCategoryTypeIdentifierAppleStandHour.csv\n#> data/HKCategoryTypeIdentifierAudioExposureEvent.csv\n#> data/HKCategoryTypeIdentifierLowHeartRateEvent.csv\n#> data/HKCategoryTypeIdentifierMindfulSession.csv\n#> data/HKCategoryTypeIdentifierSleepAnalysis.csv\n#> data/HKDataTypeSleepDurationGoal.csv\n#> data/HKQuantityTypeIdentifierActiveEnergyBurned.csv\n#> data/HKQuantityTypeIdentifierAppleExerciseTime.csv\n#> data/HKQuantityTypeIdentifierAppleSleepingBreathingDisturbances.csv\n#> data/HKQuantityTypeIdentifierAppleSleepingWristTemperature.csv\n#> data/HKQuantityTypeIdentifierAppleStandTime.csv\n#> data/HKQuantityTypeIdentifierAppleWalkingSteadiness.csv\n#> data/HKQuantityTypeIdentifierBasalEnergyBurned.csv\n#> data/HKQuantityTypeIdentifierBodyFatPercentage.csv\n#> data/HKQuantityTypeIdentifierBodyMass.csv\n#> data/HKQuantityTypeIdentifierBodyMassIndex.csv\n#> data/HKQuantityTypeIdentifierDistanceCycling.csv\n#> data/HKQuantityTypeIdentifierDistanceRowing.csv\n#> data/HKQuantityTypeIdentifierDistanceWalkingRunning.csv\n#> data/HKQuantityTypeIdentifierEnvironmentalAudioExposure.csv\n#> data/HKQuantityTypeIdentifierEnvironmentalSoundReduction.csv\n#> data/HKQuantityTypeIdentifierFlightsClimbed.csv\n#> data/HKQuantityTypeIdentifierHeadphoneAudioExposure.csv\n#> data/HKQuantityTypeIdentifierHeartRate.csv\n#> data/HKQuantityTypeIdentifierHeartRateRecoveryOneMinute.csv\n#> data/HKQuantityTypeIdentifierHeartRateVariabilitySDNN.csv\n#> data/HKQuantityTypeIdentifierHeight.csv\n#> data/HKQuantityTypeIdentifierLeanBodyMass.csv\n#> data/HKQuantityTypeIdentifierPhysicalEffort.csv\n#> data/HKQuantityTypeIdentifierRespiratoryRate.csv\n#> data/HKQuantityTypeIdentifierRestingHeartRate.csv\n#> data/HKQuantityTypeIdentifierRowingSpeed.csv\n#> data/HKQuantityTypeIdentifierRunningGroundContactTime.csv\n#> data/HKQuantityTypeIdentifierRunningPower.csv\n#> data/HKQuantityTypeIdentifierRunningSpeed.csv\n#> data/HKQuantityTypeIdentifierRunningStrideLength.csv\n#> data/HKQuantityTypeIdentifierRunningVerticalOscillation.csv\n#> data/HKQuantityTypeIdentifierSixMinuteWalkTestDistance.csv\n#> data/HKQuantityTypeIdentifierStairAscentSpeed.csv\n#> data/HKQuantityTypeIdentifierStairDescentSpeed.csv\n#> data/HKQuantityTypeIdentifierStepCount.csv\n#> data/HKQuantityTypeIdentifierTimeInDaylight.csv\n#> data/HKQuantityTypeIdentifierVO2Max.csv\n#> data/HKQuantityTypeIdentifierWalkingAsymmetryPercentage.csv\n#> data/HKQuantityTypeIdentifierWalkingDoubleSupportPercentage.csv\n#> data/HKQuantityTypeIdentifierWalkingHeartRateAverage.csv\n#> data/HKQuantityTypeIdentifierWalkingSpeed.csv\n#> data/HKQuantityTypeIdentifierWalkingStepLength.csv\n#> data/apple_health_all.tsv\nYou can see that some of these files are prefixed with\nHKQuantityTypeIdentifier\nand others with\nHKCategoryTypeIdentifier\n. One, a 221 byte file, is prefixed with\nHKDataType\nand is my\nSleepDurationGoal\n. It consists a single row with the frankly delusional value of “8”. Small differences between the Quantity and Category type files were, I think, responsible for DuckDB complaining. The named columns are the same across all the records but I think they vary just enough in terms of what the read engine wants that when it got to one of them it choked.\nThe largest of these files is an 830MB file measuring\nActiveEnergyBurned\n. Records for this quantity are generated when you start doing something long enough for either your iPhone or your Watch to notice. Here’s what it looks like when read in by itself:\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nburn_df\n#> # A tibble: 2,658,839 × 10\n#>      rowid type                                  sourceName sourceVersion device unit  creationDate        startDate           endDate             value\n#>      <int> <chr>                                 <chr>      <chr>         <chr>  <chr> <dttm>              <dttm>              <dttm>              <dbl>\n#>  1 3910445 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:43:01 2024-09-17 12:41:01 2024-09-17 12:42:02 0.389\n#>  2 3910446 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:44:23 2024-09-17 12:42:02 2024-09-17 12:42:54 1.10 \n#>  3 3910447 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:44:26 2024-09-17 12:42:54 2024-09-17 12:43:34 1.83 \n#>  4 3910448 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:47:55 2024-09-17 12:43:55 2024-09-17 12:44:46 1.62 \n#>  5 3910449 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:47:55 2024-09-17 12:44:56 2024-09-17 12:45:16 0.659\n#>  6 3910450 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:47:55 2024-09-17 12:45:16 2024-09-17 12:46:07 1.15 \n#>  7 3910451 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:47:57 2024-09-17 12:46:07 2024-09-17 12:47:08 0.432\n#>  8 3910452 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:49:42 2024-09-17 12:47:08 2024-09-17 12:48:10 2.91 \n#>  9 3910453 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:49:42 2024-09-17 12:48:10 2024-09-17 12:48:40 3.09 \n#> 10 3910454 HKQuantityTypeIdentifierActiveEnergy… Kieran’s … 10.6.1        <<HKD… Cal   2024-09-17 12:50:03 2024-09-17 12:48:40 2024-09-17 12:49:21 0.74 \n#> # ℹ 2,658,829 more rows\nThe\nstartDate\nand\nendDate\nperiods on each row were almost all between thirty and ninety seconds, though there was a long, sparse tail of outliers. I haven’t looked yet to see if these artifacts are errors or if they were created in some recognizable way. It’s not that a workout gets recorded as 60 minutes in a single row or anything—once you’re doing something, the phone or watch will create a new row every 30 to 90 seconds for as long as you continue doing it. While the first rows of the table here begin in September 2024, the rows are not ordered by date. I think it’s by date within device, or\nsourceName\n. My data go back to 2018. Again, if you were the sort of person who worked out once or twice a day, and took full advantage of Apple’s market offerings with health, and had owned an iPhone or Watch for a long time, you would end up with a\nlot\nof information about yourself.\nIs it useful information? It’s certainly extremely fine-grained. Whether all that monitoring amounts to anything particularly insightful is a harder question, particularly at the level of individuals. One of the many ironies of all broadly person-centric or social data over the past fifty years is that the people doing work on everything from social networks to epidemiology to personal health and beyond prayed for more and more fine-grained data … and then they got it. Now we are much better than we used to be at\nbleating articulate monotony\n, quantitatively, and indicating that two and one are three, that grass is green, lakes damp, and mountains steep.\nIn the case of the ActiveEnergyBurned table, 2.5 million moment-to-moment estimates of the calories I’m burning might or might not be useful. We can aggregate it a bit by grouping the values by hour and summing them up for every hour, day, month, and year that we have. Like this:\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\nburn_df <- read_csv(here(\"data\", \"HKQuantityTypeIdentifierActiveEnergyBurned.csv\"),\n                    col_types = colspec) |>\n  arrange(startDate) |>\n  select(rowid, sourceName, startDate:value) |>\n  mutate(startDate = with_tz(startDate, tzone = \"US/Eastern\"),\n         endDate = with_tz(startDate, tzone = \"US/Eastern\")) |>\n  mutate(\n    start_year = year(startDate),\n    start_month = month(startDate),\n    start_day = day(startDate),\n    start_hour = hour(startDate),\n    start_minute = minute(startDate),\n    end_year = year(endDate),\n    end_month = month(endDate),\n    end_day = day(endDate),\n    end_hour = hour(endDate),\n    end_minute = minute(endDate),\n    length = endDate - startDate)\n\nhourly_burn <- burn_df |>\n  group_by(start_year, start_month, start_day, start_hour) |>\n  summarize(burned = sum(value),\n            time = first(startDate)) |>\n  mutate(index = as.integer(as.factor(time)))\n\nhourly_burn\n#> # A tibble: 36,338 × 7\n#> # Groups:   start_year, start_month, start_day [2,333]\n#>    start_year start_month start_day start_hour burned time                index\n#>         <dbl>       <dbl>     <int>      <int>  <dbl> <dttm>              <int>\n#>  1       2018           7        14          9  73.2  2018-07-14 09:51:54     1\n#>  2       2018           7        14         10 525.   2018-07-14 10:00:13     2\n#>  3       2018           7        14         11   4.84 2018-07-14 11:37:34     3\n#>  4       2018           7        14         12  27.4  2018-07-14 12:12:27     4\n#>  5       2018           7        14         13  72.5  2018-07-14 13:00:03     5\n#>  6       2018           7        14         14  93.1  2018-07-14 14:00:51     6\n#>  7       2018           7        14         15  68.0  2018-07-14 15:00:55     7\n#>  8       2018           7        14         16  18.9  2018-07-14 16:00:08     8\n#>  9       2018           7        14         17  29.1  2018-07-14 17:00:23     9\n#> 10       2018           7        14         18  30.4  2018-07-14 18:00:19    10\n#> # ℹ 36,328 more rows\nFrom there, we can make a kind of grid of every single hour of my life since July 2018, with each box of the grid showing—if there’s data—an estimate of calories burned in that hour. Notice above how I used\nwith_tz()\nto adjust the start and end dates before going the other calculations. If I didn’t do that most of the data would be off by five hours (the difference between US/Eastern and UTC). This correction will still leave some data points off, as I do sometimes leave not only the house but even the time zone. But for now it’s good enough.\nTo get a nice picture we should fold this data into a grid of all the days and hours since July 2018, taking care to drop dates that don’t exist, like June 31st or February 29th 2019.\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ntime_grid <- expand_grid(\n  start_year = c(2018L:2025L),\n  start_month = c(1L:12L),\n  start_day = c(1L:31L),\n  start_hour = c(0:23)) |>\n  filter(!(start_month == 2 & start_day > 29)) |>\n  filter(!(start_month %in% c(4, 6, 9, 11) & start_day > 30)) |>\n  filter(!(start_year %nin% c(2020, 2024) & start_month == 2 & start_day > 28)) # careful here\n\nburned_grid <- time_grid |>\n  left_join(hourly_burn, by = c(\"start_year\", \"start_month\",\n                                \"start_day\", \"start_hour\"))\nThen we can draw a plot:\nr\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nout <- burned_grid |>\n  ggplot(aes(x = factor(start_day),\n             y = factor(start_hour),\n             fill = burned)) +\n  geom_tile(color = \"gray10\", linewidth = 0.01) +\n  coord_fixed() +\n  facet_grid(start_year ~ month_rc) +\n  scale_fill_binned_sequential(palette = \"Inferno\",\n                               na.value = \"white\") +\n  scale_x_discrete(breaks = c(5, 15, 25), labels = c(5, 15, 25)) +\n  scale_y_discrete(breaks = c(6, 12, 18, 0), labels = c(\"6am\", \"Noon\", \"6pm\", \"Midnight\")) +\n  guides(fill = guide_legend(theme = theme(legend.title.position = \"top\",\n                                           legend.text.position = \"bottom\"))) +\n  labs(x = \"Day of the Month\",\n       y = \"Hour of the Day\",\n       fill = \"Energy (Cal)\",\n       title = \"iPhone/Watch Health Data Dump\",\n       subtitle = \"Hourly sums of all ‘Active Energy Burned’ measurements, by day and year\") +\n  theme(strip.text = element_text(size = rel(1.2)),\n        axis.text.y = element_text(size = rel(0.8)),\n        plot.title = element_text(size = rel(2)))\nI’m not going to show you the whole thing—a large faceted grid of all eight years of hourly data—because that would make me, and indeed perhaps you, feel a little queasy. But here’s a typical recent month:\nA detail from the big graph.\nDays of the month are on the x-axis, hours of the day on the y-axis. Where a cell is filled in, there’s at least one activity measure for that hour. Where the grid cell is white, there’s no data. You can see two things from this detail. The first thing is that on most weekday mornings I go for a run. Now, to be clear, it’s not like I spend the rest of the day entirely comatose. But given that this is a detail from a graph covering eight years of hourly data, the only reliable signal that jumps out from the background is running for a while. The second thing is that, recently, I have been experimenting with wearing my Watch while sleeping, but more often than not I forget to put it back on after taking it off to charge in the evenings. I’m sure there are other things to be found in this data. And I haven’t even looked at some of the other files yet. (\nStairAscentSpeed.csv\n?\nTimeInDaylight.csv\n?\nWalkingAsymmetryPercentage.csv\n? So many options.) And, in fairness, some of the features of this sort of monitoring—like fall detection, and AFib triggering—are properly important and very useful. But I am still not entirely convinced that collecting this much data about me personally is going to be of much use to anyone.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR on kieranhealy.org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "Your Phone and Watch have a lot of data about you. I mean, like, a lot. Someone should really write a book all about the general issues for society that this raises. Yesterday I decided I wanted to take a look specifically at the health data on my iPho...",
    "meta_keywords": null,
    "og_description": "Your Phone and Watch have a lot of data about you. I mean, like, a lot. Someone should really write a book all about the general issues for society that this raises. Yesterday I decided I wanted to take a look specifically at the health data on my iPho...",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "Burn Notice | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 18.8,
    "sitemap_lastmod": null,
    "twitter_description": "Your Phone and Watch have a lot of data about you. I mean, like, a lot. Someone should really write a book all about the general issues for society that this raises. Yesterday I decided I wanted to take a look specifically at the health data on my iPho...",
    "twitter_title": "Burn Notice | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/02/burn-notice/",
    "word_count": 3756
  }
}