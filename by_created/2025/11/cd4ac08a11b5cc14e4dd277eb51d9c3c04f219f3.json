{
  "id": "cd4ac08a11b5cc14e4dd277eb51d9c3c04f219f3",
  "url": "https://www.r-bloggers.com/2025/07/a-primer-on-power-simulations-when-evaluating-experimental-designs/",
  "created_at_utc": "2025-11-22T19:58:09Z",
  "data": null,
  "raw_original": {
    "uuid": "7e4af72c-4748-4a8a-82a9-821d202d6609",
    "created_at": "2025-11-22 19:58:09",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/07/a-primer-on-power-simulations-when-evaluating-experimental-designs/",
      "crawled_at": "2025-11-22T10:46:03.811589",
      "external_links": [
        {
          "href": "https://joachim-gassen.github.io/2025/07/power-simulations/",
          "text": "An Accounting and Data Science Nerd's Corner"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://joachim-gassen.github.io/2025/07/power-simulations/#fn1",
          "text": "1"
        },
        {
          "href": "https://joachim-gassen.github.io/2025/07/power-simulations/#fnref1",
          "text": "↩︎"
        },
        {
          "href": "https://joachim-gassen.github.io/2025/07/power-simulations/",
          "text": "An Accounting and Data Science Nerd's Corner"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "A Primer on Power Simulations when Evaluating Experimental Designs | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimBaseLineData-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimTreated-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerStats-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveEffectSize-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveCellSize-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveFamily-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimulVsEquation-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/an-accounting-and-data-science-nerds-corner/",
          "text": "An Accounting and Data Science Nerd's Corner"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-393834 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">A Primer on Power Simulations when Evaluating Experimental Designs</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">July 5, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/an-accounting-and-data-science-nerds-corner/\">An Accounting and Data Science Nerd's Corner</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://joachim-gassen.github.io/2025/07/power-simulations/\"> An Accounting and Data Science Nerd's Corner</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>When you design experiments, you need to know how many participants it takes to get informative results. But what makes results informative? Simply put: A precise effect estimate, meaning an estimate that is unbiased and has a narrow confidence interval. Your randomized design and careful measurement should ensure that your effect estimate is unbiased. But how can you be confident that your estimate’s confidence interval is narrow “enough”?</p>\n<p>Let’s look at an artificial example. Assume that you are designing an experiment that studies whether a certain information treatment makes people more likely to invest into a given firm’s stock. You measure this investment likelihood as a stated preference on a discrete scale from 0 to 100 %. Let’s simulate some baseline data, meaning the data for participants that do not receive the information treatment.</p>\n<pre>library(tidyverse)\nlibrary(truncnorm)\nset.seed(42)\n\nsim_data &lt;- function(n = 100, mean = 50, sd = 20) {\n  y &lt;- rtruncnorm(n, 0, 100, mean, sd)\n  tibble(\n    y = round(y)\n  )\n}\n\nbaseline &lt;- sim_data()\nggplot(baseline, aes(x = y)) + geom_histogram(bins = 20) + theme_minimal()</pre>\n<p><img data-lazy-src=\"https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimBaseLineData-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimBaseLineData-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>As you can see, we had to make assumotions about the sample size (100), the distribution (truncated normal), its mean (50), and its standard deviation (20) to simulate data. These are crucial assumptions for all that follows. The resulting histogram shows that we have quite a variance in our investment likelihood.</p>\n<p>Based on our assumed distribution of our dependent variable under no treatment, we next have to formulate our assumptions about a likely effect size for our information treatment. Often, effect sizes are expressed in % Standard Deviation, also labelled as Cohen’s-d. We start with the expectation that our expected effect size is 10, meaning a Cohen’s-d value of 0.5. Such an effect is traditionally referred to as a “medium-sized effect” (small (d = 0.2), medium (d = 0.5), and large (d &gt;= 0.8)). Let’s simulate the treated sample and plot a visual.</p>\n<pre>treated &lt;- sim_data(mean = 60)\n\nsmp &lt;- bind_rows(\n  baseline %&gt;% mutate(tment = FALSE) %&gt;% select(tment, y),\n  treated %&gt;% mutate(tment = TRUE) %&gt;% select(tment, y)\n)\n\nggplot(smp, aes(x = tment, y = y)) + geom_boxplot() + theme_minimal()</pre>\n<p><img data-lazy-src=\"https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimTreated-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimTreated-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>Based on the boxplot you might want to guess that there is a treatment effect and this is exactly how Cohen has once defined a “medium” effect. It should be “visible to the naked eye of a careful observer”. But is it significant in terms of a convential t-Test?</p>\n<pre>tt &lt;- t.test(y ~ !tment, data = smp)\nprint(tt)\n## \n## \tWelch Two Sample t-test\n## \n## data:  y by !tment\n## t = 1.8965, df = 195.86, p-value = 0.05936\n## alternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n## 95 percent confidence interval:\n##  -0.1970289 10.0770289\n## sample estimates:\n## mean in group FALSE  mean in group TRUE \n##               57.16               52.22</pre>\n<p>Only marginally. And this brings us back to the topic of narrow confidence intervals and power. For the test above, the confidence interval ranges from -0.2 to 10.08. Assuming that we are mostly interested in learning whether our effect is different from zero at the conventional level of 95%, then this confidence interval is just a little to wide (it includes zero).</p>\n<p>But this might be just bad luck resulting from the simulated data draw, right? Power conceptually asks the question how likely it is that we would find a significant test result given our test design and assumptions about the data generating process as outlined above. We can derive the power for our fictuous experiment by running a Monte Carlo Simulation on our setting. This is not that hard to do:</p>\n<pre>sim_run &lt;- function(effect_size = 0.5, cell_size = 100, mean = 50, sd = 20) {\n  df &lt;- bind_rows(\n    sim_data(n = cell_size, mean = mean, sd = sd) %&gt;% \n      mutate(tment = FALSE) %&gt;% select(tment, y),\n    sim_data(n = cell_size, mean = mean + effect_size*sd, sd = sd) %&gt;% \n      mutate(tment = TRUE) %&gt;% select(tment, y)\n  )\n  tt &lt;- t.test(y ~ !tment, data = df)\n  tibble(\n    effect_size = effect_size,\n    cell_size = cell_size,\n    mean = mean,\n    sd = sd,\n    t.stat = tt$statistic,\n    p.value = tt$p.value,\n    est = unname(tt$estimate[1] - tt$estimate[2]),\n    lb = unname(tt$conf.int[1]),\n    ub = unname(tt$conf.int[2])\n  )\n}\n\nsim_runs &lt;- bind_rows(replicate(1000, sim_run(), simplify = FALSE))</pre>\n<p>Based on this simulation, we can now plot a distribution of our 1,000 effect estimates and also infer how often their confidence interval is strictly positive, meaning how often we can reject the null hypothesis of no effect.</p>\n<pre>ggplot(sim_runs, aes(x = est)) + geom_density() + theme_minimal()</pre>\n<p><img data-lazy-src=\"https://i1.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerStats-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerStats-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<pre>pwr &lt;- mean(sim_runs$lb &gt; 0)\npwr\n## [1] 0.915</pre>\n<p>So, in 91.5% of our simulated cases, the effect estimate was significantly positive at a two-sided significance level of 95%.<a class=\"footnote-ref\" href=\"https://joachim-gassen.github.io/2025/07/power-simulations/#fn1\" id=\"fnref1\" rel=\"nofollow\" target=\"_blank\"><sup>1</sup></a> Conventionally, experimentalists try to achieve a power of at least 80%. This implies that with cell sizes of 100 participants we should be comfortably high-powered to detect a medium effect size.</p>\n<p>Now: How would the power change when we change the assumed parameters, for example the cell or the effect size? To assess this quickly, one can use power functions of the underlying test statistics. These functions are based on assumptions that might differ from the ones that we used in our simulated data generating process. For example, we model a dependent variable that is discrete and based on a truncated normal distribution while the power function for two-sample t-tests assumes the data to be normally distributed. Let’s compare the function-based power estimate for our effect size with the power estimate derived from our Monte Carlo Simulation.</p>\n<pre>library(pwr)\npwr.t.test(n = 100, d = 0.5, sig.level = .05)\n## \n##      Two-sample t test power calculation \n## \n##               n = 100\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.9404272\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group</pre>\n<p>This is reasonably close but a little bit higher than the simulation power estimate of 91.5%. One very convenient feature of the <code>pwr</code> functions in R is that they will always calculate the missing parameter (power in our case above). For example, to infer the cell size that you need to reach a power of 80%:</p>\n<pre>pwr.t.test(power = 0.8, d = 0.5, sig.level = .05)\n## \n##      Two-sample t test power calculation \n## \n##               n = 63.76561\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group</pre>\n<p>We would need 64 people per cell. Let’s see how our function-based power estimate changes when we change the effect size.</p>\n<pre>my_power_fct &lt;- function(effect_size = 0.5, cell_size = 100) {\n  rv &lt;- pwr.t.test(n = cell_size, d = effect_size, sig.level = .05)\n  tibble(\n    effect_size = effect_size,\n    cell_size = cell_size,\n    power = rv$power\n  )\n}\ndf &lt;- bind_rows(lapply(seq(0.1, 0.8, by = 0.1), my_power_fct))\nggplot(df, aes(x = effect_size, y = power)) + \n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + theme_minimal()</pre>\n<p><img data-lazy-src=\"https://i1.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveEffectSize-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveEffectSize-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>You can infer from this effect size power curve that with a cell size of 100 participants and under normality assumptions, one is able to reasonably reliably detect an effect size of around 0.4 standard deviations.</p>\n<p>Another informative power curve is the one that plots power relative to cell size.</p>\n<pre>df &lt;- bind_rows(\n  lapply(seq(10, 120, by = 10), function(x) my_power_fct(cell_size = x))\n)\nggplot(df, aes(x = cell_size, y = power)) + \n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + theme_minimal()</pre>\n<p><img data-lazy-src=\"https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveCellSize-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveCellSize-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>And, if you want to increase the cognitive load of your visual, you can combine the two.</p>\n<pre>df &lt;- expand_grid(\n  cell_size = seq(10, 120, by = 10),\n  effect_size = c(0.2, 0.5, 0.8)\n) %&gt;% \n  mutate(\n    power = pmap_dbl(\n      list(n = cell_size, d = effect_size), \n      function(n, d) pwr.t.test(n = n, d = d, sig.level = .05)$power\n    )\n  )\n\nggplot(df, aes(x = cell_size, y = power, color = factor(effect_size))) + \n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + \n  labs(color = \"effect_size\") + \n  theme_minimal()</pre>\n<p><img data-lazy-src=\"https://i2.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveFamily-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/PowerCurveFamily-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>Now, given that power functions exist for many standard test designs, what is the point of infering power from simulations? The answer is that these are more flexible and allow us to incorporate details of our experimental design that power functions are unable to capture. In our case above, we have a non-normally distributed dependent variable. Let’s compare the function-based power curve with the one derived from simulations. To verify that both methods in principle give the same power curve, we also add another power simulation that uses a normally distributed dependent variable.</p>\n<pre>function_power_est &lt;- bind_rows(lapply(\n  seq(10, 120, by = 10), \n  function(x) my_power_fct(cell_size = x) %&gt;%\n    mutate(method = \"function\")\n))\n\nmc_sim_power &lt;- function(effect_size = 0.5, cell_size = 100, runs = 1000) {\n  bind_rows(\n    bind_rows(\n      replicate(runs, sim_run(effect_size, cell_size), simplify = F)\n    ) %&gt;%\n      group_by(effect_size, cell_size) %&gt;%\n      summarise(power = mean(lb &gt; 0), .groups = \"drop\")\n  )\n}\n\nsim_power_est_d100 &lt;- bind_rows(lapply(\n  seq(10, 120, by = 10), \n  function(x) mc_sim_power(cell_size = x) %&gt;%\n    mutate(method = \"simulation_d100\")\n))\n\nsim_data &lt;- function(n = 100, mean = 50, sd = 20) {\n  tibble(y = rnorm(n, mean, sd))\n}\nsim_power_est_norm &lt;- bind_rows(lapply(\n  seq(10, 120, by = 10), \n  function(x) mc_sim_power(cell_size = x) %&gt;%\n    mutate(method = \"simulation_norm\")\n))\n\ndf &lt;- bind_rows(function_power_est, sim_power_est_d100, sim_power_est_norm)\n\nggplot(df, aes(x = cell_size, y = power, color = method)) +\n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + \n  theme_minimal() + \n  theme(legend.position = \"bottom\")</pre>\n<p><img data-lazy-src=\"https://i2.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimulVsEquation-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimulVsEquation-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>As you see, the simulation using a normally distributed dependent variable yields a power curve that is virtually identical to the function-based power curve. The original simulation yields marginally lower power values. This is caused by the truncation of the dependent variable to be within 0 and 100. As you can also see from the estimate distribution above, this truncation affects treated observations (mean = 60) more than control observations (mean = 50), marginally reducing power and slightly biasing the effect size measurement downwards. This is a good example for why simulations are helpful in the experimental design stage. They not only allow you to estimate power but they can also help to spot other design issues that you might want to address prior to running your design.</p>\n<div class=\"footnotes footnotes-end-of-document\">\n<hr/>\n<ol>\n<li id=\"fn1\"><p>If you wonder why the estimate distribution does not center around the true effect size of 10: Well spotted! Keep reading until the end ;-).<a class=\"footnote-back\" href=\"https://joachim-gassen.github.io/2025/07/power-simulations/#fnref1\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n</ol>\n</div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://joachim-gassen.github.io/2025/07/power-simulations/\"> An Accounting and Data Science Nerd's Corner</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "A Primer on Power Simulations when Evaluating Experimental Designs\nPosted on\nJuly 5, 2025\nby\nAn Accounting and Data Science Nerd's Corner\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nAn Accounting and Data Science Nerd's Corner\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nWhen you design experiments, you need to know how many participants it takes to get informative results. But what makes results informative? Simply put: A precise effect estimate, meaning an estimate that is unbiased and has a narrow confidence interval. Your randomized design and careful measurement should ensure that your effect estimate is unbiased. But how can you be confident that your estimate’s confidence interval is narrow “enough”?\nLet’s look at an artificial example. Assume that you are designing an experiment that studies whether a certain information treatment makes people more likely to invest into a given firm’s stock. You measure this investment likelihood as a stated preference on a discrete scale from 0 to 100 %. Let’s simulate some baseline data, meaning the data for participants that do not receive the information treatment.\nlibrary(tidyverse)\nlibrary(truncnorm)\nset.seed(42)\n\nsim_data <- function(n = 100, mean = 50, sd = 20) {\n  y <- rtruncnorm(n, 0, 100, mean, sd)\n  tibble(\n    y = round(y)\n  )\n}\n\nbaseline <- sim_data()\nggplot(baseline, aes(x = y)) + geom_histogram(bins = 20) + theme_minimal()\nAs you can see, we had to make assumotions about the sample size (100), the distribution (truncated normal), its mean (50), and its standard deviation (20) to simulate data. These are crucial assumptions for all that follows. The resulting histogram shows that we have quite a variance in our investment likelihood.\nBased on our assumed distribution of our dependent variable under no treatment, we next have to formulate our assumptions about a likely effect size for our information treatment. Often, effect sizes are expressed in % Standard Deviation, also labelled as Cohen’s-d. We start with the expectation that our expected effect size is 10, meaning a Cohen’s-d value of 0.5. Such an effect is traditionally referred to as a “medium-sized effect” (small (d = 0.2), medium (d = 0.5), and large (d >= 0.8)). Let’s simulate the treated sample and plot a visual.\ntreated <- sim_data(mean = 60)\n\nsmp <- bind_rows(\n  baseline %>% mutate(tment = FALSE) %>% select(tment, y),\n  treated %>% mutate(tment = TRUE) %>% select(tment, y)\n)\n\nggplot(smp, aes(x = tment, y = y)) + geom_boxplot() + theme_minimal()\nBased on the boxplot you might want to guess that there is a treatment effect and this is exactly how Cohen has once defined a “medium” effect. It should be “visible to the naked eye of a careful observer”. But is it significant in terms of a convential t-Test?\ntt <- t.test(y ~ !tment, data = smp)\nprint(tt)\n## \n## \tWelch Two Sample t-test\n## \n## data:  y by !tment\n## t = 1.8965, df = 195.86, p-value = 0.05936\n## alternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n## 95 percent confidence interval:\n##  -0.1970289 10.0770289\n## sample estimates:\n## mean in group FALSE  mean in group TRUE \n##               57.16               52.22\nOnly marginally. And this brings us back to the topic of narrow confidence intervals and power. For the test above, the confidence interval ranges from -0.2 to 10.08. Assuming that we are mostly interested in learning whether our effect is different from zero at the conventional level of 95%, then this confidence interval is just a little to wide (it includes zero).\nBut this might be just bad luck resulting from the simulated data draw, right? Power conceptually asks the question how likely it is that we would find a significant test result given our test design and assumptions about the data generating process as outlined above. We can derive the power for our fictuous experiment by running a Monte Carlo Simulation on our setting. This is not that hard to do:\nsim_run <- function(effect_size = 0.5, cell_size = 100, mean = 50, sd = 20) {\n  df <- bind_rows(\n    sim_data(n = cell_size, mean = mean, sd = sd) %>% \n      mutate(tment = FALSE) %>% select(tment, y),\n    sim_data(n = cell_size, mean = mean + effect_size*sd, sd = sd) %>% \n      mutate(tment = TRUE) %>% select(tment, y)\n  )\n  tt <- t.test(y ~ !tment, data = df)\n  tibble(\n    effect_size = effect_size,\n    cell_size = cell_size,\n    mean = mean,\n    sd = sd,\n    t.stat = tt$statistic,\n    p.value = tt$p.value,\n    est = unname(tt$estimate[1] - tt$estimate[2]),\n    lb = unname(tt$conf.int[1]),\n    ub = unname(tt$conf.int[2])\n  )\n}\n\nsim_runs <- bind_rows(replicate(1000, sim_run(), simplify = FALSE))\nBased on this simulation, we can now plot a distribution of our 1,000 effect estimates and also infer how often their confidence interval is strictly positive, meaning how often we can reject the null hypothesis of no effect.\nggplot(sim_runs, aes(x = est)) + geom_density() + theme_minimal()\npwr <- mean(sim_runs$lb > 0)\npwr\n## [1] 0.915\nSo, in 91.5% of our simulated cases, the effect estimate was significantly positive at a two-sided significance level of 95%.\n1\nConventionally, experimentalists try to achieve a power of at least 80%. This implies that with cell sizes of 100 participants we should be comfortably high-powered to detect a medium effect size.\nNow: How would the power change when we change the assumed parameters, for example the cell or the effect size? To assess this quickly, one can use power functions of the underlying test statistics. These functions are based on assumptions that might differ from the ones that we used in our simulated data generating process. For example, we model a dependent variable that is discrete and based on a truncated normal distribution while the power function for two-sample t-tests assumes the data to be normally distributed. Let’s compare the function-based power estimate for our effect size with the power estimate derived from our Monte Carlo Simulation.\nlibrary(pwr)\npwr.t.test(n = 100, d = 0.5, sig.level = .05)\n## \n##      Two-sample t test power calculation \n## \n##               n = 100\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.9404272\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\nThis is reasonably close but a little bit higher than the simulation power estimate of 91.5%. One very convenient feature of the\npwr\nfunctions in R is that they will always calculate the missing parameter (power in our case above). For example, to infer the cell size that you need to reach a power of 80%:\npwr.t.test(power = 0.8, d = 0.5, sig.level = .05)\n## \n##      Two-sample t test power calculation \n## \n##               n = 63.76561\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\nWe would need 64 people per cell. Let’s see how our function-based power estimate changes when we change the effect size.\nmy_power_fct <- function(effect_size = 0.5, cell_size = 100) {\n  rv <- pwr.t.test(n = cell_size, d = effect_size, sig.level = .05)\n  tibble(\n    effect_size = effect_size,\n    cell_size = cell_size,\n    power = rv$power\n  )\n}\ndf <- bind_rows(lapply(seq(0.1, 0.8, by = 0.1), my_power_fct))\nggplot(df, aes(x = effect_size, y = power)) + \n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + theme_minimal()\nYou can infer from this effect size power curve that with a cell size of 100 participants and under normality assumptions, one is able to reasonably reliably detect an effect size of around 0.4 standard deviations.\nAnother informative power curve is the one that plots power relative to cell size.\ndf <- bind_rows(\n  lapply(seq(10, 120, by = 10), function(x) my_power_fct(cell_size = x))\n)\nggplot(df, aes(x = cell_size, y = power)) + \n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + theme_minimal()\nAnd, if you want to increase the cognitive load of your visual, you can combine the two.\ndf <- expand_grid(\n  cell_size = seq(10, 120, by = 10),\n  effect_size = c(0.2, 0.5, 0.8)\n) %>% \n  mutate(\n    power = pmap_dbl(\n      list(n = cell_size, d = effect_size), \n      function(n, d) pwr.t.test(n = n, d = d, sig.level = .05)$power\n    )\n  )\n\nggplot(df, aes(x = cell_size, y = power, color = factor(effect_size))) + \n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + \n  labs(color = \"effect_size\") + \n  theme_minimal()\nNow, given that power functions exist for many standard test designs, what is the point of infering power from simulations? The answer is that these are more flexible and allow us to incorporate details of our experimental design that power functions are unable to capture. In our case above, we have a non-normally distributed dependent variable. Let’s compare the function-based power curve with the one derived from simulations. To verify that both methods in principle give the same power curve, we also add another power simulation that uses a normally distributed dependent variable.\nfunction_power_est <- bind_rows(lapply(\n  seq(10, 120, by = 10), \n  function(x) my_power_fct(cell_size = x) %>%\n    mutate(method = \"function\")\n))\n\nmc_sim_power <- function(effect_size = 0.5, cell_size = 100, runs = 1000) {\n  bind_rows(\n    bind_rows(\n      replicate(runs, sim_run(effect_size, cell_size), simplify = F)\n    ) %>%\n      group_by(effect_size, cell_size) %>%\n      summarise(power = mean(lb > 0), .groups = \"drop\")\n  )\n}\n\nsim_power_est_d100 <- bind_rows(lapply(\n  seq(10, 120, by = 10), \n  function(x) mc_sim_power(cell_size = x) %>%\n    mutate(method = \"simulation_d100\")\n))\n\nsim_data <- function(n = 100, mean = 50, sd = 20) {\n  tibble(y = rnorm(n, mean, sd))\n}\nsim_power_est_norm <- bind_rows(lapply(\n  seq(10, 120, by = 10), \n  function(x) mc_sim_power(cell_size = x) %>%\n    mutate(method = \"simulation_norm\")\n))\n\ndf <- bind_rows(function_power_est, sim_power_est_d100, sim_power_est_norm)\n\nggplot(df, aes(x = cell_size, y = power, color = method)) +\n  geom_hline(yintercept = 0.8, lty = 2, color = \"red\") +\n  geom_line() + geom_point() + \n  theme_minimal() + \n  theme(legend.position = \"bottom\")\nAs you see, the simulation using a normally distributed dependent variable yields a power curve that is virtually identical to the function-based power curve. The original simulation yields marginally lower power values. This is caused by the truncation of the dependent variable to be within 0 and 100. As you can also see from the estimate distribution above, this truncation affects treated observations (mean = 60) more than control observations (mean = 50), marginally reducing power and slightly biasing the effect size measurement downwards. This is a good example for why simulations are helpful in the experimental design stage. They not only allow you to estimate power but they can also help to spot other design issues that you might want to address prior to running your design.\nIf you wonder why the estimate distribution does not center around the true effect size of 10: Well spotted! Keep reading until the end ;-).\n↩︎\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nAn Accounting and Data Science Nerd's Corner\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "When you design experiments, you need to know how many participants it takes to get informative results. But what makes results informative? Simply put: A precise effect estimate, meaning an estimate that is unbiased and has a narrow confidence inte...",
      "meta_keywords": null,
      "og_description": "When you design experiments, you need to know how many participants it takes to get informative results. But what makes results informative? Simply put: A precise effect estimate, meaning an estimate that is unbiased and has a narrow confidence inte...",
      "og_image": "https://joachim-gassen.github.io/post/2025-07-0-power-simulations_files/figure-html/SimBaseLineData-1.png",
      "og_title": "A Primer on Power Simulations when Evaluating Experimental Designs | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 9.1,
      "sitemap_lastmod": null,
      "twitter_description": "When you design experiments, you need to know how many participants it takes to get informative results. But what makes results informative? Simply put: A precise effect estimate, meaning an estimate that is unbiased and has a narrow confidence inte...",
      "twitter_title": "A Primer on Power Simulations when Evaluating Experimental Designs | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/07/a-primer-on-power-simulations-when-evaluating-experimental-designs/",
      "word_count": 1822
    }
  }
}