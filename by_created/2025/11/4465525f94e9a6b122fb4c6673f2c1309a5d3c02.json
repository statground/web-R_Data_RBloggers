{
  "id": "4465525f94e9a6b122fb4c6673f2c1309a5d3c02",
  "url": "https://www.r-bloggers.com/2025/04/model-diagnostics-statistics-vs-machine-learning/",
  "created_at_utc": "2025-11-22T19:58:42Z",
  "data": null,
  "raw_original": {
    "uuid": "bd0518e3-b8e3-4e59-ba16-edcb2247eef4",
    "created_at": "2025-11-22 19:58:42",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/04/model-diagnostics-statistics-vs-machine-learning/",
      "crawled_at": "2025-11-22T10:49:34.861785",
      "external_links": [
        {
          "href": "https://lorentzen.ch/index.php/2025/05/01/model-diagnostics-statistics-vs-machine-learning/",
          "text": "R – Michael's and Christian's Blog"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://doi.org/10.1007/978-3-662-63882-8",
          "text": "Regression – Models, Methods and Applications 2nd ed. (2021)"
        },
        {
          "href": "https://glum.readthedocs.io/",
          "text": "glum"
        },
        {
          "href": "https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS",
          "text": "statsmodels.regression.linear_model.OLS"
        },
        {
          "href": "https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.ipynb",
          "text": "https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.ipynb"
        },
        {
          "href": "https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.R",
          "text": "https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.R"
        },
        {
          "href": "https://lorentzen.ch/index.php/2025/05/01/model-diagnostics-statistics-vs-machine-learning/",
          "text": "R – Michael's and Christian's Blog"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Model Diagnostics: Statistics vs Machine Learning | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-2.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-3-612x1024.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-4-1024x682.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/christian-lorentzen/",
          "text": "Christian Lorentzen"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392201 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Model Diagnostics: Statistics vs Machine Learning</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">April 30, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/christian-lorentzen/\">Christian Lorentzen</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://lorentzen.ch/index.php/2025/05/01/model-diagnostics-statistics-vs-machine-learning/\"> R – Michael's and Christian's Blog</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>In this post, we show how different <strong>use cases</strong> require different <strong>model diagnostics</strong>. In short, we compare (statistical) <strong>inference</strong> and <strong>prediction</strong>.</p>\n<p>As an example, we use a simple linear model for the Munich rent index dataset, which was kindly provided by the authors of <a data-id=\"https://doi.org/10.1007/978-3-662-63882-8\" data-type=\"link\" href=\"https://doi.org/10.1007/978-3-662-63882-8\" rel=\"nofollow\" target=\"_blank\">Regression – Models, Methods and Applications 2nd ed. (2021)</a>. This dataset contains monthy rents in EUR (<code>rent</code>) for about 3000 apartments in Munich, Germany, from 1999. The apartments have several features such as living area (<code>area</code>) in squared meters, year of construction (<code>yearc</code>), quality of location (<code>location</code>, 0: average, 1: good, 2: top), quality of bath rooms (<code>bath</code>, 0:standard, 1: premium), quality of kitchen (<code>kitchen</code>, 0: standard, 1: premium), indicator for central heating (<code>cheating</code>).</p>\n<p>The target variable is <code>Y=\\text{rent}</code> and the goal of our model is to predict the mean rent, <code>E[Y]</code> (we omit the conditioning on X for brevity).</p>\n<p>Disclaimer: Before presenting the use cases, let me clearly state that I am not in the apartment rent business and everything here is merely for the purpose of demonstrating statistical good practice.</p>\n<h3 class=\"wp-block-heading\" id=\"0-inference\">Inference</h3>\n<p>The first use case is about inference of the effect of the features. Imagine the point of view of an investor who wants to know whether the installation of a central heating is worth it (financially). To lay the ground on which to base a decision, a statistician must have answers to:</p>\n<ul class=\"wp-block-list\">\n<li>What is the <em>effect</em> of the variable <code>cheating</code> on the rent.</li>\n<li>Is this effect statistically significant?</li>\n</ul>\n<h3 class=\"wp-block-heading\">Prediction</h3>\n<p>The second use case is about prediction. This time, we take the point of view of someone looking out for a new apartment to rent. In order to know whether the proposed rent by the landlord is about right or improper (too high), a reference value would be very convenient. One can either ask the neighbors or ask a model to predict the rent of the apartment in question.</p>\n<h2 class=\"wp-block-heading\">Model Fit</h2>\n<p>Before answering the above questions and doing some key diagnostics, we must load the data and fit a model. We choose a simple linear model and directly model <code>rent</code>.</p>\n<p>Notes:</p>\n<ul class=\"wp-block-list\">\n<li>For rent indices as well as house prices, one often log-transforms the target variable before modelling or one uses a log-link and an appropriate loss function (e.g. Gamma deviance).</li>\n<li>Our Python version uses <code>GeneralizedLinearRegressor</code> from the package <a href=\"https://glum.readthedocs.io/\" rel=\"nofollow\" target=\"_blank\">glum</a>. We could as well have chosen other implementations like <a href=\"https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS\" rel=\"nofollow\" target=\"_blank\">statsmodels.regression.linear_model.OLS</a>. This way, we have to implement the residual diagnostics ourselves which makes it clear what is plotted.</li>\n</ul>\n<p>For brevity, we skip imports and data loading. Our model is then fit by:</p>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre>lm = glum.GeneralizedLinearRegressor(\n    alpha=0,\n    drop_first=True,  # this is very important if alpha=0\n    formula=\"bs(area, degree=3, df=4) + yearc\"\n      \t\" + C(location) + C(bath) + C(kitchen) + C(cheating)\"\n)\nlm.fit(X_train, y_train)</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-8c981e59-b644-4bb0-8cd3-86a06952cf37-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>model = lm(\n  formula = rent ~ bs(area, degree = 3, df = 4) + yearc + location + bath + kitchen + cheating,\n  data = df_train\n)</pre>\n</div></div>\n</div>\n<h2 class=\"wp-block-heading\">Diagnostics for Inference</h2>\n<p>The coefficient table will already tell us the effect of the <code>cheating</code> variable. For more involved models like gradient boosted trees or neural nets, one can use partial dependence and shap values to assess the effect of features.</p>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre>lm.coef_table(X_train, y_train)</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-f0e7ee03-1654-4365-9827-c7d35472f6bc-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>summary(model)\nconfint(model)</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-table\"><table><thead><tr><th><strong>Variable</strong></th><th><strong>coef</strong></th><th><strong>se</strong></th><th><strong>p_value</strong></th><th><strong>ci_lower</strong></th><th><strong>ci_upper</strong></th></tr></thead><tbody><tr><td>intercept</td><td>-3682.5</td><td>327.0</td><td>0.0</td><td>-4323</td><td>-3041</td></tr><tr><td>bs(area, ..)[1]</td><td>88.5</td><td>31.3</td><td>4.6e-03</td><td>27</td><td>150</td></tr><tr><td>bs(area,..)[2]</td><td>316.8</td><td>24.5</td><td>0.0</td><td>269</td><td>365</td></tr><tr><td>bs(area, ..)[3]</td><td>547.7</td><td>62.8</td><td>0.0</td><td>425</td><td>671</td></tr><tr><td>bs(area, ..)[4]</td><td>733.7</td><td>91.7</td><td>1.3e-15</td><td>554</td><td>913</td></tr><tr><td>yearc</td><td>1.9</td><td>0.2</td><td>0.0</td><td>1.6</td><td>2.3</td></tr><tr><td>C(location)[2]</td><td>48.2</td><td>5.9</td><td>4.4e-16</td><td>37</td><td>60</td></tr><tr><td>C(location)[3]</td><td>137.9</td><td>27.7</td><td>6.6e-07</td><td>84</td><td>192</td></tr><tr><td>C(bath)[1]</td><td>50.0</td><td>16.5</td><td>2.4e-03</td><td>18</td><td>82</td></tr><tr><td>C(kitchen)[1]</td><td>98.2</td><td>18.5</td><td>1.1e-07</td><td>62</td><td>134</td></tr><tr><td>C(cheating)[1]</td><td>107.8</td><td>10.6</td><td>0.0</td><td>87.0</td><td>128.6</td></tr></tbody></table></figure>\n<p>We see that <em>ceteris paribus</em>, meaning all else equal, a central heating increases the monthly rent by about 108 EUR. Not the size of the effect of 108 EUR, but the fact that there is an effect of central heating on the rent seems statistically significant:<br/>This is indicated by the very low probability, i.e. p-value, for the null-hypothesis of <code>cheating</code> having a coefficient of zero.<br/>We also see that the confidence interval with the default confidence level of 95%: [<code>ci_lower</code>, <code>ci_upper</code>] = [87, 129].<br/>This shows the uncertainty of the estimated effect.</p>\n<p>For a building with 10 apartments and with an investment horizon of about 10 years, the estimated effect gives roughly a budget of 13000 EUR (range is roughly 10500 to 15500 with 95% confidence).</p>\n<p>A good statistician should ask several further questions:</p>\n<ul class=\"wp-block-list\">\n<li>Is the dataset at hand a good representation of the population?</li>\n<li>Are there confounders or interaction effects, in particular between <code>cheating</code> and other features?</li>\n<li>Are the assumptions for the low p-value and the confidence interval of <code>cheating</code> valid?</li>\n</ul>\n<p>Here, we will only address the last question, and even that one only partially. Which assumptions were made? The error term, <code>\\epsilon = Y - E[Y]</code>, should be homoscedastic and Normal distributed. As the error is not observable (because the <em>true model</em> for <code>E[Y]</code> is unknown), one replaces <code>E[Y]</code> by the model prediction <code>\\hat{E}[Y]</code>, this gives the residuals, <code>\\hat{\\epsilon} = Y - \\hat{E}[Y] = y - \\text{fitted values}</code>, instead. For homoscedasticity, the residuals should look like white (random) noise. Normality, on the other hand, becomes less of a concern with larger data thanks to the central limit theorem. With about 3000 data points, we are far away from <em>small data</em>, but it might still be a good idea to check for normality.</p>\n<p>The diagnostic tools to check that are residual and quantile-quatile (QQ) plots.</p>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre># See notebook for a definition of residual_plot.\nimport seaborn as sns\nfig, axes = plt.subplots(ncols=2, figsize=(4.8 * 2.1, 6.4))\nax = residual_plot(model=lm, X=X_train, y=y_train, ax=axes[0])\nsns.kdeplot(\n    x=lm.predict(X_train),\n    y=residuals(lm, X_train, y_train, kind=\"studentized\"),\n    thresh=.02,\n    fill=True,\n    ax=axes[1],\n).set(\n    xlabel=\"fitted\",\n    ylabel=\"studentized residuals\",\n    title=\"Contour Plot of Residuals\",\n)</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-63f892f6-5238-44ac-a49e-0b0ec0982b56-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>autoplot(model, which = c(1, 2))  # from library ggfortify\n# density plot of residuals\nggplot(model, aes(x = .fitted, y = .resid)) + geom_point() +\n  geom_density_2d() + geom_density_2d_filled(alpha = 0.5)</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1910\" data-lazy-sizes=\"(max-width: 850px) 100vw, 850px\" data-lazy-src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" fetchpriority=\"high\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image.png?w=450&amp;ssl=1 850w, https://lorentzen.ch/wp-content/uploads/2025/04/image-300x204.png 300w, https://lorentzen.ch/wp-content/uploads/2025/04/image-768x522.png 768w\"/><noscript><img alt=\"\" class=\"wp-image-1910\" data-recalc-dims=\"1\" decoding=\"async\" fetchpriority=\"high\" loading=\"lazy\" sizes=\"(max-width: 850px) 100vw, 850px\" src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image.png?w=450&amp;ssl=1 850w, https://lorentzen.ch/wp-content/uploads/2025/04/image-300x204.png 300w, https://lorentzen.ch/wp-content/uploads/2025/04/image-768x522.png 768w\"/></noscript><figcaption class=\"wp-element-caption\">Residual plots on the training data.</figcaption></figure>\n<p>The more data points one has the less informative is a scatter plot. Therefore, we put a contour plot on the right.</p>\n<p>Visual insights:</p>\n<ul class=\"wp-block-list\">\n<li>There seems to be a larger variability for larger fitted values. This is a hint that the homoscedasticity might be violated.</li>\n<li>The residuals seem to be centered around 0. This is a hint that the model is well calibrated (adequate).</li>\n</ul>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre># See notebook for a definition of qq_plot.\nqq_plot(lm, X_train, y_train)</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-3b3e3997-19b3-4929-9efa-27a64d777eae-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>autoplot(model, which = 2)</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1911\" data-lazy-sizes=\"(max-width: 565px) 100vw, 565px\" data-lazy-src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-1.png?w=450&amp;ssl=1 565w, https://lorentzen.ch/wp-content/uploads/2025/04/image-1-300x242.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1911\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 565px) 100vw, 565px\" src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-1.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-1.png?w=450&amp;ssl=1 565w, https://lorentzen.ch/wp-content/uploads/2025/04/image-1-300x242.png 300w\"/></noscript></figure>\n<p>The QQ-plot shows the quantiles of the theoretical assumed distribution of the residuals on the x-axis and the ordered values of the residuals on the y-axis. In the Python version, we decided to use the studentized residuals because normality of the error implies a student (t) distribution for these residuals.</p>\n<p>Concluding remarks:</p>\n<ul class=\"wp-block-list\">\n<li>We might do similar plots on the test sample, but we don’t necessarily need a test sample to answer the inference questions.</li>\n<li>It is good practice to plot the residuals vs each of the features as well.</li>\n</ul>\n<h2 class=\"wp-block-heading\">Diagnostics for Prediction</h2>\n<p>If we are only interested in predictions of the mean rent, <code>\\hat{E}[Y]</code>, we don’t care much about the probability distribution of <code>Y</code>. We just want to know if the predictions are close enough to the real mean of the rent <code>E[Y]</code>. In a similar argument as for the error term and residuals, we have to accept that <code>E[Y]</code> is not observable (it is the quantity that we want to predict). So we have to fall back to the observations of <code>Y</code> in order to judge if our model is well calibrated, i.e., close the the ideal <code>E[Y]</code>.</p>\n<p>Very importantly, here we make use of the test sample in all of our diagnostics because <strong>we fear the in-sample bias</strong>.</p>\n<p>We start simple by a look at the unconditional calibration, that is the average (negative) residual <code>\\frac{1}{n}\\sum(\\hat{E}[Y_i]-Y_i)</code>.</p>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre>compute_bias(\n    y_obs=np.concatenate([y_train, y_test]),\n    y_pred=lm.predict(pd.concat([X_train, X_test])),\n    feature=np.array([\"train\"] * X_train.shape[0] + [\"test\"] * X_test.shape[0]),\n)</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-2bb67cbf-cc26-4591-9af0-9c9cdce957c5-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>print(paste(\"Train set mean residual:\", mean(resid(model))))\nprint(paste(\"Test set mean residual: \", mean(df_test$rent - predict(model, df_test))))</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><thead><tr><th>set</th><th>mean bias</th><th>count</th><th>stderr</th><th>p-value</th></tr></thead><tbody><tr><td>train</td><td>-3.2e-12</td><td>2465</td><td>2.8</td><td>1.0</td></tr><tr><td>test</td><td>2.1</td><td>617</td><td>5.8</td><td>0.72</td></tr></tbody></table></figure>\n<p>It is no surprise that <code>bias_mean</code> in the train set is almost zero.<br/>This is the <em>balance property</em> of (generalized) linear models (with intercept term). On the test set, however, we detect a small bias of about 2 EUR per apartment on average.</p>\n<p>Next, we have a look a reliability diagrams which contain much more information about calibration and bias of a model than the unconditional calibration above. In fact, it assesses auto-calibration, i.e. how well the model uses its own information.<br/>An ideal model would lie on the dotted diagonal line.</p>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre>fig, axes = plt.subplots(ncols=2, figsize=(4.8 * 2.1, 6.4))\nplot_reliability_diagram(y_obs=y_train, y_pred=lm.predict(X_train), n_bootstrap=100, ax=axes[0])\naxes[0].set_title(axes[0].get_title() + f\" train set (n={X_train.shape[0]})\")\nplot_reliability_diagram(y_obs=y_test, y_pred=lm.predict(X_test), n_bootstrap=100, ax=axes[1])\naxes[1].set_title(axes[1].get_title() + f\" test set (n={X_test.shape[0]})\")</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-fe967c4a-18bc-4376-9063-0e33a0aed6bb-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>iso_train = isoreg(x = model$fitted.values, y = df_train$rent)\niso_test = isoreg(x = predict(model, df_test), y = df_test$rent)\nbind_rows(\n  tibble(set = \"train\", x = iso_train$x[iso_train$ord], y = iso_train$yf),\n  tibble(set = \"test\", x = iso_test$x[iso_test$ord], y = iso_test$yf),\n) |&gt;\n  ggplot(aes(x=x, y=y, color=set)) + geom_line() +\n  geom_abline(intercept = 0, slope = 1, linetype=\"dashed\") +\n  ggtitle(\"Reliability Diagram\")</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1915\" data-lazy-sizes=\"(max-width: 866px) 100vw, 866px\" data-lazy-src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-2.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-2.png?w=450&amp;ssl=1 866w, https://lorentzen.ch/wp-content/uploads/2025/04/image-2-300x200.png 300w, https://lorentzen.ch/wp-content/uploads/2025/04/image-2-768x513.png 768w\"/><noscript><img alt=\"\" class=\"wp-image-1915\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 866px) 100vw, 866px\" src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-2.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-2.png?w=450&amp;ssl=1 866w, https://lorentzen.ch/wp-content/uploads/2025/04/image-2-300x200.png 300w, https://lorentzen.ch/wp-content/uploads/2025/04/image-2-768x513.png 768w\"/></noscript></figure>\n<p>Visual insights:</p>\n<ul class=\"wp-block-list\">\n<li>Graphs on train and test set look very similar.<br/>The larger uncertainty intervals on the test set stem from the fact that is has a smaller sample size.</li>\n<li>The model seems to lie around the diagonal indicating good auto-calibration for the largest part of the range.</li>\n<li>Very high predicted values seem to be systematically too low, i.e. the graph is above the diagonal.</li>\n</ul>\n<p>Finally, we assess conditional calibration, i.e. the calibration with respect to the features. Therefore, we plot one of our favorite graphs for each feature. It consists of:</p>\n<ul class=\"wp-block-list\">\n<li>average observed value of <code>Y</code> for each (binned) value of the feature</li>\n<li>average predicted value</li>\n<li>partial dependence</li>\n<li>histogram of the feature (grey, right y-axis)</li>\n</ul>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre>fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 5*4), sharey=True)\nfor i, col in enumerate([\"area\", \"yearc\", \"bath\", \"kitchen\", \"cheating\"]):\n    plot_marginal(\n        y_obs=y_train,\n        y_pred=lm.predict(X_train),\n        X=X_train,\n        feature_name=col,\n        predict_function=lm.predict,\n        ax=axes[i][0],\n    )\n    plot_marginal(\n        y_obs=y_test,\n        y_pred=lm.predict(X_test),\n        X=X_test,\n        feature_name=col,\n        predict_function=lm.predict,\n        ax=axes[i][1],\n    )\n    axes[i][0].set_title(\"Train\")\n    axes[i][1].set_title(\"Test\")\n    if i != 0:\n        axes[i][0].get_legend().remove()\n    axes[i][1].get_legend().remove()\nfig.tight_layout()</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-1bb37af4-106c-4c46-9aef-ca5cb222373f-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>xvars = c(\"area\", \"yearc\", \"bath\", \"kitchen\", \"cheating\")\nm_train = feature_effects(model, v = xvars, data = df_train, y = df_train$rent)\nm_test = feature_effects(model, v = xvars, data = df_test, y = df_test$rent)\n\nc(m_train, m_test) |&gt; \n  plot(\n    share_y = \"rows\",\n    ncol = 2,\n    byrow = FALSE,\n    stats = c(\"y_mean\", \"pred_mean\", \"pd\"),\n    subplot_titles = FALSE,\n    # plotly = TRUE,\n    title = \"Left: Train - Right: Test\",\n  )</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-1918\" data-lazy-sizes=\"auto, (max-width: 612px) 100vw, 612px\" data-lazy-src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-3-612x1024.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-3-612x1024.png?w=450&amp;ssl=1 612w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3-179x300.png 179w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3-768x1285.png 768w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3-918x1536.png 918w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3.png 1189w\"/><noscript><img alt=\"\" class=\"wp-image-1918\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"auto, (max-width: 612px) 100vw, 612px\" src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-3-612x1024.png?w=450&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-3-612x1024.png?w=450&amp;ssl=1 612w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3-179x300.png 179w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3-768x1285.png 768w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3-918x1536.png 918w, https://lorentzen.ch/wp-content/uploads/2025/04/image-3.png 1189w\"/></noscript></figure>\n<p>Visual insights:</p>\n<ul class=\"wp-block-list\">\n<li>On the train set, the categorical features seem to have perfect calibration as average observed equals average predicted. This is again a result of the balance property. On the test set, we see a deviation, especially for the categorical level with smaller sample size. This is a good demonstration why plotting on both train and test set is a good idea.</li>\n<li>The numerical features area and year of construction seem fine, but a closer look can’t hurt.</li>\n</ul>\n<p>We next perform a bias plot, which is plotting the average difference of predicted minus observed per feature value. The values should be around zero, so we can zoom in on the y-axis.<br/>This is very similar to the residual plot, but the information is better condensed for its purpose.</p>\n<div class=\"wp-block-ub-tabbed-content wp-block-ub-tabbed-content-holder wp-block-ub-tabbed-content-horizontal-holder-mobile wp-block-ub-tabbed-content-horizontal-holder-tablet\" id=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8\" style=\"\">\n<div class=\"wp-block-ub-tabbed-content-tab-holder horizontal-tab-width-mobile horizontal-tab-width-tablet\">\n<div class=\"wp-block-ub-tabbed-content-tabs-title wp-block-ub-tabbed-content-tabs-title-mobile-horizontal-tab wp-block-ub-tabbed-content-tabs-title-tablet-horizontal-tab\" role=\"tablist\" style=\"justify-content: flex-start; \"><div aria-controls=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-panel-0\" aria-selected=\"true\" class=\"wp-block-ub-tabbed-content-tab-title-wrap active\" id=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-tab-0\" role=\"tab\" style=\"--ub-tabbed-title-background-color: #eeeeee; --ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">Python</div>\n</div><div aria-controls=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-panel-1\" aria-selected=\"false\" class=\"wp-block-ub-tabbed-content-tab-title-wrap\" id=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-tab-1\" role=\"tab\" style=\"--ub-tabbed-active-title-color: inherit; --ub-tabbed-active-title-background-color: #eeeeee; text-align: left; \" tabindex=\"-1\">\n<div class=\"wp-block-ub-tabbed-content-tab-title\">R</div>\n</div></div>\n</div>\n<div class=\"wp-block-ub-tabbed-content-tabs-content\" style=\"\"><div aria-labelledby=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-tab-0\" class=\"wp-block-ub-tabbed-content-tab-content-wrap active\" id=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-panel-0\" role=\"tabpanel\" tabindex=\"0\">\n<pre>fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 2*4), sharey=True)\naxes[0,0].set_ylim(-150, 150)\nfor i, col in enumerate([\"area\", \"yearc\"]):\n    plot_bias(\n        y_obs=y_train,\n        y_pred=lm.predict(X_train),\n        feature=X_train[col],\n        ax=axes[i][0],\n    )\n    plot_bias(\n        y_obs=y_test,\n        y_pred=lm.predict(X_test),\n        feature=X_test[col],\n        ax=axes[i][1],\n    )\n    axes[i][0].set_title(\"Train\")\n    axes[i][1].set_title(\"Test\")\nfig.tight_layout()</pre>\n</div><div aria-labelledby=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-tab-1\" class=\"wp-block-ub-tabbed-content-tab-content-wrap ub-hide\" id=\"ub-tabbed-content-b1a71625-3af8-43f0-aaa1-ae92c6fdaeb8-panel-1\" role=\"tabpanel\" tabindex=\"0\">\n<pre>c(m_train[c(\"area\", \"yearc\")], m_test[c(\"area\", \"yearc\")]) |&gt; \n  plot(\n    ylim = c(-150, 150),\n    ncol = 2,\n    byrow = FALSE,\n    stats = \"resid_mean\",\n    subplot_titles = FALSE,\n    title = \"Left: Train - Right: Test\",\n    # plotly = TRUE,\n    interval = \"ci\"\n  )</pre>\n</div></div>\n</div>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-1925\" data-lazy-sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" data-lazy-src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-4-1024x682.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-4-1024x682.png?w=450&amp;ssl=1 1024w, https://lorentzen.ch/wp-content/uploads/2025/04/image-4-300x200.png 300w, https://lorentzen.ch/wp-content/uploads/2025/04/image-4-768x511.png 768w, https://lorentzen.ch/wp-content/uploads/2025/04/image-4.png 1187w\"/><noscript><img alt=\"\" class=\"wp-image-1925\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-4-1024x682.png?w=450&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2025/04/image-4-1024x682.png?w=450&amp;ssl=1 1024w, https://lorentzen.ch/wp-content/uploads/2025/04/image-4-300x200.png 300w, https://lorentzen.ch/wp-content/uploads/2025/04/image-4-768x511.png 768w, https://lorentzen.ch/wp-content/uploads/2025/04/image-4.png 1187w\"/></noscript></figure>\n<p>Visual insights:</p>\n<ul class=\"wp-block-list\">\n<li>For large values of <code>area</code> and <code>yearc</code> in the 1940s and 1950s, there are only few observations available. Still, the model might be improved for those regions.</li>\n<li>The bias of <code>yearc</code> shows a parabolic curve. The simple linear effect in our model seems too simplistic. A refined model could use splines instead, as for <code>area</code>.</li>\n</ul>\n<p>Concluding remarks:</p>\n<ul class=\"wp-block-list\">\n<li>The predictions for <code>area</code> larger than around 120 square meters and for year of construction around the 2nd world war are less reliable.</li>\n<li>For all the rest, the bias is smaller than 50 EUR on average.<br/>This is therefore a rough estimation of the prediction uncertainty.<br/>It should be enough to prevent improperly high (or low) rents (on average).</li>\n</ul>\n<p>The full Python and R code is available under:</p>\n<ul class=\"wp-block-list\">\n<li>Python: <a href=\"https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.ipynb\" rel=\"nofollow\" target=\"_blank\">https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.ipynb</a></li>\n<li>R: <a href=\"https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.R\" rel=\"nofollow\" target=\"_blank\">https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.R</a></li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://lorentzen.ch/index.php/2025/05/01/model-diagnostics-statistics-vs-machine-learning/\"> R – Michael's and Christian's Blog</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Model Diagnostics: Statistics vs Machine Learning\nPosted on\nApril 30, 2025\nby\nChristian Lorentzen\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR – Michael's and Christian's Blog\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIn this post, we show how different\nuse cases\nrequire different\nmodel diagnostics\n. In short, we compare (statistical)\ninference\nand\nprediction\n.\nAs an example, we use a simple linear model for the Munich rent index dataset, which was kindly provided by the authors of\nRegression – Models, Methods and Applications 2nd ed. (2021)\n. This dataset contains monthy rents in EUR (\nrent\n) for about 3000 apartments in Munich, Germany, from 1999. The apartments have several features such as living area (\narea\n) in squared meters, year of construction (\nyearc\n), quality of location (\nlocation\n, 0: average, 1: good, 2: top), quality of bath rooms (\nbath\n, 0:standard, 1: premium), quality of kitchen (\nkitchen\n, 0: standard, 1: premium), indicator for central heating (\ncheating\n).\nThe target variable is\nY=\\text{rent}\nand the goal of our model is to predict the mean rent,\nE[Y]\n(we omit the conditioning on X for brevity).\nDisclaimer: Before presenting the use cases, let me clearly state that I am not in the apartment rent business and everything here is merely for the purpose of demonstrating statistical good practice.\nInference\nThe first use case is about inference of the effect of the features. Imagine the point of view of an investor who wants to know whether the installation of a central heating is worth it (financially). To lay the ground on which to base a decision, a statistician must have answers to:\nWhat is the\neffect\nof the variable\ncheating\non the rent.\nIs this effect statistically significant?\nPrediction\nThe second use case is about prediction. This time, we take the point of view of someone looking out for a new apartment to rent. In order to know whether the proposed rent by the landlord is about right or improper (too high), a reference value would be very convenient. One can either ask the neighbors or ask a model to predict the rent of the apartment in question.\nModel Fit\nBefore answering the above questions and doing some key diagnostics, we must load the data and fit a model. We choose a simple linear model and directly model\nrent\n.\nNotes:\nFor rent indices as well as house prices, one often log-transforms the target variable before modelling or one uses a log-link and an appropriate loss function (e.g. Gamma deviance).\nOur Python version uses\nGeneralizedLinearRegressor\nfrom the package\nglum\n. We could as well have chosen other implementations like\nstatsmodels.regression.linear_model.OLS\n. This way, we have to implement the residual diagnostics ourselves which makes it clear what is plotted.\nFor brevity, we skip imports and data loading. Our model is then fit by:\nPython\nR\nlm = glum.GeneralizedLinearRegressor(\n    alpha=0,\n    drop_first=True,  # this is very important if alpha=0\n    formula=\"bs(area, degree=3, df=4) + yearc\"\n      \t\" + C(location) + C(bath) + C(kitchen) + C(cheating)\"\n)\nlm.fit(X_train, y_train)\nmodel = lm(\n  formula = rent ~ bs(area, degree = 3, df = 4) + yearc + location + bath + kitchen + cheating,\n  data = df_train\n)\nDiagnostics for Inference\nThe coefficient table will already tell us the effect of the\ncheating\nvariable. For more involved models like gradient boosted trees or neural nets, one can use partial dependence and shap values to assess the effect of features.\nPython\nR\nlm.coef_table(X_train, y_train)\nsummary(model)\nconfint(model)\nVariable\ncoef\nse\np_value\nci_lower\nci_upper\nintercept\n-3682.5\n327.0\n0.0\n-4323\n-3041\nbs(area, ..)[1]\n88.5\n31.3\n4.6e-03\n27\n150\nbs(area,..)[2]\n316.8\n24.5\n0.0\n269\n365\nbs(area, ..)[3]\n547.7\n62.8\n0.0\n425\n671\nbs(area, ..)[4]\n733.7\n91.7\n1.3e-15\n554\n913\nyearc\n1.9\n0.2\n0.0\n1.6\n2.3\nC(location)[2]\n48.2\n5.9\n4.4e-16\n37\n60\nC(location)[3]\n137.9\n27.7\n6.6e-07\n84\n192\nC(bath)[1]\n50.0\n16.5\n2.4e-03\n18\n82\nC(kitchen)[1]\n98.2\n18.5\n1.1e-07\n62\n134\nC(cheating)[1]\n107.8\n10.6\n0.0\n87.0\n128.6\nWe see that\nceteris paribus\n, meaning all else equal, a central heating increases the monthly rent by about 108 EUR. Not the size of the effect of 108 EUR, but the fact that there is an effect of central heating on the rent seems statistically significant:\nThis is indicated by the very low probability, i.e. p-value, for the null-hypothesis of\ncheating\nhaving a coefficient of zero.\nWe also see that the confidence interval with the default confidence level of 95%: [\nci_lower\n,\nci_upper\n] = [87, 129].\nThis shows the uncertainty of the estimated effect.\nFor a building with 10 apartments and with an investment horizon of about 10 years, the estimated effect gives roughly a budget of 13000 EUR (range is roughly 10500 to 15500 with 95% confidence).\nA good statistician should ask several further questions:\nIs the dataset at hand a good representation of the population?\nAre there confounders or interaction effects, in particular between\ncheating\nand other features?\nAre the assumptions for the low p-value and the confidence interval of\ncheating\nvalid?\nHere, we will only address the last question, and even that one only partially. Which assumptions were made? The error term,\n\\epsilon = Y - E[Y]\n, should be homoscedastic and Normal distributed. As the error is not observable (because the\ntrue model\nfor\nE[Y]\nis unknown), one replaces\nE[Y]\nby the model prediction\n\\hat{E}[Y]\n, this gives the residuals,\n\\hat{\\epsilon} = Y - \\hat{E}[Y] = y - \\text{fitted values}\n, instead. For homoscedasticity, the residuals should look like white (random) noise. Normality, on the other hand, becomes less of a concern with larger data thanks to the central limit theorem. With about 3000 data points, we are far away from\nsmall data\n, but it might still be a good idea to check for normality.\nThe diagnostic tools to check that are residual and quantile-quatile (QQ) plots.\nPython\nR\n# See notebook for a definition of residual_plot.\nimport seaborn as sns\nfig, axes = plt.subplots(ncols=2, figsize=(4.8 * 2.1, 6.4))\nax = residual_plot(model=lm, X=X_train, y=y_train, ax=axes[0])\nsns.kdeplot(\n    x=lm.predict(X_train),\n    y=residuals(lm, X_train, y_train, kind=\"studentized\"),\n    thresh=.02,\n    fill=True,\n    ax=axes[1],\n).set(\n    xlabel=\"fitted\",\n    ylabel=\"studentized residuals\",\n    title=\"Contour Plot of Residuals\",\n)\nautoplot(model, which = c(1, 2))  # from library ggfortify\n# density plot of residuals\nggplot(model, aes(x = .fitted, y = .resid)) + geom_point() +\n  geom_density_2d() + geom_density_2d_filled(alpha = 0.5)\nResidual plots on the training data.\nThe more data points one has the less informative is a scatter plot. Therefore, we put a contour plot on the right.\nVisual insights:\nThere seems to be a larger variability for larger fitted values. This is a hint that the homoscedasticity might be violated.\nThe residuals seem to be centered around 0. This is a hint that the model is well calibrated (adequate).\nPython\nR\n# See notebook for a definition of qq_plot.\nqq_plot(lm, X_train, y_train)\nautoplot(model, which = 2)\nThe QQ-plot shows the quantiles of the theoretical assumed distribution of the residuals on the x-axis and the ordered values of the residuals on the y-axis. In the Python version, we decided to use the studentized residuals because normality of the error implies a student (t) distribution for these residuals.\nConcluding remarks:\nWe might do similar plots on the test sample, but we don’t necessarily need a test sample to answer the inference questions.\nIt is good practice to plot the residuals vs each of the features as well.\nDiagnostics for Prediction\nIf we are only interested in predictions of the mean rent,\n\\hat{E}[Y]\n, we don’t care much about the probability distribution of\nY\n. We just want to know if the predictions are close enough to the real mean of the rent\nE[Y]\n. In a similar argument as for the error term and residuals, we have to accept that\nE[Y]\nis not observable (it is the quantity that we want to predict). So we have to fall back to the observations of\nY\nin order to judge if our model is well calibrated, i.e., close the the ideal\nE[Y]\n.\nVery importantly, here we make use of the test sample in all of our diagnostics because\nwe fear the in-sample bias\n.\nWe start simple by a look at the unconditional calibration, that is the average (negative) residual\n\\frac{1}{n}\\sum(\\hat{E}[Y_i]-Y_i)\n.\nPython\nR\ncompute_bias(\n    y_obs=np.concatenate([y_train, y_test]),\n    y_pred=lm.predict(pd.concat([X_train, X_test])),\n    feature=np.array([\"train\"] * X_train.shape[0] + [\"test\"] * X_test.shape[0]),\n)\nprint(paste(\"Train set mean residual:\", mean(resid(model))))\nprint(paste(\"Test set mean residual: \", mean(df_test$rent - predict(model, df_test))))\nset\nmean bias\ncount\nstderr\np-value\ntrain\n-3.2e-12\n2465\n2.8\n1.0\ntest\n2.1\n617\n5.8\n0.72\nIt is no surprise that\nbias_mean\nin the train set is almost zero.\nThis is the\nbalance property\nof (generalized) linear models (with intercept term). On the test set, however, we detect a small bias of about 2 EUR per apartment on average.\nNext, we have a look a reliability diagrams which contain much more information about calibration and bias of a model than the unconditional calibration above. In fact, it assesses auto-calibration, i.e. how well the model uses its own information.\nAn ideal model would lie on the dotted diagonal line.\nPython\nR\nfig, axes = plt.subplots(ncols=2, figsize=(4.8 * 2.1, 6.4))\nplot_reliability_diagram(y_obs=y_train, y_pred=lm.predict(X_train), n_bootstrap=100, ax=axes[0])\naxes[0].set_title(axes[0].get_title() + f\" train set (n={X_train.shape[0]})\")\nplot_reliability_diagram(y_obs=y_test, y_pred=lm.predict(X_test), n_bootstrap=100, ax=axes[1])\naxes[1].set_title(axes[1].get_title() + f\" test set (n={X_test.shape[0]})\")\niso_train = isoreg(x = model$fitted.values, y = df_train$rent)\niso_test = isoreg(x = predict(model, df_test), y = df_test$rent)\nbind_rows(\n  tibble(set = \"train\", x = iso_train$x[iso_train$ord], y = iso_train$yf),\n  tibble(set = \"test\", x = iso_test$x[iso_test$ord], y = iso_test$yf),\n) |>\n  ggplot(aes(x=x, y=y, color=set)) + geom_line() +\n  geom_abline(intercept = 0, slope = 1, linetype=\"dashed\") +\n  ggtitle(\"Reliability Diagram\")\nVisual insights:\nGraphs on train and test set look very similar.\nThe larger uncertainty intervals on the test set stem from the fact that is has a smaller sample size.\nThe model seems to lie around the diagonal indicating good auto-calibration for the largest part of the range.\nVery high predicted values seem to be systematically too low, i.e. the graph is above the diagonal.\nFinally, we assess conditional calibration, i.e. the calibration with respect to the features. Therefore, we plot one of our favorite graphs for each feature. It consists of:\naverage observed value of\nY\nfor each (binned) value of the feature\naverage predicted value\npartial dependence\nhistogram of the feature (grey, right y-axis)\nPython\nR\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 5*4), sharey=True)\nfor i, col in enumerate([\"area\", \"yearc\", \"bath\", \"kitchen\", \"cheating\"]):\n    plot_marginal(\n        y_obs=y_train,\n        y_pred=lm.predict(X_train),\n        X=X_train,\n        feature_name=col,\n        predict_function=lm.predict,\n        ax=axes[i][0],\n    )\n    plot_marginal(\n        y_obs=y_test,\n        y_pred=lm.predict(X_test),\n        X=X_test,\n        feature_name=col,\n        predict_function=lm.predict,\n        ax=axes[i][1],\n    )\n    axes[i][0].set_title(\"Train\")\n    axes[i][1].set_title(\"Test\")\n    if i != 0:\n        axes[i][0].get_legend().remove()\n    axes[i][1].get_legend().remove()\nfig.tight_layout()\nxvars = c(\"area\", \"yearc\", \"bath\", \"kitchen\", \"cheating\")\nm_train = feature_effects(model, v = xvars, data = df_train, y = df_train$rent)\nm_test = feature_effects(model, v = xvars, data = df_test, y = df_test$rent)\n\nc(m_train, m_test) |> \n  plot(\n    share_y = \"rows\",\n    ncol = 2,\n    byrow = FALSE,\n    stats = c(\"y_mean\", \"pred_mean\", \"pd\"),\n    subplot_titles = FALSE,\n    # plotly = TRUE,\n    title = \"Left: Train - Right: Test\",\n  )\nVisual insights:\nOn the train set, the categorical features seem to have perfect calibration as average observed equals average predicted. This is again a result of the balance property. On the test set, we see a deviation, especially for the categorical level with smaller sample size. This is a good demonstration why plotting on both train and test set is a good idea.\nThe numerical features area and year of construction seem fine, but a closer look can’t hurt.\nWe next perform a bias plot, which is plotting the average difference of predicted minus observed per feature value. The values should be around zero, so we can zoom in on the y-axis.\nThis is very similar to the residual plot, but the information is better condensed for its purpose.\nPython\nR\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 2*4), sharey=True)\naxes[0,0].set_ylim(-150, 150)\nfor i, col in enumerate([\"area\", \"yearc\"]):\n    plot_bias(\n        y_obs=y_train,\n        y_pred=lm.predict(X_train),\n        feature=X_train[col],\n        ax=axes[i][0],\n    )\n    plot_bias(\n        y_obs=y_test,\n        y_pred=lm.predict(X_test),\n        feature=X_test[col],\n        ax=axes[i][1],\n    )\n    axes[i][0].set_title(\"Train\")\n    axes[i][1].set_title(\"Test\")\nfig.tight_layout()\nc(m_train[c(\"area\", \"yearc\")], m_test[c(\"area\", \"yearc\")]) |> \n  plot(\n    ylim = c(-150, 150),\n    ncol = 2,\n    byrow = FALSE,\n    stats = \"resid_mean\",\n    subplot_titles = FALSE,\n    title = \"Left: Train - Right: Test\",\n    # plotly = TRUE,\n    interval = \"ci\"\n  )\nVisual insights:\nFor large values of\narea\nand\nyearc\nin the 1940s and 1950s, there are only few observations available. Still, the model might be improved for those regions.\nThe bias of\nyearc\nshows a parabolic curve. The simple linear effect in our model seems too simplistic. A refined model could use splines instead, as for\narea\n.\nConcluding remarks:\nThe predictions for\narea\nlarger than around 120 square meters and for year of construction around the 2nd world war are less reliable.\nFor all the rest, the bias is smaller than 50 EUR on average.\nThis is therefore a rough estimation of the prediction uncertainty.\nIt should be enough to prevent improperly high (or low) rents (on average).\nThe full Python and R code is available under:\nPython:\nhttps://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.ipynb\nR:\nhttps://github.com/lorentzenchr/notebooks/blob/master/blogposts/2025-05-01%20diagnostics.R\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR – Michael's and Christian's Blog\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "In this post, we show how different use cases require different model diagnostics. In short, we compare (statistical) inference and prediction. As an example, we use a simple linear model for the Munich rent index dataset, which was kindly provided by the authors of Regression – Models, Methods and Applications 2nd ed. (2021). This dataset […]",
      "meta_keywords": null,
      "og_description": "In this post, we show how different use cases require different model diagnostics. In short, we compare (statistical) inference and prediction. As an example, we use a simple linear model for the Munich rent index dataset, which was kindly provided by the authors of Regression – Models, Methods and Applications 2nd ed. (2021). This dataset […]",
      "og_image": "https://lorentzen.ch/wp-content/uploads/2025/04/image.png",
      "og_title": "Model Diagnostics: Statistics vs Machine Learning | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 12.5,
      "sitemap_lastmod": null,
      "twitter_description": "In this post, we show how different use cases require different model diagnostics. In short, we compare (statistical) inference and prediction. As an example, we use a simple linear model for the Munich rent index dataset, which was kindly provided by the authors of Regression – Models, Methods and Applications 2nd ed. (2021). This dataset […]",
      "twitter_title": "Model Diagnostics: Statistics vs Machine Learning | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/04/model-diagnostics-statistics-vs-machine-learning/",
      "word_count": 2497
    }
  }
}