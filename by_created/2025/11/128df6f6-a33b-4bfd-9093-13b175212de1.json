{
  "uuid": "128df6f6-a33b-4bfd-9093-13b175212de1",
  "created_at": "2025-11-22 19:59:18",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/02/a-first-look-at-timegpt-using-nixtlar-2/",
    "crawled_at": "2025-11-22T10:54:30.848951",
    "external_links": [
      {
        "href": "https://rworks.dev/posts/revised_TimeGPT/",
        "text": "R Works"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://docs.nixtla.io/",
        "text": "Nixtla’s"
      },
      {
        "href": "https://arxiv.org/abs/2111.04052",
        "text": "Garza et al. (2021)"
      },
      {
        "href": "https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/timegpt.png?ssl=1",
        "text": null
      },
      {
        "href": "https://cran.r-project.org/package=nixtlar",
        "text": "nixtlar"
      },
      {
        "href": "https://cran.r-project.org/web/packages/nixtlar/vignettes/setting-up-your-api-key.html",
        "text": "vignette"
      },
      {
        "href": "https://www.pjm.com/",
        "text": "PJM Interconnection LLC"
      },
      {
        "href": "https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-4-1.png?ssl=1",
        "text": null
      },
      {
        "href": "https://i2.wp.com/rworks.dev/posts/revised_TimeGPT/fcst.png?ssl=1",
        "text": null
      },
      {
        "href": "https://i0.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-6-1.png?ssl=1",
        "text": null
      },
      {
        "href": "https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-15-1.png?ssl=1",
        "text": null
      },
      {
        "href": "https://nixtlaverse.nixtla.io/mlforecast/docs/tutorials/electricity_load_forecasting.html",
        "text": "Electricity Load Forecast Tutorial"
      },
      {
        "href": "https://www.youtube.com/watch?v=3NM-nJxm-qY",
        "text": "nyhackr"
      },
      {
        "href": "https://rworks.dev/posts/revised_TimeGPT/",
        "text": "R Works"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "A First Look at TimeGPT using nixtlar | R-bloggers",
    "images": [
      {
        "alt": "TimeGPT architecture",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "TimeGPT architecture",
        "base64": null,
        "src": "https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/timegpt.png?w=578&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-4-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/rworks.dev/posts/revised_TimeGPT/fcst.png?w=578&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-6-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-15-1.png?w=450&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/joseph-rickert/",
        "text": "Joseph Rickert"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-390723 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">A First Look at TimeGPT using nixtlar</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 18, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/joseph-rickert/\">Joseph Rickert</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://rworks.dev/posts/revised_TimeGPT/\"> R Works</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>This post is a first look at <a href=\"https://docs.nixtla.io/\" rel=\"nofollow\" target=\"_blank\">Nixtla’s</a> <code>TimeGPT</code> generative, pre-trained transformer for time series forecasting using the <code>nixtlar</code> R package.</p>\n<p>As described in <a href=\"https://arxiv.org/abs/2111.04052\" rel=\"nofollow\" target=\"_blank\">Garza et al. (2021)</a>, TimeGPT is a Transformer-based time series model with self-attention mechanisms. The architecture comprises an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. The encoder, a stack of multi-head self-attention layers followed by a feed-forward neural network, processes the input time series. The decoder, which is similar to the encoder, generates the forecast. The decoder includes an additional multi-head attention layer that takes the encoder’s output as input. The model is trained using a teacher-forcing strategy, where the decoder receives the ground-truth values during training. The model is then used for forecasting by feeding the model’s predictions back as input during inference.</p>\n<p><a class=\"lightbox\" data-gallery=\"quarto-lightbox-gallery-1\" href=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/timegpt.png?ssl=1\" rel=\"nofollow\" target=\"_blank\"><img alt=\"TimeGPT architecture\" class=\"img-fluid\" data-lazy-src=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/timegpt.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"TimeGPT architecture\" class=\"img-fluid\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/timegpt.png?w=578&amp;ssl=1\"/></noscript></a></p>\n<p>Nixtla’s website provides a considerable amount of explanatory material, documentation, and code examples in Python. The <a href=\"https://cran.r-project.org/package=nixtlar\" rel=\"nofollow\" target=\"_blank\"><code>nixtlar</code></a> package wraps the Python code to provide an R interface. The package documentation for version 0.6.2 doesn’t fully the R functions, but the vignettes provide sufficient code examples to get started.</p>\n<p><em>Before getting started with TimeGPT, you will have to register for an API key. The process is easy enough and is described in this <a href=\"https://cran.r-project.org/web/packages/nixtlar/vignettes/setting-up-your-api-key.html\" rel=\"nofollow\" target=\"_blank\">vignette</a>.</em></p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>library(tidyverse)\nlibrary(forecast)\nlibrary(prophet)\nlibrary(nixtlar)</pre>\n</details>\n</div>\n<section class=\"level2\" id=\"the-data\">\n<h2 class=\"anchored\" data-anchor-id=\"the-data\">The Data</h2>\n<p>The electricity dataset included in the <code>nixtlar</code> package contains hourly observations of electricity consumption generated sourced from the <a href=\"https://www.pjm.com/\" rel=\"nofollow\" target=\"_blank\">PJM Interconnection LLC</a>, a regional transmission organization that is part of the Eastern Interconnection grid in the United States. There are five different time series with data taken from 2012 to 2018.</p>\n<p>Note that the electricity data is in “long” format and that <code>ds</code> the time variable is character data.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>df &lt;- nixtlar::electricity\nglimpse(df)</pre>\n</details>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Rows: 8,400\nColumns: 3\n$ unique_id &lt;chr&gt; \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", …\n$ ds        &lt;chr&gt; \"2016-10-22 00:00:00\", \"2016-10-22 01:00:00\", \"2016-10-22 02…\n$ y         &lt;dbl&gt; 70.00, 37.10, 37.10, 44.75, 37.10, 35.61, 34.55, 50.49, 61.5…</pre>\n</div>\n</div>\n<p>A look at the “wide” data frame shows that the various series do not cover the same time periods.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>df_wide &lt;- df |&gt;\n  pivot_wider(names_from = unique_id, values_from = y)\n\nhead(df_wide)</pre>\n</details>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 6 × 6\n  ds                     BE    DE    FR    NP   PJM\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2016-10-22 00:00:00  70      NA  54.7    NA    NA\n2 2016-10-22 01:00:00  37.1    NA  51.2    NA    NA\n3 2016-10-22 02:00:00  37.1    NA  48.9    NA    NA\n4 2016-10-22 03:00:00  44.8    NA  45.9    NA    NA\n5 2016-10-22 04:00:00  37.1    NA  41.2    NA    NA\n6 2016-10-22 05:00:00  35.6    NA  41.4    NA    NA</pre>\n</div>\n</div>\n<p>Plots indicate that all of the series show periods of considerable volatility. The BE, DE, and FR series appear to be stationary. NP trends upward to the right, and the PJM series appears to be nonlinear.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>df2 &lt;- df |&gt; mutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |&gt;\n  group_by(unique_id)\n\np &lt;- df2 |&gt; ggplot(aes(x = time, y = y, color = unique_id)) +\n  geom_line() + facet_wrap( ~ unique_id, scales = \"free\")\n\np</pre>\n</details>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><a class=\"lightbox\" data-gallery=\"quarto-lightbox-gallery-2\" href=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-4-1.png?ssl=1\" rel=\"nofollow\" target=\"_blank\"><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-4-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-4-1.png?w=450&amp;ssl=1\"/></noscript></a></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"timegpt-forecsts\">\n<h2 class=\"anchored\" data-anchor-id=\"timegpt-forecsts\">TimeGPT Forecsts</h2>\n<p>I’ll begin by showing off the <code>nixtlar</code> forecasting function, which can handle multiple time series by forecasting eight hours ahead using all of the data. The parameter <code>h</code> specifies the number of steps ahead to forecast, and <code>level</code> specifies the confidence level for the forecast.</p>\n<p>Here is the built-in <code>nixtlar</code> plot function.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>#nixtla_client_plot(df, nixtla_client_fcst, max_insample_length = 200)</pre>\n</details>\n</div>\n<p><a class=\"lightbox\" data-gallery=\"quarto-lightbox-gallery-3\" href=\"https://i2.wp.com/rworks.dev/posts/revised_TimeGPT/fcst.png?ssl=1\" rel=\"nofollow\" target=\"_blank\"><img alt=\"\" class=\"img-fluid\" data-lazy-src=\"https://i2.wp.com/rworks.dev/posts/revised_TimeGPT/fcst.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" class=\"img-fluid\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/rworks.dev/posts/revised_TimeGPT/fcst.png?w=578&amp;ssl=1\"/></noscript></a></p>\n<p>This plot uses <code>ggplot2</code>to focus in on the forecasts of 8 points using all of the data. The level parameter indicates that both 80% and 96% confidence intervals should be computed.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre># nixtla_client_fcst &lt;- nixtla_client_forecast(df, h = 8, level = c(80,95))\n# saveRDS(nixtla_client_fcst, \"nixtla_client_fcst.rds\")\n\nnixtla_client_fcst &lt;- readRDS(\"nixtla_client_fcst.rds\")\n\nncf_df &lt;-  nixtla_client_fcst |&gt; mutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |&gt; group_by(unique_id)\n\nnames(ncf_df) &lt;- c(\"unique_id\", \"ds\", \"TimeGPT\", \"lon\", \"loe\", \"hie\", \"hin\")\n\npf &lt;- ncf_df |&gt; ggplot(aes(x = ds, y = TimeGPT, color = unique_id)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lon, ymax = hin),\n              linetype = 2,\n              alpha = 0.1) +\n  facet_wrap( ~ unique_id, scales = \"free\")\n\npf</pre>\n</details>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><a class=\"lightbox\" data-gallery=\"quarto-lightbox-gallery-4\" href=\"https://i0.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-6-1.png?ssl=1\" rel=\"nofollow\" target=\"_blank\"><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\"/></noscript></a></p>\n</figure>\n</div>\n</div>\n</div>\n<p>For the rest of this post, I’ll work only with the BE data and do some simple back testing. I will split the data into a training set and a test set containing 24 hours worth of observations. Then, I’ll fit established time series forecasting models and compare how well they do vis-a-vis the actual data and with each other. Note, I will not attempt any tuning of these models in an attempt to make it a fair, “out-of-the-box” comparison.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>NF &lt;- 24\n\nBE_df_wide &lt;- df |&gt; pivot_wider(names_from = unique_id, values_from = y) |&gt;\n  select(ds, BE) |&gt; drop_na()\n\nBE_train_df &lt;- BE_df_wide %&gt;% filter(row_number() &lt;= n() - NF)\nBE_test_df &lt;- tail(BE_df_wide, NF)\nBE_train_df &lt;- BE_train_df |&gt; rename(y = BE) |&gt; mutate(unique_id = \"BE\")\nBE_test_df &lt;- BE_test_df |&gt; rename(y = BE)</pre>\n</details>\n</div>\n<p>The <code>nixtla_client_forecast()</code> function is the main <code>nixtlar</code> forecasting function. (I have already run this function and saved the results RDS file in order not to make an API call every time the code is run during the blog building process.)</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>#nixtla_fcst &lt;- nixtla_client_forecast(BE_train_df, h = NF, level = 95)\n#saveRDS(nixtla_fcst, \"nixtla_fcst_24.rds\")\nnixtla_fcst &lt;- readRDS(\"nixtla_fcst_24.rds\")\nnames(nixtla_fcst) &lt;- c(\"unique_id\", \"ds\", \"TimeGPT\", \"lo95\", \"up95\")</pre>\n</details>\n</div>\n<p>Here, I create a data frame to hold the actual and forecast values.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>fcst_df &lt;- tail(nixtla_fcst, NF) |&gt; select(ds, TimeGPT) |&gt;\n  rename(time = ds, tgpt_fcst = TimeGPT) |&gt;\n  mutate(elec_actual = BE_test_df$y)\n\nhead(fcst_df)</pre>\n</details>\n<div class=\"cell-output cell-output-stdout\">\n<pre>                 time tgpt_fcst elec_actual\n1 2016-12-30 00:00:00  38.82010       44.30\n2 2016-12-30 01:00:00  36.29234       44.30\n3 2016-12-30 02:00:00  34.97838       41.26\n4 2016-12-30 03:00:00  32.99565       40.62\n5 2016-12-30 04:00:00  31.58322       40.07\n6 2016-12-30 05:00:00  33.27422       41.02</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"some-comparative-forecasts\">\n<h2 class=\"anchored\" data-anchor-id=\"some-comparative-forecasts\">Some Comparative Forecasts</h2>\n<p>This next section of code puts the training data into a time series format that is suitable for the <code>forecast::auto.arima()</code> and <code>forecast::auto.ets()</code> functions. Both of these functions require that the data be expressed as a <code>ts()</code> object. The original version of this post formatted the data as an <code>xts()</code> object: an error that substantially impacted the ARIMA and exponential smoothing forecasts. There is a problem though with using <code>ts()</code>: the electricity data has multiseasonal aspects which <code>ts()</code> was not designed to accommodate. The code below that creates the <code>ts()</code> object treats time as an index of days with a period of 24 and ignores all the rest of the seasonality information in the data set. This is clearly not a perfect solution, but it provides a workaround that is good enough for my purpose of comparing <code>TimeGPT</code> with a couple of very simple automatic forecasts. You will see below that ARIMA forecast is quite good.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>auto_train &lt;- BE_train_df |&gt; select(-unique_id) |&gt;\nmutate(time = 1:length(ds))|&gt; select(-ds)\nelec_ts &lt;- ts(auto_train$y, frequency = 24)</pre>\n</details>\n</div>\n<section class=\"level3\" id=\"arima-forecast-with-auto.arima\">\n<h3 class=\"anchored\" data-anchor-id=\"arima-forecast-with-auto.arima\">ARIMA Forecast with <code>auto.arima()</code></h3>\n<p>The <code>auto.arima()</code> function from the <code>forecast</code> package fits a fairly sophisticated ARIMA(2,1,1)(1.0.1)[24] model. The parameters in parentheses means two autoregressive terms, one difference, one moving average term, one seasonal autoregressive term, no seasonal differencing, and one seasonal moving average term. [24] indicates that the seasonal pattern repeats every 24 observations. (Note that because the ARIMA forecast takes several seconds to run, I am reading form a an rds file to expedite the blog process.)</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>#Run the following three lines to actually generate the forecast on your own.\n# arima_fcst &lt;- elec_ts |&gt;\n# auto.arima() |&gt;\n# forecast(h = NF , level = 95)\n#saveRDS(arima_fcst, \"arima_fcst_24.rds\")\narima_fcst &lt;- readRDS(\"arima_fcst_24.rds\")</pre>\n</details>\n</div>\n</section>\n<section class=\"level3\" id=\"exponential-smoothing-forecast-with-ets\">\n<h3 class=\"anchored\" data-anchor-id=\"exponential-smoothing-forecast-with-ets\">Exponential Smoothing Forecast with <code>ets()</code></h3>\n<p>Because I have provided no guidance, the <code>ets()</code> function from the <code>forecast</code> package fits an ETS(M,N,M) model. This is a multiplicative model without a trend component, where both the error and the seasonal components are multiplicative. The first M indicates that the error term (random fluctuation) is modeled as a multiplicative component where the error term’s effect on the forecasted value is proportional to the level of the time series.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>ets_fcst &lt;- elec_ts |&gt;\n  ets() |&gt;\n  # number of periods to forecast\n  forecast(h = NF)</pre>\n</details>\n</div>\n</section>\n<section class=\"level3\" id=\"prophet-forecast\">\n<h3 class=\"anchored\" data-anchor-id=\"prophet-forecast\">Prophet Forecast</h3>\n<p>I also ask the <code>prophet()</code> function from the <code>prophet</code> package for an automatic fit using the default parameters. Among other things, this means a linear growth curve with additive seasonality and automatic estimates for daily seasonality. As for the <code>TimeGPT</code> forecast, the model is fit using<code>BE_train_df</code> data frame in which the time variable is character data. The <code>make_future_dataframe()</code> function creates a data frame with the same structure as <code>BE_train_df</code> but with the <code>ds</code> column extended by <code>NF</code> periods.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>prophet_fit &lt;- prophet(BE_train_df)\n\nfuture &lt;- make_future_dataframe(\n  prophet_fit,\n  periods = NF,\n  freq = 3600,\n  include_history = FALSE\n)\n\nprophet_fcst &lt;- predict(prophet_fit, future)</pre>\n</details>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"results-and-discussion\">\n<h2 class=\"anchored\" data-anchor-id=\"results-and-discussion\">Results and Discussion</h2>\n<p>Before plotting, let’s have a look at the wide data frame that holds the forecasts.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>fcst_df2 &lt;- fcst_df |&gt;\n  mutate(\n    arima_fcst = as.vector(arima_fcst$mean),\n    ets_fcst = as.vector(ets_fcst$mean),\n    prophet_fcst = prophet_fcst$yhat\n  )\n\nhead(fcst_df2)</pre>\n</details>\n<div class=\"cell-output cell-output-stdout\">\n<pre>                 time tgpt_fcst elec_actual arima_fcst ets_fcst prophet_fcst\n1 2016-12-30 00:00:00  38.82010       44.30   46.19057 37.72396     32.05748\n2 2016-12-30 01:00:00  36.29234       44.30   44.99377 34.97583     29.74375\n3 2016-12-30 02:00:00  34.97838       41.26   44.06233 32.89185     22.66588\n4 2016-12-30 03:00:00  32.99565       40.62   42.45347 29.87591     15.85864\n5 2016-12-30 04:00:00  31.58322       40.07   43.05403 27.56602     15.62841\n6 2016-12-30 05:00:00  33.27422       41.02   44.28001 29.15387     23.54824</pre>\n</div>\n</div>\n<p>Then, shape the data into long format and plot.</p>\n<div class=\"cell preview-image\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>fcst_dft2_long &lt;- fcst_df2 %&gt;%\n  pivot_longer(!time, names_to = \"method\", values_to = \"mean\")\n\nq &lt;- fcst_dft2_long |&gt;\n  ggplot(aes(\n    x = time,\n    y = mean,\n    group = method,\n    color = method\n  )) +\n  geom_line() +\n  geom_point() +\n  ggtitle(\"TimeGPT vs ARIMA vs ETS vs Prophet vs actual data - 24 Point Forecast\")\n\nq</pre>\n</details>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><a class=\"lightbox\" data-gallery=\"quarto-lightbox-gallery-5\" href=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-15-1.png?ssl=1\" rel=\"nofollow\" target=\"_blank\"><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-15-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/rworks.dev/posts/revised_TimeGPT/index_files/figure-html/unnamed-chunk-15-1.png?w=450&amp;ssl=1\"/></noscript></a></p>\n</figure>\n</div>\n</div>\n</div>\n<p>The TimeGPT forecast looks quite good. I don’t think this is a big surprise, given that the Nixtla folks chose the electricity data set to show off their transformer. However, it is curious, that except for one point, the TimeGPT forecast is lower than the actual data. It is also interesting that some of the forecasted points that are farther out are a better match to the actual data than the initial forecast points.</p>\n<p>The ARIMA forecast is very good. It tracks the first few points very closely, undershoots the peaks of the actual data, but recovers after both peaks and tracks the data well towards the end of the forecast period.</p>\n<p>The exponential smoothing forecast mostly stays well below the actual data, but also dose pretty well overall.</p>\n<p>The black box prophet model overacts to the downward trends at the beginning and end of the forecast period. My guess is that with a little tuning prophet could do much better.</p>\n<p>It is also worth noting that choosing the <em>best</em> forecast also depends on your objectives. For example, in some circumstances, one might prefer a forecast that reproduces seasonal patterns or possible volatility rather than overall accuracy. For this exercise, we see that TimeGPT mimics the volatility of the actual data but that the ARIMA forecast has the lowest root mean squared error. Also note that the ARIMA and exponential smoothing forecasts are not quite <em>black-box</em> forecasts. In converting the data into <code>ts()</code> a time series object explicitly provided seasonality information. TimeGPT apparently inferred this information from character data.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Show the code</summary>\n<pre>RMSE &lt;-  function(m, o){sqrt(mean((m - o)^2))}\nrms_names &lt;- c(\"tgpt\", \"arima\", \"ets\", \"prophet\")\nrms_fcst &lt;- array(NA_real_,\n                          dim = 4,\n                          dimnames = list(rms_names))\nrms_fcst[1] &lt;- RMSE(fcst_df2$tgpt_fcst, fcst_df2$elec_actual)\nrms_fcst[2] &lt;- RMSE(fcst_df2$arima_fcst, fcst_df2$elec_actual)\nrms_fcst[3] &lt;- RMSE(fcst_df2$ets_fcst, fcst_df2$elec_actual)\nrms_fcst[4] &lt;- RMSE(fcst_df2$prophet_fcst, fcst_df2$elec_actual)\nrms_fcst</pre>\n</details>\n<div class=\"cell-output cell-output-stdout\">\n<pre>     tgpt     arima       ets   prophet \n 6.601752  4.966623  9.355767 14.948709 </pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"some-final-thoughts\">\n<h2 class=\"anchored\" data-anchor-id=\"some-final-thoughts\">Some Final Thoughts</h2>\n<p>It is clear that the TimeGPT model has upped the game for black-box time series forecasting. It is sure to become a powerful tool for exploratory work with large time series and for comparing multiple time series. It may become the <em>go-to</em> baseline forecasting tool for a wide range of time series projects. Moreover, I expect that time series experts who can fine-tune prophet and more traditional time series models will be able to develop some intuition about what TimeGPT is doing by assessing its behavior in relation to these models.</p>\n<p>I am aware that this little post may have raised more questions than it answered. If so, please try your hand at elaborating on some of the issues raised. We would be very happy to consider your time series posts for publication on R Works.</p>\n<p>Finally, for a more sophisticated analysis of these series that deals with their multiseasonality aspects, see the <a href=\"https://nixtlaverse.nixtla.io/mlforecast/docs/tutorials/electricity_load_forecasting.html\" rel=\"nofollow\" target=\"_blank\">Electricity Load Forecast Tutorial</a>. And, for some ideas about how to harness “ordinary” LLMs for time series forecasting have a look at the second half of the talk that Bryan Lewis gave to <a href=\"https://www.youtube.com/watch?v=3NM-nJxm-qY\" rel=\"nofollow\" target=\"_blank\">nyhackr</a> in April 2024.</p>\n</section>\n<section class=\"level2\" id=\"acknowledgment\">\n<h2 class=\"anchored\" data-anchor-id=\"acknowledgment\">Acknowledgment</h2>\n<p>Many thanks to Professor Rob Hyndman for flagging the ARIMA time series object error described above and for generously suggesting alternatives that led to the workaround described above. Any errors still remaining in this post are all mine.</p>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://rworks.dev/posts/revised_TimeGPT/\"> R Works</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "A First Look at TimeGPT using nixtlar\nPosted on\nFebruary 18, 2025\nby\nJoseph Rickert\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR Works\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nThis post is a first look at\nNixtla’s\nTimeGPT\ngenerative, pre-trained transformer for time series forecasting using the\nnixtlar\nR package.\nAs described in\nGarza et al. (2021)\n, TimeGPT is a Transformer-based time series model with self-attention mechanisms. The architecture comprises an encoder-decoder structure with multiple layers, each with residual connections and layer normalization. The encoder, a stack of multi-head self-attention layers followed by a feed-forward neural network, processes the input time series. The decoder, which is similar to the encoder, generates the forecast. The decoder includes an additional multi-head attention layer that takes the encoder’s output as input. The model is trained using a teacher-forcing strategy, where the decoder receives the ground-truth values during training. The model is then used for forecasting by feeding the model’s predictions back as input during inference.\nNixtla’s website provides a considerable amount of explanatory material, documentation, and code examples in Python. The\nnixtlar\npackage wraps the Python code to provide an R interface. The package documentation for version 0.6.2 doesn’t fully the R functions, but the vignettes provide sufficient code examples to get started.\nBefore getting started with TimeGPT, you will have to register for an API key. The process is easy enough and is described in this\nvignette\n.\nShow the code\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(prophet)\nlibrary(nixtlar)\nThe Data\nThe electricity dataset included in the\nnixtlar\npackage contains hourly observations of electricity consumption generated sourced from the\nPJM Interconnection LLC\n, a regional transmission organization that is part of the Eastern Interconnection grid in the United States. There are five different time series with data taken from 2012 to 2018.\nNote that the electricity data is in “long” format and that\nds\nthe time variable is character data.\nShow the code\ndf <- nixtlar::electricity\nglimpse(df)\nRows: 8,400\nColumns: 3\n$ unique_id <chr> \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", \"BE\", …\n$ ds        <chr> \"2016-10-22 00:00:00\", \"2016-10-22 01:00:00\", \"2016-10-22 02…\n$ y         <dbl> 70.00, 37.10, 37.10, 44.75, 37.10, 35.61, 34.55, 50.49, 61.5…\nA look at the “wide” data frame shows that the various series do not cover the same time periods.\nShow the code\ndf_wide <- df |>\n  pivot_wider(names_from = unique_id, values_from = y)\n\nhead(df_wide)\n# A tibble: 6 × 6\n  ds                     BE    DE    FR    NP   PJM\n  <chr>               <dbl> <dbl> <dbl> <dbl> <dbl>\n1 2016-10-22 00:00:00  70      NA  54.7    NA    NA\n2 2016-10-22 01:00:00  37.1    NA  51.2    NA    NA\n3 2016-10-22 02:00:00  37.1    NA  48.9    NA    NA\n4 2016-10-22 03:00:00  44.8    NA  45.9    NA    NA\n5 2016-10-22 04:00:00  37.1    NA  41.2    NA    NA\n6 2016-10-22 05:00:00  35.6    NA  41.4    NA    NA\nPlots indicate that all of the series show periods of considerable volatility. The BE, DE, and FR series appear to be stationary. NP trends upward to the right, and the PJM series appears to be nonlinear.\nShow the code\ndf2 <- df |> mutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |>\n  group_by(unique_id)\n\np <- df2 |> ggplot(aes(x = time, y = y, color = unique_id)) +\n  geom_line() + facet_wrap( ~ unique_id, scales = \"free\")\n\np\nTimeGPT Forecsts\nI’ll begin by showing off the\nnixtlar\nforecasting function, which can handle multiple time series by forecasting eight hours ahead using all of the data. The parameter\nh\nspecifies the number of steps ahead to forecast, and\nlevel\nspecifies the confidence level for the forecast.\nHere is the built-in\nnixtlar\nplot function.\nShow the code\n#nixtla_client_plot(df, nixtla_client_fcst, max_insample_length = 200)\nThis plot uses\nggplot2\nto focus in on the forecasts of 8 points using all of the data. The level parameter indicates that both 80% and 96% confidence intervals should be computed.\nShow the code\n# nixtla_client_fcst <- nixtla_client_forecast(df, h = 8, level = c(80,95))\n# saveRDS(nixtla_client_fcst, \"nixtla_client_fcst.rds\")\n\nnixtla_client_fcst <- readRDS(\"nixtla_client_fcst.rds\")\n\nncf_df <-  nixtla_client_fcst |> mutate(time = as.POSIXct(ds, format = \"%Y-%m-%d %H:%M:%S\")) |> group_by(unique_id)\n\nnames(ncf_df) <- c(\"unique_id\", \"ds\", \"TimeGPT\", \"lon\", \"loe\", \"hie\", \"hin\")\n\npf <- ncf_df |> ggplot(aes(x = ds, y = TimeGPT, color = unique_id)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lon, ymax = hin),\n              linetype = 2,\n              alpha = 0.1) +\n  facet_wrap( ~ unique_id, scales = \"free\")\n\npf\nFor the rest of this post, I’ll work only with the BE data and do some simple back testing. I will split the data into a training set and a test set containing 24 hours worth of observations. Then, I’ll fit established time series forecasting models and compare how well they do vis-a-vis the actual data and with each other. Note, I will not attempt any tuning of these models in an attempt to make it a fair, “out-of-the-box” comparison.\nShow the code\nNF <- 24\n\nBE_df_wide <- df |> pivot_wider(names_from = unique_id, values_from = y) |>\n  select(ds, BE) |> drop_na()\n\nBE_train_df <- BE_df_wide %>% filter(row_number() <= n() - NF)\nBE_test_df <- tail(BE_df_wide, NF)\nBE_train_df <- BE_train_df |> rename(y = BE) |> mutate(unique_id = \"BE\")\nBE_test_df <- BE_test_df |> rename(y = BE)\nThe\nnixtla_client_forecast()\nfunction is the main\nnixtlar\nforecasting function. (I have already run this function and saved the results RDS file in order not to make an API call every time the code is run during the blog building process.)\nShow the code\n#nixtla_fcst <- nixtla_client_forecast(BE_train_df, h = NF, level = 95)\n#saveRDS(nixtla_fcst, \"nixtla_fcst_24.rds\")\nnixtla_fcst <- readRDS(\"nixtla_fcst_24.rds\")\nnames(nixtla_fcst) <- c(\"unique_id\", \"ds\", \"TimeGPT\", \"lo95\", \"up95\")\nHere, I create a data frame to hold the actual and forecast values.\nShow the code\nfcst_df <- tail(nixtla_fcst, NF) |> select(ds, TimeGPT) |>\n  rename(time = ds, tgpt_fcst = TimeGPT) |>\n  mutate(elec_actual = BE_test_df$y)\n\nhead(fcst_df)\ntime tgpt_fcst elec_actual\n1 2016-12-30 00:00:00  38.82010       44.30\n2 2016-12-30 01:00:00  36.29234       44.30\n3 2016-12-30 02:00:00  34.97838       41.26\n4 2016-12-30 03:00:00  32.99565       40.62\n5 2016-12-30 04:00:00  31.58322       40.07\n6 2016-12-30 05:00:00  33.27422       41.02\nSome Comparative Forecasts\nThis next section of code puts the training data into a time series format that is suitable for the\nforecast::auto.arima()\nand\nforecast::auto.ets()\nfunctions. Both of these functions require that the data be expressed as a\nts()\nobject. The original version of this post formatted the data as an\nxts()\nobject: an error that substantially impacted the ARIMA and exponential smoothing forecasts. There is a problem though with using\nts()\n: the electricity data has multiseasonal aspects which\nts()\nwas not designed to accommodate. The code below that creates the\nts()\nobject treats time as an index of days with a period of 24 and ignores all the rest of the seasonality information in the data set. This is clearly not a perfect solution, but it provides a workaround that is good enough for my purpose of comparing\nTimeGPT\nwith a couple of very simple automatic forecasts. You will see below that ARIMA forecast is quite good.\nShow the code\nauto_train <- BE_train_df |> select(-unique_id) |>\nmutate(time = 1:length(ds))|> select(-ds)\nelec_ts <- ts(auto_train$y, frequency = 24)\nARIMA Forecast with\nauto.arima()\nThe\nauto.arima()\nfunction from the\nforecast\npackage fits a fairly sophisticated ARIMA(2,1,1)(1.0.1)[24] model. The parameters in parentheses means two autoregressive terms, one difference, one moving average term, one seasonal autoregressive term, no seasonal differencing, and one seasonal moving average term. [24] indicates that the seasonal pattern repeats every 24 observations. (Note that because the ARIMA forecast takes several seconds to run, I am reading form a an rds file to expedite the blog process.)\nShow the code\n#Run the following three lines to actually generate the forecast on your own.\n# arima_fcst <- elec_ts |>\n# auto.arima() |>\n# forecast(h = NF , level = 95)\n#saveRDS(arima_fcst, \"arima_fcst_24.rds\")\narima_fcst <- readRDS(\"arima_fcst_24.rds\")\nExponential Smoothing Forecast with\nets()\nBecause I have provided no guidance, the\nets()\nfunction from the\nforecast\npackage fits an ETS(M,N,M) model. This is a multiplicative model without a trend component, where both the error and the seasonal components are multiplicative. The first M indicates that the error term (random fluctuation) is modeled as a multiplicative component where the error term’s effect on the forecasted value is proportional to the level of the time series.\nShow the code\nets_fcst <- elec_ts |>\n  ets() |>\n  # number of periods to forecast\n  forecast(h = NF)\nProphet Forecast\nI also ask the\nprophet()\nfunction from the\nprophet\npackage for an automatic fit using the default parameters. Among other things, this means a linear growth curve with additive seasonality and automatic estimates for daily seasonality. As for the\nTimeGPT\nforecast, the model is fit using\nBE_train_df\ndata frame in which the time variable is character data. The\nmake_future_dataframe()\nfunction creates a data frame with the same structure as\nBE_train_df\nbut with the\nds\ncolumn extended by\nNF\nperiods.\nShow the code\nprophet_fit <- prophet(BE_train_df)\n\nfuture <- make_future_dataframe(\n  prophet_fit,\n  periods = NF,\n  freq = 3600,\n  include_history = FALSE\n)\n\nprophet_fcst <- predict(prophet_fit, future)\nResults and Discussion\nBefore plotting, let’s have a look at the wide data frame that holds the forecasts.\nShow the code\nfcst_df2 <- fcst_df |>\n  mutate(\n    arima_fcst = as.vector(arima_fcst$mean),\n    ets_fcst = as.vector(ets_fcst$mean),\n    prophet_fcst = prophet_fcst$yhat\n  )\n\nhead(fcst_df2)\ntime tgpt_fcst elec_actual arima_fcst ets_fcst prophet_fcst\n1 2016-12-30 00:00:00  38.82010       44.30   46.19057 37.72396     32.05748\n2 2016-12-30 01:00:00  36.29234       44.30   44.99377 34.97583     29.74375\n3 2016-12-30 02:00:00  34.97838       41.26   44.06233 32.89185     22.66588\n4 2016-12-30 03:00:00  32.99565       40.62   42.45347 29.87591     15.85864\n5 2016-12-30 04:00:00  31.58322       40.07   43.05403 27.56602     15.62841\n6 2016-12-30 05:00:00  33.27422       41.02   44.28001 29.15387     23.54824\nThen, shape the data into long format and plot.\nShow the code\nfcst_dft2_long <- fcst_df2 %>%\n  pivot_longer(!time, names_to = \"method\", values_to = \"mean\")\n\nq <- fcst_dft2_long |>\n  ggplot(aes(\n    x = time,\n    y = mean,\n    group = method,\n    color = method\n  )) +\n  geom_line() +\n  geom_point() +\n  ggtitle(\"TimeGPT vs ARIMA vs ETS vs Prophet vs actual data - 24 Point Forecast\")\n\nq\nThe TimeGPT forecast looks quite good. I don’t think this is a big surprise, given that the Nixtla folks chose the electricity data set to show off their transformer. However, it is curious, that except for one point, the TimeGPT forecast is lower than the actual data. It is also interesting that some of the forecasted points that are farther out are a better match to the actual data than the initial forecast points.\nThe ARIMA forecast is very good. It tracks the first few points very closely, undershoots the peaks of the actual data, but recovers after both peaks and tracks the data well towards the end of the forecast period.\nThe exponential smoothing forecast mostly stays well below the actual data, but also dose pretty well overall.\nThe black box prophet model overacts to the downward trends at the beginning and end of the forecast period. My guess is that with a little tuning prophet could do much better.\nIt is also worth noting that choosing the\nbest\nforecast also depends on your objectives. For example, in some circumstances, one might prefer a forecast that reproduces seasonal patterns or possible volatility rather than overall accuracy. For this exercise, we see that TimeGPT mimics the volatility of the actual data but that the ARIMA forecast has the lowest root mean squared error. Also note that the ARIMA and exponential smoothing forecasts are not quite\nblack-box\nforecasts. In converting the data into\nts()\na time series object explicitly provided seasonality information. TimeGPT apparently inferred this information from character data.\nShow the code\nRMSE <-  function(m, o){sqrt(mean((m - o)^2))}\nrms_names <- c(\"tgpt\", \"arima\", \"ets\", \"prophet\")\nrms_fcst <- array(NA_real_,\n                          dim = 4,\n                          dimnames = list(rms_names))\nrms_fcst[1] <- RMSE(fcst_df2$tgpt_fcst, fcst_df2$elec_actual)\nrms_fcst[2] <- RMSE(fcst_df2$arima_fcst, fcst_df2$elec_actual)\nrms_fcst[3] <- RMSE(fcst_df2$ets_fcst, fcst_df2$elec_actual)\nrms_fcst[4] <- RMSE(fcst_df2$prophet_fcst, fcst_df2$elec_actual)\nrms_fcst\ntgpt     arima       ets   prophet \n 6.601752  4.966623  9.355767 14.948709\nSome Final Thoughts\nIt is clear that the TimeGPT model has upped the game for black-box time series forecasting. It is sure to become a powerful tool for exploratory work with large time series and for comparing multiple time series. It may become the\ngo-to\nbaseline forecasting tool for a wide range of time series projects. Moreover, I expect that time series experts who can fine-tune prophet and more traditional time series models will be able to develop some intuition about what TimeGPT is doing by assessing its behavior in relation to these models.\nI am aware that this little post may have raised more questions than it answered. If so, please try your hand at elaborating on some of the issues raised. We would be very happy to consider your time series posts for publication on R Works.\nFinally, for a more sophisticated analysis of these series that deals with their multiseasonality aspects, see the\nElectricity Load Forecast Tutorial\n. And, for some ideas about how to harness “ordinary” LLMs for time series forecasting have a look at the second half of the talk that Bryan Lewis gave to\nnyhackr\nin April 2024.\nAcknowledgment\nMany thanks to Professor Rob Hyndman for flagging the ARIMA time series object error described above and for generously suggesting alternatives that led to the workaround described above. Any errors still remaining in this post are all mine.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR Works\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "This post is a first look at Nixtla’s TimeGPT generative, pre-trained transformer for time series forecasting using the nixtlar R package. As described in Garza et al. (2021), TimeGPT is a Transformer-based time series model with self-atten...",
    "meta_keywords": null,
    "og_description": "This post is a first look at Nixtla’s TimeGPT generative, pre-trained transformer for time series forecasting using the nixtlar R package. As described in Garza et al. (2021), TimeGPT is a Transformer-based time series model with self-atten...",
    "og_image": "https://rworks.dev/posts/revised_TimeGPT/timegpt.png",
    "og_title": "A First Look at TimeGPT using nixtlar | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 12.1,
    "sitemap_lastmod": null,
    "twitter_description": "This post is a first look at Nixtla’s TimeGPT generative, pre-trained transformer for time series forecasting using the nixtlar R package. As described in Garza et al. (2021), TimeGPT is a Transformer-based time series model with self-atten...",
    "twitter_title": "A First Look at TimeGPT using nixtlar | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/02/a-first-look-at-timegpt-using-nixtlar-2/",
    "word_count": 2428
  }
}