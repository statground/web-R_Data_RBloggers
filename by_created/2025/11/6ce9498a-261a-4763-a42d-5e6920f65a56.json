{
  "uuid": "6ce9498a-261a-4763-a42d-5e6920f65a56",
  "created_at": "2025-11-17 20:39:55",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2009/11/loading-big-ish-data-into-r/",
    "crawled_at": "2025-11-17T10:15:42.358962",
    "external_links": [
      {
        "href": "http://www.cerebralmastication.com/2009/11/loading-big-data-into-r/",
        "text": "Cerebral Mastication » R"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://twitter.com/vsbuffalo/statuses/5987999475",
        "text": null
      },
      {
        "href": "https://twitter.com/mpastell/statuses/6002853376",
        "text": "Matti Pastell (@mpastell) recommended"
      },
      {
        "href": "https://code.google.com/p/sqldf/",
        "text": "sqldf"
      },
      {
        "href": "http://www.sqlite.org/",
        "text": "sqlite"
      },
      {
        "href": "http://old.nabble.com/Re%3A-Memory-Experimentation%3A-Rule-of-Thumb-%3D-10-15-Times-the-Memory-to12076668.html#a12078165",
        "text": "this discussion"
      },
      {
        "href": "http://www.cerebralmastication.com/2009/11/loading-big-data-into-r/",
        "text": "Cerebral Mastication » R"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Loading Big (ish) Data into R | R-bloggers",
    "images": [
      {
        "alt": "2gib",
        "base64": null,
        "src": "https://i0.wp.com/www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG?w=450"
      },
      {
        "alt": "2gib",
        "base64": null,
        "src": "https://i0.wp.com/www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG?w=450"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/jd-long/",
        "text": "JD Long"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/category/uncategorized/",
        "text": "Uncategorized"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/tag/r/",
        "text": "R"
      },
      {
        "href": "https://www.r-bloggers.com/tag/sql/",
        "text": "SQL"
      },
      {
        "href": "https://www.r-bloggers.com/tag/sqldf/",
        "text": "sqldf"
      },
      {
        "href": "https://www.r-bloggers.com/tag/sqlite/",
        "text": "sqlite"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-2514 post type-post status-publish format-standard hentry category-r-bloggers category-uncategorized tag-r tag-sql tag-sqldf tag-sqlite\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Loading Big (ish) Data into R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 24, 2009</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/jd-long/\">JD Long</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a>, <a href=\"https://www.r-bloggers.com/category/uncategorized/\" rel=\"category tag\">Uncategorized</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<p class=\"syndicated-attribution\"><!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"http://www.cerebralmastication.com/2009/11/loading-big-data-into-r/\"> Cerebral Mastication » R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div></p>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.0--><p>So for the rest of this conversation big data == 2 Gigs. Done. Don’t give me any of this ‘that’s not big, THIS is big’ shit. There now, on with the cool stuff:</p>\n<p>This week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab delimited data, but I ignored that because I use mostly comma data and I wanted to test CSV. Sue me.)</p>\n<p><a data-cf-modified-09f3a68a68cba93d5368e123-=\"\" href=\"https://twitter.com/vsbuffalo/statuses/5987999475\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; pageTracker._trackPageview('/outgoing/twitter.com/vsbuffalo/statuses/5987999475?referer=');\" rel=\"nofollow\" target=\"_blank\"><img alt=\"2gib\" class=\"size-full wp-image-417 alignnone jetpack-lazy-image\" data-lazy-src=\"https://i0.wp.com/www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG?w=450&amp;is-pending-load=1\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i0.wp.com/www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG?w=450\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" style=\"border: 2px solid black; margin: 2px;\" title=\"2gib\"/><noscript><img alt=\"2gib\" class=\"size-full wp-image-417 alignnone\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i0.wp.com/www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG?w=450\" style=\"border: 2px solid black; margin: 2px;\" title=\"2gib\"/></noscript></a></p>\n<p>I thought this was a dang good question. What I have always done in the past was load my data into SQL Server or Oracle using an ETL tool and then suck it from the database to R using either native database connections or the RODBC package. <a data-cf-modified-09f3a68a68cba93d5368e123-=\"\" href=\"https://twitter.com/mpastell/statuses/6002853376\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; pageTracker._trackPageview('/outgoing/twitter.com/mpastell/statuses/6002853376?referer=');\" rel=\"nofollow\" target=\"_blank\">Matti Pastell (@mpastell) recommended </a>using the <a data-cf-modified-09f3a68a68cba93d5368e123-=\"\" href=\"https://code.google.com/p/sqldf/\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; pageTracker._trackPageview('/outgoing/code.google.com/p/sqldf/?referer=');\" rel=\"nofollow\" target=\"_blank\">sqldf </a>(SQL to data frame) package to do the import. I’ve used sqldf before, but only to allow me to use SQL syntax to manipulate R data frames. I didn’t know it could import data, but that makes sense, given how sqldf works. How does it work? Well sqldf sets up an instance of the <a data-cf-modified-09f3a68a68cba93d5368e123-=\"\" href=\"http://www.sqlite.org/\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; pageTracker._trackPageview('/outgoing/www.sqlite.org/?referer=');\" rel=\"nofollow\" target=\"_blank\">sqlite </a>database server then shoves R data into the DB, does operations on the tables, and then spits out an R data frame of the results. What I didn’t realize is that we can call sqldf from within R and have it import a text file directly into sqlite and then return the data from sqlite directly into R using a pretty fast native connection. I did a little Googling and came up with <a data-cf-modified-09f3a68a68cba93d5368e123-=\"\" href=\"http://old.nabble.com/Re%3A-Memory-Experimentation%3A-Rule-of-Thumb-%3D-10-15-Times-the-Memory-to12076668.html#a12078165\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; pageTracker._trackPageview('/outgoing/old.nabble.com/Re_3A-Memory-Experimentation_3A-Rule-of-Thumb-_3D-10-15-Times-the-Memory-to12076668.html_a12078165?referer=');\" rel=\"nofollow\" target=\"_blank\">this discussion </a>on the R mailing list.</p>\n<p>So enough background, here’s my setup: I have a Ubuntu virtual machine running with 2 cores and 10 gigs of memory. Here’s the code I ran to test:</p>\n<blockquote><p>bigdf &lt;- data.frame(dim=sample(letters, replace=T, 4e7), fact1=rnorm(4e7), fact2=rnorm(4e7, 20, 50))<br/>\nwrite.csv(bigdf, ‘bigdf.csv’, quote = F)</p></blockquote>\n<p>That code creates a data frame with 3 columns. I created a single letter text column, then two floating point columns. There are 40,000,000 records. When I run the write.csv step on my machine I get about 1.8GiB. That’s close enough to 2 gigs for me. I created the text file and then ran rm(list=ls()) to kill all objects. I then ran gc() and saw that I had hundreds of megs of something or other (I have not invested the brain cycles to understand the output that gc() gives). So I just killed and restarted R. I then ran the following:</p>\n<blockquote><p>library(sqldf)<br/>\nf &lt;- file(“bigdf.csv”)<br/>\nsystem.time(bigdf &lt;- sqldf(“select * from f”, dbname = tempfile(), file.format = list(header = T, row.names = F)))</p></blockquote>\n<p>That code loads the CSV into an sqlite DB then executes a select * query and returns the results to the R data frame bigdf. Pretty straightforward, ey? Well except for the dbname = tempfile() bit. In sqldf you can choose where it makes the sqlite db. If you don’t specify at all it makes it in memory which is what I first tried. I ran out of mem even on my 10GB box. So I read a little more and added the dbname = tempfile() which creates a temporary sqlite file on the disk. If I wanted to use an existing sqlite file I could have specified that instead.</p>\n<p>So how long did it take to run? Just under 5 minutes.</p>\n<p>So how long would the read.csv method take? Funny you should ask. I ran the following code to compare:</p>\n<blockquote><p>system.time(big.df &lt;- read.csv(‘bigdf.csv’))</p></blockquote>\n<p>And I would love to tell you how long that took to run, but it’s been running <span style=\"text-decoration: line-through;\">for half an hour</span> all night and I just don’t have that kind of patience.</p>\n<p>-JD</p>\n\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.0-->\n<p class=\"syndicated-attribution\"><div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"http://www.cerebralmastication.com/2009/11/loading-big-data-into-r/\"> Cerebral Mastication » R</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div></p> </div>\n<div class=\"post-tags clearfix\"><ul><li class=\"round-corners\"><a href=\"https://www.r-bloggers.com/tag/r/\" rel=\"tag\">R</a></li><li class=\"round-corners\"><a href=\"https://www.r-bloggers.com/tag/sql/\" rel=\"tag\">SQL</a></li><li class=\"round-corners\"><a href=\"https://www.r-bloggers.com/tag/sqldf/\" rel=\"tag\">sqldf</a></li><li class=\"round-corners\"><a href=\"https://www.r-bloggers.com/tag/sqlite/\" rel=\"tag\">sqlite</a></li></ul></div></article>",
    "main_text": "Loading Big (ish) Data into R\nPosted on\nNovember 24, 2009\nby\nJD Long\nin\nR bloggers\n,\nUncategorized\n| 0 Comments\n[This article was first published on\nCerebral Mastication » R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nSo for the rest of this conversation big data == 2 Gigs. Done. Don’t give me any of this ‘that’s not big, THIS is big’ shit. There now, on with the cool stuff:\nThis week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab delimited data, but I ignored that because I use mostly comma data and I wanted to test CSV. Sue me.)\nI thought this was a dang good question. What I have always done in the past was load my data into SQL Server or Oracle using an ETL tool and then suck it from the database to R using either native database connections or the RODBC package.\nMatti Pastell (@mpastell) recommended\nusing the\nsqldf\n(SQL to data frame) package to do the import. I’ve used sqldf before, but only to allow me to use SQL syntax to manipulate R data frames. I didn’t know it could import data, but that makes sense, given how sqldf works. How does it work? Well sqldf sets up an instance of the\nsqlite\ndatabase server then shoves R data into the DB, does operations on the tables, and then spits out an R data frame of the results. What I didn’t realize is that we can call sqldf from within R and have it import a text file directly into sqlite and then return the data from sqlite directly into R using a pretty fast native connection. I did a little Googling and came up with\nthis discussion\non the R mailing list.\nSo enough background, here’s my setup: I have a Ubuntu virtual machine running with 2 cores and 10 gigs of memory. Here’s the code I ran to test:\nbigdf <- data.frame(dim=sample(letters, replace=T, 4e7), fact1=rnorm(4e7), fact2=rnorm(4e7, 20, 50))\nwrite.csv(bigdf, ‘bigdf.csv’, quote = F)\nThat code creates a data frame with 3 columns. I created a single letter text column, then two floating point columns. There are 40,000,000 records. When I run the write.csv step on my machine I get about 1.8GiB. That’s close enough to 2 gigs for me. I created the text file and then ran rm(list=ls()) to kill all objects. I then ran gc() and saw that I had hundreds of megs of something or other (I have not invested the brain cycles to understand the output that gc() gives). So I just killed and restarted R. I then ran the following:\nlibrary(sqldf)\nf <- file(“bigdf.csv”)\nsystem.time(bigdf <- sqldf(“select * from f”, dbname = tempfile(), file.format = list(header = T, row.names = F)))\nThat code loads the CSV into an sqlite DB then executes a select * query and returns the results to the R data frame bigdf. Pretty straightforward, ey? Well except for the dbname = tempfile() bit. In sqldf you can choose where it makes the sqlite db. If you don’t specify at all it makes it in memory which is what I first tried. I ran out of mem even on my 10GB box. So I read a little more and added the dbname = tempfile() which creates a temporary sqlite file on the disk. If I wanted to use an existing sqlite file I could have specified that instead.\nSo how long did it take to run? Just under 5 minutes.\nSo how long would the read.csv method take? Funny you should ask. I ran the following code to compare:\nsystem.time(big.df <- read.csv(‘bigdf.csv’))\nAnd I would love to tell you how long that took to run, but it’s been running\nfor half an hour\nall night and I just don’t have that kind of patience.\n-JD\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nCerebral Mastication » R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nR\nSQL\nsqldf\nsqlite",
    "meta_description": "So for the rest of this conversation big data == 2 Gigs. Done. Don’t give me any of this ‘that’s not big, THIS is big’ shit. There now, on with the cool stuff: This week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab [...]",
    "meta_keywords": "r,sql,sqldf,sqlite",
    "og_description": "So for the rest of this conversation big data == 2 Gigs. Done. Don’t give me any of this ‘that’s not big, THIS is big’ shit. There now, on with the cool stuff: This week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab [...]",
    "og_image": "https://www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG",
    "og_title": "Loading Big (ish) Data into R | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 3.9,
    "sitemap_lastmod": "2009-11-24T23:14:06+00:00",
    "twitter_description": "So for the rest of this conversation big data == 2 Gigs. Done. Don’t give me any of this ‘that’s not big, THIS is big’ shit. There now, on with the cool stuff: This week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab [...]",
    "twitter_title": "Loading Big (ish) Data into R | R-bloggers",
    "url": "https://www.r-bloggers.com/2009/11/loading-big-ish-data-into-r/",
    "word_count": 784
  }
}