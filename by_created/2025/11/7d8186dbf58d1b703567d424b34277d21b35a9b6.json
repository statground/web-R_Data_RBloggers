{
  "id": "7d8186dbf58d1b703567d424b34277d21b35a9b6",
  "url": "https://www.r-bloggers.com/2025/01/the-mathematics-of-taylor-swift/",
  "created_at_utc": "2025-11-22T19:59:20Z",
  "data": null,
  "raw_original": {
    "uuid": "de898a61-7def-48bb-826e-b1b73e3e1c03",
    "created_at": "2025-11-22 19:59:20",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/01/the-mathematics-of-taylor-swift/",
      "crawled_at": "2025-11-22T10:54:38.250966",
      "external_links": [
        {
          "href": "https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html",
          "text": "Online College Math Teacher"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBBSRa6fR4LXCy7Trgqcy-suWuyrxWEihIgknyFXGsC3LAdbG7INN4xSvOAtRhPtAmGKRUEDz4rL66Yq17rY2jf1-F4KDTOYB9jUKhyphenhyphenYM6qWeeZpnLtSaXgapIytjHctG_XeSf8XXmG95_kgajIlcTJ26SrzTa_KF8YVhsO3OCCQZQhEAmOuTZ91bHySg/s555/Screenshot_1.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_LrJI3mCAhiloIUQ4thBzhzYh8OxwvotHEwsJ3vlV57piWIP0Z4ObcCXSs-WQeC4wJiUV8hyphenhyphen_Rk0Wo2kjpoZB8lFlqV3fvC2c8KSUCHVmXftWA_FBullYzJ21FuSx_J6yQhmFi5PXBAHDHQBq9Ux0TfjxAAsoomqZ-nJEd2mSNKQFPe_1dvAQd3w1KmE/s704/Screenshot_2.png?ssl=1",
          "text": null
        },
        {
          "href": "https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html#part1",
          "text": "her attractiveness"
        },
        {
          "href": "https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html#part2",
          "text": "her lipstick shades"
        },
        {
          "href": "https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html#part3",
          "text": "her lyrics"
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqoM643rLrDEpp676CmKFSz65ANKrO1H9b1nRUrGoesKIK-3gGXf67HVbeIVUb-9TOFD8DGLSdaol2mFFGR8TwpE2i3lKL6-H3eU7QsUF-ey3V8FkzIsxOAgW8Xjyv1tLvOj3WhE-8qjHodlEsG2nJpTm6WU8BeRwHiYYKdoS96Gi8i0VA7jL8ZGPzyQQ/s334/goldenratio.JPG?ssl=1",
          "text": null
        },
        {
          "href": "https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1098&context=csearticles",
          "text": "Schmid"
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_hpCpKZESznMjtTcXhZxQCnO510alUNrufnpagO-bxmVs6x7q7XUBy0ggpsJAXfJWE03-FPNbvviNeehscyKsm9aVzoxEgE9oKXU5VoWdwc3nT0cotJK2eiCKoVsqspmajrU2ksrucKVbCVs60xiU5VdnF1_KECI4dCxNmzunyghDg6ZvFqToVv90NrU/s595/Screenshot_3.png?ssl=1",
          "text": null
        },
        {
          "href": "https://www.remove.bg/",
          "text": "https://www.remove.bg/"
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSnlVmYA6b5PIHwoRYBIZRSozzqCi_lyQMiInLLRrw_7Dd-cQwrdcNrAR3qm-oEVgmyKIcnfoCMTF08GFZ4TwMOP1tp_D2t8wcP98rRhx836OcXGCMX9ndOkTGcJXpFCk1VY2BxOoJ7-V-hc9P9sEDTw0RqiV0awgcE1XWG1Zwoq57teKRjdNVT6HdL08/s1128/Screenshot_4.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5oJGgzlsIyg8ku_KjwQgOR3WgEcpmx_83zjFUF-R3L57fIt492_mDua1vLPgpqY_h1hZm6Nm0p4KHA1Prv-nxPopr2crHvMuwflTjmTPnBjdv-mHjs7pKj3L1M8X6pvadWAkY0IlYA61KGM7rdj_USMdSVJ6LHQErALzRE8PCFBelifNVNAcnyNXsX6U/s1130/Screenshot_5.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBi7LXBcufNihCQPK6-MtAnTS7OS2_uQn6uD81j5CxuKt5U1wFHlRBm7NdXAV6HMcinkiduJ0f-6bsL6Tpb96Mwmn_1Y8bKlg1b0LKMjYeBbaWZJAbpshANuufjfj8NaQqg4sKLjquJa53yPW_tide1ED4SEveQwoNcnU8H5FVd7CZStui1AFWVF6velk/s1134/Screenshot_6.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYdZsJrG6yZFc3jOy-7GGsANCtZ5oi_K0akP1yDdLy0JxWKQUWVbE8LF8BDKJlV20KT5Wqz62WHNZXY19zFxYgc9H8e2x4TUGcR6RX63TmUQgGfgq0lU05GTuuHlstyREHHPTYSlvTGqjnM_H4OxiPKjiOkQkJZK6rNg8TXrvR9TADiYdFA2E5cVwogok/s571/Screenshot_7.png?ssl=1",
          "text": null
        },
        {
          "href": "https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm",
          "text": "NRC Emotion Lexicon"
        },
        {
          "href": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3qwAszZVcg_C2iisTwTjb6DJXKg3jUD_LXTSLiLdklVRMbh482Kg6rFkpzklDjiUiEhb65tWx8Q9ztHweotsMqsOdA5P0_y1ZRqbGCszdV242-S5ljrcd3nTp3uiw7hi4Qun5sFdjfpYV0uSK4ZZAoOABdKwfK_o1Ydonp_aM7ohnnBCcy3X7oVqgOqA/s705/Screenshot_9.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5CCOxIuxWnF3HXYJ5jpPrk88qWM8XiTKPKMWoV1hgrvI4NdeabX6PeflDIGLntHBW6fEUdo4KhRBPlyEYF-WonEjWZRhfH33p6wBL_K605AANRIAcA0oa4zkEfvuQzYX7G26CrqI5P7jkIaG3L281vPoH57Md_PkKWabhy1fZHctXsZDHj5Raj5ytc8E/s678/Screenshot_10.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1Kp9f_jiMwQ33Nv9cCwmJ044jLzaep1GDyq2ZgexBmIO6ApQQmuvzwfKPu_vqyWxaKS3MSQp1JY3EDxsE7RkfQh-wviwWbwPN-0WPuL83FyahqdiH4vkg806sXRKuQ3gKdBKebqYIgRxc3yuSZNyV9ERpSfrcCOTuhaCHULsQ_09HeekAQgEqrn349xo/s715/Screenshot_11.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhe2ojHhA1VxwV35aLnkb0R9kd9rSltMK___1LtuFmc5z37dTRu93_2ttLFM878spsM-hM6DkuY-8m-Lyf4Id6XXEcqIugMEA8lazmovnxUC3El0U4qJcCOqgnybvxGx-MJeOnQpTVYqjxDhrXpe25cVzYvINj5i54L9R1LwkgB9cMBh9jBUcNBY-8c6Pk/s651/Screenshot_12.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpfv5l1vVxaCxybylB8XFjnZ13WPJ8q-nnUVmlNVPtTRVcQjcfcTsq1HHWNOLlaC9n1xydtLgkOXnBud8U18vJ2zlfsylQ8ekxln_BfGSYNgItAHQcTcoMIOg1vQjrK8vxPh6DNNV1JVh8FQ8EiGf1xPLOoUkDAZ3p_LboItMIel7De72G4nyYihQU3og/s655/Screenshot_13.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuYVI4CImkSSlMnZ1EvxQ_jPWXYAbwKjddoppNw2YdSu1PjMyQaiuEBpcgq0M_kXiDmkDzTg4ka82HV_mx20yalBTU9fRfpOGTk7wOpFxf_5wjpzxMEO3pS6DwA_AcVuWzFSSpo1JvR6pLhJqGoKx_Gia0wesBkUPxNtl10NK3lakfSoJu8AY7EnTzwM/s690/Screenshot_14.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHJh1B-D4360PyMa2f1OblCvgrdKxU-2XbOrYOrWMe5YM6Wa4s8F2o_l_vBdQEKal4YArxwBg-FL1fbHXfdRD81UB2Ye2Pp7egoyqsoJuHlvcWgJ_40Yt1SbUF9xLEcAxTUtK4KbbBFDhD4M4zuEzycq8W_Z0YYjlkicbdnwzVPm9gvOu12Cq1h_-dyZA/s551/Screenshot_15.png?ssl=1",
          "text": null
        },
        {
          "href": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDQNBzBOXxDiUsxTTANfKv4eTuEjvo4iYcBh9Hh084JS7-yw2hD3yzL1K0G5Lr1MeeMPlIm1WH2DxmOCCkIqRQwGdQL8Hh8aNnwaTnM6mbD_Q-1TI26zcAB7e74pQnl-hcBGJ2AhS0qK_ZFK_ImQsuzP15MIlUdPtgbG17xgPGTuNQdov7KMLHH7JwqG4/s501/Screenshot_16.png?ssl=1",
          "text": null
        },
        {
          "href": "https://www.vulture.com/article/taylor-swift-track-5-songs-ranked.html",
          "text": "theory"
        },
        {
          "href": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVhL4H169PiDFbKEMxHxtfx1QbneWH3cjsyR1fC9-HQB5WifSNaoyIrqwxjOoh9pHW_gtdg5o-oBNKF4vrVIlwPdnzdehFXNF8R4oXMBewcITGcQwGrxnADibzrtuuhYDnFd5hsHQiEvnNJZ6nTFba_sGiwzI1ViXBvw5EiC109sZFPxY1Dt5U94nm1so/s561/Screenshot_8.png?ssl=1",
          "text": null
        },
        {
          "href": "https://github.com/fcas80/The-Taylor-Swift-Project/tree/main",
          "text": "https://github.com/fcas80/The-Taylor-Swift-Project/tree/main"
        },
        {
          "href": "https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html",
          "text": "Online College Math Teacher"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "The Mathematics of Taylor Swift | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBBSRa6fR4LXCy7Trgqcy-suWuyrxWEihIgknyFXGsC3LAdbG7INN4xSvOAtRhPtAmGKRUEDz4rL66Yq17rY2jf1-F4KDTOYB9jUKhyphenhyphenYM6qWeeZpnLtSaXgapIytjHctG_XeSf8XXmG95_kgajIlcTJ26SrzTa_KF8YVhsO3OCCQZQhEAmOuTZ91bHySg/s320/Screenshot_1.png?resize=450%2C480&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_LrJI3mCAhiloIUQ4thBzhzYh8OxwvotHEwsJ3vlV57piWIP0Z4ObcCXSs-WQeC4wJiUV8hyphenhyphen_Rk0Wo2kjpoZB8lFlqV3fvC2c8KSUCHVmXftWA_FBullYzJ21FuSx_J6yQhmFi5PXBAHDHQBq9Ux0TfjxAAsoomqZ-nJEd2mSNKQFPe_1dvAQd3w1KmE/s320/Screenshot_2.png?resize=450%2C498&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqoM643rLrDEpp676CmKFSz65ANKrO1H9b1nRUrGoesKIK-3gGXf67HVbeIVUb-9TOFD8DGLSdaol2mFFGR8TwpE2i3lKL6-H3eU7QsUF-ey3V8FkzIsxOAgW8Xjyv1tLvOj3WhE-8qjHodlEsG2nJpTm6WU8BeRwHiYYKdoS96Gi8i0VA7jL8ZGPzyQQ/s320/goldenratio.JPG?resize=180%2C112&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_hpCpKZESznMjtTcXhZxQCnO510alUNrufnpagO-bxmVs6x7q7XUBy0ggpsJAXfJWE03-FPNbvviNeehscyKsm9aVzoxEgE9oKXU5VoWdwc3nT0cotJK2eiCKoVsqspmajrU2ksrucKVbCVs60xiU5VdnF1_KECI4dCxNmzunyghDg6ZvFqToVv90NrU/s320/Screenshot_3.png?resize=320%2C490&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSnlVmYA6b5PIHwoRYBIZRSozzqCi_lyQMiInLLRrw_7Dd-cQwrdcNrAR3qm-oEVgmyKIcnfoCMTF08GFZ4TwMOP1tp_D2t8wcP98rRhx836OcXGCMX9ndOkTGcJXpFCk1VY2BxOoJ7-V-hc9P9sEDTw0RqiV0awgcE1XWG1Zwoq57teKRjdNVT6HdL08/s320/Screenshot_4.png?resize=450%2C578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5oJGgzlsIyg8ku_KjwQgOR3WgEcpmx_83zjFUF-R3L57fIt492_mDua1vLPgpqY_h1hZm6Nm0p4KHA1Prv-nxPopr2crHvMuwflTjmTPnBjdv-mHjs7pKj3L1M8X6pvadWAkY0IlYA61KGM7rdj_USMdSVJ6LHQErALzRE8PCFBelifNVNAcnyNXsX6U/s320/Screenshot_5.png?resize=450%2C490&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBi7LXBcufNihCQPK6-MtAnTS7OS2_uQn6uD81j5CxuKt5U1wFHlRBm7NdXAV6HMcinkiduJ0f-6bsL6Tpb96Mwmn_1Y8bKlg1b0LKMjYeBbaWZJAbpshANuufjfj8NaQqg4sKLjquJa53yPW_tide1ED4SEveQwoNcnU8H5FVd7CZStui1AFWVF6velk/s320/Screenshot_6.png?resize=450%2C505&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYdZsJrG6yZFc3jOy-7GGsANCtZ5oi_K0akP1yDdLy0JxWKQUWVbE8LF8BDKJlV20KT5Wqz62WHNZXY19zFxYgc9H8e2x4TUGcR6RX63TmUQgGfgq0lU05GTuuHlstyREHHPTYSlvTGqjnM_H4OxiPKjiOkQkJZK6rNg8TXrvR9TADiYdFA2E5cVwogok/s320/Screenshot_7.png?resize=320%2C495&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3qwAszZVcg_C2iisTwTjb6DJXKg3jUD_LXTSLiLdklVRMbh482Kg6rFkpzklDjiUiEhb65tWx8Q9ztHweotsMqsOdA5P0_y1ZRqbGCszdV242-S5ljrcd3nTp3uiw7hi4Qun5sFdjfpYV0uSK4ZZAoOABdKwfK_o1Ydonp_aM7ohnnBCcy3X7oVqgOqA/s320/Screenshot_9.png?resize=450%2C608&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5CCOxIuxWnF3HXYJ5jpPrk88qWM8XiTKPKMWoV1hgrvI4NdeabX6PeflDIGLntHBW6fEUdo4KhRBPlyEYF-WonEjWZRhfH33p6wBL_K605AANRIAcA0oa4zkEfvuQzYX7G26CrqI5P7jkIaG3L281vPoH57Md_PkKWabhy1fZHctXsZDHj5Raj5ytc8E/s320/Screenshot_10.png?resize=450%2C603&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1Kp9f_jiMwQ33Nv9cCwmJ044jLzaep1GDyq2ZgexBmIO6ApQQmuvzwfKPu_vqyWxaKS3MSQp1JY3EDxsE7RkfQh-wviwWbwPN-0WPuL83FyahqdiH4vkg806sXRKuQ3gKdBKebqYIgRxc3yuSZNyV9ERpSfrcCOTuhaCHULsQ_09HeekAQgEqrn349xo/s320/Screenshot_11.png?resize=450%2C595&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhe2ojHhA1VxwV35aLnkb0R9kd9rSltMK___1LtuFmc5z37dTRu93_2ttLFM878spsM-hM6DkuY-8m-Lyf4Id6XXEcqIugMEA8lazmovnxUC3El0U4qJcCOqgnybvxGx-MJeOnQpTVYqjxDhrXpe25cVzYvINj5i54L9R1LwkgB9cMBh9jBUcNBY-8c6Pk/s320/Screenshot_12.png?resize=450%2C441&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpfv5l1vVxaCxybylB8XFjnZ13WPJ8q-nnUVmlNVPtTRVcQjcfcTsq1HHWNOLlaC9n1xydtLgkOXnBud8U18vJ2zlfsylQ8ekxln_BfGSYNgItAHQcTcoMIOg1vQjrK8vxPh6DNNV1JVh8FQ8EiGf1xPLOoUkDAZ3p_LboItMIel7De72G4nyYihQU3og/s320/Screenshot_13.png?resize=450%2C425&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuYVI4CImkSSlMnZ1EvxQ_jPWXYAbwKjddoppNw2YdSu1PjMyQaiuEBpcgq0M_kXiDmkDzTg4ka82HV_mx20yalBTU9fRfpOGTk7wOpFxf_5wjpzxMEO3pS6DwA_AcVuWzFSSpo1JvR6pLhJqGoKx_Gia0wesBkUPxNtl10NK3lakfSoJu8AY7EnTzwM/s320/Screenshot_14.png?resize=450%2C475&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHJh1B-D4360PyMa2f1OblCvgrdKxU-2XbOrYOrWMe5YM6Wa4s8F2o_l_vBdQEKal4YArxwBg-FL1fbHXfdRD81UB2Ye2Pp7egoyqsoJuHlvcWgJ_40Yt1SbUF9xLEcAxTUtK4KbbBFDhD4M4zuEzycq8W_Z0YYjlkicbdnwzVPm9gvOu12Cq1h_-dyZA/s320/Screenshot_15.png?resize=450%2C489&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDQNBzBOXxDiUsxTTANfKv4eTuEjvo4iYcBh9Hh084JS7-yw2hD3yzL1K0G5Lr1MeeMPlIm1WH2DxmOCCkIqRQwGdQL8Hh8aNnwaTnM6mbD_Q-1TI26zcAB7e74pQnl-hcBGJ2AhS0qK_ZFK_ImQsuzP15MIlUdPtgbG17xgPGTuNQdov7KMLHH7JwqG4/s320/Screenshot_16.png?resize=450%2C438&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVhL4H169PiDFbKEMxHxtfx1QbneWH3cjsyR1fC9-HQB5WifSNaoyIrqwxjOoh9pHW_gtdg5o-oBNKF4vrVIlwPdnzdehFXNF8R4oXMBewcITGcQwGrxnADibzrtuuhYDnFd5hsHQiEvnNJZ6nTFba_sGiwzI1ViXBvw5EiC109sZFPxY1Dt5U94nm1so/s320/Screenshot_8.png?resize=450%2C483&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/jerry-tuttle/",
          "text": "Jerry Tuttle"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-390239 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">The Mathematics of Taylor Swift</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 30, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/jerry-tuttle/\">Jerry Tuttle</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html\"> Online College Math Teacher</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n\n\n     \nI confess to being fascinated by Taylor Swift for far more than her music. I think she is an extraordinary person for her  philanthropy, her speaking out for victims of sexual assault, her advocacy of artists’ ownership rights, and her urging her fans to vote. <p>\n  \n       \nBut of course there’s much more, and so let’s look at the mathematics of Taylor Swift. </p><p>\n  \n       \nLet’s start with her net worth.  She is estimared to have $1.6 billion in assets.  Her largest asset is her music catalog. She did not own the masters (first recording) to her older music, but she re-recorded them and owns the masters to the re-recorded versions and to her future recordings. </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBBSRa6fR4LXCy7Trgqcy-suWuyrxWEihIgknyFXGsC3LAdbG7INN4xSvOAtRhPtAmGKRUEDz4rL66Yq17rY2jf1-F4KDTOYB9jUKhyphenhyphenYM6qWeeZpnLtSaXgapIytjHctG_XeSf8XXmG95_kgajIlcTJ26SrzTa_KF8YVhsO3OCCQZQhEAmOuTZ91bHySg/s555/Screenshot_1.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBBSRa6fR4LXCy7Trgqcy-suWuyrxWEihIgknyFXGsC3LAdbG7INN4xSvOAtRhPtAmGKRUEDz4rL66Yq17rY2jf1-F4KDTOYB9jUKhyphenhyphenYM6qWeeZpnLtSaXgapIytjHctG_XeSf8XXmG95_kgajIlcTJ26SrzTa_KF8YVhsO3OCCQZQhEAmOuTZ91bHySg/s320/Screenshot_1.png?resize=450%2C480&amp;ssl=1\" data-original-height=\"555\" data-original-width=\"450\" data-recalc-dims=\"1\" height=\"480\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"555\" data-original-width=\"450\" data-recalc-dims=\"1\" height=\"480\" src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBBSRa6fR4LXCy7Trgqcy-suWuyrxWEihIgknyFXGsC3LAdbG7INN4xSvOAtRhPtAmGKRUEDz4rL66Yq17rY2jf1-F4KDTOYB9jUKhyphenhyphenYM6qWeeZpnLtSaXgapIytjHctG_XeSf8XXmG95_kgajIlcTJ26SrzTa_KF8YVhsO3OCCQZQhEAmOuTZ91bHySg/s320/Screenshot_1.png?resize=450%2C480&amp;ssl=1\"/></noscript></a></div>\n  \n     \nShe donates large sums to a variety of organizations.  Most of these donations are not public, but here is a sample of some recent donations: <p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_LrJI3mCAhiloIUQ4thBzhzYh8OxwvotHEwsJ3vlV57piWIP0Z4ObcCXSs-WQeC4wJiUV8hyphenhyphen_Rk0Wo2kjpoZB8lFlqV3fvC2c8KSUCHVmXftWA_FBullYzJ21FuSx_J6yQhmFi5PXBAHDHQBq9Ux0TfjxAAsoomqZ-nJEd2mSNKQFPe_1dvAQd3w1KmE/s704/Screenshot_2.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_LrJI3mCAhiloIUQ4thBzhzYh8OxwvotHEwsJ3vlV57piWIP0Z4ObcCXSs-WQeC4wJiUV8hyphenhyphen_Rk0Wo2kjpoZB8lFlqV3fvC2c8KSUCHVmXftWA_FBullYzJ21FuSx_J6yQhmFi5PXBAHDHQBq9Ux0TfjxAAsoomqZ-nJEd2mSNKQFPe_1dvAQd3w1KmE/s320/Screenshot_2.png?resize=450%2C498&amp;ssl=1\" data-original-height=\"498\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"498\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_LrJI3mCAhiloIUQ4thBzhzYh8OxwvotHEwsJ3vlV57piWIP0Z4ObcCXSs-WQeC4wJiUV8hyphenhyphen_Rk0Wo2kjpoZB8lFlqV3fvC2c8KSUCHVmXftWA_FBullYzJ21FuSx_J6yQhmFi5PXBAHDHQBq9Ux0TfjxAAsoomqZ-nJEd2mSNKQFPe_1dvAQd3w1KmE/s320/Screenshot_2.png?resize=450%2C498&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n\n     \nThere are three parts to this mathematical analysis: <br/>\n         1.  <a href=\"https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html#part1\" rel=\"nofollow\" target=\"_blank\">her attractiveness</a>, <br/>\n         2.  <a href=\"https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html#part2\" rel=\"nofollow\" target=\"_blank\">her lipstick shades</a>, and <br/>\n         3.  <a href=\"https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html#part3\" rel=\"nofollow\" target=\"_blank\">her lyrics</a>.  <br/>\nEach part contains a considerable amount of imprecision in measuremenys and judgments.  This is more of a work in progress than the final word.  Nevertheless, I think it is a non-routine use of math, and it was a lot of fun. <p>\n</p><h3 id=\"part1\"> 1. How pretty is she?</h3><p>\n     \nYou can have your opinion.  But mine is based on math.</p><p>\n  \n     \nThe ancient Greeks discovered a particular number called the Golden Ratio, denoted by Greek letter Φ (phi),  that has many interesting mathematical properties, apppears \nin some patterns of nature, and is considered by many to be asthetically pleasing. The Golden Ratio results from finding the point on a line segment that splits the segement into two smaller segments with lengths a and b, such that (a + b)/a = a/b. </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqoM643rLrDEpp676CmKFSz65ANKrO1H9b1nRUrGoesKIK-3gGXf67HVbeIVUb-9TOFD8DGLSdaol2mFFGR8TwpE2i3lKL6-H3eU7QsUF-ey3V8FkzIsxOAgW8Xjyv1tLvOj3WhE-8qjHodlEsG2nJpTm6WU8BeRwHiYYKdoS96Gi8i0VA7jL8ZGPzyQQ/s334/goldenratio.JPG?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqoM643rLrDEpp676CmKFSz65ANKrO1H9b1nRUrGoesKIK-3gGXf67HVbeIVUb-9TOFD8DGLSdaol2mFFGR8TwpE2i3lKL6-H3eU7QsUF-ey3V8FkzIsxOAgW8Xjyv1tLvOj3WhE-8qjHodlEsG2nJpTm6WU8BeRwHiYYKdoS96Gi8i0VA7jL8ZGPzyQQ/s320/goldenratio.JPG?resize=180%2C112&amp;ssl=1\" data-original-height=\"112.5\" data-original-width=\"334\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"180\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"112.5\" data-original-width=\"334\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqoM643rLrDEpp676CmKFSz65ANKrO1H9b1nRUrGoesKIK-3gGXf67HVbeIVUb-9TOFD8DGLSdaol2mFFGR8TwpE2i3lKL6-H3eU7QsUF-ey3V8FkzIsxOAgW8Xjyv1tLvOj3WhE-8qjHodlEsG2nJpTm6WU8BeRwHiYYKdoS96Gi8i0VA7jL8ZGPzyQQ/s320/goldenratio.JPG?resize=180%2C112&amp;ssl=1\" width=\"180\"/></noscript></a></div><p>\n\n     \nThat ratio a/b is the Golden Ratio, Φ.  With a little algebra, Φ = (1 +  √5)/2 , which is an irrational number so it has an infinite non-repeating decimal, and rounded to three decimal places is 1.618. </p><p>\n\n     \nRenaissance artists, plastic surgeons, and makeup artists are among those who use Golden Ratios in various ways with faces to create ideally proportioned faces. Gary Meisner has wriiren extensively on the Golden Ratio, and he believes there are over 20 different ways that the Golden Ratio shows up in human faces and that “the Golden Ratio is also found very commonly in beautiful models of today across all ethnic groups”.  Biostatistican professor Dr. Kendra Schmid and her colleagues performed various measures of many faces. They began with 17 potential Golden Ratios, and they decided only six of these ratios were predictors of facial attractiveness. See \n<a href=\"https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1098&amp;context=csearticles\" rel=\"nofollow\" target=\"_blank\">Schmid</a>.</p><p>\n\n     \nThis takes us to Taylor.  I attempted to measure these six ratios on a picture of Taylor. There are many pictures of her, she does enjoy experimenting with different hairstyles, and I had to find one with a hairstyle that gave me the best chance of measuring her from her hairline and also between her ears.  The measurement is not exact for many reasons, and because we are using a two-dimensional photo of a three-dimensional object there is certainly some loss of accuracy.  Nevertheless, here are the results: </p><p>\n</p><center>\n<font color=\"black\">\n<table ,=\"\" bgcolor=\"lightblue\" border=\"black\" cellpadding=\"15\" width=\"320\">\n<tr><td>Face length / Face Width</td><td>1.4722</td></tr>\n<tr><td>Mouth width / Interocular distance</td><td>1.4000</td></tr>\n<tr><td>Mouth width / Nose width</td><td>1.5556</td></tr>\n<tr><td>Lips to chin / Interocular</td><td>1.7000</td></tr>\n<tr><td>Lips to chin / Nose width</td><td>1.8889</td></tr>\n<tr><td>Ear length / Nose width</td><td>1.5556</td></tr>\n<tr><td>Average</td><td>1.5954</td>\n</tr><tr><td>% Deviation from Φ </td><td>– 1.4%</td></tr>\n</table></font></center><p>\n\n     \nOn average, Taylor’s measurements are quite close to phi, the ideal measurement of facial attractiveness. </p><p>\n\n     \n(Note that I did the measurements manually, and they are inexact.  I understand there are Python libraries such as dlib that can detect facial ladmarks, and this might eliminate some of the imprecision.) </p><p>\n</p><h3 id=\"part2\"> 2. What lipstick shade is she wearing?</h3><p>\n     \nA common question on the Internet is what are her favorite lipsticks.  I think more interesting mathematically is: given a particular photo of Taylor, can we get the computer to mathematically identify her lipstick? </p><p>\n  \n       \nFirst, a little background about image files on the computer.  A computer image is a collection of tiny dots called pixels. Each dot is about 1/96 inch.\nEach pixel contains a color code, as a 6-digit base 16 (hex) number, or as a 6 digit base 10 red-green-blue ordered triple.\n  For example 861A23 in base 16 equals (134, 26, 35) in rgb base 10.  The first two hex digits, 86, is the red.  86 in base 16 equals 8*16<sup>1</sup> + 6*16<sup>0</sup> = 134 in base ten.  The second two hex digits, 1A, is the green, etc. </p><p>\n    \n     \nWe can ask how similar two colors are by plotting them in 3D and calculating their distance. </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_hpCpKZESznMjtTcXhZxQCnO510alUNrufnpagO-bxmVs6x7q7XUBy0ggpsJAXfJWE03-FPNbvviNeehscyKsm9aVzoxEgE9oKXU5VoWdwc3nT0cotJK2eiCKoVsqspmajrU2ksrucKVbCVs60xiU5VdnF1_KECI4dCxNmzunyghDg6ZvFqToVv90NrU/s595/Screenshot_3.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_hpCpKZESznMjtTcXhZxQCnO510alUNrufnpagO-bxmVs6x7q7XUBy0ggpsJAXfJWE03-FPNbvviNeehscyKsm9aVzoxEgE9oKXU5VoWdwc3nT0cotJK2eiCKoVsqspmajrU2ksrucKVbCVs60xiU5VdnF1_KECI4dCxNmzunyghDg6ZvFqToVv90NrU/s320/Screenshot_3.png?resize=320%2C490&amp;ssl=1\" data-original-height=\"490\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"320\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"490\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_hpCpKZESznMjtTcXhZxQCnO510alUNrufnpagO-bxmVs6x7q7XUBy0ggpsJAXfJWE03-FPNbvviNeehscyKsm9aVzoxEgE9oKXU5VoWdwc3nT0cotJK2eiCKoVsqspmajrU2ksrucKVbCVs60xiU5VdnF1_KECI4dCxNmzunyghDg6ZvFqToVv90NrU/s320/Screenshot_3.png?resize=320%2C490&amp;ssl=1\" width=\"320\"/></noscript></a></div>\n<center>D =  √ [ (134 – 174)<sup>2</sup> + (26 – 31)<sup>2</sup> + (35 – 61)<sup>2</sup> ] = 48.0</center><p>\n  \n     \nI started with five different photos of Taylor, where it appears to me she is wearing a different shade in each. I tried to crop each photo as a rectangular area as close as I could get to just her lips, and I saved each cropped rectangle as a file.  I used R packsge colouR to find the most frequent colors.  This was unsatisfactory, possibly because there was too much background color noise.  I found a website <a href=\"https://www.remove.bg/\" rel=\"nofollow\" target=\"_blank\">https://www.remove.bg/</a>\nthat removes background, and I retried colouR with the background removed files. </p><p>\n  \n     \nI did the same process with online swatches of ten lipsticks that the Internet says are among Taylor’s favorites. </p><p>\n                                                                                                         \n     \nTo my surprise, there were more than 50 different colors, most of them reasonably close, on both the Taylor photos and on the lipstick swatches. </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSnlVmYA6b5PIHwoRYBIZRSozzqCi_lyQMiInLLRrw_7Dd-cQwrdcNrAR3qm-oEVgmyKIcnfoCMTF08GFZ4TwMOP1tp_D2t8wcP98rRhx836OcXGCMX9ndOkTGcJXpFCk1VY2BxOoJ7-V-hc9P9sEDTw0RqiV0awgcE1XWG1Zwoq57teKRjdNVT6HdL08/s1128/Screenshot_4.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSnlVmYA6b5PIHwoRYBIZRSozzqCi_lyQMiInLLRrw_7Dd-cQwrdcNrAR3qm-oEVgmyKIcnfoCMTF08GFZ4TwMOP1tp_D2t8wcP98rRhx836OcXGCMX9ndOkTGcJXpFCk1VY2BxOoJ7-V-hc9P9sEDTw0RqiV0awgcE1XWG1Zwoq57teKRjdNVT6HdL08/s320/Screenshot_4.png?resize=450%2C578&amp;ssl=1\" data-original-height=\"578\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"578\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSnlVmYA6b5PIHwoRYBIZRSozzqCi_lyQMiInLLRrw_7Dd-cQwrdcNrAR3qm-oEVgmyKIcnfoCMTF08GFZ4TwMOP1tp_D2t8wcP98rRhx836OcXGCMX9ndOkTGcJXpFCk1VY2BxOoJ7-V-hc9P9sEDTw0RqiV0awgcE1XWG1Zwoq57teKRjdNVT6HdL08/s320/Screenshot_4.png?resize=450%2C578&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n  \n  \n  \nWhy are there so many colors? <p>\n</p><ul> <li>there are 16<sup>6</sup> = 16,777,216 different computer colors </li>\n<li>women tell me lipstick never looks same on a person as in a tube </li>\n<li>my dermatologist offered that lips have lines and grooves that create uneven coloring</li>\n<li>it is hard to isolate the image of just lips in a rectangle </li>\n<li>lighting creates distortions </li>\n<li>there are image resolution and quality issues </li>\n</ul>\n\n     \nNevertheless, I decided to do the following. For each of Taylor’s ten favorite lipsticks, I found the ten most frequent colors.\nFor each of the five Taylor lip photos, I found their ten most frequent colors.  Then for each photo, I calculated the average color distance from the photo to each lipstick.  The lipstick with the minimum distance from the photo was then the computer forecast matching a photo to a lipstick.\nGet it? <p>\n\n      \nHere are the five Taylor photos with the computer’s match: </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5oJGgzlsIyg8ku_KjwQgOR3WgEcpmx_83zjFUF-R3L57fIt492_mDua1vLPgpqY_h1hZm6Nm0p4KHA1Prv-nxPopr2crHvMuwflTjmTPnBjdv-mHjs7pKj3L1M8X6pvadWAkY0IlYA61KGM7rdj_USMdSVJ6LHQErALzRE8PCFBelifNVNAcnyNXsX6U/s1130/Screenshot_5.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5oJGgzlsIyg8ku_KjwQgOR3WgEcpmx_83zjFUF-R3L57fIt492_mDua1vLPgpqY_h1hZm6Nm0p4KHA1Prv-nxPopr2crHvMuwflTjmTPnBjdv-mHjs7pKj3L1M8X6pvadWAkY0IlYA61KGM7rdj_USMdSVJ6LHQErALzRE8PCFBelifNVNAcnyNXsX6U/s320/Screenshot_5.png?resize=450%2C490&amp;ssl=1\" data-original-height=\"490\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"490\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5oJGgzlsIyg8ku_KjwQgOR3WgEcpmx_83zjFUF-R3L57fIt492_mDua1vLPgpqY_h1hZm6Nm0p4KHA1Prv-nxPopr2crHvMuwflTjmTPnBjdv-mHjs7pKj3L1M8X6pvadWAkY0IlYA61KGM7rdj_USMdSVJ6LHQErALzRE8PCFBelifNVNAcnyNXsX6U/s320/Screenshot_5.png?resize=450%2C490&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBi7LXBcufNihCQPK6-MtAnTS7OS2_uQn6uD81j5CxuKt5U1wFHlRBm7NdXAV6HMcinkiduJ0f-6bsL6Tpb96Mwmn_1Y8bKlg1b0LKMjYeBbaWZJAbpshANuufjfj8NaQqg4sKLjquJa53yPW_tide1ED4SEveQwoNcnU8H5FVd7CZStui1AFWVF6velk/s1134/Screenshot_6.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBi7LXBcufNihCQPK6-MtAnTS7OS2_uQn6uD81j5CxuKt5U1wFHlRBm7NdXAV6HMcinkiduJ0f-6bsL6Tpb96Mwmn_1Y8bKlg1b0LKMjYeBbaWZJAbpshANuufjfj8NaQqg4sKLjquJa53yPW_tide1ED4SEveQwoNcnU8H5FVd7CZStui1AFWVF6velk/s320/Screenshot_6.png?resize=450%2C505&amp;ssl=1\" data-original-height=\"505\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"505\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBi7LXBcufNihCQPK6-MtAnTS7OS2_uQn6uD81j5CxuKt5U1wFHlRBm7NdXAV6HMcinkiduJ0f-6bsL6Tpb96Mwmn_1Y8bKlg1b0LKMjYeBbaWZJAbpshANuufjfj8NaQqg4sKLjquJa53yPW_tide1ED4SEveQwoNcnU8H5FVd7CZStui1AFWVF6velk/s320/Screenshot_6.png?resize=450%2C505&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYdZsJrG6yZFc3jOy-7GGsANCtZ5oi_K0akP1yDdLy0JxWKQUWVbE8LF8BDKJlV20KT5Wqz62WHNZXY19zFxYgc9H8e2x4TUGcR6RX63TmUQgGfgq0lU05GTuuHlstyREHHPTYSlvTGqjnM_H4OxiPKjiOkQkJZK6rNg8TXrvR9TADiYdFA2E5cVwogok/s571/Screenshot_7.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYdZsJrG6yZFc3jOy-7GGsANCtZ5oi_K0akP1yDdLy0JxWKQUWVbE8LF8BDKJlV20KT5Wqz62WHNZXY19zFxYgc9H8e2x4TUGcR6RX63TmUQgGfgq0lU05GTuuHlstyREHHPTYSlvTGqjnM_H4OxiPKjiOkQkJZK6rNg8TXrvR9TADiYdFA2E5cVwogok/s320/Screenshot_7.png?resize=320%2C495&amp;ssl=1\" data-original-height=\"495\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"320\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"495\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYdZsJrG6yZFc3jOy-7GGsANCtZ5oi_K0akP1yDdLy0JxWKQUWVbE8LF8BDKJlV20KT5Wqz62WHNZXY19zFxYgc9H8e2x4TUGcR6RX63TmUQgGfgq0lU05GTuuHlstyREHHPTYSlvTGqjnM_H4OxiPKjiOkQkJZK6rNg8TXrvR9TADiYdFA2E5cVwogok/s320/Screenshot_7.png?resize=320%2C495&amp;ssl=1\" width=\"320\"/></noscript></a></div>\n\n     \nIn summary, the computer thinks photo 1 has the minimum distance, 320, to its match.  Do you agree this is the best match? <p>\n</p><center>\n<font color=\"black\">\n<table ,=\"\" bgcolor=\"lightblue\" border=\"black\" cellpadding=\"15\" width=\"320\">\n<tr style=\"color: black; font-weight: bold;\"><td>Photo</td><td>Shade</td><td>Distance</td></tr>\n<tr><td>1</td><td>Ruby Woo</td><td>320</td></tr>\n<tr><td>2</td><td>Elson 4</td><td>625</td></tr>\n<tr><td>3</td><td>Blood Lust</td><td>388</td></tr>\n<tr><td>4</td><td>Flame</td><td>392</td></tr>\n<tr><td>5</td><td>Flame</td><td>399</td></tr>\n</table></font></center><p>\n</p><p>\n</p><h3 id=\"part3\">3.  Which of her songs are emotional?</h3><p>\n\n     \nThe R package ‘taylor’ contains lyrics for 11 albums and 240 songs.\nI will examine TS songs for their emotional content, as follows: </p><p>\n</p><ul>\n<li>% emotional words per song</li>\n<li>most frequent words per song</li>\n<li>trend in emotions over time</li>\n<li>examine theory that 5th song in every album is most emotional</li>\n</ul>\n\n     \nWhat makes a song emotional?  Some possibilities are: emotional words, volume, tempo. pitch, rhythm, and \ninstrumentation.  A few musicians I spoke to have a longer list.  But I don’t have the data to measure\nany of these, except emotional words. <p>\n  \n     \nLinguists have created lists of emotional words.  Mohammad and Turney created the \n  <a href=\"https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\" rel=\"nofollow\" target=\"_blank\">NRC Emotion Lexicon</a> \n  list of over 14,000 English words and their associations with ten basic emotions (anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, and trust). A word may have more than one emotion.  For example, faith has the emotions \nanticipation, joy, positive, and trust.</p><p>\n    \n     \nLike most datasets, Taylor Swift songs required a considerable amount of data cleaning. In a text analysis like this, data cleaning includes removing punctuation and capitals, deleting stop words (a, oh, the, etc.) that add no value, and fixing informal speech and \n colloquialisms (dyin’, gonna, etc.).  Taylor uses a large number of colloquialisms, often by dropping an ending g in an “ing” word.  Also part of the cleaning is that words are stemmed so that love, lovable, loving, and lovely all reduce to “lov”; and then the stemmed words are lemmatized and returned to their root word love.  The R packages SnowballC and textstem perform stemming and lemmatizing respectively.  However, this combination did not always produce a valid lemmatized word, and I manually adjusted about 500 words which I placed into a file lem_replacements_df.csv.  The final result is over 23,000 words from her 240 songs, which I am calling processed words. </p><p>\n   \n     \nFor all 240 songs combined, I calculated Taylor’s most frequent words, her most frequent emotional words, her number of words for each of the 10 emotions, and the ratio r = number of emotional words / number of processed) words.  The results are contained in the following three plots: </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3qwAszZVcg_C2iisTwTjb6DJXKg3jUD_LXTSLiLdklVRMbh482Kg6rFkpzklDjiUiEhb65tWx8Q9ztHweotsMqsOdA5P0_y1ZRqbGCszdV242-S5ljrcd3nTp3uiw7hi4Qun5sFdjfpYV0uSK4ZZAoOABdKwfK_o1Ydonp_aM7ohnnBCcy3X7oVqgOqA/s705/Screenshot_9.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3qwAszZVcg_C2iisTwTjb6DJXKg3jUD_LXTSLiLdklVRMbh482Kg6rFkpzklDjiUiEhb65tWx8Q9ztHweotsMqsOdA5P0_y1ZRqbGCszdV242-S5ljrcd3nTp3uiw7hi4Qun5sFdjfpYV0uSK4ZZAoOABdKwfK_o1Ydonp_aM7ohnnBCcy3X7oVqgOqA/s320/Screenshot_9.png?resize=450%2C608&amp;ssl=1\" data-original-height=\"608\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"608\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3qwAszZVcg_C2iisTwTjb6DJXKg3jUD_LXTSLiLdklVRMbh482Kg6rFkpzklDjiUiEhb65tWx8Q9ztHweotsMqsOdA5P0_y1ZRqbGCszdV242-S5ljrcd3nTp3uiw7hi4Qun5sFdjfpYV0uSK4ZZAoOABdKwfK_o1Ydonp_aM7ohnnBCcy3X7oVqgOqA/s320/Screenshot_9.png?resize=450%2C608&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5CCOxIuxWnF3HXYJ5jpPrk88qWM8XiTKPKMWoV1hgrvI4NdeabX6PeflDIGLntHBW6fEUdo4KhRBPlyEYF-WonEjWZRhfH33p6wBL_K605AANRIAcA0oa4zkEfvuQzYX7G26CrqI5P7jkIaG3L281vPoH57Md_PkKWabhy1fZHctXsZDHj5Raj5ytc8E/s678/Screenshot_10.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5CCOxIuxWnF3HXYJ5jpPrk88qWM8XiTKPKMWoV1hgrvI4NdeabX6PeflDIGLntHBW6fEUdo4KhRBPlyEYF-WonEjWZRhfH33p6wBL_K605AANRIAcA0oa4zkEfvuQzYX7G26CrqI5P7jkIaG3L281vPoH57Md_PkKWabhy1fZHctXsZDHj5Raj5ytc8E/s320/Screenshot_10.png?resize=450%2C603&amp;ssl=1\" data-original-height=\"603\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"603\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5CCOxIuxWnF3HXYJ5jpPrk88qWM8XiTKPKMWoV1hgrvI4NdeabX6PeflDIGLntHBW6fEUdo4KhRBPlyEYF-WonEjWZRhfH33p6wBL_K605AANRIAcA0oa4zkEfvuQzYX7G26CrqI5P7jkIaG3L281vPoH57Md_PkKWabhy1fZHctXsZDHj5Raj5ytc8E/s320/Screenshot_10.png?resize=450%2C603&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1Kp9f_jiMwQ33Nv9cCwmJ044jLzaep1GDyq2ZgexBmIO6ApQQmuvzwfKPu_vqyWxaKS3MSQp1JY3EDxsE7RkfQh-wviwWbwPN-0WPuL83FyahqdiH4vkg806sXRKuQ3gKdBKebqYIgRxc3yuSZNyV9ERpSfrcCOTuhaCHULsQ_09HeekAQgEqrn349xo/s715/Screenshot_11.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1Kp9f_jiMwQ33Nv9cCwmJ044jLzaep1GDyq2ZgexBmIO6ApQQmuvzwfKPu_vqyWxaKS3MSQp1JY3EDxsE7RkfQh-wviwWbwPN-0WPuL83FyahqdiH4vkg806sXRKuQ3gKdBKebqYIgRxc3yuSZNyV9ERpSfrcCOTuhaCHULsQ_09HeekAQgEqrn349xo/s320/Screenshot_11.png?resize=450%2C595&amp;ssl=1\" data-original-height=\"595\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"595\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1Kp9f_jiMwQ33Nv9cCwmJ044jLzaep1GDyq2ZgexBmIO6ApQQmuvzwfKPu_vqyWxaKS3MSQp1JY3EDxsE7RkfQh-wviwWbwPN-0WPuL83FyahqdiH4vkg806sXRKuQ3gKdBKebqYIgRxc3yuSZNyV9ERpSfrcCOTuhaCHULsQ_09HeekAQgEqrn349xo/s320/Screenshot_11.png?resize=450%2C595&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n\n     \nThe above is a baseline for all songs combined. The r value is .364. I asked a friend whose teen-aged daughter is a Swiftie, to suggest a few Taylor songs that she considers especially emotional, and I created the same types of plots for some of these songs.  I’ll share the plots for the song “My Tears Ricochet”, which had a greater than average r value:\n\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhe2ojHhA1VxwV35aLnkb0R9kd9rSltMK___1LtuFmc5z37dTRu93_2ttLFM878spsM-hM6DkuY-8m-Lyf4Id6XXEcqIugMEA8lazmovnxUC3El0U4qJcCOqgnybvxGx-MJeOnQpTVYqjxDhrXpe25cVzYvINj5i54L9R1LwkgB9cMBh9jBUcNBY-8c6Pk/s651/Screenshot_12.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhe2ojHhA1VxwV35aLnkb0R9kd9rSltMK___1LtuFmc5z37dTRu93_2ttLFM878spsM-hM6DkuY-8m-Lyf4Id6XXEcqIugMEA8lazmovnxUC3El0U4qJcCOqgnybvxGx-MJeOnQpTVYqjxDhrXpe25cVzYvINj5i54L9R1LwkgB9cMBh9jBUcNBY-8c6Pk/s320/Screenshot_12.png?resize=450%2C441&amp;ssl=1\" data-original-height=\"441\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"441\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhe2ojHhA1VxwV35aLnkb0R9kd9rSltMK___1LtuFmc5z37dTRu93_2ttLFM878spsM-hM6DkuY-8m-Lyf4Id6XXEcqIugMEA8lazmovnxUC3El0U4qJcCOqgnybvxGx-MJeOnQpTVYqjxDhrXpe25cVzYvINj5i54L9R1LwkgB9cMBh9jBUcNBY-8c6Pk/s320/Screenshot_12.png?resize=450%2C441&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpfv5l1vVxaCxybylB8XFjnZ13WPJ8q-nnUVmlNVPtTRVcQjcfcTsq1HHWNOLlaC9n1xydtLgkOXnBud8U18vJ2zlfsylQ8ekxln_BfGSYNgItAHQcTcoMIOg1vQjrK8vxPh6DNNV1JVh8FQ8EiGf1xPLOoUkDAZ3p_LboItMIel7De72G4nyYihQU3og/s655/Screenshot_13.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpfv5l1vVxaCxybylB8XFjnZ13WPJ8q-nnUVmlNVPtTRVcQjcfcTsq1HHWNOLlaC9n1xydtLgkOXnBud8U18vJ2zlfsylQ8ekxln_BfGSYNgItAHQcTcoMIOg1vQjrK8vxPh6DNNV1JVh8FQ8EiGf1xPLOoUkDAZ3p_LboItMIel7De72G4nyYihQU3og/s320/Screenshot_13.png?resize=450%2C425&amp;ssl=1\" data-original-height=\"425\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"425\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpfv5l1vVxaCxybylB8XFjnZ13WPJ8q-nnUVmlNVPtTRVcQjcfcTsq1HHWNOLlaC9n1xydtLgkOXnBud8U18vJ2zlfsylQ8ekxln_BfGSYNgItAHQcTcoMIOg1vQjrK8vxPh6DNNV1JVh8FQ8EiGf1xPLOoUkDAZ3p_LboItMIel7De72G4nyYihQU3og/s320/Screenshot_13.png?resize=450%2C425&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n<div class=\"separator\" style=\"clear: both;\"><a href=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuYVI4CImkSSlMnZ1EvxQ_jPWXYAbwKjddoppNw2YdSu1PjMyQaiuEBpcgq0M_kXiDmkDzTg4ka82HV_mx20yalBTU9fRfpOGTk7wOpFxf_5wjpzxMEO3pS6DwA_AcVuWzFSSpo1JvR6pLhJqGoKx_Gia0wesBkUPxNtl10NK3lakfSoJu8AY7EnTzwM/s690/Screenshot_14.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuYVI4CImkSSlMnZ1EvxQ_jPWXYAbwKjddoppNw2YdSu1PjMyQaiuEBpcgq0M_kXiDmkDzTg4ka82HV_mx20yalBTU9fRfpOGTk7wOpFxf_5wjpzxMEO3pS6DwA_AcVuWzFSSpo1JvR6pLhJqGoKx_Gia0wesBkUPxNtl10NK3lakfSoJu8AY7EnTzwM/s320/Screenshot_14.png?resize=450%2C475&amp;ssl=1\" data-original-height=\"475\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"475\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuYVI4CImkSSlMnZ1EvxQ_jPWXYAbwKjddoppNw2YdSu1PjMyQaiuEBpcgq0M_kXiDmkDzTg4ka82HV_mx20yalBTU9fRfpOGTk7wOpFxf_5wjpzxMEO3pS6DwA_AcVuWzFSSpo1JvR6pLhJqGoKx_Gia0wesBkUPxNtl10NK3lakfSoJu8AY7EnTzwM/s320/Screenshot_14.png?resize=450%2C475&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n\n     \nNote that in the above song, the counts are pretty small. Perhaps this is one explanation for why\na number of songs that we thought were emotional did not have a higher than average r value.  <p>\n\n     \nAnother question I was curious about is whether Taylor’s songs have become more emotional over time. I am using original release date of album as the time variable.  For songs that Taylor re-recorded, I am using the original release date under the assumption that her re-release may have changed the music but not the lyrics.</p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHJh1B-D4360PyMa2f1OblCvgrdKxU-2XbOrYOrWMe5YM6Wa4s8F2o_l_vBdQEKal4YArxwBg-FL1fbHXfdRD81UB2Ye2Pp7egoyqsoJuHlvcWgJ_40Yt1SbUF9xLEcAxTUtK4KbbBFDhD4M4zuEzycq8W_Z0YYjlkicbdnwzVPm9gvOu12Cq1h_-dyZA/s551/Screenshot_15.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHJh1B-D4360PyMa2f1OblCvgrdKxU-2XbOrYOrWMe5YM6Wa4s8F2o_l_vBdQEKal4YArxwBg-FL1fbHXfdRD81UB2Ye2Pp7egoyqsoJuHlvcWgJ_40Yt1SbUF9xLEcAxTUtK4KbbBFDhD4M4zuEzycq8W_Z0YYjlkicbdnwzVPm9gvOu12Cq1h_-dyZA/s320/Screenshot_15.png?resize=450%2C489&amp;ssl=1\" data-original-height=\"489\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"489\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHJh1B-D4360PyMa2f1OblCvgrdKxU-2XbOrYOrWMe5YM6Wa4s8F2o_l_vBdQEKal4YArxwBg-FL1fbHXfdRD81UB2Ye2Pp7egoyqsoJuHlvcWgJ_40Yt1SbUF9xLEcAxTUtK4KbbBFDhD4M4zuEzycq8W_Z0YYjlkicbdnwzVPm9gvOu12Cq1h_-dyZA/s320/Screenshot_15.png?resize=450%2C489&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n\n     \nThat her songs are becoming more emotional over time appears not to be true.  To the contrary, the slope of a poor fitting trend line is negative, but its F-statistic says the linear model is not significant. <p>\n\n\n     \nFinally, there is a theory that every fifth song on an album is intentionally more emotional than the others. </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i2.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDQNBzBOXxDiUsxTTANfKv4eTuEjvo4iYcBh9Hh084JS7-yw2hD3yzL1K0G5Lr1MeeMPlIm1WH2DxmOCCkIqRQwGdQL8Hh8aNnwaTnM6mbD_Q-1TI26zcAB7e74pQnl-hcBGJ2AhS0qK_ZFK_ImQsuzP15MIlUdPtgbG17xgPGTuNQdov7KMLHH7JwqG4/s501/Screenshot_16.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDQNBzBOXxDiUsxTTANfKv4eTuEjvo4iYcBh9Hh084JS7-yw2hD3yzL1K0G5Lr1MeeMPlIm1WH2DxmOCCkIqRQwGdQL8Hh8aNnwaTnM6mbD_Q-1TI26zcAB7e74pQnl-hcBGJ2AhS0qK_ZFK_ImQsuzP15MIlUdPtgbG17xgPGTuNQdov7KMLHH7JwqG4/s320/Screenshot_16.png?resize=450%2C438&amp;ssl=1\" data-original-height=\"438\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"438\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDQNBzBOXxDiUsxTTANfKv4eTuEjvo4iYcBh9Hh084JS7-yw2hD3yzL1K0G5Lr1MeeMPlIm1WH2DxmOCCkIqRQwGdQL8Hh8aNnwaTnM6mbD_Q-1TI26zcAB7e74pQnl-hcBGJ2AhS0qK_ZFK_ImQsuzP15MIlUdPtgbG17xgPGTuNQdov7KMLHH7JwqG4/s320/Screenshot_16.png?resize=450%2C438&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n\n     \nThis <a href=\"https://www.vulture.com/article/taylor-swift-track-5-songs-ranked.html\" rel=\"nofollow\" target=\"_blank\">theory</a> is discussed on the Internet and even Taylor gives it some credence.  Albums by release year provide 10 data points (there are 11 albums, but two were released in the same year). In 6 years out of 10 the track 5 r ratio exceeded the all other tracks, and in 4 years out of 10 they did not.   \nA two-sample t-test with one tail concludes that the track 5 mean is not significantly greater than the all other tracks mean, p = .4591. \nNote that track 5 has an outlier, “All you had to do was stay,” which sounds like it should have a high r, but its r was .134. <p>\n</p><h3>4.  Bonus:  &amp;$!#%</h3><p>\n     \nTo the Taylor Tots parents: \nOur Taylor has been known to use an R-rated word in her songs, or two.  Here is the frequency of some of her most frequent such words (and there are others), mostly from her Tortured Poets Department album: </p><p>\n</p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVhL4H169PiDFbKEMxHxtfx1QbneWH3cjsyR1fC9-HQB5WifSNaoyIrqwxjOoh9pHW_gtdg5o-oBNKF4vrVIlwPdnzdehFXNF8R4oXMBewcITGcQwGrxnADibzrtuuhYDnFd5hsHQiEvnNJZ6nTFba_sGiwzI1ViXBvw5EiC109sZFPxY1Dt5U94nm1so/s561/Screenshot_8.png?ssl=1\" rel=\"nofollow\" style=\"display: block; padding: 1em 0; text-align: center; \" target=\"_blank\"><img alt=\"\" border=\"0\" data-lazy-src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVhL4H169PiDFbKEMxHxtfx1QbneWH3cjsyR1fC9-HQB5WifSNaoyIrqwxjOoh9pHW_gtdg5o-oBNKF4vrVIlwPdnzdehFXNF8R4oXMBewcITGcQwGrxnADibzrtuuhYDnFd5hsHQiEvnNJZ6nTFba_sGiwzI1ViXBvw5EiC109sZFPxY1Dt5U94nm1so/s320/Screenshot_8.png?resize=450%2C483&amp;ssl=1\" data-original-height=\"483\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"\" border=\"0\" data-original-height=\"483\" data-original-width=\"450\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVhL4H169PiDFbKEMxHxtfx1QbneWH3cjsyR1fC9-HQB5WifSNaoyIrqwxjOoh9pHW_gtdg5o-oBNKF4vrVIlwPdnzdehFXNF8R4oXMBewcITGcQwGrxnADibzrtuuhYDnFd5hsHQiEvnNJZ6nTFba_sGiwzI1ViXBvw5EiC109sZFPxY1Dt5U94nm1so/s320/Screenshot_8.png?resize=450%2C483&amp;ssl=1\" width=\"450\"/></noscript></a></div>\n\n     \nAs I mentioned earlier, each of the three parts of this analysis – her attractiveness, her lipstick shades, and her lyrics – contains various imprecisions and judgments.  Perhaps some day I will go back to this project and make some improvements. Comments are welcome.\nBut I think it was a non-routine use of math.  And it was fun!\n\n\n<hr style=\"border-top: 10px solid #00008B;\"/>\n<p>\n     \nThis project uses three R script files and a number of image files.  The R code is shown below, but it is long. Both the R code files and the image files may be found at  <a href=\"https://github.com/fcas80/The-Taylor-Swift-Project/tree/main\" rel=\"nofollow\" target=\"_blank\">https://github.com/fcas80/The-Taylor-Swift-Project/tree/main</a>\n</p><pre>\n# file 1 of 3: lovely.txt\n\nlibrary(png)\nlibrary(ggplot2)\nlibrary(grid)\n\nimage_path &lt;- \"my_path/ts1_phi_removebg.png\"\nimg &lt;- readPNG(image_path, native=TRUE)   # width 584, height 457\nheight &lt;- nrow(img)   # 383\nwidth &lt;- ncol(img)    # 331\nraster_img &lt;- grid::rasterGrob(img, interpolate = TRUE)\ndf &lt;- data.frame(xpos = c(0, width), ypos = c(0, height))\n\n# 1. plot photo on minimum grid\nggplot(data = df,\n       aes(xpos, ypos)) +\n  xlim(0, width) + ylim(0, height) +\n  geom_blank() +\n  annotation_custom(raster_img, xmin=0, xmax=width, ymin=0, ymax=height) + \n  theme(axis.title.x = element_blank(), \n      axis.title.y = element_blank())\n\n# 2. plot photo on more detailed grid; measurements are manual and imprecise\nggplot(data = df,\n       aes(xpos, ypos)) +\n  xlim(0, width) + ylim(0, height) +\n  geom_blank() +\n  annotation_custom(raster_img, xmin=0, xmax=width, ymin=0, ymax=height) +\n  geom_hline(yintercept = seq(0, height, by = 10), color = \"gray\", linwidth = 0.5) +\n  geom_vline(xintercept = seq(0, width, by = 10), color = \"gray\", linwidth = 0.5) +\n  scale_x_continuous(breaks = seq(0, width, by = 20)) +\n  scale_y_continuous(breaks = seq(0, height, by = 20)) +\n  annotate(\"segment\", x = 90, y = 265, xend = 90, yend = 0, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 70, y = 90, xend = 250, yend = 90, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 130, y = 170, xend = 180, yend = 170, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 135, y = 105, xend = 180, yend = 105, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 125, y = 70, xend = 195, yend = 70, color = \"blue\", linwidth = 3) +\n  annotate(\"segment\", x = 160, y = 85, xend = 160, yend = 0, color = \"blue\", linwidth = 3) +\n  annotate(\"segment\", x = 50, y = 160, xend = 50, yend = 90, color = \"red\", linwidth = 3) +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank())\n\nsegments_df &lt;- data.frame(\n  segment_name = c(\"face_length\", \"face_width\", \"dist_bet_eyes\", \n                   \"nose_width\", \"mouth_width\", \"lips_to_chin\", \"ear_length\"),\n  x = c(90, 70, 130, 135, 125, 160, 50),\n  y = c(265, 90, 170, 105, 70, 85, 160),\n  xend = c(90, 250, 180, 180, 195, 160, 50),\n  yend = c(0, 90, 170, 105, 70, 0, 90))\nsegments_df$dist &lt;- sqrt((segments_df$x - segments_df$xend)^2 + (segments_df$y - segments_df$yend)^2)\nsegments_df\n\nratios_df &lt;- data.frame(\n  ratio_name = c(\"face length / width\", \"mouth width / interocular\", \"mouth width / nose width\", \n       \"lips to chin / interocular\", \"lips to chin / nose width\", \"ear length / nose width\"),\n  ratio = rep(0, times=6))\n\nratios_df$ratio[1] &lt;- round(segments_df$dist[1] / segments_df$dist[2], 4)   # face length / width\nratios_df$ratio[2] &lt;- round(segments_df$dist[5] / segments_df$dist[3], 4)   # mouth width / interocular\nratios_df$ratio[3] &lt;- round(segments_df$dist[5] / segments_df$dist[4], 4)   # mouth width / nose width\nratios_df$ratio[4] &lt;- round(segments_df$dist[6] / segments_df$dist[3], 4)   # lips to chin / interocular\nratios_df$ratio[5] &lt;- round(segments_df$dist[6] / segments_df$dist[4], 4)   # lips to chin / nose width\nratios_df$ratio[6] &lt;- round(segments_df$dist[7] / segments_df$dist[4], 4)   # ear length / nose width \nratios_df\nm &lt;- round(mean(ratios_df$ratio),3)\nm\nerror &lt;- round((m - (1+sqrt(5))/2)/m,3)\nerror\n# END\n</pre>\n<hr style=\"border-top: 10px solid #00008B;\"/>\n<pre>\n# file 2 of 3: lipsticks.txt\n\n\nsetwd(\"my_path\")\nlibrary(colouR)\nlibrary(ggplot2)\n\n# These shades were determined from various Internet articles\nshade &lt;- c(\"Ruby Woo\", \"Morocco\", \"Dragon Girl\", \"Elson 4\", \"Always Red\", \"Kyoto Red\", \n           \"Eadie Scarlet\", \"Blood Lust\", \"Flame\", \"Red to Go\")\n\n# These swatches of shades were copied from various Internet stores\ndf_lipsticks &lt;- data.frame(shade,\n        file = c(\"Mac_Ruby_Woo.png\", \"NARS_morocco.png\", \"NARS_Dragon_Girl.png\", \"PatMcGrath_Elson4.png\", \n           \"Sephora_always_red.png\", \"Tatcha_Kyoto_Red.png\", \"Gucci_Velvet_Eadie_Scarlet.png\", \n           \"PatMcGrath_Blood_Lust.png\", \"TomFord_flame.png\", \"Armani_red_to_go.png\" ))\n\n# These files were created by copying various photos of TS, cropping rectangle of lips, then\n# using https://www.remove.bg/ to remove backgrounds  \nfile = c(\"ts1_lips-removebg-preview.png\", \"ts2_lips-removebg-preview.png\",\n       \"ts3_lips-removebg-preview.png\", \"ts4_lips-removebg-preview.png\",\"ts5_lips-removebg-preview.png\")\nname = c(\"TS photo 1\", \"TS photo 2\", \"TS photo 3\", \"TS photo 4\" ,\"TS photo 5\")\ndf_photos &lt;- data.frame(file, name)\n\n############\n\n# p10 for ten lipstick shades, but some only have 1 color\np10 &lt;- function(image, name) {\n  # Get primary color\n  primary_color &lt;- colouR::getTopCol(image, n = 1, avgCols = FALSE, exclude = TRUE)\n  \n  top10 &lt;- tryCatch(\n    {\n      colouR::getTopCol(image, n = 10, avgCols = FALSE, exclude = TRUE)\n    },\n    error = function(e) {\n      primary_color\n    }\n  )\n  \n  if (nrow(top10) &lt; 10) {\n    top10 &lt;- rbind(top10, top10[rep(1, 10 - nrow(top10)),])\n  }\n  \n  plot &lt;- ggplot(top10, aes(x = hex, y = freq, fill = hex)) +\n    geom_bar(stat = 'identity') +\n    scale_fill_manual(values = top10$hex) +\n    labs(title = paste(name, \"Top 10 colors by frequency\")) +\n    xlab(\"HEX color code\") + ylab(\"Frequency\") +\n    theme(\n      legend.position = \"none\",\n      plot.title = element_text(size = 15, face = \"bold\"),\n      axis.title = element_text(size = 15, face = \"bold\"),\n      axis.text.x = element_text(angle = 45, hjust = 1, size = 12, face = \"bold\")\n    )\n  print(plot)\n  return(top10)\n}   # close p function\n\nshade_dataframe &lt;- data.frame(matrix(ncol = 10, nrow = 0))\n\nfor (i in 1:10) {\n  result &lt;- p10(df_lipsticks$file[i], df_lipsticks$shade[i])\n  \n  # Repeat the first color if there are fewer than 10 colors\n  if (nrow(result) &lt; 10) {\n    result &lt;- rbind(result, result[rep(1, 10 - nrow(result)),])\n  }\n  \n  shade_dataframe &lt;- rbind(shade_dataframe, t(result$hex))\n}\n\ncolnames(shade_dataframe) &lt;- paste0(\"Color\", 1:10)\nprint(shade_dataframe)\n\n#############\n\np5 &lt;- function(image, name) {\n   top10 &lt;- colouR::getTopCol(image, n = 10, avgCols = FALSE, exclude = TRUE)\n   return(top10)\n}\n\ndf_ts_colors &lt;- data.frame()\nfor(i in 1:nrow(df_photos)){\n   top10_colors &lt;- p5(df_photos$file[i], df_photos$name[i])\n   top10_colors &lt;- top10_colors$hex\n   df_ts_colors &lt;- if(i == 1) {\n     df_ts_colors &lt;- top10_colors\n    } else {rbind(df_ts_colors, top10_colors)\n   }\n}\ncolnames(df_ts_colors) &lt;- c(\"color1\",\"color2\",\"color3\",\"color4\",\"color5\",\n                         \"color6\",\"color7\",\"color8\",\"color9\",\"color10\")\nrownames(df_ts_colors) &lt;- df_photos$name\ndf_ts_colors &lt;- t(df_ts_colors)\nprint(df_ts_colors)\n\n# begin total RGB distance of a TS photo to centroid of lipstick shade\n\n# sample 3d plot\nlibrary(scatterplot3d)\nm &lt;- data.frame(\n     x = c(174,134),\n     y = c(31,26),\n     z = c(61,35))\n\ns3d &lt;- scatterplot3d(m$x, m$y, m$z, pch = 19, color = \"blue\", \n    xlim = c(100, 200), ylim = c(0, 50), zlim = c(0, 100), \n    xlab = expression(bold(\"R-axis\")), ylab = expression(bold(\"G-axis\")), zlab = expression(bold(\"B-axis\")), \n    main = \"Plot of two colors in RGB space\", cex.main = 2)\n\ns3d$points3d(m$x, m$y, m$z, type = \"l\", lty = 1, lwd = 2, col = \"red\")\n\n# Use 'text' for adding labels with bold font\ntext(s3d$xyz.convert(m$x, m$y, m$z), labels = paste0(\"(\", m$x, \",\", m$y, \",\", m$z, \")\"), \n     pos = 3, cex = 2, col = \"#00008B\", font = 2)\n\nA &lt;- shade_dataframe$Color1\nB &lt;- shade_dataframe$Color2\nC &lt;- shade_dataframe$Color3\nD &lt;- shade_dataframe$Color4\nE &lt;- shade_dataframe$Color5\nF &lt;- shade_dataframe$Color6\nG &lt;- shade_dataframe$Color7\nH &lt;- shade_dataframe$Color8\nI &lt;- shade_dataframe$Color9\nJ &lt;- shade_dataframe$Color10\n\n\nhex_to_rgb &lt;- function(hex) {\n  rgb &lt;- t(col2rgb(hex))\n  return(data.frame(r=rgb[1,], g=rgb[2,], b=rgb[3,]))\n}\n\n# Function to calculate Euclidean distance between two RGB vectors\neuclidean_distance &lt;- function(rgb1, rgb2) {\n  sqrt(sum((rgb1 - rgb2)^2))\n}\n\n# Function to calculate centroid of a vector of hex colors\ncalculate_centroid &lt;- function(hex_colors) {\n  rgb_values &lt;- hex_to_rgb(hex_colors)\n  centroid &lt;- colMeans(rgb_values)\n  return(centroid)\n}\n\n# Calculate centroids of shades, e.g., centroid_A &lt;- calculate_centroid(A)\n  centroids &lt;- lapply(list(A, B, C, D, E, F, G, H, I, J), calculate_centroid)\n\nfor (j in 1:nrow(df_photos)) {\n   x &lt;- df_ts_colors[,j]\n   centroid_x &lt;- calculate_centroid(x)\n# Calculate total distance of x to each centroid\n   total_distance_to_centroid &lt;- function(x, centroid) {\n     x_rgb &lt;- hex_to_rgb(x)\n     total &lt;- 0\n     for (i in 1:nrow(x_rgb)) {\n       total &lt;- total + euclidean_distance(x_rgb[i, ], centroid)\n     }\n     return(total)\n   }\n\n  distances &lt;- sapply(centroids, total_distance_to_centroid, x = x)\n  w &lt;- which(distances == min(distances))\n  most_similar &lt;- shade[w]\n  cat(\"The lipstick most similar to\", df_photos$name[j], \"is:\", most_similar, \n      \"with distance of \", min(distances), \"\\n\")\n }   #  end for loop on photos\n  \n# END\n</pre>\n<hr style=\"border-top: 10px solid #00008B;\"/>\n<pre>\nfile 3 of 3:  lyrics.txt\n\nlibrary(taylor)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\ndata(stop_words)\nlibrary(textclean)\nlibrary(SnowballC) # For stemming\nlibrary(textstem) # For lemmatization\n\n######################################\n# functions, preliminaries\n\n# function to print more rows\nmore_rows &lt;- function(filename){\n  options(tibble.print_max = nrow(filename), tibble.print_min = nrow(filename))\n  print(filename)\n}\n\n# function to fix informal speech (colloquialisms); also contractions, punctuation, lower case\ncolloq &lt;- function(df) {\n  pattern &lt;- \"ahh|Ah-uh|ahah|eh|haah|haha|iiii|Oh|oh|Oh-oh|oh-oh|ooh-oh|oohah|Uh|Uh-oh|uh-oh-oh-oh|ya|La-la-la|lala|la-la-la|la-la-la-la|Mm-mm|Mmm-mm|mm-mm|mm-mm-mm-mm|Ha-ha-ha\"\n  df &lt;- df %&gt;%\n    mutate(lyric = str_remove_all(lyric, paste0(\"\\\\b(\", pattern, \")\\\\b\"))) %&gt;%\n    mutate(lyric = str_replace_all(lyric, \n               c(\"ain't\" = \"is not\", \n                 \"Beggin'\" = \"begging\", \"beggin'\" = \"begging\", \"birth right\" = \"birthright\", \"blood-soaked\" = \"blood soaked\",\n                 \"'bout\" = \"about\", \"burnin'\" = \"burning\", \"callin'\" = \"calling\", \"'Cause\" = \"because\", \"'cause\" = \"because\", \"Cept\" = \"Except\", \"cursee\" = \"cure\",\n                 \"convers\" = \"conversation\",\"crashin'\" = \"crashing\", \"doin'\" = \"doing\", \"driftin'\" = \"drifting\", \"dyin'\" = \"dying\", \"'em\" = \"them\", \"feelin'\" = \"feeling\", \"flyin'\" = \"flying\",\"feverishlying\" = \"feverishly\", \"'fore\" = \"before\", \"'Fore\" = \"before\", \"foreyesgn\" = \"foreign\",\n                 \"everythi-i-ing\" = \"everything\",\"fuckin'\"=\"fuck\", \"gettin'\" = \"getting\", \"gonna\" = \"going to\", \"gotta\" = \"got to\", \"happ'nin'\" = \"happening\", \"haven't\" = \"have not\", \"Holdin'\" = \"holding\", \"hero's\" = \"hero\",\n                 \"Hopin'\" = \"hoping\", \"hopin'\" = \"hoping\",\"I'ma\" = \"I am going to\", \"kinda\"=\"kind of\", \"king's\" = \"king\", \"keepin\" = \"keeping\", \"Laughin'\" = \"laughing\", \"lookin'\" = \"looking\", \"losin\" = \"losing\", \"losingg\" = \"losing\", \"lovin'\" = \"loving\", \"lucki\" = \"lucky\", \"no-one\" = \"no one\", \"Nothin'\" = \"nothing\", \"nothin'\" = \"nothing\",\"one-hand\" = \"one hand\",\n                 \"mornin'\" = \"morning\", \"'nother\"=\"another\", \"nothin'\" = \"nothing\", \"pickin'\" = \"picking\", \"post-mortem\" = \"postmortem\", \"prayin'\" = \"praying\", \"Prayin'\" = \"praying\", \"pretti\" = \"pretty\", \"ridin'\" = \"riding\", \"Sneakin'\" = \"sneaking\",\n                 \"outta\" = \"out of\", \"'round\" = \"around\", \"self-implode\" = \"self implode\", \"shoulda\" = \"should have\", \"standin'\" = \"standing\", \"summer's\" = \"summer is\", \"There's\" = \"there is\", \"Thinkin'\" = \"thinking\", \n                 \"thankin'\" = \"thanking\", \"thinkin'\" = \"thinking\", \"'Til\" = \"until\", \"'til\" = \"until\", \"tryin'\" = \"trying\", \"tryna\" = \"trying to\", \"Tryna\" = \"trying to\",\"twin-sized\" = \"twin sized\", \n                 \"waitin'\" = \"waiting\", \"white-knuckle\" = \"white knuckle\",\n                 \"wanna\" = \"want to\", \"weepin'\" = \"weeping\", \"whatcha\" = \"what are you\", \"Where's\" = \"where is\", \"Why'd\" = \"why did\",\n                 \"wide-eyed\" = \"wide eyed\", \"wonderin'\" = \"wondering\", \"Wonderin'\" = \"wondering\"))) %&gt;%\n    mutate(lyric = map_chr(lyric, clean_text))\n  \n  return(df)\n}\n\n\ncustom_stop_words &lt;- bind_rows(stop_words, \n           tibble(word = c(\"ah\", \"dadadada\", \"ha\", \"haha\", \"hey\", \"hi\", \"huh\", \"la\", \"MMM\", \"mmm\", \"mmmm\", \"mm\", \"na\", \"oh\", \"okay\",\"OK\", \"ok\", \"ooh\", \n           \"uh\", \"whoa\", \"woah\", \"yeah\"), lexicon = \"custom\"))\n\n\n# Function to clean text  (included in colloq function)\nclean_text &lt;- function(text) { \n  text %&gt;% \n    replace_contraction() %&gt;% \n    str_remove_all(\"[[:punct:]]\") %&gt;% \n    str_to_lower()\n}\n\nreplacements_df &lt;- read.csv(\"my_path/lem_replacements_df.csv\", header = TRUE)\n\n# Function to stem and lemmatize\npro &lt;- function(words_df) {\n  words_df &lt;- words_df %&gt;% filter(!is.na(word))\n  processed_words &lt;- words_df %&gt;%\n    mutate(stemmed_word = wordStem(word),\n           lemmatized_word = lemmatize_words(stemmed_word)) %&gt;%\n    filter(lemmatized_word != \"\") %&gt;%  # Filter out empty strings\n    left_join(replacements_df, by = c(\"lemmatized_word\" = \"stemmed_word\")) %&gt;%\n    mutate(lemmatized_word = ifelse(is.na(lemmatized_word.y), lemmatized_word, lemmatized_word.y)) %&gt;%\n    select(-lemmatized_word.y)  # Remove the extra column created during the join\n}\n\nnrc_sentiments &lt;- get_sentiments(\"nrc\") %&gt;%\n  filter(word != \"count\" &amp; word != \"heel\" &amp; word != \"truck\" &amp; word != \"wear\" &amp; word != \"word\")\n\ncommon_theme &lt;- theme(\n  legend.position=\"NULL\",\n  plot.title = element_text(size = 15, face = \"bold\"),\n  plot.subtitle = element_text(size = 12.5, face = \"bold\"),\n  axis.title = element_text(size = 15, face = \"bold\"),\n  axis.text = element_text(size = 15, face = \"bold\"),\n  legend.title = element_text(size = 15, face = \"bold\"),\n  legend.text = element_text(size = 15, face = \"bold\"))\n\nsentiment_colors &lt;- c(\"anger\" = \"red\", \"anticipation\" = \"green\", \"disgust\" = \"brown\",\n                      \"fear\" = \"purple\", \"joy\" = \"yellow\", \"negative\" = \"gray\", \"positive\" = \"lightblue\",\n                      \"sadness\" = \"blue\", \"surprise\" = \"pink\", \"trust\" = \"turquoise\")\n\nemotion_plot &lt;- function(sentiment_summary, song, r){\n  ggplot(sentiment_summary, aes(x = sentiment, y = word_count, fill = sentiment)) + \n    geom_col(color = \"black\") + \n    scale_fill_manual(values = sentiment_colors) +\n    labs(title = paste(song, \"# words by emotion\"),\n         subtitle = paste(\"emotional words / all (processed) words =\", r), \n         x = \"Emotion\", \n         y = \"Number of Words\") + \n    common_theme + \n    theme(axis.text.x = element_text(angle = 45, hjust=1))\n}\n\nlemmatized_colors &lt;- c(\"red\", \"blue\", \"green\", \"orange\", \n                       \"purple\", \"cyan\", \"magenta\", \"yellow\", \"brown\", \"pink\")\n\nword_plot &lt;- function(word_summary, song, title){\n  ggplot(word_summary, aes(x = lemmatized_word, y = total_count, fill = lemmatized_word)) + \n    geom_col(color = \"black\") + \n    scale_fill_manual(values = lemmatized_colors) +\n    labs(title = paste(song, title),\n         x = \"\", \n         y = \"Frequency\") + \n    common_theme +\n    theme(axis.text.x = element_text(angle = 45, hjust=1))\n}\n\nall_albums &lt;- c(\"Taylor Swift\", \"Fearless (Taylor's Version)\", \"Speak Now (Taylor's Version)\", \"Red (Taylor's Version)\", \"1989 (Taylor's Version)\",\n                \"reputation\", \"Lover\", \"folklore\", \"evermore\",  \"Midnights\", \"THE TORTURED POETS DEPARTMENT\")\nnumber_of_tracks &lt;- c(15, 26, 22, 30, 22, 15, 18, 17, 17, 19, 31)\noriginal_release_year &lt;- c(2006, 2008, 2010, 2012, 2014, 2017, 2019, 2020, 2020, 2022, 2024)\n# The years are original release years, i.e., not \"Taylor's version\" years\n\n#######################################\n\n# Find song by word in title - could be lower case!\nx &lt;- \"ricochet\"\ndf1 &lt;- taylor_album_songs %&gt;%\n   select(album_name, track_name) %&gt;%\n   filter(grepl(x, track_name))\nn &lt;- df1$album_name[1]\na &lt;- which(all_albums == n)\na\n\n# Find track number\nx &lt;- df1$track_name[1]\nt &lt;- taylor_album_songs %&gt;%\n  filter(album_name == n) %&gt;%\n  select(album_name, track_number, track_name) %&gt;%\n  filter(track_name == x) %&gt;%\n  select(track_number) %&gt;%\n  pull() %&gt;% as.numeric()\nt\n\nOr instead, choose album a &amp; track t   eg: Style: a=11, t=3\n# a &lt;- 8\n# t &lt;- 5\n\n#######################################\nsong &lt;- taylor_album_songs %&gt;% \n  # filter(album_name == all_albums[a] ) %&gt;%  # all songs in an album\n  filter(album_name == all_albums[a] &amp; track_number == t) %&gt;% \n  pull(track_name[1])   # NOTE the track_name is always 1\nif (length(song) == 0) print(\"No such track number/song\") else print(song)\nsong &lt;- gsub(\"\\\\(Taylor's Version\\\\)|\\\\[Taylor's Version\\\\]\", '', song)\nalbum &lt;- all_albums[a]\n\ndf &lt;- taylor_album_songs %&gt;%\n  filter(album_name == all_albums[a] &amp; track_number == t) %&gt;%\n  select(lyrics) %&gt;%\n  unnest(lyrics) %&gt;%\n  select(line, lyric)\nmore_rows(df)   # Examine for colloquialisms; add to function colloq\n\ndf &lt;- colloq(df)\nmore_rows(df)\n\nwords_df &lt;- df %&gt;%\n  unnest_tokens(word, lyric) %&gt;%\n  anti_join(custom_stop_words, by = \"word\")\nmore_rows(words_df)\n\nprocessed_words &lt;- pro(words_df) \nmore_rows(processed_words) # Examine for words incorrectly lemmatized; add to function pro\n\nprocessed_words_count &lt;- processed_words %&gt;%\n  select(lemmatized_word) %&gt;%\n  group_by(lemmatized_word) %&gt;%\n  summarize(total_count = n()) %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  head(10)\nmore_rows(processed_words_count) \n\ntotal_processed_words &lt;- nrow(processed_words) \n\nsentiment_analysis &lt;- processed_words %&gt;%\n  inner_join(nrc_sentiments, by = c(\"lemmatized_word\" = \"word\"),\n             relationship = \"many-to-many\") %&gt;%\n  count(lemmatized_word, sentiment, sort = TRUE)\n\ndistinct_sentiment_words &lt;- sentiment_analysis %&gt;% \n  pull(lemmatized_word) %&gt;%\n  n_distinct()   \n\ntotal_sentiment_words &lt;- sentiment_analysis %&gt;%\n  select(lemmatized_word, n) %&gt;%\n  distinct() %&gt;%\n  group_by(lemmatized_word) %&gt;%\n  summarize(total_count = sum(n)) %&gt;%\n  summarize(total_count = sum(total_count)) %&gt;%\n  pull(total_count)   \n\nr &lt;- round(total_sentiment_words / total_processed_words, 3)\ncat(\"emotional words = \", total_sentiment_words, \", total processed words = \", total_processed_words, \", ratio = \", r )\n\nsentiment_summary &lt;- sentiment_analysis %&gt;%\n  group_by(sentiment) %&gt;%\n  summarise(word_count = sum(n))\nsentiment_summary\n\nword_summary &lt;- sentiment_analysis %&gt;%\n  select(lemmatized_word, n) %&gt;%\n  distinct() %&gt;%\n  group_by(lemmatized_word) %&gt;%\n  summarize(total_count = sum(n)) %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  head(10)\nword_summary   # check for word with unusual sentiment (truck = trust?); delete from NRC \n\nword_plot(processed_words_count, song, \n          title=\"10 most frequent processed words\")\nword_plot(word_summary, song, \n          title = \"10 most frequent emotional words\")\nemotion_plot(sentiment_summary, song, r)\n\n# End individual song analysis;  for all songs combined, in df &lt;- taylor_album_songs %&gt;%\n# delete filter(album_name == all_albums[a] &amp; track_number == t) %&gt;%, and also song &lt;- \"\"\n\ndirty_words &lt;- processed_words %&gt;%\n  select(lemmatized_word) %&gt;%\n  filter(, lemmatized_word==\"fuck\" | lemmatized_word==\"shit\" | lemmatized_word==\"slut\" | lemmatized_word==\"bitch\") %&gt;%\n  group_by(lemmatized_word) %&gt;%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"fuck\", \"f**k\")) %&gt;%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"shit\", \"sh*t\")) %&gt;%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"slut\", \"sl*t\")) %&gt;%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"bitch\", \"b*tch\")) %&gt;%\n  summarize(total_count = n())\ndirty_words   # limited to these four; there are others :)\n\ndirty_colors &lt;- c(\"#FF0000\",\"#000000\",\"#800080\",\"#FFA500\")\nggplot(dirty_words, aes(x = lemmatized_word, y = total_count, fill = lemmatized_word)) + \n  geom_col(color = \"black\") + \n  scale_fill_manual(values = dirty_colors) +\n  labs(title = \"TS # of R-rated Words\",\n       x = \"\", \n       y = \"Number of Words\") + \n  common_theme \n\n# END\n\n\n</pre>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://onlinecollegemathteacher.blogspot.com/2025/01/the-mathematics-of-taylor-swift.html\"> Online College Math Teacher</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "The Mathematics of Taylor Swift\nPosted on\nJanuary 30, 2025\nby\nJerry Tuttle\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nOnline College Math Teacher\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nI confess to being fascinated by Taylor Swift for far more than her music. I think she is an extraordinary person for her  philanthropy, her speaking out for victims of sexual assault, her advocacy of artists’ ownership rights, and her urging her fans to vote.\nBut of course there’s much more, and so let’s look at the mathematics of Taylor Swift.\nLet’s start with her net worth.  She is estimared to have $1.6 billion in assets.  Her largest asset is her music catalog. She did not own the masters (first recording) to her older music, but she re-recorded them and owns the masters to the re-recorded versions and to her future recordings.\nShe donates large sums to a variety of organizations.  Most of these donations are not public, but here is a sample of some recent donations:\nThere are three parts to this mathematical analysis:\n1.\nher attractiveness\n,\n2.\nher lipstick shades\n, and\n3.\nher lyrics\n.\nEach part contains a considerable amount of imprecision in measuremenys and judgments.  This is more of a work in progress than the final word.  Nevertheless, I think it is a non-routine use of math, and it was a lot of fun.\n1. How pretty is she?\nYou can have your opinion.  But mine is based on math.\nThe ancient Greeks discovered a particular number called the Golden Ratio, denoted by Greek letter Φ (phi),  that has many interesting mathematical properties, apppears \nin some patterns of nature, and is considered by many to be asthetically pleasing. The Golden Ratio results from finding the point on a line segment that splits the segement into two smaller segments with lengths a and b, such that (a + b)/a = a/b.\nThat ratio a/b is the Golden Ratio, Φ.  With a little algebra, Φ = (1 +  √5)/2 , which is an irrational number so it has an infinite non-repeating decimal, and rounded to three decimal places is 1.618.\nRenaissance artists, plastic surgeons, and makeup artists are among those who use Golden Ratios in various ways with faces to create ideally proportioned faces. Gary Meisner has wriiren extensively on the Golden Ratio, and he believes there are over 20 different ways that the Golden Ratio shows up in human faces and that “the Golden Ratio is also found very commonly in beautiful models of today across all ethnic groups”.  Biostatistican professor Dr. Kendra Schmid and her colleagues performed various measures of many faces. They began with 17 potential Golden Ratios, and they decided only six of these ratios were predictors of facial attractiveness. See\nSchmid\n.\nThis takes us to Taylor.  I attempted to measure these six ratios on a picture of Taylor. There are many pictures of her, she does enjoy experimenting with different hairstyles, and I had to find one with a hairstyle that gave me the best chance of measuring her from her hairline and also between her ears.  The measurement is not exact for many reasons, and because we are using a two-dimensional photo of a three-dimensional object there is certainly some loss of accuracy.  Nevertheless, here are the results:\nFace length / Face Width\n1.4722\nMouth width / Interocular distance\n1.4000\nMouth width / Nose width\n1.5556\nLips to chin / Interocular\n1.7000\nLips to chin / Nose width\n1.8889\nEar length / Nose width\n1.5556\nAverage\n1.5954\n% Deviation from Φ\n– 1.4%\nOn average, Taylor’s measurements are quite close to phi, the ideal measurement of facial attractiveness.\n(Note that I did the measurements manually, and they are inexact.  I understand there are Python libraries such as dlib that can detect facial ladmarks, and this might eliminate some of the imprecision.)\n2. What lipstick shade is she wearing?\nA common question on the Internet is what are her favorite lipsticks.  I think more interesting mathematically is: given a particular photo of Taylor, can we get the computer to mathematically identify her lipstick?\nFirst, a little background about image files on the computer.  A computer image is a collection of tiny dots called pixels. Each dot is about 1/96 inch.\nEach pixel contains a color code, as a 6-digit base 16 (hex) number, or as a 6 digit base 10 red-green-blue ordered triple.\n  For example 861A23 in base 16 equals (134, 26, 35) in rgb base 10.  The first two hex digits, 86, is the red.  86 in base 16 equals 8*16\n1\n+ 6*16\n0\n= 134 in base ten.  The second two hex digits, 1A, is the green, etc.\nWe can ask how similar two colors are by plotting them in 3D and calculating their distance.\nD =  √ [ (134 – 174)\n2\n+ (26 – 31)\n2\n+ (35 – 61)\n2\n] = 48.0\nI started with five different photos of Taylor, where it appears to me she is wearing a different shade in each. I tried to crop each photo as a rectangular area as close as I could get to just her lips, and I saved each cropped rectangle as a file.  I used R packsge colouR to find the most frequent colors.  This was unsatisfactory, possibly because there was too much background color noise.  I found a website\nhttps://www.remove.bg/\nthat removes background, and I retried colouR with the background removed files.\nI did the same process with online swatches of ten lipsticks that the Internet says are among Taylor’s favorites.\nTo my surprise, there were more than 50 different colors, most of them reasonably close, on both the Taylor photos and on the lipstick swatches.\nWhy are there so many colors?\nthere are 16\n6\n= 16,777,216 different computer colors\nwomen tell me lipstick never looks same on a person as in a tube\nmy dermatologist offered that lips have lines and grooves that create uneven coloring\nit is hard to isolate the image of just lips in a rectangle\nlighting creates distortions\nthere are image resolution and quality issues\nNevertheless, I decided to do the following. For each of Taylor’s ten favorite lipsticks, I found the ten most frequent colors.\nFor each of the five Taylor lip photos, I found their ten most frequent colors.  Then for each photo, I calculated the average color distance from the photo to each lipstick.  The lipstick with the minimum distance from the photo was then the computer forecast matching a photo to a lipstick.\nGet it?\nHere are the five Taylor photos with the computer’s match:\nIn summary, the computer thinks photo 1 has the minimum distance, 320, to its match.  Do you agree this is the best match?\nPhoto\nShade\nDistance\n1\nRuby Woo\n320\n2\nElson 4\n625\n3\nBlood Lust\n388\n4\nFlame\n392\n5\nFlame\n399\n3.  Which of her songs are emotional?\nThe R package ‘taylor’ contains lyrics for 11 albums and 240 songs.\nI will examine TS songs for their emotional content, as follows:\n% emotional words per song\nmost frequent words per song\ntrend in emotions over time\nexamine theory that 5th song in every album is most emotional\nWhat makes a song emotional?  Some possibilities are: emotional words, volume, tempo. pitch, rhythm, and \ninstrumentation.  A few musicians I spoke to have a longer list.  But I don’t have the data to measure\nany of these, except emotional words.\nLinguists have created lists of emotional words.  Mohammad and Turney created the\nNRC Emotion Lexicon\nlist of over 14,000 English words and their associations with ten basic emotions (anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, and trust). A word may have more than one emotion.  For example, faith has the emotions \nanticipation, joy, positive, and trust.\nLike most datasets, Taylor Swift songs required a considerable amount of data cleaning. In a text analysis like this, data cleaning includes removing punctuation and capitals, deleting stop words (a, oh, the, etc.) that add no value, and fixing informal speech and \n colloquialisms (dyin’, gonna, etc.).  Taylor uses a large number of colloquialisms, often by dropping an ending g in an “ing” word.  Also part of the cleaning is that words are stemmed so that love, lovable, loving, and lovely all reduce to “lov”; and then the stemmed words are lemmatized and returned to their root word love.  The R packages SnowballC and textstem perform stemming and lemmatizing respectively.  However, this combination did not always produce a valid lemmatized word, and I manually adjusted about 500 words which I placed into a file lem_replacements_df.csv.  The final result is over 23,000 words from her 240 songs, which I am calling processed words.\nFor all 240 songs combined, I calculated Taylor’s most frequent words, her most frequent emotional words, her number of words for each of the 10 emotions, and the ratio r = number of emotional words / number of processed) words.  The results are contained in the following three plots:\nThe above is a baseline for all songs combined. The r value is .364. I asked a friend whose teen-aged daughter is a Swiftie, to suggest a few Taylor songs that she considers especially emotional, and I created the same types of plots for some of these songs.  I’ll share the plots for the song “My Tears Ricochet”, which had a greater than average r value:\nNote that in the above song, the counts are pretty small. Perhaps this is one explanation for why\na number of songs that we thought were emotional did not have a higher than average r value.\nAnother question I was curious about is whether Taylor’s songs have become more emotional over time. I am using original release date of album as the time variable.  For songs that Taylor re-recorded, I am using the original release date under the assumption that her re-release may have changed the music but not the lyrics.\nThat her songs are becoming more emotional over time appears not to be true.  To the contrary, the slope of a poor fitting trend line is negative, but its F-statistic says the linear model is not significant.\nFinally, there is a theory that every fifth song on an album is intentionally more emotional than the others.\nThis\ntheory\nis discussed on the Internet and even Taylor gives it some credence.  Albums by release year provide 10 data points (there are 11 albums, but two were released in the same year). In 6 years out of 10 the track 5 r ratio exceeded the all other tracks, and in 4 years out of 10 they did not.   \nA two-sample t-test with one tail concludes that the track 5 mean is not significantly greater than the all other tracks mean, p = .4591. \nNote that track 5 has an outlier, “All you had to do was stay,” which sounds like it should have a high r, but its r was .134.\n4.  Bonus:  &$!#%\nTo the Taylor Tots parents: \nOur Taylor has been known to use an R-rated word in her songs, or two.  Here is the frequency of some of her most frequent such words (and there are others), mostly from her Tortured Poets Department album:\nAs I mentioned earlier, each of the three parts of this analysis – her attractiveness, her lipstick shades, and her lyrics – contains various imprecisions and judgments.  Perhaps some day I will go back to this project and make some improvements. Comments are welcome.\nBut I think it was a non-routine use of math.  And it was fun!\nThis project uses three R script files and a number of image files.  The R code is shown below, but it is long. Both the R code files and the image files may be found at\nhttps://github.com/fcas80/The-Taylor-Swift-Project/tree/main\n# file 1 of 3: lovely.txt\n\nlibrary(png)\nlibrary(ggplot2)\nlibrary(grid)\n\nimage_path <- \"my_path/ts1_phi_removebg.png\"\nimg <- readPNG(image_path, native=TRUE)   # width 584, height 457\nheight <- nrow(img)   # 383\nwidth <- ncol(img)    # 331\nraster_img <- grid::rasterGrob(img, interpolate = TRUE)\ndf <- data.frame(xpos = c(0, width), ypos = c(0, height))\n\n# 1. plot photo on minimum grid\nggplot(data = df,\n       aes(xpos, ypos)) +\n  xlim(0, width) + ylim(0, height) +\n  geom_blank() +\n  annotation_custom(raster_img, xmin=0, xmax=width, ymin=0, ymax=height) + \n  theme(axis.title.x = element_blank(), \n      axis.title.y = element_blank())\n\n# 2. plot photo on more detailed grid; measurements are manual and imprecise\nggplot(data = df,\n       aes(xpos, ypos)) +\n  xlim(0, width) + ylim(0, height) +\n  geom_blank() +\n  annotation_custom(raster_img, xmin=0, xmax=width, ymin=0, ymax=height) +\n  geom_hline(yintercept = seq(0, height, by = 10), color = \"gray\", linwidth = 0.5) +\n  geom_vline(xintercept = seq(0, width, by = 10), color = \"gray\", linwidth = 0.5) +\n  scale_x_continuous(breaks = seq(0, width, by = 20)) +\n  scale_y_continuous(breaks = seq(0, height, by = 20)) +\n  annotate(\"segment\", x = 90, y = 265, xend = 90, yend = 0, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 70, y = 90, xend = 250, yend = 90, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 130, y = 170, xend = 180, yend = 170, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 135, y = 105, xend = 180, yend = 105, color = \"red\", linwidth = 3) +\n  annotate(\"segment\", x = 125, y = 70, xend = 195, yend = 70, color = \"blue\", linwidth = 3) +\n  annotate(\"segment\", x = 160, y = 85, xend = 160, yend = 0, color = \"blue\", linwidth = 3) +\n  annotate(\"segment\", x = 50, y = 160, xend = 50, yend = 90, color = \"red\", linwidth = 3) +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank())\n\nsegments_df <- data.frame(\n  segment_name = c(\"face_length\", \"face_width\", \"dist_bet_eyes\", \n                   \"nose_width\", \"mouth_width\", \"lips_to_chin\", \"ear_length\"),\n  x = c(90, 70, 130, 135, 125, 160, 50),\n  y = c(265, 90, 170, 105, 70, 85, 160),\n  xend = c(90, 250, 180, 180, 195, 160, 50),\n  yend = c(0, 90, 170, 105, 70, 0, 90))\nsegments_df$dist <- sqrt((segments_df$x - segments_df$xend)^2 + (segments_df$y - segments_df$yend)^2)\nsegments_df\n\nratios_df <- data.frame(\n  ratio_name = c(\"face length / width\", \"mouth width / interocular\", \"mouth width / nose width\", \n       \"lips to chin / interocular\", \"lips to chin / nose width\", \"ear length / nose width\"),\n  ratio = rep(0, times=6))\n\nratios_df$ratio[1] <- round(segments_df$dist[1] / segments_df$dist[2], 4)   # face length / width\nratios_df$ratio[2] <- round(segments_df$dist[5] / segments_df$dist[3], 4)   # mouth width / interocular\nratios_df$ratio[3] <- round(segments_df$dist[5] / segments_df$dist[4], 4)   # mouth width / nose width\nratios_df$ratio[4] <- round(segments_df$dist[6] / segments_df$dist[3], 4)   # lips to chin / interocular\nratios_df$ratio[5] <- round(segments_df$dist[6] / segments_df$dist[4], 4)   # lips to chin / nose width\nratios_df$ratio[6] <- round(segments_df$dist[7] / segments_df$dist[4], 4)   # ear length / nose width \nratios_df\nm <- round(mean(ratios_df$ratio),3)\nm\nerror <- round((m - (1+sqrt(5))/2)/m,3)\nerror\n# END\n# file 2 of 3: lipsticks.txt\n\nsetwd(\"my_path\")\nlibrary(colouR)\nlibrary(ggplot2)\n\n# These shades were determined from various Internet articles\nshade <- c(\"Ruby Woo\", \"Morocco\", \"Dragon Girl\", \"Elson 4\", \"Always Red\", \"Kyoto Red\", \n           \"Eadie Scarlet\", \"Blood Lust\", \"Flame\", \"Red to Go\")\n\n# These swatches of shades were copied from various Internet stores\ndf_lipsticks <- data.frame(shade,\n        file = c(\"Mac_Ruby_Woo.png\", \"NARS_morocco.png\", \"NARS_Dragon_Girl.png\", \"PatMcGrath_Elson4.png\", \n           \"Sephora_always_red.png\", \"Tatcha_Kyoto_Red.png\", \"Gucci_Velvet_Eadie_Scarlet.png\", \n           \"PatMcGrath_Blood_Lust.png\", \"TomFord_flame.png\", \"Armani_red_to_go.png\" ))\n\n# These files were created by copying various photos of TS, cropping rectangle of lips, then\n# using https://www.remove.bg/ to remove backgrounds  \nfile = c(\"ts1_lips-removebg-preview.png\", \"ts2_lips-removebg-preview.png\",\n       \"ts3_lips-removebg-preview.png\", \"ts4_lips-removebg-preview.png\",\"ts5_lips-removebg-preview.png\")\nname = c(\"TS photo 1\", \"TS photo 2\", \"TS photo 3\", \"TS photo 4\" ,\"TS photo 5\")\ndf_photos <- data.frame(file, name)\n\n############\n\n# p10 for ten lipstick shades, but some only have 1 color\np10 <- function(image, name) {\n  # Get primary color\n  primary_color <- colouR::getTopCol(image, n = 1, avgCols = FALSE, exclude = TRUE)\n  \n  top10 <- tryCatch(\n    {\n      colouR::getTopCol(image, n = 10, avgCols = FALSE, exclude = TRUE)\n    },\n    error = function(e) {\n      primary_color\n    }\n  )\n  \n  if (nrow(top10) < 10) {\n    top10 <- rbind(top10, top10[rep(1, 10 - nrow(top10)),])\n  }\n  \n  plot <- ggplot(top10, aes(x = hex, y = freq, fill = hex)) +\n    geom_bar(stat = 'identity') +\n    scale_fill_manual(values = top10$hex) +\n    labs(title = paste(name, \"Top 10 colors by frequency\")) +\n    xlab(\"HEX color code\") + ylab(\"Frequency\") +\n    theme(\n      legend.position = \"none\",\n      plot.title = element_text(size = 15, face = \"bold\"),\n      axis.title = element_text(size = 15, face = \"bold\"),\n      axis.text.x = element_text(angle = 45, hjust = 1, size = 12, face = \"bold\")\n    )\n  print(plot)\n  return(top10)\n}   # close p function\n\nshade_dataframe <- data.frame(matrix(ncol = 10, nrow = 0))\n\nfor (i in 1:10) {\n  result <- p10(df_lipsticks$file[i], df_lipsticks$shade[i])\n  \n  # Repeat the first color if there are fewer than 10 colors\n  if (nrow(result) < 10) {\n    result <- rbind(result, result[rep(1, 10 - nrow(result)),])\n  }\n  \n  shade_dataframe <- rbind(shade_dataframe, t(result$hex))\n}\n\ncolnames(shade_dataframe) <- paste0(\"Color\", 1:10)\nprint(shade_dataframe)\n\n#############\n\np5 <- function(image, name) {\n   top10 <- colouR::getTopCol(image, n = 10, avgCols = FALSE, exclude = TRUE)\n   return(top10)\n}\n\ndf_ts_colors <- data.frame()\nfor(i in 1:nrow(df_photos)){\n   top10_colors <- p5(df_photos$file[i], df_photos$name[i])\n   top10_colors <- top10_colors$hex\n   df_ts_colors <- if(i == 1) {\n     df_ts_colors <- top10_colors\n    } else {rbind(df_ts_colors, top10_colors)\n   }\n}\ncolnames(df_ts_colors) <- c(\"color1\",\"color2\",\"color3\",\"color4\",\"color5\",\n                         \"color6\",\"color7\",\"color8\",\"color9\",\"color10\")\nrownames(df_ts_colors) <- df_photos$name\ndf_ts_colors <- t(df_ts_colors)\nprint(df_ts_colors)\n\n# begin total RGB distance of a TS photo to centroid of lipstick shade\n\n# sample 3d plot\nlibrary(scatterplot3d)\nm <- data.frame(\n     x = c(174,134),\n     y = c(31,26),\n     z = c(61,35))\n\ns3d <- scatterplot3d(m$x, m$y, m$z, pch = 19, color = \"blue\", \n    xlim = c(100, 200), ylim = c(0, 50), zlim = c(0, 100), \n    xlab = expression(bold(\"R-axis\")), ylab = expression(bold(\"G-axis\")), zlab = expression(bold(\"B-axis\")), \n    main = \"Plot of two colors in RGB space\", cex.main = 2)\n\ns3d$points3d(m$x, m$y, m$z, type = \"l\", lty = 1, lwd = 2, col = \"red\")\n\n# Use 'text' for adding labels with bold font\ntext(s3d$xyz.convert(m$x, m$y, m$z), labels = paste0(\"(\", m$x, \",\", m$y, \",\", m$z, \")\"), \n     pos = 3, cex = 2, col = \"#00008B\", font = 2)\n\nA <- shade_dataframe$Color1\nB <- shade_dataframe$Color2\nC <- shade_dataframe$Color3\nD <- shade_dataframe$Color4\nE <- shade_dataframe$Color5\nF <- shade_dataframe$Color6\nG <- shade_dataframe$Color7\nH <- shade_dataframe$Color8\nI <- shade_dataframe$Color9\nJ <- shade_dataframe$Color10\n\nhex_to_rgb <- function(hex) {\n  rgb <- t(col2rgb(hex))\n  return(data.frame(r=rgb[1,], g=rgb[2,], b=rgb[3,]))\n}\n\n# Function to calculate Euclidean distance between two RGB vectors\neuclidean_distance <- function(rgb1, rgb2) {\n  sqrt(sum((rgb1 - rgb2)^2))\n}\n\n# Function to calculate centroid of a vector of hex colors\ncalculate_centroid <- function(hex_colors) {\n  rgb_values <- hex_to_rgb(hex_colors)\n  centroid <- colMeans(rgb_values)\n  return(centroid)\n}\n\n# Calculate centroids of shades, e.g., centroid_A <- calculate_centroid(A)\n  centroids <- lapply(list(A, B, C, D, E, F, G, H, I, J), calculate_centroid)\n\nfor (j in 1:nrow(df_photos)) {\n   x <- df_ts_colors[,j]\n   centroid_x <- calculate_centroid(x)\n# Calculate total distance of x to each centroid\n   total_distance_to_centroid <- function(x, centroid) {\n     x_rgb <- hex_to_rgb(x)\n     total <- 0\n     for (i in 1:nrow(x_rgb)) {\n       total <- total + euclidean_distance(x_rgb[i, ], centroid)\n     }\n     return(total)\n   }\n\n  distances <- sapply(centroids, total_distance_to_centroid, x = x)\n  w <- which(distances == min(distances))\n  most_similar <- shade[w]\n  cat(\"The lipstick most similar to\", df_photos$name[j], \"is:\", most_similar, \n      \"with distance of \", min(distances), \"\\n\")\n }   #  end for loop on photos\n  \n# END\nfile 3 of 3:  lyrics.txt\n\nlibrary(taylor)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\ndata(stop_words)\nlibrary(textclean)\nlibrary(SnowballC) # For stemming\nlibrary(textstem) # For lemmatization\n\n######################################\n# functions, preliminaries\n\n# function to print more rows\nmore_rows <- function(filename){\n  options(tibble.print_max = nrow(filename), tibble.print_min = nrow(filename))\n  print(filename)\n}\n\n# function to fix informal speech (colloquialisms); also contractions, punctuation, lower case\ncolloq <- function(df) {\n  pattern <- \"ahh|Ah-uh|ahah|eh|haah|haha|iiii|Oh|oh|Oh-oh|oh-oh|ooh-oh|oohah|Uh|Uh-oh|uh-oh-oh-oh|ya|La-la-la|lala|la-la-la|la-la-la-la|Mm-mm|Mmm-mm|mm-mm|mm-mm-mm-mm|Ha-ha-ha\"\n  df <- df %>%\n    mutate(lyric = str_remove_all(lyric, paste0(\"\\\\b(\", pattern, \")\\\\b\"))) %>%\n    mutate(lyric = str_replace_all(lyric, \n               c(\"ain't\" = \"is not\", \n                 \"Beggin'\" = \"begging\", \"beggin'\" = \"begging\", \"birth right\" = \"birthright\", \"blood-soaked\" = \"blood soaked\",\n                 \"'bout\" = \"about\", \"burnin'\" = \"burning\", \"callin'\" = \"calling\", \"'Cause\" = \"because\", \"'cause\" = \"because\", \"Cept\" = \"Except\", \"cursee\" = \"cure\",\n                 \"convers\" = \"conversation\",\"crashin'\" = \"crashing\", \"doin'\" = \"doing\", \"driftin'\" = \"drifting\", \"dyin'\" = \"dying\", \"'em\" = \"them\", \"feelin'\" = \"feeling\", \"flyin'\" = \"flying\",\"feverishlying\" = \"feverishly\", \"'fore\" = \"before\", \"'Fore\" = \"before\", \"foreyesgn\" = \"foreign\",\n                 \"everythi-i-ing\" = \"everything\",\"fuckin'\"=\"fuck\", \"gettin'\" = \"getting\", \"gonna\" = \"going to\", \"gotta\" = \"got to\", \"happ'nin'\" = \"happening\", \"haven't\" = \"have not\", \"Holdin'\" = \"holding\", \"hero's\" = \"hero\",\n                 \"Hopin'\" = \"hoping\", \"hopin'\" = \"hoping\",\"I'ma\" = \"I am going to\", \"kinda\"=\"kind of\", \"king's\" = \"king\", \"keepin\" = \"keeping\", \"Laughin'\" = \"laughing\", \"lookin'\" = \"looking\", \"losin\" = \"losing\", \"losingg\" = \"losing\", \"lovin'\" = \"loving\", \"lucki\" = \"lucky\", \"no-one\" = \"no one\", \"Nothin'\" = \"nothing\", \"nothin'\" = \"nothing\",\"one-hand\" = \"one hand\",\n                 \"mornin'\" = \"morning\", \"'nother\"=\"another\", \"nothin'\" = \"nothing\", \"pickin'\" = \"picking\", \"post-mortem\" = \"postmortem\", \"prayin'\" = \"praying\", \"Prayin'\" = \"praying\", \"pretti\" = \"pretty\", \"ridin'\" = \"riding\", \"Sneakin'\" = \"sneaking\",\n                 \"outta\" = \"out of\", \"'round\" = \"around\", \"self-implode\" = \"self implode\", \"shoulda\" = \"should have\", \"standin'\" = \"standing\", \"summer's\" = \"summer is\", \"There's\" = \"there is\", \"Thinkin'\" = \"thinking\", \n                 \"thankin'\" = \"thanking\", \"thinkin'\" = \"thinking\", \"'Til\" = \"until\", \"'til\" = \"until\", \"tryin'\" = \"trying\", \"tryna\" = \"trying to\", \"Tryna\" = \"trying to\",\"twin-sized\" = \"twin sized\", \n                 \"waitin'\" = \"waiting\", \"white-knuckle\" = \"white knuckle\",\n                 \"wanna\" = \"want to\", \"weepin'\" = \"weeping\", \"whatcha\" = \"what are you\", \"Where's\" = \"where is\", \"Why'd\" = \"why did\",\n                 \"wide-eyed\" = \"wide eyed\", \"wonderin'\" = \"wondering\", \"Wonderin'\" = \"wondering\"))) %>%\n    mutate(lyric = map_chr(lyric, clean_text))\n  \n  return(df)\n}\n\ncustom_stop_words <- bind_rows(stop_words, \n           tibble(word = c(\"ah\", \"dadadada\", \"ha\", \"haha\", \"hey\", \"hi\", \"huh\", \"la\", \"MMM\", \"mmm\", \"mmmm\", \"mm\", \"na\", \"oh\", \"okay\",\"OK\", \"ok\", \"ooh\", \n           \"uh\", \"whoa\", \"woah\", \"yeah\"), lexicon = \"custom\"))\n\n# Function to clean text  (included in colloq function)\nclean_text <- function(text) { \n  text %>% \n    replace_contraction() %>% \n    str_remove_all(\"[[:punct:]]\") %>% \n    str_to_lower()\n}\n\nreplacements_df <- read.csv(\"my_path/lem_replacements_df.csv\", header = TRUE)\n\n# Function to stem and lemmatize\npro <- function(words_df) {\n  words_df <- words_df %>% filter(!is.na(word))\n  processed_words <- words_df %>%\n    mutate(stemmed_word = wordStem(word),\n           lemmatized_word = lemmatize_words(stemmed_word)) %>%\n    filter(lemmatized_word != \"\") %>%  # Filter out empty strings\n    left_join(replacements_df, by = c(\"lemmatized_word\" = \"stemmed_word\")) %>%\n    mutate(lemmatized_word = ifelse(is.na(lemmatized_word.y), lemmatized_word, lemmatized_word.y)) %>%\n    select(-lemmatized_word.y)  # Remove the extra column created during the join\n}\n\nnrc_sentiments <- get_sentiments(\"nrc\") %>%\n  filter(word != \"count\" & word != \"heel\" & word != \"truck\" & word != \"wear\" & word != \"word\")\n\ncommon_theme <- theme(\n  legend.position=\"NULL\",\n  plot.title = element_text(size = 15, face = \"bold\"),\n  plot.subtitle = element_text(size = 12.5, face = \"bold\"),\n  axis.title = element_text(size = 15, face = \"bold\"),\n  axis.text = element_text(size = 15, face = \"bold\"),\n  legend.title = element_text(size = 15, face = \"bold\"),\n  legend.text = element_text(size = 15, face = \"bold\"))\n\nsentiment_colors <- c(\"anger\" = \"red\", \"anticipation\" = \"green\", \"disgust\" = \"brown\",\n                      \"fear\" = \"purple\", \"joy\" = \"yellow\", \"negative\" = \"gray\", \"positive\" = \"lightblue\",\n                      \"sadness\" = \"blue\", \"surprise\" = \"pink\", \"trust\" = \"turquoise\")\n\nemotion_plot <- function(sentiment_summary, song, r){\n  ggplot(sentiment_summary, aes(x = sentiment, y = word_count, fill = sentiment)) + \n    geom_col(color = \"black\") + \n    scale_fill_manual(values = sentiment_colors) +\n    labs(title = paste(song, \"# words by emotion\"),\n         subtitle = paste(\"emotional words / all (processed) words =\", r), \n         x = \"Emotion\", \n         y = \"Number of Words\") + \n    common_theme + \n    theme(axis.text.x = element_text(angle = 45, hjust=1))\n}\n\nlemmatized_colors <- c(\"red\", \"blue\", \"green\", \"orange\", \n                       \"purple\", \"cyan\", \"magenta\", \"yellow\", \"brown\", \"pink\")\n\nword_plot <- function(word_summary, song, title){\n  ggplot(word_summary, aes(x = lemmatized_word, y = total_count, fill = lemmatized_word)) + \n    geom_col(color = \"black\") + \n    scale_fill_manual(values = lemmatized_colors) +\n    labs(title = paste(song, title),\n         x = \"\", \n         y = \"Frequency\") + \n    common_theme +\n    theme(axis.text.x = element_text(angle = 45, hjust=1))\n}\n\nall_albums <- c(\"Taylor Swift\", \"Fearless (Taylor's Version)\", \"Speak Now (Taylor's Version)\", \"Red (Taylor's Version)\", \"1989 (Taylor's Version)\",\n                \"reputation\", \"Lover\", \"folklore\", \"evermore\",  \"Midnights\", \"THE TORTURED POETS DEPARTMENT\")\nnumber_of_tracks <- c(15, 26, 22, 30, 22, 15, 18, 17, 17, 19, 31)\noriginal_release_year <- c(2006, 2008, 2010, 2012, 2014, 2017, 2019, 2020, 2020, 2022, 2024)\n# The years are original release years, i.e., not \"Taylor's version\" years\n\n#######################################\n\n# Find song by word in title - could be lower case!\nx <- \"ricochet\"\ndf1 <- taylor_album_songs %>%\n   select(album_name, track_name) %>%\n   filter(grepl(x, track_name))\nn <- df1$album_name[1]\na <- which(all_albums == n)\na\n\n# Find track number\nx <- df1$track_name[1]\nt <- taylor_album_songs %>%\n  filter(album_name == n) %>%\n  select(album_name, track_number, track_name) %>%\n  filter(track_name == x) %>%\n  select(track_number) %>%\n  pull() %>% as.numeric()\nt\n\nOr instead, choose album a & track t   eg: Style: a=11, t=3\n# a <- 8\n# t <- 5\n\n#######################################\nsong <- taylor_album_songs %>% \n  # filter(album_name == all_albums[a] ) %>%  # all songs in an album\n  filter(album_name == all_albums[a] & track_number == t) %>% \n  pull(track_name[1])   # NOTE the track_name is always 1\nif (length(song) == 0) print(\"No such track number/song\") else print(song)\nsong <- gsub(\"\\\\(Taylor's Version\\\\)|\\\\[Taylor's Version\\\\]\", '', song)\nalbum <- all_albums[a]\n\ndf <- taylor_album_songs %>%\n  filter(album_name == all_albums[a] & track_number == t) %>%\n  select(lyrics) %>%\n  unnest(lyrics) %>%\n  select(line, lyric)\nmore_rows(df)   # Examine for colloquialisms; add to function colloq\n\ndf <- colloq(df)\nmore_rows(df)\n\nwords_df <- df %>%\n  unnest_tokens(word, lyric) %>%\n  anti_join(custom_stop_words, by = \"word\")\nmore_rows(words_df)\n\nprocessed_words <- pro(words_df) \nmore_rows(processed_words) # Examine for words incorrectly lemmatized; add to function pro\n\nprocessed_words_count <- processed_words %>%\n  select(lemmatized_word) %>%\n  group_by(lemmatized_word) %>%\n  summarize(total_count = n()) %>%\n  arrange(desc(total_count)) %>%\n  head(10)\nmore_rows(processed_words_count) \n\ntotal_processed_words <- nrow(processed_words) \n\nsentiment_analysis <- processed_words %>%\n  inner_join(nrc_sentiments, by = c(\"lemmatized_word\" = \"word\"),\n             relationship = \"many-to-many\") %>%\n  count(lemmatized_word, sentiment, sort = TRUE)\n\ndistinct_sentiment_words <- sentiment_analysis %>% \n  pull(lemmatized_word) %>%\n  n_distinct()   \n\ntotal_sentiment_words <- sentiment_analysis %>%\n  select(lemmatized_word, n) %>%\n  distinct() %>%\n  group_by(lemmatized_word) %>%\n  summarize(total_count = sum(n)) %>%\n  summarize(total_count = sum(total_count)) %>%\n  pull(total_count)   \n\nr <- round(total_sentiment_words / total_processed_words, 3)\ncat(\"emotional words = \", total_sentiment_words, \", total processed words = \", total_processed_words, \", ratio = \", r )\n\nsentiment_summary <- sentiment_analysis %>%\n  group_by(sentiment) %>%\n  summarise(word_count = sum(n))\nsentiment_summary\n\nword_summary <- sentiment_analysis %>%\n  select(lemmatized_word, n) %>%\n  distinct() %>%\n  group_by(lemmatized_word) %>%\n  summarize(total_count = sum(n)) %>%\n  arrange(desc(total_count)) %>%\n  head(10)\nword_summary   # check for word with unusual sentiment (truck = trust?); delete from NRC \n\nword_plot(processed_words_count, song, \n          title=\"10 most frequent processed words\")\nword_plot(word_summary, song, \n          title = \"10 most frequent emotional words\")\nemotion_plot(sentiment_summary, song, r)\n\n# End individual song analysis;  for all songs combined, in df <- taylor_album_songs %>%\n# delete filter(album_name == all_albums[a] & track_number == t) %>%, and also song <- \"\"\n\ndirty_words <- processed_words %>%\n  select(lemmatized_word) %>%\n  filter(, lemmatized_word==\"fuck\" | lemmatized_word==\"shit\" | lemmatized_word==\"slut\" | lemmatized_word==\"bitch\") %>%\n  group_by(lemmatized_word) %>%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"fuck\", \"f**k\")) %>%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"shit\", \"sh*t\")) %>%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"slut\", \"sl*t\")) %>%\n  mutate(lemmatized_word = str_replace(lemmatized_word, \"bitch\", \"b*tch\")) %>%\n  summarize(total_count = n())\ndirty_words   # limited to these four; there are others :)\n\ndirty_colors <- c(\"#FF0000\",\"#000000\",\"#800080\",\"#FFA500\")\nggplot(dirty_words, aes(x = lemmatized_word, y = total_count, fill = lemmatized_word)) + \n  geom_col(color = \"black\") + \n  scale_fill_manual(values = dirty_colors) +\n  labs(title = \"TS # of R-rated Words\",\n       x = \"\", \n       y = \"Number of Words\") + \n  common_theme \n\n# END\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nOnline College Math Teacher\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "I confess to being fascinated by Taylor Swift for far more than her music. I think she is an extraordinary person for her philanthropy, her speaking out for victims of sexual assault, her advocacy of artists' ownershi...",
      "meta_keywords": null,
      "og_description": "I confess to being fascinated by Taylor Swift for far more than her music. I think she is an extraordinary person for her philanthropy, her speaking out for victims of sexual assault, her advocacy of artists' ownershi...",
      "og_image": "https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBBSRa6fR4LXCy7Trgqcy-suWuyrxWEihIgknyFXGsC3LAdbG7INN4xSvOAtRhPtAmGKRUEDz4rL66Yq17rY2jf1-F4KDTOYB9jUKhyphenhyphenYM6qWeeZpnLtSaXgapIytjHctG_XeSf8XXmG95_kgajIlcTJ26SrzTa_KF8YVhsO3OCCQZQhEAmOuTZ91bHySg/s320/Screenshot_1.png",
      "og_title": "The Mathematics of Taylor Swift | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 22.2,
      "sitemap_lastmod": null,
      "twitter_description": "I confess to being fascinated by Taylor Swift for far more than her music. I think she is an extraordinary person for her philanthropy, her speaking out for victims of sexual assault, her advocacy of artists' ownershi...",
      "twitter_title": "The Mathematics of Taylor Swift | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/01/the-mathematics-of-taylor-swift/",
      "word_count": 4432
    }
  }
}