{
  "uuid": "c0a88e04-0288-4620-8975-4b7e02dadf35",
  "created_at": "2025-11-22 19:58:29",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/05/filter/",
    "crawled_at": "2025-11-22T10:48:11.127987",
    "external_links": [
      {
        "href": "https://mlr-org.com/gallery/appliedml/2025-05-05-adv-feature-preproc-filter/",
        "text": "mlr-org"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://github.com/mlr-org/mlr3website/",
        "text": "GitHub"
      },
      {
        "href": "https://mlr3filters.mlr-org.com/",
        "text": "link"
      },
      {
        "href": "https://mlr-org.com/gallery/appliedml/2025-05-05-adv-feature-preproc-filter/",
        "text": "mlr-org"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Filter | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/giuseppe-casalicchio/",
        "text": "Giuseppe Casalicchio"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-392882 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Filter</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 27, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/giuseppe-casalicchio/\">Giuseppe Casalicchio</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-05-05-adv-feature-preproc-filter/\"> mlr-org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n<strong>JavaScript is required to unlock solutions.</strong><br/>\n    Please enable JavaScript and reload the page,<br/>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" rel=\"nofollow\" target=\"_blank\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n<section class=\"level1\" id=\"goal\">\n<h1>Goal</h1>\n<p>Learn how to rank features of a supervised task by their importance / strength of relationship with the target variable using a feature filter method.</p>\n</section>\n<section class=\"level1\" id=\"german-credit-dataset\">\n<h1>German Credit Dataset</h1>\n<p>We create the task as for the resampling exercise: The German Credit Data set.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(\"mlr3verse\")\nlibrary(\"data.table\")\ntask = tsk(\"german_credit\")</pre>\n</div>\n</section>\n<section class=\"level1\" id=\"exercises\">\n<h1>Exercises</h1>\n<p>Within the <code>mlr3</code> ecosystem, feature filters are implemented in the <code>mlr3filters</code> package and are typically used in combination with <code>mlr3pipelines</code> to be able to include the whole preprocessing step in a pipeline. In exercises 1 to 3, we apply feature filtering to preprocess the data of a task without using a pipeline. In exercise 4, we will set up a pipeline that combines a learner with the feature filtering as preprocessing step.</p>\n<section class=\"level2\" id=\"exercise-1-find-a-suitable-feature-filter\">\n<h2 class=\"anchored\" data-anchor-id=\"exercise-1-find-a-suitable-feature-filter\">Exercise 1: Find a suitable Feature Filter</h2>\n<!-- Feature filters are comprised of a set of methods for feature selection that aim at quantifying the ''usefulness'' of a feature in a supervised task. -->\n<!-- Often, it is desirable to reduce the number of features to both decrease the computational cost of fitting a learner and in some cases even improving the performance of the model. -->\n<!-- Based on the metric of a feature filter, features can be ranked and the ones with the strongest relationship with the target variable can be selected to be included in the modelling process. -->\n<!-- Typically, feature filters are used when a large number of similar features are available. -->\n<!-- Nevertheless, feature filters also are useful when only a medium number of features is available, as they allow for quantifying the importance of a feature in a supervised setting providing insight into the relationship of a feature and the target variable. -->\n<!-- Here, we will use feature filters to illuminate the strength of the relationship between features and the target variable. -->\n<!-- #FIXME: comment /additional info + link on logistic regression -->\n<p>Make yourself familiar with the <code>mlr3filters</code> package (<a href=\"https://mlr3filters.mlr-org.com/\" rel=\"nofollow\" target=\"_blank\">link</a>). Which <code>Filter</code>s are applicable to all feature types from the task we created above?</p>\n<details>\n<summary>\n<strong>Hint:</strong>\n</summary>\n<p>Some filters are only applicable to either classification or regression or either numeric or categorical features. Therefore, we are looking for a <code>Filter</code> that is applicable to our classification task and that can be computed for <code>integer</code> and <code>factor</code> features (as these types of features are present in task, see <code>task$feature_types</code>).</p>\n<p>The website linked above includes a table that provides detailed information for each <code>Filter</code>.</p>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-1\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-1-contents callout-collapse collapse\" id=\"callout-1\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-35e8adca8bcecc9bd008a2f7-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"<p>Our task is a classification task and we have <code>integer</code>,
and <code>factor</code> and features:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span>task_type</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] &quot;classif&quot;</code></pre>
</div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ftypes <span class="ot">=</span> <span class="fu">unique</span>(task<span class="sc">$</span>feature_types<span class="sc">$</span>type)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ftypes</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] &quot;integer&quot; &quot;factor&quot;  &quot;ordered&quot;</code></pre>
</div>
<div class="sourceCode" id="cb5"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># subset all filters that support integer and factor features</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3filters)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>filters <span class="ot">=</span> <span class="fu">as.data.table</span>(mlr_filters)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>check_ftypes <span class="ot">=</span> <span class="fu">sapply</span>(filters<span class="sc">$</span>feature_types, <span class="cf">function</span>(x) <span class="fu">all</span>(ftypes <span class="sc">%in%</span> x))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>filters[check_ftypes, ]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Key: &lt;key&gt;
                  key                                                    label   task_types task_properties
               &lt;char&gt;                                                   &lt;char&gt;       &lt;list&gt;          &lt;list&gt;
 1:              cmim      Minimal Conditional Mutual Information Maximization classif,regr                
 2:              disr                       Double Input Symmetrical Relevance classif,regr                
 3:        importance                                         Importance Score      classif                
 4:  information_gain                                         Information Gain classif,regr                
 5:               jmi                                 Joint Mutual Information classif,regr                
 6:              jmim            Minimal Joint Mutual Information Maximization classif,regr                
 7:               mim                          Mutual Information Maximization classif,regr                
 8:              mrmr                     Minimum Redundancy Maximal Relevancy classif,regr                
 9:             njmim Minimal Normalised Joint Mutual Information Maximization classif,regr                
10:       performance                                   Predictive Performance      classif                
11:       permutation                                        Permutation Score      classif                
12:            relief                                                   RELIEF classif,regr                
13: selected_features                               Embedded Feature Selection      classif                
                             params                                        feature_types          packages
                             &lt;list&gt;                                               &lt;list&gt;            &lt;list&gt;
 1:                         threads                       integer,numeric,factor,ordered           praznik
 2:                         threads                       integer,numeric,factor,ordered           praznik
 3:                          method logical,integer,numeric,character,factor,ordered,...              mlr3
 4: type,equal,discIntegers,threads                       integer,numeric,factor,ordered     FSelectorRcpp
 5:                         threads                       integer,numeric,factor,ordered           praznik
 6:                         threads                       integer,numeric,factor,ordered           praznik
 7:                         threads                       integer,numeric,factor,ordered           praznik
 8:                         threads                       integer,numeric,factor,ordered           praznik
 9:                         threads                       integer,numeric,factor,ordered           praznik
10:                          method logical,integer,numeric,character,factor,ordered,... mlr3,mlr3measures
11:                 standardize,nmc logical,integer,numeric,character,factor,ordered,... mlr3,mlr3measures
12:      neighboursCount,sampleSize                       integer,numeric,factor,ordered     FSelectorRcpp
13:                          method logical,integer,numeric,character,factor,ordered,...              mlr3</code></pre>
</div>
</div>
<p>Looking at the table <a
href="https://mlr3filters.mlr-org.com/">here</a>, potential filters
are:</p>
<p><code>cmim</code>, <code>disr</code>, <code>importance</code>,
<code>information_gain</code>, <code>jmi</code>, <code>jmim</code>,
<code>mim</code>, <code>mrmr</code>, <code>njmim</code>,
<code>performance</code>, <code>permutation</code>, <code>relief</code>,
<code>selected_features</code>.</p>
<p>You can read their documentation by looking at
<code>?mlr_filters_&lt;id&gt;</code>, (<code>&lt;id&gt;</code> should be
replaced with the filter id, e.g., <code>cmim</code>).</p>
<p>Note that <code>importance</code>, <code>performance</code>,
<code>permutation</code>, and <code>selected_features</code> are special
in the sense that they require <code>Learner</code>s themselves.</p>\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"exercise-2-information-gain-filter\">\n<h2 class=\"anchored\" data-anchor-id=\"exercise-2-information-gain-filter\">Exercise 2: Information Gain Filter</h2>\n<p>We now want to use the <code>information_gain</code> filter which requires to install the <code>FSelectorRcpp</code> package. This filter quantifies the gain in information by considering the following difference: <code>H(Target) + H(Feature) - H(Target, Feature)</code> Here, <code>H(X)</code> is the Shannon entropy for variable <code>X</code> and <code>H(X, Y)</code> is the joint Shannon entropy for variable <code>X</code> conditioned on <code>Y</code>.</p>\n<p>Create an information gain filter and compute the information gain for each feature.</p>\n<p>Visualize the score for each feature and decide how many and which features to include.</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Use <code>flt(\"information_gain\")</code> to create an <code>information_gain</code> filter and calculate the filter scores of the features. See <code>?mlr_filters_information_gain</code> (or equivalently <code>flt(\"information_gain\")$help()</code>) for more details on how to use a filter. If it does not work, you can use e.g. <code>flt(\"importance\", learner = lrn(\"classif.rpart\"))</code> which uses the feature importance of a <code>classif.rpart</code> decision tree to rank the features for the feature filter.</p>\n<p>For visualization, you can, for example, create a scree plot (similar as in principle component analysis) that plots the filter score for each feature on the y-axis and the features on the x-axis.</p>\n<p>Using a rule of thumb, e.g., the ‘’elbow rule’’ you can determine the number of features to include.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3filters)\nlibrary(mlr3viz)\nlibrary(FSelectorRcpp)\nfilter = flt(...)\nfilter$calculate()\nautoplot(...)</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-2\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-2-contents callout-collapse collapse\" id=\"callout-2\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-35e8adca8bcecc9bd008a2f7-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5saWJyYXJ5PC9zcGFuPihtbHIzZmlsdGVycyk8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMiI+PGEgaHJlZj0iI2NiMS0yIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5saWJyYXJ5PC9zcGFuPihtbHIzdml6KTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS0zIj48YSBocmVmPSIjY2IxLTMiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT48c3BhbiBjbGFzcz0iZnUiPmxpYnJhcnk8L3NwYW4+KEZTZWxlY3RvclJjcHApPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTQiPjxhIGhyZWY9IiNjYjEtNCIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPmZpbHRlciA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+Zmx0PC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O2luZm9ybWF0aW9uX2dhaW4mcXVvdDs8L3NwYW4+KTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS01Ij48YSBocmVmPSIjY2IxLTUiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT5maWx0ZXI8c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+PHNwYW4gY2xhc3M9ImZ1Ij5jYWxjdWxhdGU8L3NwYW4+KHRhc2spPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTYiPjxhIGhyZWY9IiNjYjEtNiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPjxzcGFuIGNsYXNzPSJmdSI+YXV0b3Bsb3Q8L3NwYW4+KGZpbHRlcikgIDxzcGFuIGNsYXNzPSJjbyI+IyBzdGF0dXMsIGNyZWRpdF9oaXN0b3J5IGFuZCBzYXZpbmdzPC9zcGFuPjwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0LWRpc3BsYXkiPgo8cD48aW1nIHNyYz0iaW5kZXhfZmlsZXMvZmlndXJlLWh0bWwvdW5uYW1lZC1jaHVuay03LTEucG5nIgpkYXRhLWZpZy1hbGlnbj0iY2VudGVyIiB3aWR0aD0iNjcyIiAvPjwvcD4KPC9kaXY+CjwvZGl2Pg==\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"exercise-3-create-and-apply-a-pipeopfilter-to-a-task\">\n<h2 class=\"anchored\" data-anchor-id=\"exercise-3-create-and-apply-a-pipeopfilter-to-a-task\">Exercise 3: Create and Apply a PipeOpFilter to a Task</h2>\n<p>Since the k-NN learner suffers from the curse of dimensionality, we want set up a preprocessing <code>PipeOp</code> to subset our set of features to the 5 most important ones according to the information gain filter (see <code>flt(\"information_gain\")$help()</code>). In general, you can see a list of other possible filters by looking at the dictionary <code>as.data.table(mlr_filters)</code>. You can construct a <code>PipeOp</code> object with the <code>po()</code> function from the <code>mlr3pipelines</code> package. See <code>mlr_pipeops$keys()</code> for possible choices. Create a <code>PipeOp</code> that filters features of the <code>german_credit</code> task and creates a new task containing only the 5 most important ones according to the information gain filter.</p>\n<!-- <details> -->\n<!-- <summary>**Details on the ANOVA F-test filter:**</summary> -->\n<!-- The filter conducts an analysis of variance for each feature, where the feature explains the target class variable. -->\n<!-- The score is determined by the F statistic's value. -->\n<!-- The more different the mean values of a feature between the target classes are, the higher is the F statistic. -->\n<!-- </details> -->\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<ul>\n<li>The filter can be created by <code>flt(\"information_gain\")</code> (see also the help page <code>flt(\"information_gain\")$help()</code>).</li>\n<li>In our case, we have to pass the <code>\"filter\"</code> key to the first argument of the <code>po()</code> function and the filter previously created with the <code>flt</code> function to the <code>filter</code> argument of the <code>po()</code> function to construct a <code>PipeOpFilter</code> object that performs feature filtering (see also code examples in the help page <code>?PipeOpFilter</code>).</li>\n<li>The help page of <code>?PipeOpFilter</code> also reveals the parameters we can specify. For example, to select the 5 most important features, we can set <code>filter.nfeat</code>. This can be done using the <code>param_vals</code> argument of the <code>po()</code> function during construction or by adding the parameter value to the <code>param_set$values</code> field of an already created <code>PipeOpFilter</code> object (see also code examples in the help page).</li>\n<li>The created <code>PipeOpFilter</code> object can be applied to a <code>Task</code> object to create the filtered <code>Task</code>. To do so, we can use the <code>$train(input)</code> field of the <code>PipeOpFilter</code> object and pass a <strong>list</strong> containing the task we want to filter.</li>\n</ul>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3pipelines)\n# Set the filter.nfeat parameter directly when constructing the PipeOp:\npofilter = po(\"...\",\n  filter = flt(...),\n   ... = list(filter.nfeat = ...))\n\n# Alternative (first create the filter PipeOp and then set the parameter):\npofilter = po(\"...\", filter = flt(...))\npofilter$...$filter.nfeat = ...\n\n# Train the PipeOpFilter on the task\nfiltered_task = pofilter$train(input = list(...))\nfiltered_task\ntask</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-3\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-3-contents callout-collapse collapse\" id=\"callout-3\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-35e8adca8bcecc9bd008a2f7-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"exercise-4-combine-pipeopfilter-with-a-learner\">\n<h2 class=\"anchored\" data-anchor-id=\"exercise-4-combine-pipeopfilter-with-a-learner\">Exercise 4: Combine PipeOpFilter with a Learner</h2>\n<p>Do the following tasks:</p>\n<ol type=\"1\">\n<li>Combine the <code>PipeOpFilter</code> from the previous exercise with a k-NN learner to create a so-called <code>Graph</code> (it can contain multiple preprocessing steps) using the <code>%&gt;&gt;%</code> operator.</li>\n<li>Convert the <code>Graph</code> to a <code>GraphLearner</code> so that it behaves like a new learner that first does feature filtering and then trains a model on the filtered data and run the <code>resample()</code> function to estimate the performance of the <code>GraphLearner</code> with a 5-fold cross-validation.</li>\n<li>Change the value of the <code>nfeat.filter</code> parameter (which was set to 5 in the previous exercise) and run again <code>resample()</code>.</li>\n</ol>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<ul>\n<li>Create a kNN learner using <code>lrn()</code>. Remember that the shortcut for a kNN classifier ist <code>\"classif.kknn\"</code>.</li>\n<li>You can concatenate different preprocessing steps and a learner using the <code>%&gt;&gt;%</code> operator.</li>\n<li>Use <code>as_learner</code> to create a <code>GraphLearner</code> (see also the code examples in the help page <code>?GraphLearner</code>).</li>\n</ul>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3learners)\ngraph = ... %&gt;&gt;% lrn(\"...\")\nglrn = as_learner(...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$aggregate()\n\n# Change `nfeat.filter` and run resampling again using same train-test splits\n...\nrr2 = resample(task = ..., learner = ..., resampling = rr$resampling)\nrr2$aggregate()</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-4\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-4-contents callout-collapse collapse\" id=\"callout-4\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-35e8adca8bcecc9bd008a2f7-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"summary\">\n<h1>Summary</h1>\n<p>We learned how to use feature filters to rank the features w.r.t. a feature filter method in a supervised setting and how to subset a task accordingly.</p>\n<p>Ideally, feature filtering is directly incorporated into the learning procedure by making use of a pipeline so that performance estimation after feature filtering is not biased.</p>\n<!-- In later exercises, we will see how the performance of a whole pipeline can be properly evaluated. -->\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-05-05-adv-feature-preproc-filter/\"> mlr-org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Filter\nPosted on\nMay 27, 2025\nby\nGiuseppe Casalicchio\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nmlr-org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nJavaScript is required to unlock solutions.\nPlease enable JavaScript and reload the page,\nor download the source files from\nGitHub\nand run the code locally.\nGoal\nLearn how to rank features of a supervised task by their importance / strength of relationship with the target variable using a feature filter method.\nGerman Credit Dataset\nWe create the task as for the resampling exercise: The German Credit Data set.\nlibrary(\"mlr3verse\")\nlibrary(\"data.table\")\ntask = tsk(\"german_credit\")\nExercises\nWithin the\nmlr3\necosystem, feature filters are implemented in the\nmlr3filters\npackage and are typically used in combination with\nmlr3pipelines\nto be able to include the whole preprocessing step in a pipeline. In exercises 1 to 3, we apply feature filtering to preprocess the data of a task without using a pipeline. In exercise 4, we will set up a pipeline that combines a learner with the feature filtering as preprocessing step.\nExercise 1: Find a suitable Feature Filter\nMake yourself familiar with the\nmlr3filters\npackage (\nlink\n). Which\nFilter\ns are applicable to all feature types from the task we created above?\nHint:\nSome filters are only applicable to either classification or regression or either numeric or categorical features. Therefore, we are looking for a\nFilter\nthat is applicable to our classification task and that can be computed for\ninteger\nand\nfactor\nfeatures (as these types of features are present in task, see\ntask$feature_types\n).\nThe website linked above includes a table that provides detailed information for each\nFilter\n.\nSolution\nUnlock solution\nExercise 2: Information Gain Filter\nWe now want to use the\ninformation_gain\nfilter which requires to install the\nFSelectorRcpp\npackage. This filter quantifies the gain in information by considering the following difference:\nH(Target) + H(Feature) - H(Target, Feature)\nHere,\nH(X)\nis the Shannon entropy for variable\nX\nand\nH(X, Y)\nis the joint Shannon entropy for variable\nX\nconditioned on\nY\n.\nCreate an information gain filter and compute the information gain for each feature.\nVisualize the score for each feature and decide how many and which features to include.\nHint 1:\nUse\nflt(\"information_gain\")\nto create an\ninformation_gain\nfilter and calculate the filter scores of the features. See\n?mlr_filters_information_gain\n(or equivalently\nflt(\"information_gain\")$help()\n) for more details on how to use a filter. If it does not work, you can use e.g.\nflt(\"importance\", learner = lrn(\"classif.rpart\"))\nwhich uses the feature importance of a\nclassif.rpart\ndecision tree to rank the features for the feature filter.\nFor visualization, you can, for example, create a scree plot (similar as in principle component analysis) that plots the filter score for each feature on the y-axis and the features on the x-axis.\nUsing a rule of thumb, e.g., the ‘’elbow rule’’ you can determine the number of features to include.\nHint 2:\nlibrary(mlr3filters)\nlibrary(mlr3viz)\nlibrary(FSelectorRcpp)\nfilter = flt(...)\nfilter$calculate()\nautoplot(...)\nSolution\nUnlock solution\nExercise 3: Create and Apply a PipeOpFilter to a Task\nSince the k-NN learner suffers from the curse of dimensionality, we want set up a preprocessing\nPipeOp\nto subset our set of features to the 5 most important ones according to the information gain filter (see\nflt(\"information_gain\")$help()\n). In general, you can see a list of other possible filters by looking at the dictionary\nas.data.table(mlr_filters)\n. You can construct a\nPipeOp\nobject with the\npo()\nfunction from the\nmlr3pipelines\npackage. See\nmlr_pipeops$keys()\nfor possible choices. Create a\nPipeOp\nthat filters features of the\ngerman_credit\ntask and creates a new task containing only the 5 most important ones according to the information gain filter.\nHint 1:\nThe filter can be created by\nflt(\"information_gain\")\n(see also the help page\nflt(\"information_gain\")$help()\n).\nIn our case, we have to pass the\n\"filter\"\nkey to the first argument of the\npo()\nfunction and the filter previously created with the\nflt\nfunction to the\nfilter\nargument of the\npo()\nfunction to construct a\nPipeOpFilter\nobject that performs feature filtering (see also code examples in the help page\n?PipeOpFilter\n).\nThe help page of\n?PipeOpFilter\nalso reveals the parameters we can specify. For example, to select the 5 most important features, we can set\nfilter.nfeat\n. This can be done using the\nparam_vals\nargument of the\npo()\nfunction during construction or by adding the parameter value to the\nparam_set$values\nfield of an already created\nPipeOpFilter\nobject (see also code examples in the help page).\nThe created\nPipeOpFilter\nobject can be applied to a\nTask\nobject to create the filtered\nTask\n. To do so, we can use the\n$train(input)\nfield of the\nPipeOpFilter\nobject and pass a\nlist\ncontaining the task we want to filter.\nHint 2:\nlibrary(mlr3pipelines)\n# Set the filter.nfeat parameter directly when constructing the PipeOp:\npofilter = po(\"...\",\n  filter = flt(...),\n   ... = list(filter.nfeat = ...))\n\n# Alternative (first create the filter PipeOp and then set the parameter):\npofilter = po(\"...\", filter = flt(...))\npofilter$...$filter.nfeat = ...\n\n# Train the PipeOpFilter on the task\nfiltered_task = pofilter$train(input = list(...))\nfiltered_task\ntask\nSolution\nUnlock solution\nExercise 4: Combine PipeOpFilter with a Learner\nDo the following tasks:\nCombine the\nPipeOpFilter\nfrom the previous exercise with a k-NN learner to create a so-called\nGraph\n(it can contain multiple preprocessing steps) using the\n%>>%\noperator.\nConvert the\nGraph\nto a\nGraphLearner\nso that it behaves like a new learner that first does feature filtering and then trains a model on the filtered data and run the\nresample()\nfunction to estimate the performance of the\nGraphLearner\nwith a 5-fold cross-validation.\nChange the value of the\nnfeat.filter\nparameter (which was set to 5 in the previous exercise) and run again\nresample()\n.\nHint 1:\nCreate a kNN learner using\nlrn()\n. Remember that the shortcut for a kNN classifier ist\n\"classif.kknn\"\n.\nYou can concatenate different preprocessing steps and a learner using the\n%>>%\noperator.\nUse\nas_learner\nto create a\nGraphLearner\n(see also the code examples in the help page\n?GraphLearner\n).\nHint 2:\nlibrary(mlr3learners)\ngraph = ... %>>% lrn(\"...\")\nglrn = as_learner(...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$aggregate()\n\n# Change `nfeat.filter` and run resampling again using same train-test splits\n...\nrr2 = resample(task = ..., learner = ..., resampling = rr$resampling)\nrr2$aggregate()\nSolution\nUnlock solution\nSummary\nWe learned how to use feature filters to rank the features w.r.t. a feature filter method in a supervised setting and how to subset a task accordingly.\nIdeally, feature filtering is directly incorporated into the learning procedure by making use of a pipeline so that performance estimation after feature filtering is not biased.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nmlr-org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Learn how to rank features of a supervised task by their importance / strength of relationship with the target variable using a feature filter method. German Credit Dataset We create the task as for the resampling exercise: The German Credit Data set. library(\"mlr3verse\") library(\"data.table\") task = tsk(\"german_credit\") Exercises Within the mlr3 ecosystem, feature filters are implemented in the mlr3filters package and are typically used in combination with mlr3pipelines to be able to include the whole preprocessing step in a pipeline. In exercises 1 to 3, we apply feature filtering to preprocess the data of a task without using a pipeline. In exercise 4, we will set up a pipeline that combines a learner with the feature filtering as preprocessing step. Exercise 1: Find a suitable Feature Filter Make yourself familiar with the mlr3filters package (link). Which Filters are applicable to all feature types from the task we created above? Hint: Some filters are only applicable to either classification or regression or either numeric or categorical features. Therefore, we are looking for a Filter that is applicable to our classification task and that can be computed for integer and factor features (as these types of features are present in task, see task$feature_types). The website linked above includes a table that provides detailed information for each Filter. Solution Unlock solution Exercise 2: Information Gain Filter We now want to use the information_gain filter which requires to install the FSelectorRcpp package. This filter quantifies the gain in information by considering the following difference: H(Target) + H(Feature) - H(Target, Feature) Here, H(X) is the Shannon entropy for variable X and H(X, Y) is the joint Shannon entropy for variable X conditioned on Y. Create an information gain filter and compute the information gain for each feature. Visualize the score for each feature and decide how many and which features to include. Hint 1: Use flt(\"information_gain\") to create an information_gain filter and calculate the filter scores of the features. See ?mlr_filters_information_gain (or equivalently flt(\"information_gain\")$help()) for more details on how to use a filter. If it does not work, you can use e.g. flt(\"importance\", learner = lrn(\"classif.rpart\")) which uses the feature importance of a classif.rpart decision tree to rank the features for the feature filter. For visualization, you can, for example, create a scree plot (similar as in principle component analysis) that plots the filter score for each feature on the y-axis and the features on the x-axis. Using a rule of thumb, e.g., the ‘’elbow rule’’ you can determine the number of features to include. Hint 2: library(mlr3filters) library(mlr3viz) library(FSelectorRcpp) filter = flt(...) filter$calculate() autoplot(...) Solution Unlock solution Exercise 3: Create and Apply a PipeOpFilter to a Task Since the k-NN learner suffers from the curse of dimensionality, we want set up a preprocessing PipeOp to subset our set of features to the 5 most important ones according to the information gain filter (see flt(\"information_gain\")$help()). In general, you can see a list of other possible filters by looking at the dictionary as.data.table(mlr_filters). You can construct a PipeOp object with the po() function from the mlr3pipelines package. See mlr_pipeops$keys() for possible choices. Create a PipeOp that filters features of the german_credit task and creates a new task containing only the 5 most important ones according to the information gain filter. Hint 1: The filter can be created by flt(\"information_gain\") (see also the help page flt(\"information_gain\")$help()). In our case, we have to pass the \"filter\" key to the first argument of the po() function and the filter previously created with the flt function to the filter argument of the po() function to construct a PipeOpFilter object that performs feature filtering (see also code examples in the help page ?PipeOpFilter). The help page of ?PipeOpFilter also reveals the parameters we can specify. For example, to select the 5 most important features, we can set filter.nfeat. This can be done using the param_vals argument of the po() function during construction or by adding the parameter value to the param_set$values field of an already created PipeOpFilter object (see also code examples in the help page). The created PipeOpFilter object can be applied to a Task object to create the filtered Task. To do so, we can use the $train(input) field of the PipeOpFilter object and pass a list containing the task we want to filter. Hint 2: library(mlr3pipelines) # Set the filter.nfeat parameter directly when constructing the PipeOp: pofilter = po(\"...\", filter = flt(...), ... = list(filter.nfeat = ...)) # Alternative (first create the filter PipeOp and then set the parameter): pofilter = po(\"...\", filter = flt(...)) pofilter$...$filter.nfeat = ... # Train the PipeOpFilter on the task filtered_task = pofilter$train(input = list(...)) filtered_task task Solution Unlock solution Exercise 4: Combine PipeOpFilter with a Learner Do the following tasks: Combine the PipeOpFilter from the previous exercise with a k-NN learner to create a so-called Graph (it can contain multiple preprocessing steps) using the %>>% operator. Convert the Graph to a GraphLearner so that it behaves like a new learner that first does feature filtering and then trains a model on the filtered data and run the resample() function to estimate the performance of the GraphLearner with a 5-fold cross-validation. Change the value of the nfeat.filter parameter (which was set to 5 in the previous exercise) and run again resample(). Hint 1: Create a kNN learner using lrn(). Remember that the shortcut for a kNN classifier ist \"classif.kknn\". You can concatenate different preprocessing steps and a learner using the %>>% operator. Use as_learner to create a GraphLearner (see also the code examples in the help page ?GraphLearner). Hint 2: library(mlr3learners) graph = ... %>>% lrn(\"...\") glrn = as_learner(...) rr = resample(task = ..., learner = ..., resampling = ...) rr$aggregate() # Change `nfeat.filter` and run resampling again using same train-test splits ... rr2 = resample(task = ..., learner = ..., resampling = rr$resampling) rr2$aggregate() Solution Unlock solution Summary We learned how to use feature filters to rank the features w.r.t. a feature filter method in a supervised setting and how to subset a task accordingly. Ideally, feature filtering is directly incorporated into the learning procedure by making use of a pipeline so that performance estimation after feature filtering is not biased.",
    "meta_keywords": null,
    "og_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Learn how to rank features of a supervised task by their importance / strength of relationship with the target variable using a feature filter method. German Credit Dataset We create the task as for the resampling exercise: The German Credit Data set. library(\"mlr3verse\") library(\"data.table\") task = tsk(\"german_credit\") Exercises Within the mlr3 ecosystem, feature filters are implemented in the mlr3filters package and are typically used in combination with mlr3pipelines to be able to include the whole preprocessing step in a pipeline. In exercises 1 to 3, we apply feature filtering to preprocess the data of a task without using a pipeline. In exercise 4, we will set up a pipeline that combines a learner with the feature filtering as preprocessing step. Exercise 1: Find a suitable Feature Filter Make yourself familiar with the mlr3filters package (link). Which Filters are applicable to all feature types from the task we created above? Hint: Some filters are only applicable to either classification or regression or either numeric or categorical features. Therefore, we are looking for a Filter that is applicable to our classification task and that can be computed for integer and factor features (as these types of features are present in task, see task$feature_types). The website linked above includes a table that provides detailed information for each Filter. Solution Unlock solution Exercise 2: Information Gain Filter We now want to use the information_gain filter which requires to install the FSelectorRcpp package. This filter quantifies the gain in information by considering the following difference: H(Target) + H(Feature) - H(Target, Feature) Here, H(X) is the Shannon entropy for variable X and H(X, Y) is the joint Shannon entropy for variable X conditioned on Y. Create an information gain filter and compute the information gain for each feature. Visualize the score for each feature and decide how many and which features to include. Hint 1: Use flt(\"information_gain\") to create an information_gain filter and calculate the filter scores of the features. See ?mlr_filters_information_gain (or equivalently flt(\"information_gain\")$help()) for more details on how to use a filter. If it does not work, you can use e.g. flt(\"importance\", learner = lrn(\"classif.rpart\")) which uses the feature importance of a classif.rpart decision tree to rank the features for the feature filter. For visualization, you can, for example, create a scree plot (similar as in principle component analysis) that plots the filter score for each feature on the y-axis and the features on the x-axis. Using a rule of thumb, e.g., the ‘’elbow rule’’ you can determine the number of features to include. Hint 2: library(mlr3filters) library(mlr3viz) library(FSelectorRcpp) filter = flt(...) filter$calculate() autoplot(...) Solution Unlock solution Exercise 3: Create and Apply a PipeOpFilter to a Task Since the k-NN learner suffers from the curse of dimensionality, we want set up a preprocessing PipeOp to subset our set of features to the 5 most important ones according to the information gain filter (see flt(\"information_gain\")$help()). In general, you can see a list of other possible filters by looking at the dictionary as.data.table(mlr_filters). You can construct a PipeOp object with the po() function from the mlr3pipelines package. See mlr_pipeops$keys() for possible choices. Create a PipeOp that filters features of the german_credit task and creates a new task containing only the 5 most important ones according to the information gain filter. Hint 1: The filter can be created by flt(\"information_gain\") (see also the help page flt(\"information_gain\")$help()). In our case, we have to pass the \"filter\" key to the first argument of the po() function and the filter previously created with the flt function to the filter argument of the po() function to construct a PipeOpFilter object that performs feature filtering (see also code examples in the help page ?PipeOpFilter). The help page of ?PipeOpFilter also reveals the parameters we can specify. For example, to select the 5 most important features, we can set filter.nfeat. This can be done using the param_vals argument of the po() function during construction or by adding the parameter value to the param_set$values field of an already created PipeOpFilter object (see also code examples in the help page). The created PipeOpFilter object can be applied to a Task object to create the filtered Task. To do so, we can use the $train(input) field of the PipeOpFilter object and pass a list containing the task we want to filter. Hint 2: library(mlr3pipelines) # Set the filter.nfeat parameter directly when constructing the PipeOp: pofilter = po(\"...\", filter = flt(...), ... = list(filter.nfeat = ...)) # Alternative (first create the filter PipeOp and then set the parameter): pofilter = po(\"...\", filter = flt(...)) pofilter$...$filter.nfeat = ... # Train the PipeOpFilter on the task filtered_task = pofilter$train(input = list(...)) filtered_task task Solution Unlock solution Exercise 4: Combine PipeOpFilter with a Learner Do the following tasks: Combine the PipeOpFilter from the previous exercise with a k-NN learner to create a so-called Graph (it can contain multiple preprocessing steps) using the %>>% operator. Convert the Graph to a GraphLearner so that it behaves like a new learner that first does feature filtering and then trains a model on the filtered data and run the resample() function to estimate the performance of the GraphLearner with a 5-fold cross-validation. Change the value of the nfeat.filter parameter (which was set to 5 in the previous exercise) and run again resample(). Hint 1: Create a kNN learner using lrn(). Remember that the shortcut for a kNN classifier ist \"classif.kknn\". You can concatenate different preprocessing steps and a learner using the %>>% operator. Use as_learner to create a GraphLearner (see also the code examples in the help page ?GraphLearner). Hint 2: library(mlr3learners) graph = ... %>>% lrn(\"...\") glrn = as_learner(...) rr = resample(task = ..., learner = ..., resampling = ...) rr$aggregate() # Change `nfeat.filter` and run resampling again using same train-test splits ... rr2 = resample(task = ..., learner = ..., resampling = rr$resampling) rr2$aggregate() Solution Unlock solution Summary We learned how to use feature filters to rank the features w.r.t. a feature filter method in a supervised setting and how to subset a task accordingly. Ideally, feature filtering is directly incorporated into the learning procedure by making use of a pipeline so that performance estimation after feature filtering is not biased.",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "Filter | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 6.1,
    "sitemap_lastmod": null,
    "twitter_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Learn how to rank features of a supervised task by their importance / strength of relationship with the target variable using a feature filter method. German Credit Dataset We create the task as for the resampling exercise: The German Credit Data set. library(\"mlr3verse\") library(\"data.table\") task = tsk(\"german_credit\") Exercises Within the mlr3 ecosystem, feature filters are implemented in the mlr3filters package and are typically used in combination with mlr3pipelines to be able to include the whole preprocessing step in a pipeline. In exercises 1 to 3, we apply feature filtering to preprocess the data of a task without using a pipeline. In exercise 4, we will set up a pipeline that combines a learner with the feature filtering as preprocessing step. Exercise 1: Find a suitable Feature Filter Make yourself familiar with the mlr3filters package (link). Which Filters are applicable to all feature types from the task we created above? Hint: Some filters are only applicable to either classification or regression or either numeric or categorical features. Therefore, we are looking for a Filter that is applicable to our classification task and that can be computed for integer and factor features (as these types of features are present in task, see task$feature_types). The website linked above includes a table that provides detailed information for each Filter. Solution Unlock solution Exercise 2: Information Gain Filter We now want to use the information_gain filter which requires to install the FSelectorRcpp package. This filter quantifies the gain in information by considering the following difference: H(Target) + H(Feature) - H(Target, Feature) Here, H(X) is the Shannon entropy for variable X and H(X, Y) is the joint Shannon entropy for variable X conditioned on Y. Create an information gain filter and compute the information gain for each feature. Visualize the score for each feature and decide how many and which features to include. Hint 1: Use flt(\"information_gain\") to create an information_gain filter and calculate the filter scores of the features. See ?mlr_filters_information_gain (or equivalently flt(\"information_gain\")$help()) for more details on how to use a filter. If it does not work, you can use e.g. flt(\"importance\", learner = lrn(\"classif.rpart\")) which uses the feature importance of a classif.rpart decision tree to rank the features for the feature filter. For visualization, you can, for example, create a scree plot (similar as in principle component analysis) that plots the filter score for each feature on the y-axis and the features on the x-axis. Using a rule of thumb, e.g., the ‘’elbow rule’’ you can determine the number of features to include. Hint 2: library(mlr3filters) library(mlr3viz) library(FSelectorRcpp) filter = flt(...) filter$calculate() autoplot(...) Solution Unlock solution Exercise 3: Create and Apply a PipeOpFilter to a Task Since the k-NN learner suffers from the curse of dimensionality, we want set up a preprocessing PipeOp to subset our set of features to the 5 most important ones according to the information gain filter (see flt(\"information_gain\")$help()). In general, you can see a list of other possible filters by looking at the dictionary as.data.table(mlr_filters). You can construct a PipeOp object with the po() function from the mlr3pipelines package. See mlr_pipeops$keys() for possible choices. Create a PipeOp that filters features of the german_credit task and creates a new task containing only the 5 most important ones according to the information gain filter. Hint 1: The filter can be created by flt(\"information_gain\") (see also the help page flt(\"information_gain\")$help()). In our case, we have to pass the \"filter\" key to the first argument of the po() function and the filter previously created with the flt function to the filter argument of the po() function to construct a PipeOpFilter object that performs feature filtering (see also code examples in the help page ?PipeOpFilter). The help page of ?PipeOpFilter also reveals the parameters we can specify. For example, to select the 5 most important features, we can set filter.nfeat. This can be done using the param_vals argument of the po() function during construction or by adding the parameter value to the param_set$values field of an already created PipeOpFilter object (see also code examples in the help page). The created PipeOpFilter object can be applied to a Task object to create the filtered Task. To do so, we can use the $train(input) field of the PipeOpFilter object and pass a list containing the task we want to filter. Hint 2: library(mlr3pipelines) # Set the filter.nfeat parameter directly when constructing the PipeOp: pofilter = po(\"...\", filter = flt(...), ... = list(filter.nfeat = ...)) # Alternative (first create the filter PipeOp and then set the parameter): pofilter = po(\"...\", filter = flt(...)) pofilter$...$filter.nfeat = ... # Train the PipeOpFilter on the task filtered_task = pofilter$train(input = list(...)) filtered_task task Solution Unlock solution Exercise 4: Combine PipeOpFilter with a Learner Do the following tasks: Combine the PipeOpFilter from the previous exercise with a k-NN learner to create a so-called Graph (it can contain multiple preprocessing steps) using the %>>% operator. Convert the Graph to a GraphLearner so that it behaves like a new learner that first does feature filtering and then trains a model on the filtered data and run the resample() function to estimate the performance of the GraphLearner with a 5-fold cross-validation. Change the value of the nfeat.filter parameter (which was set to 5 in the previous exercise) and run again resample(). Hint 1: Create a kNN learner using lrn(). Remember that the shortcut for a kNN classifier ist \"classif.kknn\". You can concatenate different preprocessing steps and a learner using the %>>% operator. Use as_learner to create a GraphLearner (see also the code examples in the help page ?GraphLearner). Hint 2: library(mlr3learners) graph = ... %>>% lrn(\"...\") glrn = as_learner(...) rr = resample(task = ..., learner = ..., resampling = ...) rr$aggregate() # Change `nfeat.filter` and run resampling again using same train-test splits ... rr2 = resample(task = ..., learner = ..., resampling = rr$resampling) rr2$aggregate() Solution Unlock solution Summary We learned how to use feature filters to rank the features w.r.t. a feature filter method in a supervised setting and how to subset a task accordingly. Ideally, feature filtering is directly incorporated into the learning procedure by making use of a pipeline so that performance estimation after feature filtering is not biased.",
    "twitter_title": "Filter | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/05/filter/",
    "word_count": 1211
  }
}