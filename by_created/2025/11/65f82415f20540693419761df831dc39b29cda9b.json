{
  "id": "65f82415f20540693419761df831dc39b29cda9b",
  "url": "https://www.r-bloggers.com/2025/01/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/",
  "created_at_utc": "2025-11-22T19:59:27Z",
  "data": null,
  "raw_original": {
    "uuid": "ce825170-1029-4f27-bf8d-868c18425af5",
    "created_at": "2025-11-22 19:59:27",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/01/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/",
      "crawled_at": "2025-11-22T10:55:36.914044",
      "external_links": [
        {
          "href": "https://www.gilesd-j.com/2025/01/20/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/",
          "text": "R-Programming – Giles Dickenson-Jones"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://bsky.app/profile/gdeejay.bsky.social",
          "text": "Bluesky"
        },
        {
          "href": "https://www.linkedin.com/in/giles-dickenson-jones/",
          "text": "Linkedin"
        },
        {
          "href": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics/cases",
          "text": "shove ‘AI’ features into their products"
        },
        {
          "href": "https://www.gilesd-j.com/2024/01/18/how-i-chatgpt-as-a-public-policy-professional/",
          "text": "I wrote up a guide on how I use ChatGPT"
        },
        {
          "href": "https://theconversation.com/wondering-what-ai-actually-is-here-are-the-7-things-it-can-do-for-you-239843",
          "text": "can"
        },
        {
          "href": "https://www.newscientist.com/article/2444870-ai-models-cant-learn-as-they-go-along-like-humans-do/",
          "text": "cannot"
        },
        {
          "href": "https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf",
          "text": "noting we’re still figuring this out"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Large_language_model",
          "text": "A LLM is a type of machine learning model that can be helpful for natural language processing and generation tasks"
        },
        {
          "href": "https://www.sumsar.net/blog/call-chatgpt-from-r/",
          "text": "tapping into API"
        },
        {
          "href": "https://themockup.blog/posts/2020-05-22-parsing-json-in-r-with-jsonlite/",
          "text": "wrangling JSON data"
        },
        {
          "href": "https://github.com/hadley",
          "text": "Hadley Wickham"
        },
        {
          "href": "https://github.com/tidyverse/ellmer",
          "text": "the ellmer package"
        },
        {
          "href": "https://github.com/tidyverse/ellmer",
          "text": "github"
        },
        {
          "href": "https://ellmer.tidyverse.org/",
          "text": "tidyverse"
        },
        {
          "href": "https://www.anthropic.com/claude",
          "text": "Anthropic’s Claude model"
        },
        {
          "href": "https://lmstudio.ai/terms",
          "text": "terms of service"
        },
        {
          "href": "https://lmstudio.ai/download",
          "text": "here"
        },
        {
          "href": "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/",
          "text": "check out Hugging Face’s leaderboard"
        },
        {
          "href": "https://github.com/tidyverse/ellmer",
          "text": "see here"
        },
        {
          "href": "https://lmstudio.ai/docs/api/openai-api",
          "text": "the documentation"
        },
        {
          "href": "https://github.com/tidyverse/ellmer",
          "text": "on the Github page"
        },
        {
          "href": "https://floating-point-gui.de/basic/",
          "text": "floating point errors"
        },
        {
          "href": "https://garymarcus.substack.com/p/math-is-hard-if-you-are-an-llm-and",
          "text": "notoriously bad at arithmetic"
        },
        {
          "href": "https://arxiv.org/pdf/2311.05232",
          "text": "‘hallucinate’"
        },
        {
          "href": "https://arxiv.org/html/2404.18824v1",
          "text": "might have been trained on coding problems resembling our task"
        },
        {
          "href": "https://www.gilesd-j.com/2025/01/20/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/",
          "text": "R-Programming – Giles Dickenson-Jones"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "From code to conversation: Localized AI fun with LM Studio and the ellmer package | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/wmvs29uce2r01.jpg?resize=259%2C194&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-10-1024x674.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-11.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/LM_Studio_5eOfvM0isc.gif?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/LM_Studio_XBdiumVgKM.gif?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-12.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-15.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/giles-dickenson-jones-2/",
          "text": "giles.dickenson.jones"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-389971 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">From code to conversation: Localized AI fun with LM Studio and the ellmer package</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 19, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/giles-dickenson-jones-2/\">giles.dickenson.jones</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.gilesd-j.com/2025/01/20/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/\"> R-Programming – Giles Dickenson-Jones</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p class=\"\"><strong>Summary: </strong><em>in this post I demonstrate how you can interact with locally hosted LLM models in R using the ellmer package and LM Studio.</em> </p>\n<p class=\"\"><em>If you didn’t guess already, the title of this post was suggested by a Large Language Model (LLM). The bartowski/Llama-3.2-3B-Instruct-GGUF LLM to be precise. Unfortunately for you, the rest of this blog will be written by me.</em></p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>Intro</strong></h3>\n<p class=\"\">If you’ve connected with me on <a href=\"https://bsky.app/profile/gdeejay.bsky.social\" rel=\"nofollow\" target=\"_blank\">Bluesky </a>or <a href=\"https://www.linkedin.com/in/giles-dickenson-jones/\" rel=\"nofollow\" target=\"_blank\">Linkedin</a> you’ll know I think the benefits and impact of Artificial Intelligence (AI) are frequently exaggerated. With companies increasingly trying to <a href=\"https://www.unesco.org/en/artificial-intelligence/recommendation-ethics/cases\" rel=\"nofollow\" target=\"_blank\">shove ‘AI’ features into their products </a>to satisfy executives that have no idea how it works. But, you’ll also know that I’m not blind to their utility, being a frequent user of Claude and ChatGPT for refining code, steel-manning methodologies and improving my writing. In fact, <a href=\"https://www.gilesd-j.com/2024/01/18/how-i-chatgpt-as-a-public-policy-professional/\" rel=\"nofollow\" target=\"_blank\">I wrote up a guide on how I use ChatGPT </a>early in the AI craze. </p>\n<p class=\"\">Which is why whenever I show others how to use AI tools, I spent an inordinate amount of time making it clear what they <a href=\"https://theconversation.com/wondering-what-ai-actually-is-here-are-the-7-things-it-can-do-for-you-239843\" rel=\"nofollow\" target=\"_blank\">can </a>and <a href=\"https://www.newscientist.com/article/2444870-ai-models-cant-learn-as-they-go-along-like-humans-do/\" rel=\"nofollow\" target=\"_blank\">cannot </a>do (<a href=\"https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf\" rel=\"nofollow\" target=\"_blank\">noting we’re still figuring this out</a>). Let’s just say I’m ‘skeptically enamored’ by the technology.  </p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>Interacting with LLM models in R</strong></h3>\n<p class=\"\"><a href=\"https://en.wikipedia.org/wiki/Large_language_model\" rel=\"nofollow\" target=\"_blank\">A LLM is a type of machine learning model that can be helpful for natural language processing and generation tasks</a>. Although they can be useful for a variety of tasks, I frequently employ them when I need to generate a set of boilerplate text, clean unstructured data or extract information from a document. I’ve therefore been trying to intelligently integrate LLMs more directly into my R workflow. </p>\n<p class=\"\"><em>Unfortunately</em>, this requires tapping into an LLM’s Application Programming Interface (API). <em>Fortunately</em>, <a href=\"https://www.sumsar.net/blog/call-chatgpt-from-r/\" rel=\"nofollow\" target=\"_blank\">tapping into API</a>s is possible in R. <em>Unfortunately</em>, this frequently requires interpreting poorly written developer document and <a href=\"https://themockup.blog/posts/2020-05-22-parsing-json-in-r-with-jsonlite/\" rel=\"nofollow\" target=\"_blank\">wrangling JSON data</a>. </p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img alt=\"\" class=\"wp-image-2684\" data-lazy-src=\"https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/wmvs29uce2r01.jpg?resize=259%2C194&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" fetchpriority=\"high\" height=\"194\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"259\"/><noscript><img alt=\"\" class=\"wp-image-2684\" data-recalc-dims=\"1\" decoding=\"async\" fetchpriority=\"high\" height=\"194\" loading=\"lazy\" src=\"https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/wmvs29uce2r01.jpg?resize=259%2C194&amp;ssl=1\" width=\"259\"/></noscript><figcaption class=\"wp-element-caption\">“<em>The data is also in JSON format”</em></figcaption></figure></div>\n<p class=\"\">But, <em>fortunately </em>you won’t have to suffer like I did thanks to <a href=\"https://github.com/hadley\" rel=\"nofollow\" target=\"_blank\">Hadley Wickham</a> and the team behind the <a href=\"https://github.com/tidyverse/ellmer\" rel=\"nofollow\" target=\"_blank\">the ellmer package</a>. To learn more about the package’s capabilities check out the <a href=\"https://github.com/tidyverse/ellmer\" rel=\"nofollow\" target=\"_blank\">github </a>or <a href=\"https://ellmer.tidyverse.org/\" rel=\"nofollow\" target=\"_blank\">tidyverse</a> page, but in essence it greatly simplifies interacting with the most popular LLM models.  </p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>The problem</strong></h3>\n<p class=\"\">I’ve been interacting with LLMs in R a lot recently, as I’m building an analysis pipeline that requires extracting metadata from unstructured text documents. I’ve mainly been using <a href=\"https://www.anthropic.com/claude\" rel=\"nofollow\" target=\"_blank\">Anthropic’s Claude model</a> as I find it to be more reliable than its competitors. However, the API request limits can make building a new analysis pipeline cumbersome. </p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>The solution(?)</strong></h3>\n<p class=\"\">Although I could just try to make less requests of Anthropic’s API, the problem seemed like as good an excuse as any to figure how to deploy a local LLM model that could serve as a temporary substitute until the pipeline is working. Once everything works I’d then switch back to using Claude.</p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>LM Studio</strong></h3>\n<p class=\"\">LM Studio is an application for downloading and running LLM models locally. Although it’s ‘free’  for personal use, there are limits if you’re using it for commercial purposes (see: <a href=\"https://lmstudio.ai/terms\" rel=\"nofollow\" target=\"_blank\">terms of service</a>). You can download the latest version of LM Studio <a href=\"https://lmstudio.ai/download\" rel=\"nofollow\" target=\"_blank\">here</a>. </p>\n<p class=\"\">When you first open LM Studio you should see a screen similar to below (without the annotations). The the areas of the app we’re interested include:</p>\n<ul class=\"wp-block-list\">\n<li class=\"has-black-color has-text-color has-link-color has-medium-font-size wp-elements-5e40d1ec28ccde81e6b28d46ec783215\"><strong>Chat: </strong>a chat style interface for interacting with models we’ve downloaded. </li>\n<li class=\"has-black-color has-text-color has-link-color has-medium-font-size wp-elements-aa7025532b4a658864e3fc487e5fb8ea\"><strong>Developer</strong>: options for setting up a local server for interacting with a model.</li>\n<li class=\"has-black-color has-text-color has-link-color has-medium-font-size wp-elements-4029fae211ea4d408471cd8d0c856e9e\"><strong>Models:</strong> lists models you’ve downloaded and provides an interface for altering model defaults. </li>\n<li class=\"has-black-color has-text-color has-link-color has-medium-font-size wp-elements-7a8b68a10be61222b63815f51cc6eee0\"><strong>Discover: </strong>provides an interface for finding and downloading models.</li>\n<li class=\"has-black-color has-text-color has-link-color has-medium-font-size wp-elements-c1ff6ad6b26e2c1d0ee45bb12730afdc\"><strong>Model list: </strong>for selecting a model you’d like to load. </li>\n</ul>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img alt=\"\" class=\"wp-image-2704\" data-lazy-sizes=\"(max-width: 1024px) 100vw, 1024px\" data-lazy-src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-10-1024x674.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-10-1024x674.png?w=450&amp;ssl=1 1024w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-10-300x197.png 300w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-10-768x505.png 768w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-10.png 1204w\"/><noscript><img alt=\"\" class=\"wp-image-2704\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-10-1024x674.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-10-1024x674.png?w=450&amp;ssl=1 1024w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-10-300x197.png 300w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-10-768x505.png 768w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-10.png 1204w\"/></noscript></figure></div>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"> <strong>Finding and downloading a model</strong></h3>\n<p class=\"\">Once you’ve installed LM Studio, you’ll probably want to download an LLM model. For our purposes we’ll go with a small version of Meta’s Llama model, which you can find by searching for ‘llama-3.2-3b-instruct’ via the ‘discover’ menu in LM Studio:</p>\n<p class=\"\">(You can also <a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/\" rel=\"nofollow\" target=\"_blank\">check out Hugging Face’s leaderboard </a>for a ranking of open LLM models).</p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img alt=\"\" class=\"wp-image-2705\" data-lazy-sizes=\"(max-width: 985px) 100vw, 985px\" data-lazy-src=\"https://i2.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-11.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-11.png?w=450&amp;ssl=1 985w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-11-300x196.png 300w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-11-768x501.png 768w\"/><noscript><img alt=\"\" class=\"wp-image-2705\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 985px) 100vw, 985px\" src=\"https://i2.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-11.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-11.png?w=450&amp;ssl=1 985w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-11-300x196.png 300w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-11-768x501.png 768w\"/></noscript><figcaption class=\"wp-element-caption\"><em>Downloading the <em>llama-3.2-3b-instruct</em> model</em></figcaption></figure></div>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\">L<strong>oading and interacting with a model</strong></h3>\n<p class=\"\">Once you’ve downloaded the model, there are a number of ways to interact with it. The simplest way is to loaded it via the dropdown menu and chat with it directly using the ‘Chat’ panel:</p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img alt=\"\" class=\"wp-image-2706\" data-lazy-src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/LM_Studio_5eOfvM0isc.gif?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" class=\"wp-image-2706\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/LM_Studio_5eOfvM0isc.gif?w=450&amp;ssl=1\"/></noscript><figcaption class=\"wp-element-caption\"><em>llama-3.2-3b-instruct</em>: <em>probably not optimized for jokes. </em></figcaption></figure></div>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>Setting up a local LM Studio server</strong></h3>\n<p class=\"\">As useful as having an endless supply of bad jokes is, we’ll have to cover this in a later post. Our focus today is setting up a local server that we can access in R. </p>\n<p class=\"\">This is embarrassingly simple and requires navigating to the developer tab, loading the model using the drop down menu and starting the server by toggling the switch on the top-left. Pay particular attention to the information in the ‘API Usage’ panel as we’ll refer to this when we interact with the server in R.</p>\n<p class=\"\"><strong>Note: </strong>I’ve set the seed to ‘123’ when loading the model. I’ve also specified a temperature of ‘0’ in the model settings (see: ‘My Models’ &gt; llama-3.2-3b-instruct &gt; ‘Edit model default config’ &gt; Inference &gt; Temperature). </p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img alt=\"\" class=\"wp-image-2708\" data-lazy-src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/LM_Studio_XBdiumVgKM.gif?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" class=\"wp-image-2708\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/LM_Studio_XBdiumVgKM.gif?w=450&amp;ssl=1\"/></noscript><figcaption class=\"wp-element-caption\"><em>Setting up a local LLM server</em></figcaption></figure></div>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\"><strong>Interacting with LLM models in R</strong></h3>\n<p class=\"\">If you’ve made it this far you should have an LLM model running in the background. To interact with it, we’ll be using the ellmer package which can be installed using install.packages(‘ellmer’). I’ve mainly been using ellmer to interact with Anthropic’s Claude model, but the package supports a variety of functions for specific providers such as OpenAI, Groq and Google Gemini (<a href=\"https://github.com/tidyverse/ellmer\" rel=\"nofollow\" target=\"_blank\">see here</a>).</p>\n<p class=\"\">We’ll be using the chat_vllm() function which allows us to connect to our LLM server, but the basic setup is similar: you setup a chat object that specifies the default system prompt, the model to use and your API key. For the chat_vllm() function we’ll also need to set the base_url so it knows the address of the server.</p>\n<p class=\"\">In the code below, after loading the required packages, we’ve specified key inputs required by the chat_vllm() function. Notice that the values of the base url and model are directly sourced from LM Studio in the ‘API Usage’ section of the developer panel. The API key is simply ‘lm-studio’ as per <a href=\"https://lmstudio.ai/docs/api/openai-api\" rel=\"nofollow\" target=\"_blank\">the documentation</a>. </p>\n<p class=\"\">For the sake of providing a reproducible demonstration of interacting with a local LLM in R, we’re going to have it complete a really simple task that we don’t need an LLM for: take a string of numbers separated by ‘,’ and return them as a string separated by ‘;’. </p>\n<p class=\"\">These instructions are specified in the system prompt in ref_prompt (we’ll generate the numbers below). Notice that the prompt specifies to the LLM that it should not comment on the output. This is a great demonstration of how LLMs can be quirky to work with, as you need to be <em>really </em>explicit about what you want. In this case, I’ve included this line to avoid the model sharing its life story when returning the string of numbers we’ve asked for.</p>\n<p class=\"\"><strong>Code:</strong> <strong>Getting ready for some localized AI “fun”:</strong></p>\n<pre>#load packages\nlibrary(ellmer)\nlibrary(tidyverse)\n\n#set assumptions\nref_llm_url&lt;-\"http://127.0.0.1:1234/v1/\"\n\nref_model&lt;-\"llama-3.2-3b-instruct\"\n\n#specify prompt to provide LLM\nref_prompt&lt;-c(\"Output this data as numeric string using ';' as the deliminator. \n                  Do not comment on the output:\")\n#set up chat object\nfnc_chat&lt;-chat_vllm(base_url=ref_llm_url, \n                    api_key= \"lm-studio\",\n                model=ref_model,\n                system_prompt = ref_prompt)</pre>\n<p class=\"\">One thing you might notice when running this code is that the chat_vllm() function hasn’t interacted with the server yet. Instead, we’ve saved it as an R6 object which we can interact with by calling the ‘chat’ method. You can read more about this <a href=\"https://github.com/tidyverse/ellmer\" rel=\"nofollow\" target=\"_blank\">on the Github page</a>, but one advantage of this approach is that previous conversations are saved. For instance, once we’ve called the chat method below, results of the conversation are stored in ‘last_turn()’.</p>\n<p class=\"\">The code below sets up the example by creating a data frame with a series of 100 random numbers in the source_data column. I’ve set the rounding to nine to reduce the chance we’ll encounter <a href=\"https://floating-point-gui.de/basic/\" rel=\"nofollow\" target=\"_blank\">floating point errors</a> when comparing the output produced by our LLM model with the source data. </p>\n<p class=\"\">The prompt and data is then sent to the local LLM via fnc_chat$chat(). Results of the conversation have been saved under dta_llm_reply and converted to a variable in the dta_rnorm data frame. But, if you look at dta_llm_reply you’ll see what was returned by the model – a string of numbers separated by ‘;’. Because we specified not to comment on the output it also didn’t provide us with narrative information about the task, which is handy as we’re just interested in getting the string of numbers.</p>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img alt=\"\" class=\"wp-image-2723\" data-lazy-sizes=\"auto, (max-width: 766px) 100vw, 766px\" data-lazy-src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-12.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-12.png?w=450&amp;ssl=1 766w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-12-300x32.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-2723\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"auto, (max-width: 766px) 100vw, 766px\" src=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-12.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-12.png?w=450&amp;ssl=1 766w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-12-300x32.png 300w\"/></noscript></figure></div>\n<p class=\"\">The final line of the code simply demonstrates that the previous interaction is saved in the fnc_chat under $last_turn.</p>\n<p class=\"\"><strong>Code:</strong> <strong>Starting the conversation</strong>:</p>\n<pre>#generate a random string\n#(I've rounded to avoid a floating point errors)\nset.seed(123)\ndta_rnorm&lt;-data.frame(source_data=rnorm(100)) |&gt; \n  round(9)\n\n#send prompt to local LLM model and save results \ndta_llm_reply&lt;-fnc_chat$chat(paste(dta_rnorm$source_data))[1]\n\n#save results from LLM as a column in dta_rnorm  \ndta_rnorm&lt;-dta_rnorm |&gt; \n  mutate(llm_transcribed=dta_llm_reply[[1]] |&gt; \n  str_split(pattern=';') |&gt; \n  unlist() |&gt; \n  as.numeric())\n\n#show last conversation\nfnc_chat$last_turn()</pre>\n<p class=\"\">Once again, there really is no reason you would use an LLM for a task like this. After all, if we really needed to complete such a simple task we could just use str_replace(). But, it <em>does </em>provide a simple illustration of how to interact with a local LLM model in R.</p>\n<p class=\"\"><strong>Code:</strong> <strong>Checking its work</strong></p>\n<pre>#calculate the difference\ndta_rnorm&lt;-dta_rnorm |&gt; \n  mutate(diff=source_data-llm_transcribed )\n\n#Check whether the numbers match:\ntable(dta_rnorm$source_data==dta_rnorm$llm_transcribed)\n\n#plot the data series for good measure: \nplot(x=dta_rnorm$source_data, \n     y=dta_rnorm$llm_transcribed, \n     xlab=\"Source value\",\n     ylab=\"Value outputted (LLM)\", \n     main=\"Source values vs values outputted by LLM\")</pre>\n<p class=\"\">Although this is in no way an efficient way to change the deliminator used in a string, I was surprised to find it took my moderately spec’d laptop around ten seconds to execute the entire script and that the numbers outputted were all correct: </p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img alt=\"\" class=\"wp-image-2728\" data-lazy-sizes=\"auto, (max-width: 898px) 100vw, 898px\" data-lazy-src=\"https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-15.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-15.png?w=450&amp;ssl=1 898w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-15-300x252.png 300w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-15-768x644.png 768w\"/><noscript><img alt=\"\" class=\"wp-image-2728\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"auto, (max-width: 898px) 100vw, 898px\" src=\"https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-15.png?w=450&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/www.gilesd-j.com/wp-content/uploads/2025/01/image-15.png?w=450&amp;ssl=1 898w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-15-300x252.png 300w, https://www.gilesd-j.com/wp-content/uploads/2025/01/image-15-768x644.png 768w\"/></noscript></figure></div>\n<div aria-hidden=\"true\" class=\"wp-block-spacer\" style=\"height:50px\"></div>\n<h3 class=\"wp-block-heading\">Wrapping up: some final points</h3>\n<p class=\"\">If you’re wondering why I’m surprised an LLM would be able to complete this task, it’s because they are <a href=\"https://garymarcus.substack.com/p/math-is-hard-if-you-are-an-llm-and\" rel=\"nofollow\" target=\"_blank\">notoriously bad at arithmetic</a>. They’re also known to <a href=\"https://arxiv.org/pdf/2311.05232\" rel=\"nofollow\" target=\"_blank\">‘hallucinate’</a>, which I expected <em>might </em>result in it outputting numbers that looked correct, but didn’t match what we provided it with. But, I was wrong on both counts: the model seemed to output exactly what we asked for. This could be attributed to a variety of sources, such as the fact the model <a href=\"https://arxiv.org/html/2404.18824v1\" rel=\"nofollow\" target=\"_blank\">might have been trained on coding problems resembling our task</a>, but that’s a problem for another post. </p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.gilesd-j.com/2025/01/20/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/\"> R-Programming – Giles Dickenson-Jones</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "From code to conversation: Localized AI fun with LM Studio and the ellmer package\nPosted on\nJanuary 19, 2025\nby\ngiles.dickenson.jones\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR-Programming – Giles Dickenson-Jones\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nSummary:\nin this post I demonstrate how you can interact with locally hosted LLM models in R using the ellmer package and LM Studio.\nIf you didn’t guess already, the title of this post was suggested by a Large Language Model (LLM). The bartowski/Llama-3.2-3B-Instruct-GGUF LLM to be precise. Unfortunately for you, the rest of this blog will be written by me.\nIntro\nIf you’ve connected with me on\nBluesky\nor\nLinkedin\nyou’ll know I think the benefits and impact of Artificial Intelligence (AI) are frequently exaggerated. With companies increasingly trying to\nshove ‘AI’ features into their products\nto satisfy executives that have no idea how it works. But, you’ll also know that I’m not blind to their utility, being a frequent user of Claude and ChatGPT for refining code, steel-manning methodologies and improving my writing. In fact,\nI wrote up a guide on how I use ChatGPT\nearly in the AI craze.\nWhich is why whenever I show others how to use AI tools, I spent an inordinate amount of time making it clear what they\ncan\nand\ncannot\ndo (\nnoting we’re still figuring this out\n). Let’s just say I’m ‘skeptically enamored’ by the technology.\nInteracting with LLM models in R\nA LLM is a type of machine learning model that can be helpful for natural language processing and generation tasks\n. Although they can be useful for a variety of tasks, I frequently employ them when I need to generate a set of boilerplate text, clean unstructured data or extract information from a document. I’ve therefore been trying to intelligently integrate LLMs more directly into my R workflow.\nUnfortunately\n, this requires tapping into an LLM’s Application Programming Interface (API).\nFortunately\n,\ntapping into API\ns is possible in R.\nUnfortunately\n, this frequently requires interpreting poorly written developer document and\nwrangling JSON data\n.\n“\nThe data is also in JSON format”\nBut,\nfortunately\nyou won’t have to suffer like I did thanks to\nHadley Wickham\nand the team behind the\nthe ellmer package\n. To learn more about the package’s capabilities check out the\ngithub\nor\ntidyverse\npage, but in essence it greatly simplifies interacting with the most popular LLM models.\nThe problem\nI’ve been interacting with LLMs in R a lot recently, as I’m building an analysis pipeline that requires extracting metadata from unstructured text documents. I’ve mainly been using\nAnthropic’s Claude model\nas I find it to be more reliable than its competitors. However, the API request limits can make building a new analysis pipeline cumbersome.\nThe solution(?)\nAlthough I could just try to make less requests of Anthropic’s API, the problem seemed like as good an excuse as any to figure how to deploy a local LLM model that could serve as a temporary substitute until the pipeline is working. Once everything works I’d then switch back to using Claude.\nLM Studio\nLM Studio is an application for downloading and running LLM models locally. Although it’s ‘free’  for personal use, there are limits if you’re using it for commercial purposes (see:\nterms of service\n). You can download the latest version of LM Studio\nhere\n.\nWhen you first open LM Studio you should see a screen similar to below (without the annotations). The the areas of the app we’re interested include:\nChat:\na chat style interface for interacting with models we’ve downloaded.\nDeveloper\n: options for setting up a local server for interacting with a model.\nModels:\nlists models you’ve downloaded and provides an interface for altering model defaults.\nDiscover:\nprovides an interface for finding and downloading models.\nModel list:\nfor selecting a model you’d like to load.\nFinding and downloading a model\nOnce you’ve installed LM Studio, you’ll probably want to download an LLM model. For our purposes we’ll go with a small version of Meta’s Llama model, which you can find by searching for ‘llama-3.2-3b-instruct’ via the ‘discover’ menu in LM Studio:\n(You can also\ncheck out Hugging Face’s leaderboard\nfor a ranking of open LLM models).\nDownloading the\nllama-3.2-3b-instruct\nmodel\nL\noading and interacting with a model\nOnce you’ve downloaded the model, there are a number of ways to interact with it. The simplest way is to loaded it via the dropdown menu and chat with it directly using the ‘Chat’ panel:\nllama-3.2-3b-instruct\n:\nprobably not optimized for jokes.\nSetting up a local LM Studio server\nAs useful as having an endless supply of bad jokes is, we’ll have to cover this in a later post. Our focus today is setting up a local server that we can access in R.\nThis is embarrassingly simple and requires navigating to the developer tab, loading the model using the drop down menu and starting the server by toggling the switch on the top-left. Pay particular attention to the information in the ‘API Usage’ panel as we’ll refer to this when we interact with the server in R.\nNote:\nI’ve set the seed to ‘123’ when loading the model. I’ve also specified a temperature of ‘0’ in the model settings (see: ‘My Models’ > llama-3.2-3b-instruct > ‘Edit model default config’ > Inference > Temperature).\nSetting up a local LLM server\nInteracting with LLM models in R\nIf you’ve made it this far you should have an LLM model running in the background. To interact with it, we’ll be using the ellmer package which can be installed using install.packages(‘ellmer’). I’ve mainly been using ellmer to interact with Anthropic’s Claude model, but the package supports a variety of functions for specific providers such as OpenAI, Groq and Google Gemini (\nsee here\n).\nWe’ll be using the chat_vllm() function which allows us to connect to our LLM server, but the basic setup is similar: you setup a chat object that specifies the default system prompt, the model to use and your API key. For the chat_vllm() function we’ll also need to set the base_url so it knows the address of the server.\nIn the code below, after loading the required packages, we’ve specified key inputs required by the chat_vllm() function. Notice that the values of the base url and model are directly sourced from LM Studio in the ‘API Usage’ section of the developer panel. The API key is simply ‘lm-studio’ as per\nthe documentation\n.\nFor the sake of providing a reproducible demonstration of interacting with a local LLM in R, we’re going to have it complete a really simple task that we don’t need an LLM for: take a string of numbers separated by ‘,’ and return them as a string separated by ‘;’.\nThese instructions are specified in the system prompt in ref_prompt (we’ll generate the numbers below). Notice that the prompt specifies to the LLM that it should not comment on the output. This is a great demonstration of how LLMs can be quirky to work with, as you need to be\nreally\nexplicit about what you want. In this case, I’ve included this line to avoid the model sharing its life story when returning the string of numbers we’ve asked for.\nCode:\nGetting ready for some localized AI “fun”:\n#load packages\nlibrary(ellmer)\nlibrary(tidyverse)\n\n#set assumptions\nref_llm_url<-\"http://127.0.0.1:1234/v1/\"\n\nref_model<-\"llama-3.2-3b-instruct\"\n\n#specify prompt to provide LLM\nref_prompt<-c(\"Output this data as numeric string using ';' as the deliminator. \n                  Do not comment on the output:\")\n#set up chat object\nfnc_chat<-chat_vllm(base_url=ref_llm_url, \n                    api_key= \"lm-studio\",\n                model=ref_model,\n                system_prompt = ref_prompt)\nOne thing you might notice when running this code is that the chat_vllm() function hasn’t interacted with the server yet. Instead, we’ve saved it as an R6 object which we can interact with by calling the ‘chat’ method. You can read more about this\non the Github page\n, but one advantage of this approach is that previous conversations are saved. For instance, once we’ve called the chat method below, results of the conversation are stored in ‘last_turn()’.\nThe code below sets up the example by creating a data frame with a series of 100 random numbers in the source_data column. I’ve set the rounding to nine to reduce the chance we’ll encounter\nfloating point errors\nwhen comparing the output produced by our LLM model with the source data.\nThe prompt and data is then sent to the local LLM via fnc_chat$chat(). Results of the conversation have been saved under dta_llm_reply and converted to a variable in the dta_rnorm data frame. But, if you look at dta_llm_reply you’ll see what was returned by the model – a string of numbers separated by ‘;’. Because we specified not to comment on the output it also didn’t provide us with narrative information about the task, which is handy as we’re just interested in getting the string of numbers.\nThe final line of the code simply demonstrates that the previous interaction is saved in the fnc_chat under $last_turn.\nCode:\nStarting the conversation\n:\n#generate a random string\n#(I've rounded to avoid a floating point errors)\nset.seed(123)\ndta_rnorm<-data.frame(source_data=rnorm(100)) |> \n  round(9)\n\n#send prompt to local LLM model and save results \ndta_llm_reply<-fnc_chat$chat(paste(dta_rnorm$source_data))[1]\n\n#save results from LLM as a column in dta_rnorm  \ndta_rnorm<-dta_rnorm |> \n  mutate(llm_transcribed=dta_llm_reply[[1]] |> \n  str_split(pattern=';') |> \n  unlist() |> \n  as.numeric())\n\n#show last conversation\nfnc_chat$last_turn()\nOnce again, there really is no reason you would use an LLM for a task like this. After all, if we really needed to complete such a simple task we could just use str_replace(). But, it\ndoes\nprovide a simple illustration of how to interact with a local LLM model in R.\nCode:\nChecking its work\n#calculate the difference\ndta_rnorm<-dta_rnorm |> \n  mutate(diff=source_data-llm_transcribed )\n\n#Check whether the numbers match:\ntable(dta_rnorm$source_data==dta_rnorm$llm_transcribed)\n\n#plot the data series for good measure: \nplot(x=dta_rnorm$source_data, \n     y=dta_rnorm$llm_transcribed, \n     xlab=\"Source value\",\n     ylab=\"Value outputted (LLM)\", \n     main=\"Source values vs values outputted by LLM\")\nAlthough this is in no way an efficient way to change the deliminator used in a string, I was surprised to find it took my moderately spec’d laptop around ten seconds to execute the entire script and that the numbers outputted were all correct:\nWrapping up: some final points\nIf you’re wondering why I’m surprised an LLM would be able to complete this task, it’s because they are\nnotoriously bad at arithmetic\n. They’re also known to\n‘hallucinate’\n, which I expected\nmight\nresult in it outputting numbers that looked correct, but didn’t match what we provided it with. But, I was wrong on both counts: the model seemed to output exactly what we asked for. This could be attributed to a variety of sources, such as the fact the model\nmight have been trained on coding problems resembling our task\n, but that’s a problem for another post.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR-Programming – Giles Dickenson-Jones\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Summary: in this post I demonstrate how you can interact with locally hosted LLM models in R using the ellmer package and LM Studio.",
      "meta_keywords": null,
      "og_description": "Summary: in this post I demonstrate how you can interact with locally hosted LLM models in R using the ellmer package and LM Studio.",
      "og_image": "https://www.gilesd-j.com/wp-content/uploads/2025/01/wmvs29uce2r01.jpg",
      "og_title": "From code to conversation: Localized AI fun with LM Studio and the ellmer package | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 10.1,
      "sitemap_lastmod": null,
      "twitter_description": "Summary: in this post I demonstrate how you can interact with locally hosted LLM models in R using the ellmer package and LM Studio.",
      "twitter_title": "From code to conversation: Localized AI fun with LM Studio and the ellmer package | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/01/from-code-to-conversation-localized-ai-fun-with-lm-studio-and-the-ellmer-package/",
      "word_count": 2014
    }
  }
}