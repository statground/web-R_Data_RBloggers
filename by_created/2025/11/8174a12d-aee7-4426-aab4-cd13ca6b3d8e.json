{
  "uuid": "8174a12d-aee7-4426-aab4-cd13ca6b3d8e",
  "created_at": "2025-11-17 20:39:36",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2023/10/interactions-where-are-you/",
    "crawled_at": "2025-11-17T10:08:08.827642",
    "external_links": [
      {
        "href": "https://lorentzen.ch/index.php/2023/10/16/interactions-where-are-you/",
        "text": "R – Michael's and Christian's Blog"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://lorentzen.ch/index.php/2023/08/01/its-the-interactions/",
        "text": "last post"
      },
      {
        "href": "https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2023-10-16%20hstats.R",
        "text": "here"
      },
      {
        "href": "https://lorentzen.ch/index.php/2023/10/16/interactions-where-are-you/",
        "text": "R – Michael's and Christian's Blog"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Interactions – where are you? | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-3.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-4.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-5.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-6.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-7.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-8-1024x442.png?w=450&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/michael-mayer/",
        "text": "Michael Mayer"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-379183 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Interactions – where are you?</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">October 16, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/michael-mayer/\">Michael Mayer</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://lorentzen.ch/index.php/2023/10/16/interactions-where-are-you/\"> R – Michael's and Christian's Blog</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<p>This question sends shivers down the poor modelers spine… </p>\n<figure class=\"wp-block-image size-full is-resized\"><img alt=\"\" class=\"wp-image-1303\" data-lazy-sizes=\"(max-width: 468px) 100vw, 468px\" data-lazy-src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1 747w, https://lorentzen.ch/wp-content/uploads/2023/10/image-2-300x240.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1303\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 468px) 100vw, 468px\" src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1 747w, https://lorentzen.ch/wp-content/uploads/2023/10/image-2-300x240.png 300w\"/></noscript></figure>\n<p>The {hstats} R package introduced in our <a href=\"https://lorentzen.ch/index.php/2023/08/01/its-the-interactions/\" rel=\"nofollow\" target=\"_blank\">last post</a> measures their strength using Friedman’s H-statistics, a collection of statistics based on partial dependence functions.</p>\n<p>On Github, the preview version of {hstats} 1.0.0 out – I will try to bring it to CRAN in about one week (October 2023). Until then, try it via d<code>evtools::install_github(\"mayer79/hstats\")</code></p>\n<p>The current version offers:</p>\n<ul>\n<li>H statistics per feature, feature pair, and feature triple</li>\n<li>multivariate predictions at no additional cost</li>\n<li>a convenient API</li>\n<li>other important tools from explainable ML: \n<ul>\n<li>performance calculations</li>\n<li>permutation importance (e.g., to select features for calculating H-statistics)</li>\n<li>partial dependence plots (including grouping, multivariate, multivariable)</li>\n<li>individual conditional expectations (ICE)</li>\n</ul>\n</li>\n<li>Case-weights are available for all methods, which is important, e.g., in insurance applications.</li>\n</ul>\n<p>This post has two parts:</p>\n<ol>\n<li>Example with house-prices and XGBoost</li>\n<li><strong>Naive</strong> benchmark against {iml}, {DALEX}, and my old {flashlight}.</li>\n</ol>\n<h2 class=\"wp-block-heading\">1. Example</h2>\n<p>Let’s model logarithmic sales prices of houses sold in Miami Dade County, a dataset prepared by Prof. Dr. Steven Bourassa, and available in {shapviz}. We use XGBoost with <em>interaction constraints</em> to provide a model additive in all structure information, but allowing for interactions between latitude/longitude for a flexible representation of geographic effects.</p>\n<p>The following code prepares the data, splits the data into train and validation, and then fits an XGBoost model.</p>\n<pre>library(hstats)\nlibrary(shapviz)\nlibrary(xgboost)\nlibrary(ggplot2)\n\n# Data preparation\ncolnames(miami) &lt;- tolower(colnames(miami))\nmiami &lt;- transform(miami, log_price = log(sale_prc))\nx &lt;- c(\"tot_lvg_area\", \"lnd_sqfoot\", \"latitude\", \"longitude\",\n       \"structure_quality\", \"age\", \"month_sold\")\ncoord &lt;- c(\"longitude\", \"latitude\")\n\n# Modeling\nset.seed(1)\nix &lt;- sample(nrow(miami), 0.8 * nrow(miami))\ntrain &lt;- data.frame(miami[ix, ])\nvalid &lt;- data.frame(miami[-ix, ])\ny_train &lt;- train$log_price\ny_valid &lt;- valid$log_price\nX_train &lt;- data.matrix(train[x])\nX_valid &lt;- data.matrix(valid[x])\n\ndtrain &lt;- xgb.DMatrix(X_train, label = y_train)\ndvalid &lt;- xgb.DMatrix(X_valid, label = y_valid)\n\nic &lt;- c(\n  list(which(x %in% coord) - 1),\n  as.list(which(!x %in% coord) - 1)\n)\n\n# Fit via early stopping\nfit &lt;- xgb.train(\n  params = list(\n    learning_rate = 0.15, \n    objective = \"reg:squarederror\", \n    max_depth = 5,\n    interaction_constraints = ic\n  ),\n  data = dtrain,\n  watchlist = list(valid = dvalid),\n  early_stopping_rounds = 20,\n  nrounds = 1000,\n  callbacks = list(cb.print.evaluation(period = 100))\n)</pre>\n<p>Now it is time for a compact analysis with {hstats} to interpret the model:</p>\n<pre>average_loss(fit, X = X_valid, y = y_valid)  # 0.0247 MSE -&gt; 0.157 RMSE\n\nperm_importance(fit, X = X_valid, y = y_valid) |&gt; \n  plot()\n\n# Or combining some features\nv_groups &lt;- list(\n  coord = c(\"longitude\", \"latitude\"),\n  size = c(\"lnd_sqfoot\", \"tot_lvg_area\"),\n  condition = c(\"age\", \"structure_quality\")\n)\nperm_importance(fit, v = v_groups, X = X_valid, y = y_valid) |&gt; \n  plot()\n\nH &lt;- hstats(fit, v = x, X = X_valid)\nH\nplot(H)\nplot(H, zero = FALSE)\nh2_pairwise(H, zero = FALSE, squared = FALSE, normalize = FALSE)\npartial_dep(fit, v = \"tot_lvg_area\", X = X_valid) |&gt; \n  plot()\npartial_dep(fit, v = \"tot_lvg_area\", X = X_valid, BY = \"structure_quality\") |&gt; \n  plot(show_points = FALSE)\nplot(ii &lt;- ice(fit, v = \"tot_lvg_area\", X = X_valid))\nplot(ii, center = TRUE)\n\n# Spatial plots\ng &lt;- unique(X_valid[, coord])\npp &lt;- partial_dep(fit, v = coord, X = X_valid, grid = g)\nplot(pp, d2_geom = \"point\", alpha = 0.5, size = 1) + \n  coord_equal()\n\n# Takes some seconds because it generates the last plot per structure quality\npartial_dep(fit, v = coord, X = X_valid, grid = g, BY = \"structure_quality\") |&gt;\n  plot(pp, d2_geom = \"point\", alpha = 0.5) +\n  coord_equal()\n)</pre>\n<h3 class=\"wp-block-heading\">Results summarized by plots</h3>\n<h4 class=\"wp-block-heading\">Permutation importance</h4>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1301\" data-lazy-sizes=\"(max-width: 714px) 100vw, 714px\" data-lazy-src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image.png?w=450&amp;ssl=1 714w, https://lorentzen.ch/wp-content/uploads/2023/10/image-300x249.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1301\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 714px) 100vw, 714px\" src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image.png?w=450&amp;ssl=1 714w, https://lorentzen.ch/wp-content/uploads/2023/10/image-300x249.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 1: Permutation importance (4 repetitions) on the validation data. Error bars show standard errors of the estimated increase in MSE from shuffling feature values.</figcaption></figure>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1302\" data-lazy-sizes=\"(max-width: 727px) 100vw, 727px\" data-lazy-src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-1.png?w=450&amp;ssl=1 727w, https://lorentzen.ch/wp-content/uploads/2023/10/image-1-300x242.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1302\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 727px) 100vw, 727px\" src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-1.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-1.png?w=450&amp;ssl=1 727w, https://lorentzen.ch/wp-content/uploads/2023/10/image-1-300x242.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 2: Feature groups can be shuffled together – accounting for issues of permutation importance with highly correlated features</figcaption></figure>\n<h4 class=\"wp-block-heading\">H-Statistics</h4>\n<p>Let’s now move on to interaction statistics.</p>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1303\" data-lazy-sizes=\"(max-width: 747px) 100vw, 747px\" data-lazy-src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1 747w, https://lorentzen.ch/wp-content/uploads/2023/10/image-2-300x240.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1303\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 747px) 100vw, 747px\" src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-2.png?w=450&amp;ssl=1 747w, https://lorentzen.ch/wp-content/uploads/2023/10/image-2-300x240.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 3: Overall and pairwise H-statistics. Overall H^2 gives the proportion of prediction variability explained by all interactions of the feature. By default, {hstats} picks the five features with largest H^2 and calculates their pairwise H^2. This explains why not all 21 feature pairs appear in the figure on the right-hand side. Pairwise H^2 is differently scaled than overall H^2: It gives the proportion of joint effect variability of the two features explained by their interaction.</figcaption></figure>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1304\" data-lazy-sizes=\"(max-width: 730px) 100vw, 730px\" data-lazy-src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-3.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-3.png?w=450&amp;ssl=1 730w, https://lorentzen.ch/wp-content/uploads/2023/10/image-3-300x242.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1304\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 730px) 100vw, 730px\" src=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-3.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-3.png?w=450&amp;ssl=1 730w, https://lorentzen.ch/wp-content/uploads/2023/10/image-3-300x242.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 4: Use “zero = FALSE” to drop variable (pairs) with value 0.</figcaption></figure>\n<h4 class=\"wp-block-heading\">PDPs and ICEs</h4>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1305\" data-lazy-sizes=\"(max-width: 721px) 100vw, 721px\" data-lazy-src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-4.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-4.png?w=450&amp;ssl=1 721w, https://lorentzen.ch/wp-content/uploads/2023/10/image-4-300x247.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1305\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 721px) 100vw, 721px\" src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-4.png?w=450&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-4.png?w=450&amp;ssl=1 721w, https://lorentzen.ch/wp-content/uploads/2023/10/image-4-300x247.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 5: A partial dependence plot of living area.</figcaption></figure>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1306\" data-lazy-sizes=\"(max-width: 733px) 100vw, 733px\" data-lazy-src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-5.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-5.png?w=450&amp;ssl=1 733w, https://lorentzen.ch/wp-content/uploads/2023/10/image-5-300x242.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1306\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 733px) 100vw, 733px\" src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-5.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-5.png?w=450&amp;ssl=1 733w, https://lorentzen.ch/wp-content/uploads/2023/10/image-5-300x242.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 6: Stratification shows indeed: no interactions between structure quality and living area.</figcaption></figure>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1308\" data-lazy-sizes=\"(max-width: 742px) 100vw, 742px\" data-lazy-src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-6.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-6.png?w=450&amp;ssl=1 742w, https://lorentzen.ch/wp-content/uploads/2023/10/image-6-300x243.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1308\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 742px) 100vw, 742px\" src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-6.png?w=450&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-6.png?w=450&amp;ssl=1 742w, https://lorentzen.ch/wp-content/uploads/2023/10/image-6-300x243.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 7: ICE plots also show no interations with any other feature. The interaction constraints of XGBoost did a good job.</figcaption></figure>\n<figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1309\" data-lazy-sizes=\"(max-width: 609px) 100vw, 609px\" data-lazy-src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-7.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-7.png?w=450&amp;ssl=1 609w, https://lorentzen.ch/wp-content/uploads/2023/10/image-7-300x296.png 300w\"/><noscript><img alt=\"\" class=\"wp-image-1309\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 609px) 100vw, 609px\" src=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-7.png?w=450&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-7.png?w=450&amp;ssl=1 609w, https://lorentzen.ch/wp-content/uploads/2023/10/image-7-300x296.png 300w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 8: This two-dimensional PDP evaluated over all unique coordinates shows a realistic profile of house prices in Miami Dade County (mind the log scale).</figcaption></figure>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-1310\" data-lazy-sizes=\"(max-width: 1024px) 100vw, 1024px\" data-lazy-src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-8-1024x442.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-8-1024x442.png?w=450&amp;ssl=1 1024w, https://lorentzen.ch/wp-content/uploads/2023/10/image-8-300x129.png 300w, https://lorentzen.ch/wp-content/uploads/2023/10/image-8-768x331.png 768w, https://lorentzen.ch/wp-content/uploads/2023/10/image-8.png 1108w\"/><noscript><img alt=\"\" class=\"wp-image-1310\" data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" sizes=\"(max-width: 1024px) 100vw, 1024px\" src=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-8-1024x442.png?w=450&amp;ssl=1\" srcset_temp=\"https://i2.wp.com/lorentzen.ch/wp-content/uploads/2023/10/image-8-1024x442.png?w=450&amp;ssl=1 1024w, https://lorentzen.ch/wp-content/uploads/2023/10/image-8-300x129.png 300w, https://lorentzen.ch/wp-content/uploads/2023/10/image-8-768x331.png 768w, https://lorentzen.ch/wp-content/uploads/2023/10/image-8.png 1108w\"/></noscript><figcaption class=\"wp-element-caption\">Figure 8: Same, but grouped by structure quality (5 is best). Since there is no interaction between location and structure quality, the plots are just shifted versions of each other. (You can’t really see it on the plots.)</figcaption></figure>\n<h2 class=\"wp-block-heading\">Naive Benchmark</h2>\n<p>All methods in {hstats} are optimized for speed. But how fast are they compared to other implementations? Note that: this is just a simple benchmark run on a Windows notebook with Intel i7-8650U CPU. </p>\n<p>Note that {iml} offers a parallel backend, but we could not make it run with XGBoost and Windows. Let me know how fast it is using parallelism and Linux! </p>\n<h4 class=\"wp-block-heading\">Setup + benchmark on permutation importance</h4>\n<p>Always using the full validation dataset and 10 repetitions.</p>\n<pre>library(iml)  # Might benefit of multiprocessing, but on Windows with XGB models, this is not easy\nlibrary(DALEX)\nlibrary(ingredients)\nlibrary(flashlight)\nlibrary(microbenchmark)\n\nset.seed(1)\n\n# iml\npredf &lt;- function(object, newdata) predict(object, data.matrix(newdata[x]))\nmod &lt;- Predictor$new(fit, data = as.data.frame(X_valid), y = y_valid, \n                     predict.function = predf)\n\n# DALEX\nex &lt;- DALEX::explain(fit, data = X_valid, y = y_valid)\n\n# flashlight (my slightly old fashioned package)\nfl &lt;- flashlight(\n  model = fit, data = valid, y = \"log_price\", predict_function = predf, label = \"lm\"\n)\n  \n# Permutation importance: 10 repeats over full validation data (~2700 rows)\nmicrobenchmark(\n  iml = FeatureImp$new(mod, n.repetitions = 10, loss = \"mse\", compare = \"difference\"),\n  dalex = feature_importance(ex, B = 10, type = \"difference\", n_sample = Inf),\n  flashlight = light_importance(fl, v = x, n_max = Inf, m_repetitions = 10),\n  hstats = perm_importance(fit, X = X_valid, y = y_valid, perms = 10),\n  times = 4\n)\n\n# Unit: milliseconds\n# expr             min        lq      mean    median        uq       max neval  cld\n# iml        1558.3352 1585.3964 1651.9098 1625.5042 1718.4233 1798.2958     4 a   \n# dalex       556.1398  573.8428  594.5660  592.1752  615.2893  637.7739     4  b  \n# flashlight 1207.8085 1238.2424 1347.5105 1340.0633 1456.7787 1502.1071     4   c \n# hstats      146.0656  146.9564  151.3652  149.4352  155.7741  160.5249     4    d\n</pre>\n<p>{hstats} is about <strong>four times </strong>as fast as the second, {DALEX}.</p>\n<h4 class=\"wp-block-heading\">Partial dependence</h4>\n<p>Here, we study the time for crunching partial dependence of a continuous feature and a discrete feature.</p>\n<pre># Partial dependence (cont)\nv &lt;- \"tot_lvg_area\"\nmicrobenchmark(\n  iml = FeatureEffect$new(mod, feature = v, grid.size = 50, method = \"pdp\"),\n  dalex = partial_dependence(ex, variables = v, N = Inf, grid_points = 50),\n  flashlight = light_profile(fl, v = v, pd_n_max = Inf, n_bins = 50),\n  hstats = partial_dep(fit, v = v, X = X_valid, grid_size = 50, n_max = Inf),\n  times = 4\n)\n# Unit: milliseconds\n# expr            min       lq     mean    median        uq       max neval  cld\n# iml        941.7763 968.5576 993.0481 1002.5849 1017.5386 1025.2462     4 a   \n# dalex      694.8007 740.1619 767.1501  788.6172  794.1384  796.5654     4  b  \n# flashlight 327.6056 328.7617 330.4069  330.5388  332.0522  332.9445     4   c \n# hstats     216.4040 217.0602 217.5606  217.8603  218.0611  218.1179     4    d\n\n# Partial dependence (discrete)\nv &lt;- \"structure_quality\"\nmicrobenchmark(\n  iml = FeatureEffect$new(mod, feature = v, method = \"pdp\", grid.points = 1:5),\n  dalex = partial_dependence(ex, variables = v, N = Inf, variable_type = \"categorical\", grid_points = 5),\n  flashlight = light_profile(fl, v = v, pd_n_max = Inf),\n  hstats = partial_dep(fit, v = v, X = X_valid, n_max = Inf),\n  times = 4\n)\n\n# Unit: milliseconds\n# expr            min        lq      mean    median        uq      max neval  cld\n# iml         90.3690  91.08965  94.18403  92.57250  97.27840 101.2221     4 a   \n# dalex      174.2517 174.97330 179.43483 175.87115 183.89635 191.7453     4  b  \n# flashlight  43.9318  45.05070  48.09375  46.64275  51.13680  55.1577     4   c \n# hstats      24.5972  24.64975  25.01325  24.94085  25.37675  25.5741     4    d\n</pre>\n<p>{hstats} is <strong>1.5 to 2 times faster</strong> than {flashlight}, and about four times as fast as the other packages.</p>\n<h4 class=\"wp-block-heading\">H-statistics</h4>\n<p>How fast can overall H-statistics be computed? How fast can it do pairwise calculations? </p>\n<p>{DALEX} does not offer these statistics yet, {iml} focuses on overall H per feature. It was the first model-agnostic implementation of H-statistics I am aware of. It uses quantile approximation by default, but we purposely force it to calculate exact, in order to compare the numbers. Thus, we made it slower than it actually is. Forgive me, Christoph :-).</p>\n<pre># H-Stats -&gt; we use a subset of 500 rows\nX_v500 &lt;- X_valid[1:500, ]\nmod500 &lt;- Predictor$new(fit, data = as.data.frame(X_v500), predict.function = predf)\nfl500 &lt;- flashlight(fl, data = as.data.frame(valid[1:500, ]))\n\n# iml  # 90 s (no pairwise possible)\nsystem.time(\n  iml_overall &lt;- Interaction$new(mod500, grid.size = 500)\n)\n\n# flashlight: 14s total, doing only one pairwise calculation, otherwise would take 63s\nsystem.time(  # 12s\n  fl_overall &lt;- light_interaction(fl500, v = x, grid_size = Inf, n_max = Inf)\n)\nsystem.time(  # 2s\n  fl_pairwise &lt;- light_interaction(\n    fl500, v = coord, grid_size = Inf, n_max = Inf, pairwise = TRUE\n  )\n)\n\n# hstats: 3s total\nsystem.time({\n  H &lt;- hstats(fit, v = x, X = X_v500, n_max = Inf)\n  hstats_overall &lt;- h2_overall(H, squared = FALSE, zero = FALSE)\n  hstats_pairwise &lt;- h2_pairwise(H, squared = FALSE, zero = FALSE)\n}\n)\n\n# Overall statistics correspond exactly\niml_overall$results |&gt; filter(.interaction &gt; 1e-6)\n#     .feature .interaction\n# 1:  latitude    0.2458269\n# 2: longitude    0.2458269\n\nfl_overall$data |&gt; subset(value &gt; 0, select = c(variable, value))\n#   variable  value\n# 1 latitude  0.246\n# 2 longitude 0.246\n\nhstats_overall\n# longitude  latitude \n# 0.2458269 0.2458269 \n\n# Pairwise results match as well\nfl_pairwise$data |&gt; subset(value &gt; 0, select = c(variable, value))\n# latitude:longitude 0.394\n\nhstats_pairwise\n# latitude:longitude \n# 0.3942526     4    d\n</pre>\n<ul>\n<li>{hstats} is about <strong>four times as fast</strong> as {flashlight}</li>\n<li>Since one often want to study relative and absolute H statistics, in practice, the speed-up would be about a factor of eight.</li>\n<li>In multi-classification/multi-output settings with m categories, the speed-up would be even m times larger.</li>\n<li>Forcing all three packages to calculate exact statistics, <strong>all results match.</strong></li>\n</ul>\n<h2 class=\"wp-block-heading\">Wrap-Up</h2>\n<ul>\n<li>{hstats} is much faster than other XAI packages, at least in our use-case. This includes H-statistics, permutation importance, and partial dependence. <strong>Note that making good benchmarks is not my strength, so forgive any bias in the results.</strong></li>\n<li>With multivariate output, the potential is even larger.</li>\n<li>H-Statistics match other implementations.</li>\n</ul>\n<p>Try it out! </p>\n<p>The full R code in one piece is <a href=\"https://github.com/lorentzenchr/notebooks/blob/master/blogposts/2023-10-16%20hstats.R\" rel=\"nofollow\" target=\"_blank\">here</a>.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://lorentzen.ch/index.php/2023/10/16/interactions-where-are-you/\"> R – Michael's and Christian's Blog</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
    "main_text": "Interactions – where are you?\nPosted on\nOctober 16, 2023\nby\nMichael Mayer\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR – Michael's and Christian's Blog\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nThis question sends shivers down the poor modelers spine…\nThe {hstats} R package introduced in our\nlast post\nmeasures their strength using Friedman’s H-statistics, a collection of statistics based on partial dependence functions.\nOn Github, the preview version of {hstats} 1.0.0 out – I will try to bring it to CRAN in about one week (October 2023). Until then, try it via d\nevtools::install_github(\"mayer79/hstats\")\nThe current version offers:\nH statistics per feature, feature pair, and feature triple\nmultivariate predictions at no additional cost\na convenient API\nother important tools from explainable ML:\nperformance calculations\npermutation importance (e.g., to select features for calculating H-statistics)\npartial dependence plots (including grouping, multivariate, multivariable)\nindividual conditional expectations (ICE)\nCase-weights are available for all methods, which is important, e.g., in insurance applications.\nThis post has two parts:\nExample with house-prices and XGBoost\nNaive\nbenchmark against {iml}, {DALEX}, and my old {flashlight}.\n1. Example\nLet’s model logarithmic sales prices of houses sold in Miami Dade County, a dataset prepared by Prof. Dr. Steven Bourassa, and available in {shapviz}. We use XGBoost with\ninteraction constraints\nto provide a model additive in all structure information, but allowing for interactions between latitude/longitude for a flexible representation of geographic effects.\nThe following code prepares the data, splits the data into train and validation, and then fits an XGBoost model.\nlibrary(hstats)\nlibrary(shapviz)\nlibrary(xgboost)\nlibrary(ggplot2)\n\n# Data preparation\ncolnames(miami) <- tolower(colnames(miami))\nmiami <- transform(miami, log_price = log(sale_prc))\nx <- c(\"tot_lvg_area\", \"lnd_sqfoot\", \"latitude\", \"longitude\",\n       \"structure_quality\", \"age\", \"month_sold\")\ncoord <- c(\"longitude\", \"latitude\")\n\n# Modeling\nset.seed(1)\nix <- sample(nrow(miami), 0.8 * nrow(miami))\ntrain <- data.frame(miami[ix, ])\nvalid <- data.frame(miami[-ix, ])\ny_train <- train$log_price\ny_valid <- valid$log_price\nX_train <- data.matrix(train[x])\nX_valid <- data.matrix(valid[x])\n\ndtrain <- xgb.DMatrix(X_train, label = y_train)\ndvalid <- xgb.DMatrix(X_valid, label = y_valid)\n\nic <- c(\n  list(which(x %in% coord) - 1),\n  as.list(which(!x %in% coord) - 1)\n)\n\n# Fit via early stopping\nfit <- xgb.train(\n  params = list(\n    learning_rate = 0.15, \n    objective = \"reg:squarederror\", \n    max_depth = 5,\n    interaction_constraints = ic\n  ),\n  data = dtrain,\n  watchlist = list(valid = dvalid),\n  early_stopping_rounds = 20,\n  nrounds = 1000,\n  callbacks = list(cb.print.evaluation(period = 100))\n)\nNow it is time for a compact analysis with {hstats} to interpret the model:\naverage_loss(fit, X = X_valid, y = y_valid)  # 0.0247 MSE -> 0.157 RMSE\n\nperm_importance(fit, X = X_valid, y = y_valid) |> \n  plot()\n\n# Or combining some features\nv_groups <- list(\n  coord = c(\"longitude\", \"latitude\"),\n  size = c(\"lnd_sqfoot\", \"tot_lvg_area\"),\n  condition = c(\"age\", \"structure_quality\")\n)\nperm_importance(fit, v = v_groups, X = X_valid, y = y_valid) |> \n  plot()\n\nH <- hstats(fit, v = x, X = X_valid)\nH\nplot(H)\nplot(H, zero = FALSE)\nh2_pairwise(H, zero = FALSE, squared = FALSE, normalize = FALSE)\npartial_dep(fit, v = \"tot_lvg_area\", X = X_valid) |> \n  plot()\npartial_dep(fit, v = \"tot_lvg_area\", X = X_valid, BY = \"structure_quality\") |> \n  plot(show_points = FALSE)\nplot(ii <- ice(fit, v = \"tot_lvg_area\", X = X_valid))\nplot(ii, center = TRUE)\n\n# Spatial plots\ng <- unique(X_valid[, coord])\npp <- partial_dep(fit, v = coord, X = X_valid, grid = g)\nplot(pp, d2_geom = \"point\", alpha = 0.5, size = 1) + \n  coord_equal()\n\n# Takes some seconds because it generates the last plot per structure quality\npartial_dep(fit, v = coord, X = X_valid, grid = g, BY = \"structure_quality\") |>\n  plot(pp, d2_geom = \"point\", alpha = 0.5) +\n  coord_equal()\n)\nResults summarized by plots\nPermutation importance\nFigure 1: Permutation importance (4 repetitions) on the validation data. Error bars show standard errors of the estimated increase in MSE from shuffling feature values.\nFigure 2: Feature groups can be shuffled together – accounting for issues of permutation importance with highly correlated features\nH-Statistics\nLet’s now move on to interaction statistics.\nFigure 3: Overall and pairwise H-statistics. Overall H^2 gives the proportion of prediction variability explained by all interactions of the feature. By default, {hstats} picks the five features with largest H^2 and calculates their pairwise H^2. This explains why not all 21 feature pairs appear in the figure on the right-hand side. Pairwise H^2 is differently scaled than overall H^2: It gives the proportion of joint effect variability of the two features explained by their interaction.\nFigure 4: Use “zero = FALSE” to drop variable (pairs) with value 0.\nPDPs and ICEs\nFigure 5: A partial dependence plot of living area.\nFigure 6: Stratification shows indeed: no interactions between structure quality and living area.\nFigure 7: ICE plots also show no interations with any other feature. The interaction constraints of XGBoost did a good job.\nFigure 8: This two-dimensional PDP evaluated over all unique coordinates shows a realistic profile of house prices in Miami Dade County (mind the log scale).\nFigure 8: Same, but grouped by structure quality (5 is best). Since there is no interaction between location and structure quality, the plots are just shifted versions of each other. (You can’t really see it on the plots.)\nNaive Benchmark\nAll methods in {hstats} are optimized for speed. But how fast are they compared to other implementations? Note that: this is just a simple benchmark run on a Windows notebook with Intel i7-8650U CPU.\nNote that {iml} offers a parallel backend, but we could not make it run with XGBoost and Windows. Let me know how fast it is using parallelism and Linux!\nSetup + benchmark on permutation importance\nAlways using the full validation dataset and 10 repetitions.\nlibrary(iml)  # Might benefit of multiprocessing, but on Windows with XGB models, this is not easy\nlibrary(DALEX)\nlibrary(ingredients)\nlibrary(flashlight)\nlibrary(microbenchmark)\n\nset.seed(1)\n\n# iml\npredf <- function(object, newdata) predict(object, data.matrix(newdata[x]))\nmod <- Predictor$new(fit, data = as.data.frame(X_valid), y = y_valid, \n                     predict.function = predf)\n\n# DALEX\nex <- DALEX::explain(fit, data = X_valid, y = y_valid)\n\n# flashlight (my slightly old fashioned package)\nfl <- flashlight(\n  model = fit, data = valid, y = \"log_price\", predict_function = predf, label = \"lm\"\n)\n  \n# Permutation importance: 10 repeats over full validation data (~2700 rows)\nmicrobenchmark(\n  iml = FeatureImp$new(mod, n.repetitions = 10, loss = \"mse\", compare = \"difference\"),\n  dalex = feature_importance(ex, B = 10, type = \"difference\", n_sample = Inf),\n  flashlight = light_importance(fl, v = x, n_max = Inf, m_repetitions = 10),\n  hstats = perm_importance(fit, X = X_valid, y = y_valid, perms = 10),\n  times = 4\n)\n\n# Unit: milliseconds\n# expr             min        lq      mean    median        uq       max neval  cld\n# iml        1558.3352 1585.3964 1651.9098 1625.5042 1718.4233 1798.2958     4 a   \n# dalex       556.1398  573.8428  594.5660  592.1752  615.2893  637.7739     4  b  \n# flashlight 1207.8085 1238.2424 1347.5105 1340.0633 1456.7787 1502.1071     4   c \n# hstats      146.0656  146.9564  151.3652  149.4352  155.7741  160.5249     4    d\n{hstats} is about\nfour times\nas fast as the second, {DALEX}.\nPartial dependence\nHere, we study the time for crunching partial dependence of a continuous feature and a discrete feature.\n# Partial dependence (cont)\nv <- \"tot_lvg_area\"\nmicrobenchmark(\n  iml = FeatureEffect$new(mod, feature = v, grid.size = 50, method = \"pdp\"),\n  dalex = partial_dependence(ex, variables = v, N = Inf, grid_points = 50),\n  flashlight = light_profile(fl, v = v, pd_n_max = Inf, n_bins = 50),\n  hstats = partial_dep(fit, v = v, X = X_valid, grid_size = 50, n_max = Inf),\n  times = 4\n)\n# Unit: milliseconds\n# expr            min       lq     mean    median        uq       max neval  cld\n# iml        941.7763 968.5576 993.0481 1002.5849 1017.5386 1025.2462     4 a   \n# dalex      694.8007 740.1619 767.1501  788.6172  794.1384  796.5654     4  b  \n# flashlight 327.6056 328.7617 330.4069  330.5388  332.0522  332.9445     4   c \n# hstats     216.4040 217.0602 217.5606  217.8603  218.0611  218.1179     4    d\n\n# Partial dependence (discrete)\nv <- \"structure_quality\"\nmicrobenchmark(\n  iml = FeatureEffect$new(mod, feature = v, method = \"pdp\", grid.points = 1:5),\n  dalex = partial_dependence(ex, variables = v, N = Inf, variable_type = \"categorical\", grid_points = 5),\n  flashlight = light_profile(fl, v = v, pd_n_max = Inf),\n  hstats = partial_dep(fit, v = v, X = X_valid, n_max = Inf),\n  times = 4\n)\n\n# Unit: milliseconds\n# expr            min        lq      mean    median        uq      max neval  cld\n# iml         90.3690  91.08965  94.18403  92.57250  97.27840 101.2221     4 a   \n# dalex      174.2517 174.97330 179.43483 175.87115 183.89635 191.7453     4  b  \n# flashlight  43.9318  45.05070  48.09375  46.64275  51.13680  55.1577     4   c \n# hstats      24.5972  24.64975  25.01325  24.94085  25.37675  25.5741     4    d\n{hstats} is\n1.5 to 2 times faster\nthan {flashlight}, and about four times as fast as the other packages.\nH-statistics\nHow fast can overall H-statistics be computed? How fast can it do pairwise calculations?\n{DALEX} does not offer these statistics yet, {iml} focuses on overall H per feature. It was the first model-agnostic implementation of H-statistics I am aware of. It uses quantile approximation by default, but we purposely force it to calculate exact, in order to compare the numbers. Thus, we made it slower than it actually is. Forgive me, Christoph :-).\n# H-Stats -> we use a subset of 500 rows\nX_v500 <- X_valid[1:500, ]\nmod500 <- Predictor$new(fit, data = as.data.frame(X_v500), predict.function = predf)\nfl500 <- flashlight(fl, data = as.data.frame(valid[1:500, ]))\n\n# iml  # 90 s (no pairwise possible)\nsystem.time(\n  iml_overall <- Interaction$new(mod500, grid.size = 500)\n)\n\n# flashlight: 14s total, doing only one pairwise calculation, otherwise would take 63s\nsystem.time(  # 12s\n  fl_overall <- light_interaction(fl500, v = x, grid_size = Inf, n_max = Inf)\n)\nsystem.time(  # 2s\n  fl_pairwise <- light_interaction(\n    fl500, v = coord, grid_size = Inf, n_max = Inf, pairwise = TRUE\n  )\n)\n\n# hstats: 3s total\nsystem.time({\n  H <- hstats(fit, v = x, X = X_v500, n_max = Inf)\n  hstats_overall <- h2_overall(H, squared = FALSE, zero = FALSE)\n  hstats_pairwise <- h2_pairwise(H, squared = FALSE, zero = FALSE)\n}\n)\n\n# Overall statistics correspond exactly\niml_overall$results |> filter(.interaction > 1e-6)\n#     .feature .interaction\n# 1:  latitude    0.2458269\n# 2: longitude    0.2458269\n\nfl_overall$data |> subset(value > 0, select = c(variable, value))\n#   variable  value\n# 1 latitude  0.246\n# 2 longitude 0.246\n\nhstats_overall\n# longitude  latitude \n# 0.2458269 0.2458269 \n\n# Pairwise results match as well\nfl_pairwise$data |> subset(value > 0, select = c(variable, value))\n# latitude:longitude 0.394\n\nhstats_pairwise\n# latitude:longitude \n# 0.3942526     4    d\n{hstats} is about\nfour times as fast\nas {flashlight}\nSince one often want to study relative and absolute H statistics, in practice, the speed-up would be about a factor of eight.\nIn multi-classification/multi-output settings with m categories, the speed-up would be even m times larger.\nForcing all three packages to calculate exact statistics,\nall results match.\nWrap-Up\n{hstats} is much faster than other XAI packages, at least in our use-case. This includes H-statistics, permutation importance, and partial dependence.\nNote that making good benchmarks is not my strength, so forgive any bias in the results.\nWith multivariate output, the potential is even larger.\nH-Statistics match other implementations.\nTry it out!\nThe full R code in one piece is\nhere\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR – Michael's and Christian's Blog\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "This question sends shivers down the poor modelers spine… The {hstats} R package introduced in our last post measures their strength using Friedman’s H-statistics, a collection of statistics based on partial dependence functions. On Github, the preview version of {hstats} 1.0.0 out – I will try to bring it to CRAN in about one week […]",
    "meta_keywords": null,
    "og_description": "This question sends shivers down the poor modelers spine… The {hstats} R package introduced in our last post measures their strength using Friedman’s H-statistics, a collection of statistics based on partial dependence functions. On Github, the preview version of {hstats} 1.0.0 out – I will try to bring it to CRAN in about one week […]",
    "og_image": "https://lorentzen.ch/wp-content/uploads/2023/10/image-2.png",
    "og_title": "Interactions – where are you? | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 9.5,
    "sitemap_lastmod": "2023-10-16T15:22:48+00:00",
    "twitter_description": "This question sends shivers down the poor modelers spine… The {hstats} R package introduced in our last post measures their strength using Friedman’s H-statistics, a collection of statistics based on partial dependence functions. On Github, the preview version of {hstats} 1.0.0 out – I will try to bring it to CRAN in about one week […]",
    "twitter_title": "Interactions – where are you? | R-bloggers",
    "url": "https://www.r-bloggers.com/2023/10/interactions-where-are-you/",
    "word_count": 1906
  }
}