{
  "id": "aa030e98cb366cc32595c11cc1955dc86f457b8e",
  "url": "https://www.r-bloggers.com/2025/04/how-the-dsge-sausage-is-made/",
  "created_at_utc": "2025-11-22T19:58:45Z",
  "data": null,
  "raw_original": {
    "uuid": "591d305e-ba01-4aec-a869-30e5909af26a",
    "created_at": "2025-11-22 19:58:45",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/04/how-the-dsge-sausage-is-made/",
      "crawled_at": "2025-11-22T10:49:52.690768",
      "external_links": [
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html",
          "text": "datascienceconfidential - r"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#fitting-the-model-to-data",
          "text": "illustrated below"
        },
        {
          "href": "https://www.nzae.org.nz/wp-content/uploads/2015/01/Sander.pdf",
          "text": "NZSim model"
        },
        {
          "href": "https://www.treasury.govt.nz/sites/default/files/2024-04/twp24-01.pdf",
          "text": "at Te Tai Ōhanga The Treasury"
        },
        {
          "href": "https://www.bruegel.org/blog-post/dsge-model-quarrel-again",
          "text": "label everybody who doesn’t use them as dilettantes"
        },
        {
          "href": "https://criticalfinance.org/2017/11/19/dilettantes-shouldnt-get-excited/",
          "text": "a toxic cocktail"
        },
        {
          "href": "https://sites.nd.edu/esims/courses/ph-d-macro-theory-ii/",
          "text": "PhD Macro Theory II lecture notes of Eric Sims"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#hand-calculations",
          "text": "part of this post"
        },
        {
          "href": "https://karlwhelan.com/blog/?page_id=2214",
          "text": "lecture notes by Karl Whelan"
        },
        {
          "href": "https://matteocoronese.eu/teaching/advanced-macroeconomics",
          "text": "slides by Matteo Coronese"
        },
        {
          "href": "https://doi.org/10.2307/1927758",
          "text": "Samuelson’s original paper"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote1",
          "text": "1"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Multiplier-accelerator_model",
          "text": "Samuelson’s model"
        },
        {
          "href": "https://math.arizona.edu/~jwatkins/h-ode.pdf",
          "text": "equation describing the motion of a spring under an external force"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations",
          "text": "Lotka-Volterra predator-prey model"
        },
        {
          "href": "https://en.wikipedia.org/wiki/%22Hello,_World!%22_program",
          "text": "“hello world”"
        },
        {
          "href": "https://sites.nd.edu/esims/files/2024/01/notes_linear_models_sp2024.pdf",
          "text": "notes by Sims"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote2",
          "text": "2"
        },
        {
          "href": "https://www.dynare.org/",
          "text": "Dynare"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#fitting-the-model-to-data",
          "text": "below"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#fitting-the-model-to-data",
          "text": "below"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Real_economy",
          "text": "real economy"
        },
        {
          "href": "https://bayesiancomputationbook.com/markdown/chp_08.html",
          "text": "Approximate Bayesian Computation"
        },
        {
          "href": "https://cran.r-project.org/web/packages/dlm/index.html",
          "text": "package by Giovanni Petris"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote3",
          "text": "3"
        },
        {
          "href": "https://www.rbnz.govt.nz/statistics/series/economic-indicators/gross-domestic-product",
          "text": "RBNZ website"
        },
        {
          "href": "https://www.bruegel.org/blog-post/dsge-model-quarrel-again",
          "text": "some people don’t seem to like or trust DSGE models"
        },
        {
          "href": "https://johnhcochrane.blogspot.com/2023/07/new-york-times-on-hank-and-questions.html",
          "text": "Perhaps it is"
        },
        {
          "href": "https://en.wikipedia.org/wiki/The_Fable_of_the_Bees",
          "text": "lessons of economics"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote4",
          "text": "4"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote5",
          "text": "5"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#a-model-of-the-economy",
          "text": "in the following section"
        },
        {
          "href": "https://pages.stern.nyu.edu/~cedmond/ge07pt/ps1_answers.pdf",
          "text": "a standard choice of parameter in the production function"
        },
        {
          "href": "https://www.goodreads.com/book/show/18377995-how-to-speak-money",
          "text": "How to Speak Money"
        },
        {
          "href": "https://users.ox.ac.uk/~exet2581/Boe/dsge_all.pdf",
          "text": "training materials from the Bank of England"
        },
        {
          "href": "https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html",
          "text": "datascienceconfidential - r"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "How the DSGE sausage is made | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/datascienceconfidential.github.io/blog/images/2025/samuelson_example.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/datascienceconfidential.github.io/blog/images/2025/rbc_example.png?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/datascienceconfidential-r/",
          "text": "datascienceconfidential - r"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392143 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">How the DSGE sausage is made</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">April 27, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/datascienceconfidential-r/\">datascienceconfidential - r</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html\"> datascienceconfidential - r</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><h2 id=\"introduction\">Introduction</h2>\n<p>Dynamic Stochastic General Equilibrium (DSGE) models are a class of models which attempt to model the entire economy of a nation. The purpose of this post is to hand-code a DSGE model in R, fit it to data, and check that the results agree with the software packages Dynare and gEcon.</p>\n<p>I got interested in DSGE models after seeing a job ad mentioning them last year. I’m always looking for interesting applications of Bayesian statistics, and it turns out that central bank economists like to use Bayesian methods to fit these models, for reasons which will be <a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#fitting-the-model-to-data\" rel=\"nofollow\" target=\"_blank\">illustrated below</a>. I’m also interested in modelling the macroeconomy, which is what DSGE models are designed to do. DSGE models are nowadays fairly entrenched in economics education. For example, the popular economics textbook <em>Macroeconomics</em> by Mankiw contains a section entitled <em>Towards DSGE Models</em> which illustrates the calculation of some impulse response functions for a dynamic model.</p>\n<p>In New Zealand, quite a few people are using DSGE models. At the Reserve Bank there is the <a href=\"https://www.nzae.org.nz/wp-content/uploads/2015/01/Sander.pdf\" rel=\"nofollow\" target=\"_blank\">NZSim model</a> (which I’m particularly curious about). DSGE models are also being used <a href=\"https://www.treasury.govt.nz/sites/default/files/2024-04/twp24-01.pdf\" rel=\"nofollow\" target=\"_blank\">at Te Tai Ōhanga The Treasury</a> to predict things like the economic impact of natural disasters.</p>\n<p>Proponents of DSGE models say that they’re the only sensible way to model the macroeconomy, even going so far as to <a href=\"https://www.bruegel.org/blog-post/dsge-model-quarrel-again\" rel=\"nofollow\" target=\"_blank\">label everybody who doesn’t use them as dilettantes</a>. Sceptics see them as <a href=\"https://criticalfinance.org/2017/11/19/dilettantes-shouldnt-get-excited/\" rel=\"nofollow\" target=\"_blank\">a toxic cocktail</a> of intimidating mathematical complexity and horrifying economic naïveté. Who is right? The only way to find out is to roll up our sleeves and actually try to answer the following questions:</p>\n<ul>\n<li>What is a DSGE model?</li>\n<li>What is the motivation for developing DSGE models? Why are they better than other models?</li>\n<li>How do you formulate and solve a DSGE model?</li>\n<li>How do you fit a DSGE model to data?</li>\n</ul>\n<p>There are plenty of software packages which can fit DSGE models, so I could just run some code and pretend I know what I’m doing. But in order to really understand these models it’s necessary to compute one by hand. The purpose of this post is to go through the process of “solving” and fitting a DSGE model step by step and compare the results with the output of some software packages. The best source I have found for explicit computations are the superb <a href=\"https://sites.nd.edu/esims/courses/ph-d-macro-theory-ii/\" rel=\"nofollow\" target=\"_blank\">PhD Macro Theory II lecture notes of Eric Sims</a>, which I will be following closely for <a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#hand-calculations\" rel=\"nofollow\" target=\"_blank\">part of this post</a>. But I think there might be some value in explaining the same calculations from the point of view of someone who is not an insider. In particular, I will spend some time explaining points where I got stuck or confused.</p>\n<p>Apart from the notes by Sims, I also got a lot of benefit from reading the <a href=\"https://karlwhelan.com/blog/?page_id=2214\" rel=\"nofollow\" target=\"_blank\">lecture notes by Karl Whelan</a> and (for Dynare) the <a href=\"https://matteocoronese.eu/teaching/advanced-macroeconomics\" rel=\"nofollow\" target=\"_blank\">slides by Matteo Coronese</a>. Bear in mind that I’m not an expert! So if you find some mistakes in this post, I would be very grateful if you could point them out.</p>\n<p>Before explaining the motivation, the name DSGE stands for Dynamic (evolving in time), Stochastic (involving some source of randomness), and General Equilibirum (all markets clear).</p>\n<h2 id=\"motivation\">Motivation</h2>\n<p>Let’s start with a motivating question. Where do business cycles come from? Why does the economy go through cycles of boom and bust? The simplest model which exhibits business cycles is a beautiful toy model developed by Samuelson in 1939 and known as the Multiplier-Accelerator Interaction Model. (By the way, the best exposition of this model is the one in <a href=\"https://doi.org/10.2307/1927758\" rel=\"nofollow\" target=\"_blank\">Samuelson’s original paper</a>. It also appears in various textbooks, but they often ignore some of the important aspects.)</p>\n<p>Here’s a modified version<sup><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote1\" rel=\"nofollow\" target=\"_blank\">1</a></sup> of <a href=\"https://en.wikipedia.org/wiki/Multiplier-accelerator_model\" rel=\"nofollow\" target=\"_blank\">Samuelson’s model</a>. Suppose we have an economy with output $Y_t$ which is made up of consumption $C_t$ and investment $I_t$, giving the general equilibrium condition</p>\n\n\\[Y_t = C_t + I_t\\]\n\n<p>and suppose that consumption is a fixed proportion of output $C_t = cY_t$. Suppose that there is a long-term steady state $Y_t = \\overline{Y}$, $I_t = \\overline{I}$, $C_t = \\overline{C}$ and consider fluctuations around this steady state, which are denoted by lower case letters, so that $Y_t = \\overline{Y} + y_t$ etc.</p>\n<p>How is $I_t$ determined? At time $t$, assume that investors collectively choose to invest based on their observation of the most recent performance of the economy. The difference between output in the last two periods is $Y_{t-1} – Y_{t-2} = y_{t-1} – y_{t-2}$. Assume that the departure of investment from its steady state value is given by a multiple</p>\n\n\\[i_t = a(y_{t-1} – y_{t-2})\\]\n\n<p>the idea being that if output has recently grown then there will be more investment, but if output has shrunk then there will be less. Then you get</p>\n\n\\[y_t = \\frac{1}{1-c}(\\overline{I} + (1-c)\\overline{Y}) + \\frac{a}{1-c}(y_{t-1} – y_{t-2})\\]\n\n<p>or in other words, there are $\\alpha, \\beta$ with</p>\n\n\\[y_t = \\alpha + \\beta y_{t-1} – \\beta y_{t-2}.\\]\n\n<p>You can solve this model exactly using the theory of difference equations, or you can simply plot some examples. For example, suppose $\\alpha = 1, \\beta = 0.9$, and suppose we start in a boom, so that output in the last two periods was $Y_1 = \\overline{Y} + 1$ and $Y_2 = \\overline{Y} + 2$. Then this is what happens:</p>\n<pre>alpha &lt;- 1\nbeta &lt;- 0.9\nn &lt;- 100\ny12 &lt;- c(1, 2)\n\ny &lt;- rep(0, n)\ny[1:2] &lt;- y12\n\nfor (i in 3:n) y[i] &lt;- alpha + beta * y[i-1] - beta * y[i-2]\n\nplot(1:n, y, las=1, ylab=\"\", xlab=\"Time\", type=\"l\")\nmtext(\"y\", side=2, line=3, las=2)\n</pre>\n<div style=\"width:85%; margin:0 auto;\">\n<img data-lazy-src=\"https://i0.wp.com/datascienceconfidential.github.io/blog/images/2025/samuelson_example.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/datascienceconfidential.github.io/blog/images/2025/samuelson_example.png?w=578&amp;ssl=1\"/></noscript>\n</div>\n<p>You get a business cycle! Beautiful! In fact, if you write $\\Delta y := y_{t} - y_{t-1}$ then the equation for $y$ becomes</p>\n\n\\[-\\beta \\Delta^2 y + \\beta \\Delta y + y = -\\alpha\\]\n\n<p>which is a discrete version of the <a href=\"https://math.arizona.edu/~jwatkins/h-ode.pdf\" rel=\"nofollow\" target=\"_blank\">equation describing the motion of a spring under an external force</a>.</p>\n<p>However, there is a fly in the ointment. If we were biologists, we could add a little bit of stochasticity and stop here. Cycles like the ones in the <a href=\"https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations\" rel=\"nofollow\" target=\"_blank\">Lotka-Volterra predator-prey model</a> of animal populations are observed in real life. But economists, rather flatteringly, give us (the participants in the economy) more credit than this. If we were living in our toy economy, we would notice that business cycles occur regularly and adjust our investment behaviour accordingly. Because we know about the business cycles, there won’t actually be any business cycles! So this is not a good model of human behaviour.</p>\n<p>The question now becomes: is it possible to construct a model of the economy in which the people inside the model <em>know</em> that they are inside the model, and yet business cycles can still occur? And this is the sort of question that DSGE models address.</p>\n<h2 id=\"forward-looking-models\">Forward-looking models</h2>\n<p>A general DSGE-type model can be put into the form</p>\n\n\\[E_t[f(y_t, y_{t+1})] = 0\\]\n\n<p>where $f$ is some function (depending on some stochasticity) and $y_t$ is a vector of observations. The operator $E_t$ denotes expectations conditioned on information available up to time $t$ but not beyond. This equation expresses the idea that whatever we decide to do at time $t$ will depend on what we expect to happen in the future, namely at time $t+1$.</p>\n<p>In order to warm up, let’s consider some simple examples of such models. About the simplest thing we could consider is</p>\n\n\\[y_t = aE_t[y_{t+1}].\\]\n\n<p>Applying iterated expectations to this equation gives us</p>\n\n\\[y_t = a^2E_t[y_{t+2}] = \\cdots = a^k E_t[y_{t+k}]\\]\n\n<p>for all $k$. If $a &lt; 1$ and $E_t[y_{t+k}]$ is bounded, then we must have $y_t = 0$. For example, say $a = 1/2$. Then $E_t[y_{t+1}] = 2y_t$, so $y_t$ might be something like the price of an asset that we expect to go to the moon. If it can’t, then it turns out to be worthless.</p>\n<p>On the other hand if $a &gt; 1$, then we get an equation like $E[y_{t+1}] = 0.5y_t$ which describes something whose value is expected to decay, like a piece of rotting fruit, for example. In this case, $y_t$ could certainly be nonzero.</p>\n<p>These models might seem overly simple, but actually the strategy for analysing more complicated models will be to reduce them, via a series of steps, to one of these two cases.</p>\n<p>With this in mind, let’s consider the <a href=\"https://en.wikipedia.org/wiki/%22Hello,_World!%22_program\" rel=\"nofollow\" target=\"_blank\">“hello world”</a> example of a DSGE model.</p>\n<h2 id=\"a-model-of-the-economy\">A model of the economy</h2>\n<p>Suppose as before that time moves in discrete steps (months or quarters, perhaps) and that output, consumption, and investment at time $t$ are related by</p>\n\n\\[Y_t = C_t + I_t\\]\n\n<p>but now we’ll assume that $Y_t$ is produced by some technology which follows a Cobb-Douglas production function. Labour is taken to be fixed, so</p>\n\n\\[Y_t = A_t K_t^\\alpha\\]\n\n<p>where $K_t$ represents capital at time $t$ (capital is the buildings and tools which are used to produce things but not consumed in the process of production, so things like potash mines, grain elevators, tractors, pizza ovens and rolling pins, but not coal or flour) and $A_t$ is a fudge factor called <em>total factor productivity</em> which represents various things including the level of technology at time $t$.</p>\n<p>Capital depreciates over time at a rate $\\delta$, but new capital is created via investment, so the capital stock satisfies</p>\n\n\\[K_{t+1} = K_t - \\delta K_t + I_t.\\]\n\n<p>Technology is assumed to be subject to random shocks. It goes up if, for example, someone invents the filofax, and goes down if something occurs to lower the level of production, like a pandemic for example. We assume that these shocks follow an $AR(1)$ process in logarithms, so</p>\n\n\\[\\log A_{t+1} = \\rho \\log A_t + \\sigma_A\\varepsilon_t\\]\n\n<p>for some $0 &lt; \\rho &lt; 1$ where</p>\n\n\\[\\varepsilon_t \\sim N(0, 1).\\]\n\n<p>There is a representative agent in the economy who seeks to consume just the right amount at each time step in order to maximise their lifetime expected utility. Their utility function is assumed to be logarithmic and the discount factor is $\\beta$, so lifetime expected utility is</p>\n\n\\[E_t\\left[\\sum_{\\tau=t}^\\infty \\beta^{\\tau-t}\\log(C_\\tau) \\right]\\]\n\n<p>which some authors write as $E_0[\\sum_{t=0}^\\infty \\log(C_t)]$ and others as $E_t[\\sum_{t=0}^\\infty \\log(C_t)]$, both of which I found quite confusing.</p>\n<p>(An important point which took me a while to understand is that $C_t$ is chosen given all the information up to time $t$. Then the next value of the shock $\\varepsilon_{t+1}$ is revealed, and $C_{t+1}$ is chosen on the basis of this new knowledge. The ${C_t}$ aren’t chosen all at once. In the language of optimal control theory, this is a <em>closed loop</em> rather than an <em>open loop</em> problem.)</p>\n<p>We can eliminate $Y_t$ and $I_t$ from the equations to reduce our model to the following problem</p>\n\n\\[\\text{max } E_t\\left[\\sum_{\\tau=t}^\\infty \\beta^{\\tau-t}\\log(C_\\tau) \\right]\\]\n\n<p>subject to (for all $t$)</p>\n\n\\[\\begin{align*}\nK_{t+1} &amp;= A_t K_t^\\alpha - C_t + (1-\\delta)K_t\\\\\n\\log A_{t+1} &amp;= \\rho \\log A_t + \\sigma_A \\varepsilon_t\\\\\n\\varepsilon_t &amp;\\sim N(0, 1)\n\\end{align*}\\]\n\n<p>There are two ways of making progress on this problem. Both of them require some functional analysis to make fully rigorous. The right approach is to solve the Bellman Equation, but the approach taken by all the textbooks and lecture notes is to use Lagrange multipliers.</p>\n<p>Unfortunately we need to find some way to deal with the pesky expectation operator. Let’s pretend that there are some large finite number of possible paths of the random shocks, and denote each such path by $s$. Let $\\pi(s)$ be the probability of path $s$. Then we want to maximise</p>\n\n\\[\\sum_s \\pi(s) \\sum_\\tau \\beta^{\\tau-t}\\log(C_\\tau(s))\\]\n\n<p>subject to a very large number of constraints (one for each $s$ and $\\tau$)</p>\n\n\\[K_{\\tau+1}(s) = A_\\tau(s)K_\\tau(s)^\\alpha - C_\\tau(s) + (1-\\delta)K_\\tau(s)\\]\n\n<p>Note that variables with a subscript $t$ are known at time $t$, so we can drop their dependence on $s$. Also, $K_{t+1}$ is known at time $t$. We can form the Lagrangian</p>\n\n\\[\\mathcal{L} = \\log(C_t) + \\lambda_t(A_tK_t^\\alpha -C_t + \\\\(1-\\delta)K_t - K_{t+1}) + \\\\\n\\beta\\sum_s \\pi(s)\\log(C_{t+1}(s)) \\\\\n+ \\beta \\sum_s \\pi(s) \\lambda_{t+1}(s)(A_{t+1}(s)K_{t+1}^\\alpha\\\\\n-C_{t+1}(s) + (1-\\delta)K_{t+1} - K_{t+2}(s) ) + \\cdots\\]\n\n<p>where $\\lambda_t$ and $\\beta\\pi(s)\\lambda_{t+1}(s)$ are Lagrange multipliers, and the dots represent more terms which don’t involve $C_t$ or $K_{t+1}$.</p>\n<p>Setting the derivative to zero with respect to $C_t$ yields</p>\n\n\\[\\frac{1}{C_t} = \\lambda_t,\\]\n\n<p>Setting the derivative to zero with respect to $C_{t+1}(s)$ yields</p>\n\n\\[\\frac{1}{C_{t+1}(s)} = \\lambda_{t+1}(s),\\]\n\n<p>and doing the same with $K_{t+1}$ yields</p>\n\n\\[\\beta\\sum_s \\pi(s) \\lambda_{t+1}(s)(A_{t+1}(s)\\alpha K_{t+1}^{\\alpha-1} +1-\\delta) = \\lambda_t.\\]\n\n<p>Putting it all together gives the <em>Euler Equation</em></p>\n\n\\[\\frac{1}{C_t} = \\beta E_t\\left[\\frac{1}{C_{t+1}}(\\alpha A_{t+1}K_{t+1}^{\\alpha-1} + 1 -\\delta)\\right]\\]\n\n<p>Usually the textbooks don’t make the dependence on $s$ explicit, and treat $\\lambda_{t+1}$ as though it’s a scalar, inserting the expectation operator at the end.</p>\n<p>We’re left with a system of three equations: the Euler Equation, the <em>Law of Motion of Capital</em></p>\n\n\\[K_{t+1} =A_tK_t^\\alpha - C_t + (1-\\delta)K_t\\]\n\n<p>and the <em>Law of Motion of Technology</em></p>\n\n\\[\\log A_{t+1} = \\rho \\log A_t + \\sigma_A \\varepsilon_t.\\]\n\n<p>The model described by these equations is called the Neoclassical Growth Model, or (a special case of) a Real Business Cycle Model.</p>\n<p>Our task is to solve the model, which means that we want to give a function which tells us how to calculate $C_{t+1}$ from the information available at time $t$. Such a function is called a <em>policy function</em>. If $\\delta = 1$, the policy function can be found exactly. If $\\delta &lt; 1$, we need to approximate it.</p>\n<h2 id=\"perturbation-solution\">Perturbation Solution</h2>\n<p>Just like in the Samuelson model, let’s assume that the economy has a steady state and we’re interested in small deviations from the steady state. Instead of additive, we’ll assume that the deviations are logarithmic.</p>\n<p>The steady state can be found by setting $C_t = \\overline{C}$, $K_t = \\overline{K}$, $A_t = \\overline{A}$, $\\varepsilon_t = 0$ for all $t$. From the Law of Motion of Technology, we have</p>\n\n\\[\\log \\overline{A} = \\rho \\log \\overline{A} + 0\\]\n\n<p>and so $\\overline{A} = 1$ (since we assume that $\\rho &lt; 1$). From the Euler Equation, we get</p>\n\n\\[\\frac{1}{\\beta} = \\alpha \\overline{K}^{\\alpha-1} + 1 - \\delta\\]\n\n<p>and so</p>\n\n\\[\\overline{K} = \\left(\\frac{1}{\\alpha}\\left(\\frac{1}{\\beta} - 1 + \\delta \\right)\\right)^{\\frac{1}{\\alpha-1}}\\]\n\n<p>and from the Law of Motion of Capital, we get</p>\n\n\\[\\overline{C} = \\overline{K}^\\alpha - \\delta \\overline{K}\\]\n\n<p>Now we define $c_t = \\log(C_t/\\overline{C})$, $k_t = \\log(K_t/\\overline{K})$, $a_t = \\log(A_t/\\overline{A})$. Following a method due to Uhlig, we can replace $C_t$ by $\\overline{C} e^{c_t}$ and the same for the other variables, and make a first order approximation. To first order, the equation</p>\n\n\\[E_t g(C_t, K_t, A_t, C_{t+1}, K_{t+1}, A_{t+1}) = 0\\]\n\n<p>will become</p>\n\n\\[E_t \\nabla g (\\overline{X})\n\\cdot (\\overline{C}c_t, \\overline{K}k_t, \\overline{A}a_t, \\overline{C}c_{t+1}, \\overline{K}k_{t+1}, \\overline{A}a_{t+1}) = 0\\]\n\n<p>where $\\overline{X} = (\\overline{C}, \\overline{K}, \\overline{A}, \\overline{C}, \\overline{K}, \\overline{A})$ and the dot denotes the dot product. This process is called log-linearisation.</p>\n<p>As an example, let’s compute the log-linearisation for the Law of Motion of Capital. We have</p>\n\n\\[g = -C_t + A_tK_t^\\alpha + (1-\\delta)K_t - K_{t+1}\\]\n\n<p>and</p>\n\n\\[\\nabla g = (-1, \\alpha K_t^{\\alpha-1} + 1 -\\delta, K_t^\\alpha, 0, -1, 0)\\]\n\n<p>so</p>\n\n\\[\\nabla g (\\overline{X}) \\cdot (\\overline{C}c_t, \\overline{K}k_t, \\overline{A}a_t, \\overline{C}c_{t+1}, \\overline{K}k_{t+1}, \\overline{A}a_{t+1}) = \\\\\n-\\overline{C}c_t + \\frac{\\overline{K}}{\\beta}k_t + \\overline{K}^\\alpha a_t - \\overline{K}k_{t+1} = 0\\]\n\n<p>or equivalently</p>\n\n\\[k_{t+1} = -\\frac{\\overline{C}}{\\overline{K}}c_t + \\frac{1}{\\beta}k_t + \\overline{K}^{\\alpha -1} a_t.\\]\n\n<p>Doing the same for the other two equations yields the following result</p>\n\n\\[E_t\\left[\\begin{matrix} {c}_{t+1}\\\\\n{k}_{t+1}\\\\\n{a}_{t+1}\\end{matrix}\\right] = \\left[\n\\begin{matrix}\na &amp; b &amp; c\\\\\n-\\overline{C}/\\overline{K} &amp; 1/\\beta &amp; \\overline{K}^{\\alpha-1}\\\\\n0 &amp; 0 &amp; \\rho\n\\end{matrix}\n\\right]\n\\left[\\begin{matrix}\n{c}_t \\\\\n{k}_t \\\\\n{a}_t\n\\end{matrix}\\right]\\]\n\n<p>where $a, b$ and $c$ are given by the following gnarly expressions</p>\n\n\\[\\begin{align*}\na &amp;= 1 - \\beta\\alpha (\\alpha-1) \\overline{C} \\times \\overline{K}^{\\alpha-2}\\\\\nb &amp;= \\alpha (\\alpha-1) \\overline{K}^{\\alpha-1}\\\\\nc &amp;=  \\rho\\beta\\alpha\\overline{K}^{\\alpha-1} + \\beta\\alpha (\\alpha-1) (\\overline{K}^{\\alpha-1})^2\n\\end{align*}\\]\n\n<p>The next step is not hard to guess. If we could diagonalise the matrix which relates time $t+1$ to time $t$, then we’d have three single-variable equations of the form $E_t[w_{t+1}] = aw_t$ and we could solve them the way we did before.</p>\n<h3 id=\"hand-calculations\">Hand Calculations</h3>\n<p>(Based on the <a href=\"https://sites.nd.edu/esims/files/2024/01/notes_linear_models_sp2024.pdf\" rel=\"nofollow\" target=\"_blank\">notes by Sims</a>)</p>\n<p>Let’s consider a numerical example. Suppose<sup><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote2\" rel=\"nofollow\" target=\"_blank\">2</a></sup> $\\alpha = 0.33$, $\\beta = 0.99$, $\\delta = 0.1$, $\\rho = 0.95$, $\\sigma_A = 0.01$. Then</p>\n\n\\[\\left[\n\\begin{matrix}\na &amp; b &amp; c\\\\\n-\\overline{C}/\\overline{K} &amp; 1/\\beta &amp; \\overline{K}^{\\alpha-1}\\\\\n0 &amp; 0 &amp; \\rho\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n\\phantom{-}1.017 &amp; -0.074 &amp; 0.079\\\\\n-0.234 &amp; \\phantom{-}1.010 &amp; 0.334\\\\\n0 &amp; 0 &amp; 0.950\n\\end{matrix}\n\\right]\\]\n\n<p>This matrix can be decomposed as $Q^{-1}DQ$ where</p>\n\n\\[D = \\left[\n\\begin{matrix}\n1.144 &amp; 0 &amp; 0\\\\\n0 &amp; 0.95 &amp; 0\\\\\n0 &amp; 0 &amp; 0.882\n\\end{matrix}\n\\right]\\]\n\n<p>and</p>\n\n\\[Q = \\left[\n\\begin{matrix}\n1.027 &amp; -0.562 &amp; -0.545\\\\\n0 &amp; 0 &amp; \\phantom{-}3.941\\\\\n1.014 &amp; 0.585 &amp; -4.067\n\\end{matrix}\n\\right]\\]\n\n<p>which means that</p>\n\n\\[w_t := 1.027c_t - 0.562k_t - 0.545 a_t\\]\n\n<p>satisfies</p>\n\n\\[E_t[ w_t] = 1.144 w_t,\\]\n\n<p>but we already saw that, under some mild conditions stating that $E[w_t]$ can’t “go to the moon”, this means that $w_t = 0$ and so</p>\n\n\\[c_t = 0.547k_t + 0.530a_t\\]\n\n<p>for all $t$. This is the first-order approximation to the <em>policy function</em>. Furthermore,</p>\n\n\\[\\begin{align*}\nk_{t+1} &amp;= -0.234c_t +1.010k_t + 0.334a_t\\\\\n&amp;= -0.234(0.547k_t + 0.530a_t) + 1.010k_t + 0.334a_t\\\\\n&amp;= 0.882k_t + 0.210a_t\n\\end{align*}\\]\n\n<p>This is the first order approximation to the <em>transition function</em>, which is the function which computes $K_{t+1}$ in terms of $K_t$ and $A_t$.</p>\n<p>Shift indices to define $k_t’ := k_{t+1}$. Then since $a_t = 0.95a_{t-1} + 0.01\\varepsilon_t$, we have</p>\n\n\\[\\left[\n\\begin{matrix}\nc_t\\\\\n{k}_t'\\\\\na_t\\\\\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n0.547 &amp; 0.504\\\\\n0.882 &amp; 0.199\\\\\n0 &amp; 0.95\\\\\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nk_{t-1}'\\\\\na_{t-1}'\n\\end{matrix}\n\\right] +\n\\left[\n\\begin{matrix}\n0.005304\\\\\n0.002079\\\\\n0.01\\\\\n\\end{matrix}\n\\right] \\varepsilon_{t}\\]\n\n<p>and the expectation operator has magically vanished and we are left with a linear difference equation.</p>\n<h3 id=\"dynare\">Dynare</h3>\n<p><a href=\"https://www.dynare.org/\" rel=\"nofollow\" target=\"_blank\">Dynare</a> (I think it’s pronounced dee-<em>naire</em>; it’s French) is a software package for Matlab and Octave which, among other things, fits DSGE models. First, save the following code in a file <code>hello_dynare.mod</code>.</p>\n<pre>var k a c;\n\nvarexo e; // shocks\n\nparameters alpha beta delta rho sA;\nalpha = 0.33;\nbeta = 0.99;\ndelta = 0.1;\nrho = 0.95;\nsA = 0.01;\n\nmodel;\n\n/* Euler equation */\nexp(c)^(-1) = beta*exp(c(+1))^(-1)*(alpha*exp(a(+1))*exp(k)^(alpha-1) + (1-delta));\n\n/* law of motion of capital */\nexp(k) = exp(a)*exp(k(-1))^(alpha) - exp(c) + (1-delta)*exp(k(-1));\n\n/* law of motion of technology */\na = rho*a(-1) + sA*e;\n\nend;\n\nshocks;\nvar e = 1;\nend;\n\nsteady;\ncheck;\n\nstoch_simul(order=1, periods=200, nograph, nodisplay);\n</pre>\n<p>Then run it in the Octave GUI with the command</p>\n<pre>dynare hello_dynare.mod\n</pre>\n<p>Among the output, you’ll see</p>\n<pre>POLICY AND TRANSITION FUNCTIONS\n                                   k               a               c\nConstant                    1.638350               0        0.184374\nk(-1)                       0.882253               0        0.547200\na(-1)                       0.199228        0.950000        0.503894\ne                           0.002097        0.010000        0.005304\n</pre>\n<p>which is exactly what we just calculated! (Somewhat confusingly, the first row gives the steady state values. But no doubt there is a good reason for this.)</p>\n<h3 id=\"gecon\">gEcon</h3>\n<p>The R package <code>gEcon</code> is similar to Dynare. I’m not sure whether it is quite as powerful, but it has the advantage that you can type in the model without having to derive the Euler Equation yourself (although don’t make a mistake or you will get a segmentation fault and R will crash!) In the code, <code>U[]</code> represents the value function and the <code>objective</code> block is the Bellman Equation.</p>\n<p>To run it, save the following file as <code>hello_gecon.gcn</code></p>\n<pre>##################################################\n# Neoclassical growth model in gEcon\n##################################################\n\nblock CONSUMER\n{\n    definitions\n    {\n        u[] = log(C[]);\n    };\n    controls\n    {\n        K_s[], C[];\n    };\n    objective\n    {\n        U[] = u[] + beta * E[][U[1]];\n    };\n    constraints\n    {\n        K_s[] = A[] * K_s[-1]^alpha + (1 - delta) * K_s[-1] - C[] ;\n    };\n    calibration\n    {\n        delta = 0.1;\n        beta = 0.99;\n        alpha = 0.33;\n    };\n};\n\nblock EXOG \n{\n    identities\n    {\n        A[] = exp(rho * log(A[-1]) + 0.01 * epsilon_A[]);\n    };\n    shocks\n    {\n        epsilon_A[];\n    };\n    calibration\n    {\n        rho = 0.95;\n    };\n};\n</pre>\n<p>and run</p>\n<pre>library(gEcon)\n\nrbc &lt;- make_model(\"hello_gecon.gcn\")\nrbc &lt;- steady_state(rbc)\nrbc &lt;- solve_pert(model = rbc, loglin = TRUE)\nget_pert_solution(rbc)\n\n</pre>\n<p>Once again, you can identify the matrix we derived in the following output:</p>\n<pre>Matrix P:\n\n       A[-1] K_s[-1]\nA[]   0.9500  0.0000\nK_s[] 0.1992  0.8823\n\n\nMatrix Q:\n\n    epsilon_A\nA      0.0100\nK_s    0.0021\n\n\nMatrix R:\n\n     A[-1] K_s[-1]\nC[] 0.5039  0.5472\nU[] 1.2366  0.2345\n\n\nMatrix S:\n\n  epsilon_A\nC    0.0053\nU    0.0130\n</pre>\n<h2 id=\"simulation\">Simulation</h2>\n<p>Let’s write R functions to find the first-order solution for a given choice of parameters and to simulate from the model.</p>\n<pre>model_matrices &lt;- function(alpha, beta, rho, delta, sigma_A){\n  \n  # steady state\n  Abar &lt;- 1\n  Kbar &lt;- (1/alpha * (1/beta -1 +delta))^(1/(alpha-1))\n  Cbar &lt;- Kbar^alpha + (-delta) * Kbar\n  \n  a &lt;- 1 - beta * alpha * (alpha -1) * Cbar * Kbar^(alpha-2)\n  b &lt;- alpha * (alpha-1) * Kbar^(alpha-1)\n  c &lt;- rho * beta * alpha * Kbar^(alpha-1)\n  c &lt;- c + beta*alpha*(alpha-1)*(Kbar^(alpha-1))^2\n  \n  A &lt;- matrix(c(a, b, c, -Cbar/Kbar, 1/beta, Kbar^(alpha-1), 0, 0, rho), byrow=2, ncol=3)\n  \n  eigs &lt;- eigen(A)\n  \n  # check Blanchard-Kahn conditions\n  # eigs$values are always ordered from largest to smallest in R\n  BK_violated &lt;- F\n  if ((abs(eigs$values[1]) &lt; 1) | (abs(eigs$values[2]) &gt; 1)){\n    BK_violated &lt;- T\n  }\n  \n  P &lt;- eigs$vectors\n  \n  fr &lt;- solve(P)[1, ] # corresponds to largest eigenvalue\n  u &lt;- A[2,1]*(-fr[2:3]/fr[1]) + A[2,2:3]\n  Tt &lt;- matrix(c(u*c(1, rho), 0, rho), ncol=2, byrow=T)\n  Ht &lt;- sigma_A * matrix(c(0, 0, Tt[,2]/rho), nc=2, byrow=F)\n  \n  list(Tt = Tt, Ht = Ht, BK_violated = BK_violated)\n}\n</pre>\n<p>This function is pretty straightforward. It just does the computation which we already went through in a special case. It makes an additional check that the largest eigenvalue is really $&gt;1$ and the others are $&lt;1$. These are the famous <em>Blanchard-Kahn conditions</em> which guarantee that there is exactly one steady state.</p>\n<p>The function outputs <code>Tt</code> which consists of the bottom two rows of the policy/transition function matrix, and <code>Ht</code> which is the vector of coefficients of $\\varepsilon_t$ with an additional column of zeroes on the left. This format is chosen to match the conventions of the <code>dlm</code> package which will be used <a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#fitting-the-model-to-data\" rel=\"nofollow\" target=\"_blank\">below</a> for fitting the model.</p>\n<p>Checking in our example:</p>\n<pre>model_matrices(0.33, 0.99, 0.95, 0.1, 0.01)\n$Tt\n          [,1]      [,2]\n[1,] 0.8822534 0.1992279\n[2,] 0.0000000 0.9500000\n\n$Ht\n     [,1]        [,2]\n[1,]    0 0.002097136\n[2,]    0 0.010000000\n\n$BK_violated\n[1] FALSE\n</pre>\n<p>To simulate from the model, what we really want to output is $y_t := \\log(Y_t/\\overline{Y})$ where $\\overline{Y}$ is the steady-state value of output. Since $Y_t = A_t K_t^\\alpha$, we have $y_t = a_t + \\alpha k_t$. The following function outputs simulated values of $y_t, k_t$ and $a_t$.</p>\n<pre>sim &lt;- function(n, alpha, beta, rho, sigma, delta, sigma_A, start=c(0,0)){\n  \n  # n : number of simulations\n  # start : starting values of (k, a)\n  \n  M &lt;- model_matrices(alpha, beta, rho, sigma, delta, sigma_A)\n  Tt &lt;- M$Tt\n  Ht &lt;- M$Ht\n  \n  y_sim &lt;- rep(0, n)\n  ka_sim &lt;- matrix(0, nr=2, nc=n)\n  ka_sim[,1] &lt;- start\n  \n  for (i in 2:n){\n    ka_sim[,i] &lt;- Tt %*% ka_sim[,i-1] + Ht %*% rnorm(2)\n    y_sim[i] &lt;- ka_sim[,i] %*% c(alpha, 1) \n  }\n  list(y = y_sim, ka = ka_sim)\n}\n</pre>\n<p>(Note that we do <code>Ht %*% rnorm(2)</code>; the first component of <code>rnorm(2)</code> gets multiplied by zero and does nothing. Again, this is to match the <code>dlm</code> package which I’ll be using <a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#fitting-the-model-to-data\" rel=\"nofollow\" target=\"_blank\">below</a>.)</p>\n<p>Now we can look at the output of the model. Note that here we’re using the first-order approximation to simulate. I think this is a bit like using Euler’s method to plot the solution of an ODE. It usually works pretty well, but it might go off track eventually.</p>\n<pre>alpha &lt;- 0.33\nbeta &lt;- 0.99\nrho &lt;- 0.95\ndelta &lt;- 0.1\nsigma_A &lt;- 0.01\n\nset.seed(101)\ns &lt;- sim(200, alpha, beta, rho, delta, sigma_A)\nplot.ts(s$y, las=1, ylab=\"\")\n</pre>\n<div style=\"width:85%; margin:0 auto;\">\n<img data-lazy-src=\"https://i1.wp.com/datascienceconfidential.github.io/blog/images/2025/rbc_example.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/datascienceconfidential.github.io/blog/images/2025/rbc_example.png?w=578&amp;ssl=1\"/></noscript>\n</div>\n<p>Even though the people (or rather person; representative agent) in the model knows everything about the model and behaves optimally, we still get business cycles! That’s realistic, but it’s not the reason why the model is called a real business cycle model. The name comes from the fact that it’s a model of the <a href=\"https://en.wikipedia.org/wiki/Real_economy\" rel=\"nofollow\" target=\"_blank\">real economy</a>; it ignores prices, wages, and monetary frictions.</p>\n<p>Anyway, despite the fact that it is realistic in some ways, it turns out that such models are very unrealistic in other ways, and the problems with them can’t be fixed, so they have fallen out of fashion since the 1980s.</p>\n<h2 id=\"fitting-the-model-to-data\">Fitting the model to data</h2>\n<p>Fitting the model to real-life data is important because the ultimate goal of DSGE modelling is to answer questions like <em>“how much will output rise if the government increases spending by x?”</em>.</p>\n<p>There seem to be two approaches to model fitting. The first, called <em>calibration</em>, is just to guess what the parameters might be and try to make the model output match some salient features of the observed data. If this was formalised, I suppose it would be something like <a href=\"https://bayesiancomputationbook.com/markdown/chp_08.html\" rel=\"nofollow\" target=\"_blank\">Approximate Bayesian Computation</a> (called <em>indirect inference</em> in economics). The second way of fitting the model, which makes more sense from the point of view of a statistician, is to write down a likelihood function and then do likelihood-based inference.</p>\n<p>However, there is a complication in writing down the likelihood function. Recall that our example model (leaving out $c_t$) looks like</p>\n\n\\[\\left[\n\\begin{matrix}\n{k}_t'\\\\\na_t\\\\\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n0.882 &amp; 0.199\\\\\n0 &amp; 0.95\\\\\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nk_{t-1}'\\\\\na_{t-1}'\n\\end{matrix}\n\\right] +\n\\left[\n\\begin{matrix}\n0.002079\\\\\n0.01\\\\\n\\end{matrix}\n\\right] \\varepsilon_{t}\\]\n\n<p>but we don’t actually observe $k_t$ (the log deviation of the capital stock from its steady state) or $a_t$ (the log deviation of total factor productivity from its steady state). What we maybe do have is an estimate of output $Y_t$ since we can use the GDP, and we can estimate its log deviation $y_t$ from its steady state. However, we can’t make further progress without positing some sort of statistical relationship between $y_t = a_t + \\alpha k_t$, $k_t$ and $a_t$.</p>\n<p>Fortunately, there’s a standard approach to this problem called the Kalman Filter. I experimented with a few different Kalman Filter R packages, but I got confused because they all call things different names and I couldn’t get two of them to give the same output. Finally I found the <code>dlm</code> <a href=\"https://cran.r-project.org/web/packages/dlm/index.html\" rel=\"nofollow\" target=\"_blank\">package by Giovanni Petris</a> which does exactly what I need (generally I like these kind of self-contained R packages which do one thing well rather than attempting to do everything).</p>\n<pre>library(dlm)\n\nbuild_dlm &lt;- function(alpha, beta, rho, delta, sigma_A){\n  \n  # obtain a DLM model object from the parameters\n  \n  M &lt;- model_matrices(alpha, beta, rho, delta, sigma_A)\n  Tt &lt;- M$Tt\n  Ht &lt;- M$Ht\n  HHt &lt;- M$Ht %*% t(M$Ht)\n  \n  # starting point for Kalman Filter variance\n  P0 &lt;- matrix(solve(diag(4) - kronecker(Tt, Tt)) %*% c(HHt), nc=2, byrow=F)\n  \n  dlm(m0 = matrix(c(0,0), nc=1),\n      C0 = P0,\n      FF = matrix(c(alpha, 1), nc=2),\n      V = matrix(0),\n      GG = Tt,\n      W = sigma_A^2 * diag(2))\n}\n</pre>\n<p>To construct the <code>dlm</code> object, <code>m0</code> is the starting point for the hidden (state) variables (in our case $(k_0, a_0)$), <code>P0</code> is the starting point for the variance of the hidden variables, <code>FF</code> is the matrix which defines <code>y</code> in terms of <code>k</code> and <code>a</code>, <code>V</code> is a constant which can be added to <code>y</code>, <code>GG</code> is the transition matrix for the state variables, and <code>W</code> is the variance-covariance matrix for the shocks. The formula for <code>P0</code> is the solution of the so-called <em>Lyapunov Equation</em><sup><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote3\" rel=\"nofollow\" target=\"_blank\">3</a></sup> which calculates the long-run limit of the variance-covariance matrix of the state variables.</p>\n<p>Once you have the <code>dlm</code> object, there are functions <code>dlmLL</code> for calculating the negative log-likelihood and <code>dlmMLE</code> for the maximum likelihood estimator.</p>\n<p>You can check that <code>dlmMLE</code> really recovers the model parameters by writing a piece of code like this (I’m going to do a little bit of calibration of my own by fixing $\\alpha=0.33$ and taking $\\beta = 0.99$ in this example.)</p>\n<pre>alpha &lt;- 0.33\nbeta &lt;- 0.99\ndelta &lt;- 0.1\nrho &lt;- 0.95\nsigma_A &lt;- 0.01\n\nN_rep &lt;- 100\nN_sim &lt;- 1000\nbuild &lt;- function(par) build_dlm(alpha, \n                                beta, \n                                exp(par[1])/(1+exp(par[1])),\n                                exp(par[2])/(1+exp(par[2])), \n                                exp(par[3]))\n\nout &lt;- matrix(0, nc=N_rep, nr=3)\n\nset.seed(102)\n\nfor (i in 1:N_rep){\n  # simulate from model\n  s &lt;- sim(N_sim, alpha, beta, rho, delta, sigma_A)\n  \n  # calculate MLE\n  opt &lt;- dlmMLE(y=s$y, parm=c(0,0,0), build)\n\n  # save MLE in output matrix\n  out[,i] &lt;- c((1/(1+exp(-opt$par[1:2]))), exp(opt$par[3]))\n}\n\nrowMeans(out)\n# [1] 0.94887794 0.10367580 0.01011981\n</pre>\n<p>Note that it’s much better to transform the variables and use unconstrained optimisation rather than trying to use <code>nlminb</code> and the simplex method.</p>\n<h3 id=\"real-life-data\">Real-life data</h3>\n<p>In order to see whether the model breaks down with real-life data, I downloaded the New Zealand GDP series from the <a href=\"https://www.rbnz.govt.nz/statistics/series/economic-indicators/gross-domestic-product\" rel=\"nofollow\" target=\"_blank\">RBNZ website</a>. I’m not quite sure which series is most appropriate, but I decided to take seasonally-adjusted real production-based GDP as my $Y_t$. Because we are looking at long-term fluctuations of $Y_t$ about its steady state, the long-term growth trend in the series has to be removed (hopefully this also takes care of population growth).</p>\n<pre># raw data\nY &lt;- ts(c(27979, 28130, 28258, 28148, 28016, 28080, 27835, \n28189, 28353, 28034, 27980, 27933, 27929, 28183, 28512, 27821, \n27632, 27721, 27900, 27983, 27986, 27771, 28120, 28581, 29213, \n29806, 30085, 30544, 30746, 31271, 31686, 32068, 32478, 32779, \n33000, 33466, 33761, 33981, 34424, 34296, 34967, 34887, 34827, \n34643, 34827, 34859, 35205, 35616, 35899, 36900, 37350, 37893, \n37904, 38053, 38122, 38252, 38790, 39079, 39657, 39998, 40527, \n41009, 41583, 41840, 42041, 42946, 43509, 44254, 44644, 44779, \n44947, 45469, 46202, 46439, 46250, 46990, 47185, 47471, 47864, \n48501, 48922, 49311, 49409, 49200, 49098, 48936, 48622, 48197, \n48167, 48397, 49014, 49170, 49509, 49387, 49105, 49618, 49802, \n50337, 50632, 50943, 51233, 51314, 52043, 51887, 52445, 52738, \n52931, 53746, 53982, 54695, 55540, 55725, 56152, 56692, 57269, \n57979, 58496, 59089, 59243, 59921, 60407, 60869, 61437, 62046, \n62639, 62716, 63665, 64243, 64263, 64756, 65403, 64687, 57967, \n66117, 66375, 67757, 68349, 65452, 68107, 68104, 68848, 70203, \n70208, 70156, 70688, 70717, 70857, 71111, 70346, 69604, 70056\n), start=c(1987, 1), freq=4)\n\n# preprocessing\ny &lt;- log(Y)\nzz &lt;- lm(y ~ x, data=data.frame(x=1:151))$resid\n\n# fit model\nopt &lt;- dlmMLE(y=zz, parm=c(0,0,0), build)\n\n# results\nc((1/(1+exp(-opt$par[1:2]))), exp(opt$par[3]))\n# [1] 0.83546378 0.07661051 0.01548509\n</pre>\n<p>It’s hard to know whether the parameters for the $AR(1)$ process which drive the shocks in total factor productivity are sensible, but a capital depreciation rate of $7.7\\%$ seems reasonable.</p>\n<h4 id=\"maximum-likelihood-estimation-with-gecon\">Maximum Likelihood Estimation with gEcon</h4>\n<p>In gEcon, we need to add an <code>EQUILIBRIUM</code> block and change the <code>EXOG</code> block as follows:</p>\n<pre>block EQUILIBRIUM \n{\n    identities {\n        log_Y[] = log(A[]) + alpha * log(K_s[]);\n    };\n};\n\nblock EXOG \n{\n    identities\n    {\n        A[] = exp(rho * log(A[-1]) + epsilon_A[]);\n    };\n    shocks\n    {\n        epsilon_A[];\n    };\n    calibration\n    {\n        rho = 0.95;\n    };\n};\n</pre>\n<p>Now run the code to solve the model again and add commands to perform inference.</p>\n<pre>rbc &lt;- make_model(\"hello_gecon.gcn\")\nrbc &lt;- steady_state(rbc)\nrbc &lt;- solve_pert(model = rbc, loglin = TRUE)\n\n# zz as above\nzz &lt;- lm(y ~ x, data=data.frame(x=1:151))$resid\ndf = data.frame(log_Y = zz)\n\nml_estimation_result &lt;- ml_estimation(data_set = ts(df),\nobservables = c(\"log_Y\"),\nmodel = rbc,\nest_par = c(\"rho\", \"delta\", \"sd(epsilon_A)\"),\ninitial_vals = c(\"sd(epsilon_A)\" = 0.01,\n\"rho\" = 0.9, \"delta\" = 0.1))\n\nsummary(ml_estimation_result)\n# The estimation results:\n# The likelihood optimisation statistics:\n#              ML estimate:   Std. err:\n# rho             0.83796827 0.055224398\n# delta           0.07310659 0.059381472\n# sd(epsilon_A)   0.01039685 0.005797989\n</pre>\n<p>The results are pretty close to our hand calculation using <code>dlm</code>, although not quite the same.</p>\n<h4 id=\"maximum-likelihood-estimation-with-dynare\">Maximum Likelihood Estimation with Dynare</h4>\n<p>To fit the model in Dynare, we need to create a csv file with a single column containing the values in <code>zz</code> with a named header which I chose to call <code>y3</code>. Then you need to modify the <code>.mod</code> file and save it as <code>estimate_rbc_model.mod</code>. Then you can run it in Octave as before.</p>\n<pre>var k a c y;\n\nvarexo e; // shocks\n\nparameters alpha beta delta rho sA;\nalpha = 0.33;\nbeta = 0.95;\ndelta = 0.1;\nrho = 0.95;\nsA = 0.01;\n\nmodel;\n\n/* Euler equation */\nexp(c)^(-1) = beta*exp(c(+1))^(-1)*(alpha*exp(a(+1))*exp(k)^(alpha-1) + (1-delta));\n\n/* law of motion of capital */\nexp(k) = exp(a)*exp(k(-1))^(alpha) - exp(c) + (1-delta)*exp(k(-1));\n\n/* law of motion of technology */\na = rho*a(-1) + e;\n\n/* output */\ny = a + alpha*k;\n\nend;\n\nsteady_state_model;\na = 0;\nk = log(((1/beta - 1 + delta)/alpha)^(1/(alpha - 1)));\nc = log(exp(k)^alpha - delta * exp(k));\ny = a + alpha*k;\nend;\ncheck;\n\nvarobs y;\n\nestimated_params;\n  rho, 0.5, 0, 1;\n  delta, 0.5, 0, 1;\n  stderr e, 0.5, 0, 1;\nend;\n\nestimation(datafile=y3, mode_compute = 4, nograph, nodisplay) y;\n</pre>\n<p>The output from Dynare is quite different to the other two packages. It gives a value of $27.7\\%$ for depreciation.</p>\n<pre>RESULTS FROM MAXIMUM LIKELIHOOD ESTIMATION\nparameters\n        Estimate    s.d. t-stat\n\nrho       0.5935  0.0633  9.3710\ndelta     0.2770  0.0032 85.4451\n\nstandard deviation of shocks\n        Estimate    s.d. t-stat\n\ne         0.0149  0.0010 15.2407\n</pre>\n<p>Looking at the code, it’s clear that no two of these three methods are using the same likelihood function. This could be because they start the Kalman Filter in a different place, because they handle violation of the Blanchard-Kahn conditions differently, because of an error in my code, or for some other reason.</p>\n<p>This does provide a nice illustration of why economists prefer to use Bayesian methods to estimate these models. Even in this very simple example, the likelihood surface is very flat. In general, there will be lots of local maxima and it doesn’t make much sense to use maximum likelihood. Bayesian methods make more sense. It seems that economists tend to use rather strong priors, since there tend to be good reasons why parameters are likely to lie within fairly narrow boundaries. Bayesian methods are available in both Dynare and gEcon, or you can write your own. Once you have the likelihood function, you can do whatever you want!</p>\n<h2 id=\"critiques-of-the-dsge-methodology\">Critiques of the DSGE methodology</h2>\n<p>We’ve finally answered the questions from the introduction.</p>\n<ul>\n<li>What is a DSGE model?</li>\n</ul>\n<p>It’s a model of how the macroeconomy evolves in time in which all markets clear, all agents behave optimally, and there are some unpredictable random shocks involved.</p>\n<ul>\n<li>What is the motivation for developing DSGE models? Why are they better than other models?</li>\n</ul>\n<p>Economic agents in DSGE models take the future into account when making decisions. This may make DSGE models more realistic than other models, in theory.</p>\n<ul>\n<li>How do you formulate and solve a DSGE model?</li>\n</ul>\n<p>You write down a stochastic optimal control problem, find some necessary first-order conditions, make a log-linear (or sometimes higher order) approximation under the assumption that there’s a unqiue steady state, derive a VAR model for the perturbations about the steady state, and solve it.</p>\n<ul>\n<li>How do you fit a DSGE model to data?</li>\n</ul>\n<p>You fit the first order approximation rather than the original model. You need the Kalman Filter to relate your observations to the state variables. And, if using a likelihood-based method, it had better be Bayesian.</p>\n<p>This leaves the question of why <a href=\"https://www.bruegel.org/blog-post/dsge-model-quarrel-again\" rel=\"nofollow\" target=\"_blank\">some people don’t seem to like or trust DSGE models</a>. One reason is that many DSGE models, just like our example model, assume that the economy contains <em>representative agents</em>: exactly one consumer, one firm, etc. One problem with earlier models like the good old IS-LM model is that people are not assumed to behave optimally. The DSGE approach tackles this problem by assuming that people behave optimally, but that there is only one of them.</p>\n<p>It’s not clear that this is necessarily better. You might say that the DSGE approach is looking at the average behaviour of a large number of people and assuming that they collectively behave optimally, but is it true that a large number of people with different utility functions each making decisions for themselves would act to maximise their collective utility? <a href=\"https://johnhcochrane.blogspot.com/2023/07/new-york-times-on-hank-and-questions.html\" rel=\"nofollow\" target=\"_blank\">Perhaps it is</a>. That’s supposed to be one of the <a href=\"https://en.wikipedia.org/wiki/The_Fable_of_the_Bees\" rel=\"nofollow\" target=\"_blank\">lessons of economics</a>, right? But is it true that individual people really behave optimally? Behavioural economists would say no. And also, if you treat the collective utility of everyone as the only number of interest, then you can’t calculate any statistics about inequality, which is something which I happen to be interested in.<sup><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote4\" rel=\"nofollow\" target=\"_blank\">4</a></sup></p>\n<p>Concerns of this sort led economists to develop extensions of DSGE models which do allow different types of agents. The most famous are the Heterogeneous Agent New Keynesian<sup><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#myfootnote5\" rel=\"nofollow\" target=\"_blank\">5</a></sup> or HANK models. HANK models can’t be fitted using the methods we used in our example, but there are other approaches, which I’m hoping to find out about.</p>\n<hr/>\n<h2 id=\"footnotes\">Footnotes</h2>\n<p><a name=\"myfootnote1\">1</a> In the original model, consumption at time $t$ depends on output at time $t-1$. I want consumption at time $t$ to be proportional to output at time $t$ in order to match the RBC model introduced <a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html#a-model-of-the-economy\" rel=\"nofollow\" target=\"_blank\">in the following section</a>. The choice doesn’t make a difference to the qualitative properties of the model.</p>\n<p><a name=\"myfootnote2\">2</a> The value $\\alpha = 0.33$ is <a href=\"https://pages.stern.nyu.edu/~cedmond/ge07pt/ps1_answers.pdf\" rel=\"nofollow\" target=\"_blank\">a standard choice of parameter in the production function</a> and $\\beta = 0.99$ is taken to match the NZSim paper mentioned in the introduction.</p>\n<p><a name=\"myfootnote3\">3</a> You can check this using the following R code</p>\n<pre>s &lt;- sim(10000, alpha, beta, rho, delta, sigma_A)\nM &lt;- model_matrices(alpha, beta, rho, delta, sigma_A)\n\n# Lyapunov Equation solution\nmatrix(solve(diag(4) - kronecker(M$Tt, M$Tt)) %*% c(M$Ht %*% t(M$Ht)), nc=2, byrow=F)\n\n# long-run variance\nvar(t(s$ka))\n</pre>\n<p><a name=\"myfootnote4\">4</a> Could this even be a feature rather than a bug? In <a href=\"https://www.goodreads.com/book/show/18377995-how-to-speak-money\" rel=\"nofollow\" target=\"_blank\"><em>How to Speak Money</em></a>, John Lanchester states that increasing inequality is the driving force that makes neoliberal policies work. So perhaps it’s not surprising that in the last forty years, as neoliberal economic policies were sweeping the developed world, economists were coming up with a modelling methodology which made it impossible even to <em>talk</em> about inequality?</p>\n<p><a name=\"myfootnote5\">5</a> The basic New Keynesian model is another possible “hello world” DSGE model. It’s used, for example, as an introductory model in <a href=\"https://users.ox.ac.uk/~exet2581/Boe/dsge_all.pdf\" rel=\"nofollow\" target=\"_blank\">training materials from the Bank of England</a>. But, although it consists of just three equations, deriving them takes about forty pages of mathematics.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://datascienceconfidential.github.io/economics/r/2025/04/28/how-the-dsge-sausage-is-made.html\"> datascienceconfidential - r</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "How the DSGE sausage is made\nPosted on\nApril 27, 2025\nby\ndatascienceconfidential - r\nin\nR bloggers\n| 0 Comments\n[This article was first published on\ndatascienceconfidential - r\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIntroduction\nDynamic Stochastic General Equilibrium (DSGE) models are a class of models which attempt to model the entire economy of a nation. The purpose of this post is to hand-code a DSGE model in R, fit it to data, and check that the results agree with the software packages Dynare and gEcon.\nI got interested in DSGE models after seeing a job ad mentioning them last year. I’m always looking for interesting applications of Bayesian statistics, and it turns out that central bank economists like to use Bayesian methods to fit these models, for reasons which will be\nillustrated below\n. I’m also interested in modelling the macroeconomy, which is what DSGE models are designed to do. DSGE models are nowadays fairly entrenched in economics education. For example, the popular economics textbook\nMacroeconomics\nby Mankiw contains a section entitled\nTowards DSGE Models\nwhich illustrates the calculation of some impulse response functions for a dynamic model.\nIn New Zealand, quite a few people are using DSGE models. At the Reserve Bank there is the\nNZSim model\n(which I’m particularly curious about). DSGE models are also being used\nat Te Tai Ōhanga The Treasury\nto predict things like the economic impact of natural disasters.\nProponents of DSGE models say that they’re the only sensible way to model the macroeconomy, even going so far as to\nlabel everybody who doesn’t use them as dilettantes\n. Sceptics see them as\na toxic cocktail\nof intimidating mathematical complexity and horrifying economic naïveté. Who is right? The only way to find out is to roll up our sleeves and actually try to answer the following questions:\nWhat is a DSGE model?\nWhat is the motivation for developing DSGE models? Why are they better than other models?\nHow do you formulate and solve a DSGE model?\nHow do you fit a DSGE model to data?\nThere are plenty of software packages which can fit DSGE models, so I could just run some code and pretend I know what I’m doing. But in order to really understand these models it’s necessary to compute one by hand. The purpose of this post is to go through the process of “solving” and fitting a DSGE model step by step and compare the results with the output of some software packages. The best source I have found for explicit computations are the superb\nPhD Macro Theory II lecture notes of Eric Sims\n, which I will be following closely for\npart of this post\n. But I think there might be some value in explaining the same calculations from the point of view of someone who is not an insider. In particular, I will spend some time explaining points where I got stuck or confused.\nApart from the notes by Sims, I also got a lot of benefit from reading the\nlecture notes by Karl Whelan\nand (for Dynare) the\nslides by Matteo Coronese\n. Bear in mind that I’m not an expert! So if you find some mistakes in this post, I would be very grateful if you could point them out.\nBefore explaining the motivation, the name DSGE stands for Dynamic (evolving in time), Stochastic (involving some source of randomness), and General Equilibirum (all markets clear).\nMotivation\nLet’s start with a motivating question. Where do business cycles come from? Why does the economy go through cycles of boom and bust? The simplest model which exhibits business cycles is a beautiful toy model developed by Samuelson in 1939 and known as the Multiplier-Accelerator Interaction Model. (By the way, the best exposition of this model is the one in\nSamuelson’s original paper\n. It also appears in various textbooks, but they often ignore some of the important aspects.)\nHere’s a modified version\n1\nof\nSamuelson’s model\n. Suppose we have an economy with output $Y_t$ which is made up of consumption $C_t$ and investment $I_t$, giving the general equilibrium condition\n\\[Y_t = C_t + I_t\\]\nand suppose that consumption is a fixed proportion of output $C_t = cY_t$. Suppose that there is a long-term steady state $Y_t = \\overline{Y}$, $I_t = \\overline{I}$, $C_t = \\overline{C}$ and consider fluctuations around this steady state, which are denoted by lower case letters, so that $Y_t = \\overline{Y} + y_t$ etc.\nHow is $I_t$ determined? At time $t$, assume that investors collectively choose to invest based on their observation of the most recent performance of the economy. The difference between output in the last two periods is $Y_{t-1} – Y_{t-2} = y_{t-1} – y_{t-2}$. Assume that the departure of investment from its steady state value is given by a multiple\n\\[i_t = a(y_{t-1} – y_{t-2})\\]\nthe idea being that if output has recently grown then there will be more investment, but if output has shrunk then there will be less. Then you get\n\\[y_t = \\frac{1}{1-c}(\\overline{I} + (1-c)\\overline{Y}) + \\frac{a}{1-c}(y_{t-1} – y_{t-2})\\]\nor in other words, there are $\\alpha, \\beta$ with\n\\[y_t = \\alpha + \\beta y_{t-1} – \\beta y_{t-2}.\\]\nYou can solve this model exactly using the theory of difference equations, or you can simply plot some examples. For example, suppose $\\alpha = 1, \\beta = 0.9$, and suppose we start in a boom, so that output in the last two periods was $Y_1 = \\overline{Y} + 1$ and $Y_2 = \\overline{Y} + 2$. Then this is what happens:\nalpha <- 1\nbeta <- 0.9\nn <- 100\ny12 <- c(1, 2)\n\ny <- rep(0, n)\ny[1:2] <- y12\n\nfor (i in 3:n) y[i] <- alpha + beta * y[i-1] - beta * y[i-2]\n\nplot(1:n, y, las=1, ylab=\"\", xlab=\"Time\", type=\"l\")\nmtext(\"y\", side=2, line=3, las=2)\nYou get a business cycle! Beautiful! In fact, if you write $\\Delta y := y_{t} - y_{t-1}$ then the equation for $y$ becomes\n\\[-\\beta \\Delta^2 y + \\beta \\Delta y + y = -\\alpha\\]\nwhich is a discrete version of the\nequation describing the motion of a spring under an external force\n.\nHowever, there is a fly in the ointment. If we were biologists, we could add a little bit of stochasticity and stop here. Cycles like the ones in the\nLotka-Volterra predator-prey model\nof animal populations are observed in real life. But economists, rather flatteringly, give us (the participants in the economy) more credit than this. If we were living in our toy economy, we would notice that business cycles occur regularly and adjust our investment behaviour accordingly. Because we know about the business cycles, there won’t actually be any business cycles! So this is not a good model of human behaviour.\nThe question now becomes: is it possible to construct a model of the economy in which the people inside the model\nknow\nthat they are inside the model, and yet business cycles can still occur? And this is the sort of question that DSGE models address.\nForward-looking models\nA general DSGE-type model can be put into the form\n\\[E_t[f(y_t, y_{t+1})] = 0\\]\nwhere $f$ is some function (depending on some stochasticity) and $y_t$ is a vector of observations. The operator $E_t$ denotes expectations conditioned on information available up to time $t$ but not beyond. This equation expresses the idea that whatever we decide to do at time $t$ will depend on what we expect to happen in the future, namely at time $t+1$.\nIn order to warm up, let’s consider some simple examples of such models. About the simplest thing we could consider is\n\\[y_t = aE_t[y_{t+1}].\\]\nApplying iterated expectations to this equation gives us\n\\[y_t = a^2E_t[y_{t+2}] = \\cdots = a^k E_t[y_{t+k}]\\]\nfor all $k$. If $a < 1$ and $E_t[y_{t+k}]$ is bounded, then we must have $y_t = 0$. For example, say $a = 1/2$. Then $E_t[y_{t+1}] = 2y_t$, so $y_t$ might be something like the price of an asset that we expect to go to the moon. If it can’t, then it turns out to be worthless.\nOn the other hand if $a > 1$, then we get an equation like $E[y_{t+1}] = 0.5y_t$ which describes something whose value is expected to decay, like a piece of rotting fruit, for example. In this case, $y_t$ could certainly be nonzero.\nThese models might seem overly simple, but actually the strategy for analysing more complicated models will be to reduce them, via a series of steps, to one of these two cases.\nWith this in mind, let’s consider the\n“hello world”\nexample of a DSGE model.\nA model of the economy\nSuppose as before that time moves in discrete steps (months or quarters, perhaps) and that output, consumption, and investment at time $t$ are related by\n\\[Y_t = C_t + I_t\\]\nbut now we’ll assume that $Y_t$ is produced by some technology which follows a Cobb-Douglas production function. Labour is taken to be fixed, so\n\\[Y_t = A_t K_t^\\alpha\\]\nwhere $K_t$ represents capital at time $t$ (capital is the buildings and tools which are used to produce things but not consumed in the process of production, so things like potash mines, grain elevators, tractors, pizza ovens and rolling pins, but not coal or flour) and $A_t$ is a fudge factor called\ntotal factor productivity\nwhich represents various things including the level of technology at time $t$.\nCapital depreciates over time at a rate $\\delta$, but new capital is created via investment, so the capital stock satisfies\n\\[K_{t+1} = K_t - \\delta K_t + I_t.\\]\nTechnology is assumed to be subject to random shocks. It goes up if, for example, someone invents the filofax, and goes down if something occurs to lower the level of production, like a pandemic for example. We assume that these shocks follow an $AR(1)$ process in logarithms, so\n\\[\\log A_{t+1} = \\rho \\log A_t + \\sigma_A\\varepsilon_t\\]\nfor some $0 < \\rho < 1$ where\n\\[\\varepsilon_t \\sim N(0, 1).\\]\nThere is a representative agent in the economy who seeks to consume just the right amount at each time step in order to maximise their lifetime expected utility. Their utility function is assumed to be logarithmic and the discount factor is $\\beta$, so lifetime expected utility is\n\\[E_t\\left[\\sum_{\\tau=t}^\\infty \\beta^{\\tau-t}\\log(C_\\tau) \\right]\\]\nwhich some authors write as $E_0[\\sum_{t=0}^\\infty \\log(C_t)]$ and others as $E_t[\\sum_{t=0}^\\infty \\log(C_t)]$, both of which I found quite confusing.\n(An important point which took me a while to understand is that $C_t$ is chosen given all the information up to time $t$. Then the next value of the shock $\\varepsilon_{t+1}$ is revealed, and $C_{t+1}$ is chosen on the basis of this new knowledge. The ${C_t}$ aren’t chosen all at once. In the language of optimal control theory, this is a\nclosed loop\nrather than an\nopen loop\nproblem.)\nWe can eliminate $Y_t$ and $I_t$ from the equations to reduce our model to the following problem\n\\[\\text{max } E_t\\left[\\sum_{\\tau=t}^\\infty \\beta^{\\tau-t}\\log(C_\\tau) \\right]\\]\nsubject to (for all $t$)\n\\[\\begin{align*}\nK_{t+1} &= A_t K_t^\\alpha - C_t + (1-\\delta)K_t\\\\\n\\log A_{t+1} &= \\rho \\log A_t + \\sigma_A \\varepsilon_t\\\\\n\\varepsilon_t &\\sim N(0, 1)\n\\end{align*}\\]\nThere are two ways of making progress on this problem. Both of them require some functional analysis to make fully rigorous. The right approach is to solve the Bellman Equation, but the approach taken by all the textbooks and lecture notes is to use Lagrange multipliers.\nUnfortunately we need to find some way to deal with the pesky expectation operator. Let’s pretend that there are some large finite number of possible paths of the random shocks, and denote each such path by $s$. Let $\\pi(s)$ be the probability of path $s$. Then we want to maximise\n\\[\\sum_s \\pi(s) \\sum_\\tau \\beta^{\\tau-t}\\log(C_\\tau(s))\\]\nsubject to a very large number of constraints (one for each $s$ and $\\tau$)\n\\[K_{\\tau+1}(s) = A_\\tau(s)K_\\tau(s)^\\alpha - C_\\tau(s) + (1-\\delta)K_\\tau(s)\\]\nNote that variables with a subscript $t$ are known at time $t$, so we can drop their dependence on $s$. Also, $K_{t+1}$ is known at time $t$. We can form the Lagrangian\n\\[\\mathcal{L} = \\log(C_t) + \\lambda_t(A_tK_t^\\alpha -C_t + \\\\(1-\\delta)K_t - K_{t+1}) + \\\\\n\\beta\\sum_s \\pi(s)\\log(C_{t+1}(s)) \\\\\n+ \\beta \\sum_s \\pi(s) \\lambda_{t+1}(s)(A_{t+1}(s)K_{t+1}^\\alpha\\\\\n-C_{t+1}(s) + (1-\\delta)K_{t+1} - K_{t+2}(s) ) + \\cdots\\]\nwhere $\\lambda_t$ and $\\beta\\pi(s)\\lambda_{t+1}(s)$ are Lagrange multipliers, and the dots represent more terms which don’t involve $C_t$ or $K_{t+1}$.\nSetting the derivative to zero with respect to $C_t$ yields\n\\[\\frac{1}{C_t} = \\lambda_t,\\]\nSetting the derivative to zero with respect to $C_{t+1}(s)$ yields\n\\[\\frac{1}{C_{t+1}(s)} = \\lambda_{t+1}(s),\\]\nand doing the same with $K_{t+1}$ yields\n\\[\\beta\\sum_s \\pi(s) \\lambda_{t+1}(s)(A_{t+1}(s)\\alpha K_{t+1}^{\\alpha-1} +1-\\delta) = \\lambda_t.\\]\nPutting it all together gives the\nEuler Equation\n\\[\\frac{1}{C_t} = \\beta E_t\\left[\\frac{1}{C_{t+1}}(\\alpha A_{t+1}K_{t+1}^{\\alpha-1} + 1 -\\delta)\\right]\\]\nUsually the textbooks don’t make the dependence on $s$ explicit, and treat $\\lambda_{t+1}$ as though it’s a scalar, inserting the expectation operator at the end.\nWe’re left with a system of three equations: the Euler Equation, the\nLaw of Motion of Capital\n\\[K_{t+1} =A_tK_t^\\alpha - C_t + (1-\\delta)K_t\\]\nand the\nLaw of Motion of Technology\n\\[\\log A_{t+1} = \\rho \\log A_t + \\sigma_A \\varepsilon_t.\\]\nThe model described by these equations is called the Neoclassical Growth Model, or (a special case of) a Real Business Cycle Model.\nOur task is to solve the model, which means that we want to give a function which tells us how to calculate $C_{t+1}$ from the information available at time $t$. Such a function is called a\npolicy function\n. If $\\delta = 1$, the policy function can be found exactly. If $\\delta < 1$, we need to approximate it.\nPerturbation Solution\nJust like in the Samuelson model, let’s assume that the economy has a steady state and we’re interested in small deviations from the steady state. Instead of additive, we’ll assume that the deviations are logarithmic.\nThe steady state can be found by setting $C_t = \\overline{C}$, $K_t = \\overline{K}$, $A_t = \\overline{A}$, $\\varepsilon_t = 0$ for all $t$. From the Law of Motion of Technology, we have\n\\[\\log \\overline{A} = \\rho \\log \\overline{A} + 0\\]\nand so $\\overline{A} = 1$ (since we assume that $\\rho < 1$). From the Euler Equation, we get\n\\[\\frac{1}{\\beta} = \\alpha \\overline{K}^{\\alpha-1} + 1 - \\delta\\]\nand so\n\\[\\overline{K} = \\left(\\frac{1}{\\alpha}\\left(\\frac{1}{\\beta} - 1 + \\delta \\right)\\right)^{\\frac{1}{\\alpha-1}}\\]\nand from the Law of Motion of Capital, we get\n\\[\\overline{C} = \\overline{K}^\\alpha - \\delta \\overline{K}\\]\nNow we define $c_t = \\log(C_t/\\overline{C})$, $k_t = \\log(K_t/\\overline{K})$, $a_t = \\log(A_t/\\overline{A})$. Following a method due to Uhlig, we can replace $C_t$ by $\\overline{C} e^{c_t}$ and the same for the other variables, and make a first order approximation. To first order, the equation\n\\[E_t g(C_t, K_t, A_t, C_{t+1}, K_{t+1}, A_{t+1}) = 0\\]\nwill become\n\\[E_t \\nabla g (\\overline{X})\n\\cdot (\\overline{C}c_t, \\overline{K}k_t, \\overline{A}a_t, \\overline{C}c_{t+1}, \\overline{K}k_{t+1}, \\overline{A}a_{t+1}) = 0\\]\nwhere $\\overline{X} = (\\overline{C}, \\overline{K}, \\overline{A}, \\overline{C}, \\overline{K}, \\overline{A})$ and the dot denotes the dot product. This process is called log-linearisation.\nAs an example, let’s compute the log-linearisation for the Law of Motion of Capital. We have\n\\[g = -C_t + A_tK_t^\\alpha + (1-\\delta)K_t - K_{t+1}\\]\nand\n\\[\\nabla g = (-1, \\alpha K_t^{\\alpha-1} + 1 -\\delta, K_t^\\alpha, 0, -1, 0)\\]\nso\n\\[\\nabla g (\\overline{X}) \\cdot (\\overline{C}c_t, \\overline{K}k_t, \\overline{A}a_t, \\overline{C}c_{t+1}, \\overline{K}k_{t+1}, \\overline{A}a_{t+1}) = \\\\\n-\\overline{C}c_t + \\frac{\\overline{K}}{\\beta}k_t + \\overline{K}^\\alpha a_t - \\overline{K}k_{t+1} = 0\\]\nor equivalently\n\\[k_{t+1} = -\\frac{\\overline{C}}{\\overline{K}}c_t + \\frac{1}{\\beta}k_t + \\overline{K}^{\\alpha -1} a_t.\\]\nDoing the same for the other two equations yields the following result\n\\[E_t\\left[\\begin{matrix} {c}_{t+1}\\\\\n{k}_{t+1}\\\\\n{a}_{t+1}\\end{matrix}\\right] = \\left[\n\\begin{matrix}\na & b & c\\\\\n-\\overline{C}/\\overline{K} & 1/\\beta & \\overline{K}^{\\alpha-1}\\\\\n0 & 0 & \\rho\n\\end{matrix}\n\\right]\n\\left[\\begin{matrix}\n{c}_t \\\\\n{k}_t \\\\\n{a}_t\n\\end{matrix}\\right]\\]\nwhere $a, b$ and $c$ are given by the following gnarly expressions\n\\[\\begin{align*}\na &= 1 - \\beta\\alpha (\\alpha-1) \\overline{C} \\times \\overline{K}^{\\alpha-2}\\\\\nb &= \\alpha (\\alpha-1) \\overline{K}^{\\alpha-1}\\\\\nc &=  \\rho\\beta\\alpha\\overline{K}^{\\alpha-1} + \\beta\\alpha (\\alpha-1) (\\overline{K}^{\\alpha-1})^2\n\\end{align*}\\]\nThe next step is not hard to guess. If we could diagonalise the matrix which relates time $t+1$ to time $t$, then we’d have three single-variable equations of the form $E_t[w_{t+1}] = aw_t$ and we could solve them the way we did before.\nHand Calculations\n(Based on the\nnotes by Sims\n)\nLet’s consider a numerical example. Suppose\n2\n$\\alpha = 0.33$, $\\beta = 0.99$, $\\delta = 0.1$, $\\rho = 0.95$, $\\sigma_A = 0.01$. Then\n\\[\\left[\n\\begin{matrix}\na & b & c\\\\\n-\\overline{C}/\\overline{K} & 1/\\beta & \\overline{K}^{\\alpha-1}\\\\\n0 & 0 & \\rho\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n\\phantom{-}1.017 & -0.074 & 0.079\\\\\n-0.234 & \\phantom{-}1.010 & 0.334\\\\\n0 & 0 & 0.950\n\\end{matrix}\n\\right]\\]\nThis matrix can be decomposed as $Q^{-1}DQ$ where\n\\[D = \\left[\n\\begin{matrix}\n1.144 & 0 & 0\\\\\n0 & 0.95 & 0\\\\\n0 & 0 & 0.882\n\\end{matrix}\n\\right]\\]\nand\n\\[Q = \\left[\n\\begin{matrix}\n1.027 & -0.562 & -0.545\\\\\n0 & 0 & \\phantom{-}3.941\\\\\n1.014 & 0.585 & -4.067\n\\end{matrix}\n\\right]\\]\nwhich means that\n\\[w_t := 1.027c_t - 0.562k_t - 0.545 a_t\\]\nsatisfies\n\\[E_t[ w_t] = 1.144 w_t,\\]\nbut we already saw that, under some mild conditions stating that $E[w_t]$ can’t “go to the moon”, this means that $w_t = 0$ and so\n\\[c_t = 0.547k_t + 0.530a_t\\]\nfor all $t$. This is the first-order approximation to the\npolicy function\n. Furthermore,\n\\[\\begin{align*}\nk_{t+1} &= -0.234c_t +1.010k_t + 0.334a_t\\\\\n&= -0.234(0.547k_t + 0.530a_t) + 1.010k_t + 0.334a_t\\\\\n&= 0.882k_t + 0.210a_t\n\\end{align*}\\]\nThis is the first order approximation to the\ntransition function\n, which is the function which computes $K_{t+1}$ in terms of $K_t$ and $A_t$.\nShift indices to define $k_t’ := k_{t+1}$. Then since $a_t = 0.95a_{t-1} + 0.01\\varepsilon_t$, we have\n\\[\\left[\n\\begin{matrix}\nc_t\\\\\n{k}_t'\\\\\na_t\\\\\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n0.547 & 0.504\\\\\n0.882 & 0.199\\\\\n0 & 0.95\\\\\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nk_{t-1}'\\\\\na_{t-1}'\n\\end{matrix}\n\\right] +\n\\left[\n\\begin{matrix}\n0.005304\\\\\n0.002079\\\\\n0.01\\\\\n\\end{matrix}\n\\right] \\varepsilon_{t}\\]\nand the expectation operator has magically vanished and we are left with a linear difference equation.\nDynare\nDynare\n(I think it’s pronounced dee-\nnaire\n; it’s French) is a software package for Matlab and Octave which, among other things, fits DSGE models. First, save the following code in a file\nhello_dynare.mod\n.\nvar k a c;\n\nvarexo e; // shocks\n\nparameters alpha beta delta rho sA;\nalpha = 0.33;\nbeta = 0.99;\ndelta = 0.1;\nrho = 0.95;\nsA = 0.01;\n\nmodel;\n\n/* Euler equation */\nexp(c)^(-1) = beta*exp(c(+1))^(-1)*(alpha*exp(a(+1))*exp(k)^(alpha-1) + (1-delta));\n\n/* law of motion of capital */\nexp(k) = exp(a)*exp(k(-1))^(alpha) - exp(c) + (1-delta)*exp(k(-1));\n\n/* law of motion of technology */\na = rho*a(-1) + sA*e;\n\nend;\n\nshocks;\nvar e = 1;\nend;\n\nsteady;\ncheck;\n\nstoch_simul(order=1, periods=200, nograph, nodisplay);\nThen run it in the Octave GUI with the command\ndynare hello_dynare.mod\nAmong the output, you’ll see\nPOLICY AND TRANSITION FUNCTIONS\n                                   k               a               c\nConstant                    1.638350               0        0.184374\nk(-1)                       0.882253               0        0.547200\na(-1)                       0.199228        0.950000        0.503894\ne                           0.002097        0.010000        0.005304\nwhich is exactly what we just calculated! (Somewhat confusingly, the first row gives the steady state values. But no doubt there is a good reason for this.)\ngEcon\nThe R package\ngEcon\nis similar to Dynare. I’m not sure whether it is quite as powerful, but it has the advantage that you can type in the model without having to derive the Euler Equation yourself (although don’t make a mistake or you will get a segmentation fault and R will crash!) In the code,\nU[]\nrepresents the value function and the\nobjective\nblock is the Bellman Equation.\nTo run it, save the following file as\nhello_gecon.gcn\n##################################################\n# Neoclassical growth model in gEcon\n##################################################\n\nblock CONSUMER\n{\n    definitions\n    {\n        u[] = log(C[]);\n    };\n    controls\n    {\n        K_s[], C[];\n    };\n    objective\n    {\n        U[] = u[] + beta * E[][U[1]];\n    };\n    constraints\n    {\n        K_s[] = A[] * K_s[-1]^alpha + (1 - delta) * K_s[-1] - C[] ;\n    };\n    calibration\n    {\n        delta = 0.1;\n        beta = 0.99;\n        alpha = 0.33;\n    };\n};\n\nblock EXOG \n{\n    identities\n    {\n        A[] = exp(rho * log(A[-1]) + 0.01 * epsilon_A[]);\n    };\n    shocks\n    {\n        epsilon_A[];\n    };\n    calibration\n    {\n        rho = 0.95;\n    };\n};\nand run\nlibrary(gEcon)\n\nrbc <- make_model(\"hello_gecon.gcn\")\nrbc <- steady_state(rbc)\nrbc <- solve_pert(model = rbc, loglin = TRUE)\nget_pert_solution(rbc)\nOnce again, you can identify the matrix we derived in the following output:\nMatrix P:\n\n       A[-1] K_s[-1]\nA[]   0.9500  0.0000\nK_s[] 0.1992  0.8823\n\nMatrix Q:\n\n    epsilon_A\nA      0.0100\nK_s    0.0021\n\nMatrix R:\n\n     A[-1] K_s[-1]\nC[] 0.5039  0.5472\nU[] 1.2366  0.2345\n\nMatrix S:\n\n  epsilon_A\nC    0.0053\nU    0.0130\nSimulation\nLet’s write R functions to find the first-order solution for a given choice of parameters and to simulate from the model.\nmodel_matrices <- function(alpha, beta, rho, delta, sigma_A){\n  \n  # steady state\n  Abar <- 1\n  Kbar <- (1/alpha * (1/beta -1 +delta))^(1/(alpha-1))\n  Cbar <- Kbar^alpha + (-delta) * Kbar\n  \n  a <- 1 - beta * alpha * (alpha -1) * Cbar * Kbar^(alpha-2)\n  b <- alpha * (alpha-1) * Kbar^(alpha-1)\n  c <- rho * beta * alpha * Kbar^(alpha-1)\n  c <- c + beta*alpha*(alpha-1)*(Kbar^(alpha-1))^2\n  \n  A <- matrix(c(a, b, c, -Cbar/Kbar, 1/beta, Kbar^(alpha-1), 0, 0, rho), byrow=2, ncol=3)\n  \n  eigs <- eigen(A)\n  \n  # check Blanchard-Kahn conditions\n  # eigs$values are always ordered from largest to smallest in R\n  BK_violated <- F\n  if ((abs(eigs$values[1]) < 1) | (abs(eigs$values[2]) > 1)){\n    BK_violated <- T\n  }\n  \n  P <- eigs$vectors\n  \n  fr <- solve(P)[1, ] # corresponds to largest eigenvalue\n  u <- A[2,1]*(-fr[2:3]/fr[1]) + A[2,2:3]\n  Tt <- matrix(c(u*c(1, rho), 0, rho), ncol=2, byrow=T)\n  Ht <- sigma_A * matrix(c(0, 0, Tt[,2]/rho), nc=2, byrow=F)\n  \n  list(Tt = Tt, Ht = Ht, BK_violated = BK_violated)\n}\nThis function is pretty straightforward. It just does the computation which we already went through in a special case. It makes an additional check that the largest eigenvalue is really $>1$ and the others are $<1$. These are the famous\nBlanchard-Kahn conditions\nwhich guarantee that there is exactly one steady state.\nThe function outputs\nTt\nwhich consists of the bottom two rows of the policy/transition function matrix, and\nHt\nwhich is the vector of coefficients of $\\varepsilon_t$ with an additional column of zeroes on the left. This format is chosen to match the conventions of the\ndlm\npackage which will be used\nbelow\nfor fitting the model.\nChecking in our example:\nmodel_matrices(0.33, 0.99, 0.95, 0.1, 0.01)\n$Tt\n          [,1]      [,2]\n[1,] 0.8822534 0.1992279\n[2,] 0.0000000 0.9500000\n\n$Ht\n     [,1]        [,2]\n[1,]    0 0.002097136\n[2,]    0 0.010000000\n\n$BK_violated\n[1] FALSE\nTo simulate from the model, what we really want to output is $y_t := \\log(Y_t/\\overline{Y})$ where $\\overline{Y}$ is the steady-state value of output. Since $Y_t = A_t K_t^\\alpha$, we have $y_t = a_t + \\alpha k_t$. The following function outputs simulated values of $y_t, k_t$ and $a_t$.\nsim <- function(n, alpha, beta, rho, sigma, delta, sigma_A, start=c(0,0)){\n  \n  # n : number of simulations\n  # start : starting values of (k, a)\n  \n  M <- model_matrices(alpha, beta, rho, sigma, delta, sigma_A)\n  Tt <- M$Tt\n  Ht <- M$Ht\n  \n  y_sim <- rep(0, n)\n  ka_sim <- matrix(0, nr=2, nc=n)\n  ka_sim[,1] <- start\n  \n  for (i in 2:n){\n    ka_sim[,i] <- Tt %*% ka_sim[,i-1] + Ht %*% rnorm(2)\n    y_sim[i] <- ka_sim[,i] %*% c(alpha, 1) \n  }\n  list(y = y_sim, ka = ka_sim)\n}\n(Note that we do\nHt %*% rnorm(2)\n; the first component of\nrnorm(2)\ngets multiplied by zero and does nothing. Again, this is to match the\ndlm\npackage which I’ll be using\nbelow\n.)\nNow we can look at the output of the model. Note that here we’re using the first-order approximation to simulate. I think this is a bit like using Euler’s method to plot the solution of an ODE. It usually works pretty well, but it might go off track eventually.\nalpha <- 0.33\nbeta <- 0.99\nrho <- 0.95\ndelta <- 0.1\nsigma_A <- 0.01\n\nset.seed(101)\ns <- sim(200, alpha, beta, rho, delta, sigma_A)\nplot.ts(s$y, las=1, ylab=\"\")\nEven though the people (or rather person; representative agent) in the model knows everything about the model and behaves optimally, we still get business cycles! That’s realistic, but it’s not the reason why the model is called a real business cycle model. The name comes from the fact that it’s a model of the\nreal economy\n; it ignores prices, wages, and monetary frictions.\nAnyway, despite the fact that it is realistic in some ways, it turns out that such models are very unrealistic in other ways, and the problems with them can’t be fixed, so they have fallen out of fashion since the 1980s.\nFitting the model to data\nFitting the model to real-life data is important because the ultimate goal of DSGE modelling is to answer questions like\n“how much will output rise if the government increases spending by x?”\n.\nThere seem to be two approaches to model fitting. The first, called\ncalibration\n, is just to guess what the parameters might be and try to make the model output match some salient features of the observed data. If this was formalised, I suppose it would be something like\nApproximate Bayesian Computation\n(called\nindirect inference\nin economics). The second way of fitting the model, which makes more sense from the point of view of a statistician, is to write down a likelihood function and then do likelihood-based inference.\nHowever, there is a complication in writing down the likelihood function. Recall that our example model (leaving out $c_t$) looks like\n\\[\\left[\n\\begin{matrix}\n{k}_t'\\\\\na_t\\\\\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n0.882 & 0.199\\\\\n0 & 0.95\\\\\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nk_{t-1}'\\\\\na_{t-1}'\n\\end{matrix}\n\\right] +\n\\left[\n\\begin{matrix}\n0.002079\\\\\n0.01\\\\\n\\end{matrix}\n\\right] \\varepsilon_{t}\\]\nbut we don’t actually observe $k_t$ (the log deviation of the capital stock from its steady state) or $a_t$ (the log deviation of total factor productivity from its steady state). What we maybe do have is an estimate of output $Y_t$ since we can use the GDP, and we can estimate its log deviation $y_t$ from its steady state. However, we can’t make further progress without positing some sort of statistical relationship between $y_t = a_t + \\alpha k_t$, $k_t$ and $a_t$.\nFortunately, there’s a standard approach to this problem called the Kalman Filter. I experimented with a few different Kalman Filter R packages, but I got confused because they all call things different names and I couldn’t get two of them to give the same output. Finally I found the\ndlm\npackage by Giovanni Petris\nwhich does exactly what I need (generally I like these kind of self-contained R packages which do one thing well rather than attempting to do everything).\nlibrary(dlm)\n\nbuild_dlm <- function(alpha, beta, rho, delta, sigma_A){\n  \n  # obtain a DLM model object from the parameters\n  \n  M <- model_matrices(alpha, beta, rho, delta, sigma_A)\n  Tt <- M$Tt\n  Ht <- M$Ht\n  HHt <- M$Ht %*% t(M$Ht)\n  \n  # starting point for Kalman Filter variance\n  P0 <- matrix(solve(diag(4) - kronecker(Tt, Tt)) %*% c(HHt), nc=2, byrow=F)\n  \n  dlm(m0 = matrix(c(0,0), nc=1),\n      C0 = P0,\n      FF = matrix(c(alpha, 1), nc=2),\n      V = matrix(0),\n      GG = Tt,\n      W = sigma_A^2 * diag(2))\n}\nTo construct the\ndlm\nobject,\nm0\nis the starting point for the hidden (state) variables (in our case $(k_0, a_0)$),\nP0\nis the starting point for the variance of the hidden variables,\nFF\nis the matrix which defines\ny\nin terms of\nk\nand\na\n,\nV\nis a constant which can be added to\ny\n,\nGG\nis the transition matrix for the state variables, and\nW\nis the variance-covariance matrix for the shocks. The formula for\nP0\nis the solution of the so-called\nLyapunov Equation\n3\nwhich calculates the long-run limit of the variance-covariance matrix of the state variables.\nOnce you have the\ndlm\nobject, there are functions\ndlmLL\nfor calculating the negative log-likelihood and\ndlmMLE\nfor the maximum likelihood estimator.\nYou can check that\ndlmMLE\nreally recovers the model parameters by writing a piece of code like this (I’m going to do a little bit of calibration of my own by fixing $\\alpha=0.33$ and taking $\\beta = 0.99$ in this example.)\nalpha <- 0.33\nbeta <- 0.99\ndelta <- 0.1\nrho <- 0.95\nsigma_A <- 0.01\n\nN_rep <- 100\nN_sim <- 1000\nbuild <- function(par) build_dlm(alpha, \n                                beta, \n                                exp(par[1])/(1+exp(par[1])),\n                                exp(par[2])/(1+exp(par[2])), \n                                exp(par[3]))\n\nout <- matrix(0, nc=N_rep, nr=3)\n\nset.seed(102)\n\nfor (i in 1:N_rep){\n  # simulate from model\n  s <- sim(N_sim, alpha, beta, rho, delta, sigma_A)\n  \n  # calculate MLE\n  opt <- dlmMLE(y=s$y, parm=c(0,0,0), build)\n\n  # save MLE in output matrix\n  out[,i] <- c((1/(1+exp(-opt$par[1:2]))), exp(opt$par[3]))\n}\n\nrowMeans(out)\n# [1] 0.94887794 0.10367580 0.01011981\nNote that it’s much better to transform the variables and use unconstrained optimisation rather than trying to use\nnlminb\nand the simplex method.\nReal-life data\nIn order to see whether the model breaks down with real-life data, I downloaded the New Zealand GDP series from the\nRBNZ website\n. I’m not quite sure which series is most appropriate, but I decided to take seasonally-adjusted real production-based GDP as my $Y_t$. Because we are looking at long-term fluctuations of $Y_t$ about its steady state, the long-term growth trend in the series has to be removed (hopefully this also takes care of population growth).\n# raw data\nY <- ts(c(27979, 28130, 28258, 28148, 28016, 28080, 27835, \n28189, 28353, 28034, 27980, 27933, 27929, 28183, 28512, 27821, \n27632, 27721, 27900, 27983, 27986, 27771, 28120, 28581, 29213, \n29806, 30085, 30544, 30746, 31271, 31686, 32068, 32478, 32779, \n33000, 33466, 33761, 33981, 34424, 34296, 34967, 34887, 34827, \n34643, 34827, 34859, 35205, 35616, 35899, 36900, 37350, 37893, \n37904, 38053, 38122, 38252, 38790, 39079, 39657, 39998, 40527, \n41009, 41583, 41840, 42041, 42946, 43509, 44254, 44644, 44779, \n44947, 45469, 46202, 46439, 46250, 46990, 47185, 47471, 47864, \n48501, 48922, 49311, 49409, 49200, 49098, 48936, 48622, 48197, \n48167, 48397, 49014, 49170, 49509, 49387, 49105, 49618, 49802, \n50337, 50632, 50943, 51233, 51314, 52043, 51887, 52445, 52738, \n52931, 53746, 53982, 54695, 55540, 55725, 56152, 56692, 57269, \n57979, 58496, 59089, 59243, 59921, 60407, 60869, 61437, 62046, \n62639, 62716, 63665, 64243, 64263, 64756, 65403, 64687, 57967, \n66117, 66375, 67757, 68349, 65452, 68107, 68104, 68848, 70203, \n70208, 70156, 70688, 70717, 70857, 71111, 70346, 69604, 70056\n), start=c(1987, 1), freq=4)\n\n# preprocessing\ny <- log(Y)\nzz <- lm(y ~ x, data=data.frame(x=1:151))$resid\n\n# fit model\nopt <- dlmMLE(y=zz, parm=c(0,0,0), build)\n\n# results\nc((1/(1+exp(-opt$par[1:2]))), exp(opt$par[3]))\n# [1] 0.83546378 0.07661051 0.01548509\nIt’s hard to know whether the parameters for the $AR(1)$ process which drive the shocks in total factor productivity are sensible, but a capital depreciation rate of $7.7\\%$ seems reasonable.\nMaximum Likelihood Estimation with gEcon\nIn gEcon, we need to add an\nEQUILIBRIUM\nblock and change the\nEXOG\nblock as follows:\nblock EQUILIBRIUM \n{\n    identities {\n        log_Y[] = log(A[]) + alpha * log(K_s[]);\n    };\n};\n\nblock EXOG \n{\n    identities\n    {\n        A[] = exp(rho * log(A[-1]) + epsilon_A[]);\n    };\n    shocks\n    {\n        epsilon_A[];\n    };\n    calibration\n    {\n        rho = 0.95;\n    };\n};\nNow run the code to solve the model again and add commands to perform inference.\nrbc <- make_model(\"hello_gecon.gcn\")\nrbc <- steady_state(rbc)\nrbc <- solve_pert(model = rbc, loglin = TRUE)\n\n# zz as above\nzz <- lm(y ~ x, data=data.frame(x=1:151))$resid\ndf = data.frame(log_Y = zz)\n\nml_estimation_result <- ml_estimation(data_set = ts(df),\nobservables = c(\"log_Y\"),\nmodel = rbc,\nest_par = c(\"rho\", \"delta\", \"sd(epsilon_A)\"),\ninitial_vals = c(\"sd(epsilon_A)\" = 0.01,\n\"rho\" = 0.9, \"delta\" = 0.1))\n\nsummary(ml_estimation_result)\n# The estimation results:\n# The likelihood optimisation statistics:\n#              ML estimate:   Std. err:\n# rho             0.83796827 0.055224398\n# delta           0.07310659 0.059381472\n# sd(epsilon_A)   0.01039685 0.005797989\nThe results are pretty close to our hand calculation using\ndlm\n, although not quite the same.\nMaximum Likelihood Estimation with Dynare\nTo fit the model in Dynare, we need to create a csv file with a single column containing the values in\nzz\nwith a named header which I chose to call\ny3\n. Then you need to modify the\n.mod\nfile and save it as\nestimate_rbc_model.mod\n. Then you can run it in Octave as before.\nvar k a c y;\n\nvarexo e; // shocks\n\nparameters alpha beta delta rho sA;\nalpha = 0.33;\nbeta = 0.95;\ndelta = 0.1;\nrho = 0.95;\nsA = 0.01;\n\nmodel;\n\n/* Euler equation */\nexp(c)^(-1) = beta*exp(c(+1))^(-1)*(alpha*exp(a(+1))*exp(k)^(alpha-1) + (1-delta));\n\n/* law of motion of capital */\nexp(k) = exp(a)*exp(k(-1))^(alpha) - exp(c) + (1-delta)*exp(k(-1));\n\n/* law of motion of technology */\na = rho*a(-1) + e;\n\n/* output */\ny = a + alpha*k;\n\nend;\n\nsteady_state_model;\na = 0;\nk = log(((1/beta - 1 + delta)/alpha)^(1/(alpha - 1)));\nc = log(exp(k)^alpha - delta * exp(k));\ny = a + alpha*k;\nend;\ncheck;\n\nvarobs y;\n\nestimated_params;\n  rho, 0.5, 0, 1;\n  delta, 0.5, 0, 1;\n  stderr e, 0.5, 0, 1;\nend;\n\nestimation(datafile=y3, mode_compute = 4, nograph, nodisplay) y;\nThe output from Dynare is quite different to the other two packages. It gives a value of $27.7\\%$ for depreciation.\nRESULTS FROM MAXIMUM LIKELIHOOD ESTIMATION\nparameters\n        Estimate    s.d. t-stat\n\nrho       0.5935  0.0633  9.3710\ndelta     0.2770  0.0032 85.4451\n\nstandard deviation of shocks\n        Estimate    s.d. t-stat\n\ne         0.0149  0.0010 15.2407\nLooking at the code, it’s clear that no two of these three methods are using the same likelihood function. This could be because they start the Kalman Filter in a different place, because they handle violation of the Blanchard-Kahn conditions differently, because of an error in my code, or for some other reason.\nThis does provide a nice illustration of why economists prefer to use Bayesian methods to estimate these models. Even in this very simple example, the likelihood surface is very flat. In general, there will be lots of local maxima and it doesn’t make much sense to use maximum likelihood. Bayesian methods make more sense. It seems that economists tend to use rather strong priors, since there tend to be good reasons why parameters are likely to lie within fairly narrow boundaries. Bayesian methods are available in both Dynare and gEcon, or you can write your own. Once you have the likelihood function, you can do whatever you want!\nCritiques of the DSGE methodology\nWe’ve finally answered the questions from the introduction.\nWhat is a DSGE model?\nIt’s a model of how the macroeconomy evolves in time in which all markets clear, all agents behave optimally, and there are some unpredictable random shocks involved.\nWhat is the motivation for developing DSGE models? Why are they better than other models?\nEconomic agents in DSGE models take the future into account when making decisions. This may make DSGE models more realistic than other models, in theory.\nHow do you formulate and solve a DSGE model?\nYou write down a stochastic optimal control problem, find some necessary first-order conditions, make a log-linear (or sometimes higher order) approximation under the assumption that there’s a unqiue steady state, derive a VAR model for the perturbations about the steady state, and solve it.\nHow do you fit a DSGE model to data?\nYou fit the first order approximation rather than the original model. You need the Kalman Filter to relate your observations to the state variables. And, if using a likelihood-based method, it had better be Bayesian.\nThis leaves the question of why\nsome people don’t seem to like or trust DSGE models\n. One reason is that many DSGE models, just like our example model, assume that the economy contains\nrepresentative agents\n: exactly one consumer, one firm, etc. One problem with earlier models like the good old IS-LM model is that people are not assumed to behave optimally. The DSGE approach tackles this problem by assuming that people behave optimally, but that there is only one of them.\nIt’s not clear that this is necessarily better. You might say that the DSGE approach is looking at the average behaviour of a large number of people and assuming that they collectively behave optimally, but is it true that a large number of people with different utility functions each making decisions for themselves would act to maximise their collective utility?\nPerhaps it is\n. That’s supposed to be one of the\nlessons of economics\n, right? But is it true that individual people really behave optimally? Behavioural economists would say no. And also, if you treat the collective utility of everyone as the only number of interest, then you can’t calculate any statistics about inequality, which is something which I happen to be interested in.\n4\nConcerns of this sort led economists to develop extensions of DSGE models which do allow different types of agents. The most famous are the Heterogeneous Agent New Keynesian\n5\nor HANK models. HANK models can’t be fitted using the methods we used in our example, but there are other approaches, which I’m hoping to find out about.\nFootnotes\n1\nIn the original model, consumption at time $t$ depends on output at time $t-1$. I want consumption at time $t$ to be proportional to output at time $t$ in order to match the RBC model introduced\nin the following section\n. The choice doesn’t make a difference to the qualitative properties of the model.\n2\nThe value $\\alpha = 0.33$ is\na standard choice of parameter in the production function\nand $\\beta = 0.99$ is taken to match the NZSim paper mentioned in the introduction.\n3\nYou can check this using the following R code\ns <- sim(10000, alpha, beta, rho, delta, sigma_A)\nM <- model_matrices(alpha, beta, rho, delta, sigma_A)\n\n# Lyapunov Equation solution\nmatrix(solve(diag(4) - kronecker(M$Tt, M$Tt)) %*% c(M$Ht %*% t(M$Ht)), nc=2, byrow=F)\n\n# long-run variance\nvar(t(s$ka))\n4\nCould this even be a feature rather than a bug? In\nHow to Speak Money\n, John Lanchester states that increasing inequality is the driving force that makes neoliberal policies work. So perhaps it’s not surprising that in the last forty years, as neoliberal economic policies were sweeping the developed world, economists were coming up with a modelling methodology which made it impossible even to\ntalk\nabout inequality?\n5\nThe basic New Keynesian model is another possible “hello world” DSGE model. It’s used, for example, as an introductory model in\ntraining materials from the Bank of England\n. But, although it consists of just three equations, deriving them takes about forty pages of mathematics.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\ndatascienceconfidential - r\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Introduction Dynamic Stochastic General Equilibrium (DSGE) models are a class of models which attempt to model the entire economy of a nation. The purpose of this post is to hand-code a DSGE model in R, fit it to data, and check that the results agree...",
      "meta_keywords": null,
      "og_description": "Introduction Dynamic Stochastic General Equilibrium (DSGE) models are a class of models which attempt to model the entire economy of a nation. The purpose of this post is to hand-code a DSGE model in R, fit it to data, and check that the results agree...",
      "og_image": "https://datascienceconfidential.github.io/blog/images/2025/samuelson_example.png",
      "og_title": "How the DSGE sausage is made | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 34.7,
      "sitemap_lastmod": null,
      "twitter_description": "Introduction Dynamic Stochastic General Equilibrium (DSGE) models are a class of models which attempt to model the entire economy of a nation. The purpose of this post is to hand-code a DSGE model in R, fit it to data, and check that the results agree...",
      "twitter_title": "How the DSGE sausage is made | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/04/how-the-dsge-sausage-is-made/",
      "word_count": 6943
    }
  }
}