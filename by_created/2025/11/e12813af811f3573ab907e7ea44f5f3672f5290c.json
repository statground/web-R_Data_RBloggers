{
  "id": "e12813af811f3573ab907e7ea44f5f3672f5290c",
  "url": "https://www.r-bloggers.com/2024/01/overview-of-clustering-methods-in-r/",
  "created_at_utc": "2025-11-17T20:38:37Z",
  "data": null,
  "raw_original": {
    "uuid": "f5c739fb-fdb0-40f9-9c31-3c64579a4120",
    "created_at": "2025-11-17 20:38:37",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2024/01/overview-of-clustering-methods-in-r/",
      "crawled_at": "2025-11-17T09:18:57.930566",
      "external_links": [
        {
          "href": "https://petolau.github.io/Overview-clustering-methods-in-R/",
          "text": "Peter Laurinec"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://en.wikipedia.org/wiki/DBSCAN",
          "text": "DBSCAN"
        },
        {
          "href": "https://en.wikipedia.org/wiki/OPTICS_algorithm",
          "text": "OPTICS"
        },
        {
          "href": "https://en.wikipedia.org/wiki/DBSCAN",
          "text": "DBSCAN"
        },
        {
          "href": "https://petolau.github.io/Overview-clustering-methods-in-R/",
          "text": "Peter Laurinec"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Overview of clustering methods in R | R-bloggers",
      "images": [
        {
          "alt": "plot of chunk unnamed-chunk-1",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-1",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-1-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-2",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-2",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-2-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-3",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-3",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-3-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-4",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-4",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-4-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-5",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-5",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-5-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-6",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-6",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-6-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-7",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-7",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-7-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-8",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-8",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-8-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-9",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-9",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-9-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-10",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-10",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-10-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-11",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-11",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-11-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-12",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-12",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-12-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-13",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-13",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-13-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-14",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-14",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-14-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-15",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-15",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-15-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-16",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-16",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-16-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-17",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-17",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-17-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-18",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-18",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-18-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-18",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-18",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-18-2.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-19",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-19",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-19-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-20",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-20",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-20-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-21",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-21",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-21-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-22",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-22",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-22-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-23",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-23",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-23-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-25",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-25",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-25-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-27",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-27",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-27-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-28",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-28",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-28-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-29",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-29",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-29-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-30",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-30",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-30-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-31",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-31",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-31-1.png?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/peter-laurinec/",
          "text": "Peter Laurinec"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-381440 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Overview of clustering methods in R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 10, 2024</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/peter-laurinec/\">Peter Laurinec</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://petolau.github.io/Overview-clustering-methods-in-R/\"> Peter Laurinec</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9--><p><strong>Clustering</strong> is a very popular technique in data science because of its unsupervised characteristic – we don’t need true labels of groups in data.\nIn this blog post, I will give you a “quick” survey of various clustering methods applied to synthetic but also real datasets.</p>\n<h2 id=\"what-is-clustering\">What is clustering?</h2>\n<p><strong>Cluster analysis</strong> or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).</p>\n<p>It is a technique of <strong>unsupervised learning</strong>, so clustering is used when no a priori information about data is available.\nThis makes clustering a very strong technique for gaining insights into data and making more accurate decisions.</p>\n<h3 id=\"what-is-it-good-for\">What is it good for?</h3>\n<p>Clustering is used for:</p>\n<ul>\n<li>To gain insight into data, generate hypotheses, detect anomalies, and identify salient features,</li>\n<li>To identify the degree of similarity among objects (i.e. organisms),</li>\n<li>As a method for organizing the data and summarising it through cluster prototypes (compression).</li>\n</ul>\n<h4 id=\"classification-to-groups\">Classification to groups</h4>\n<p>The first use case is to group data, e.g. classify them into groups.\nFor explanation purposes, I will generate synthetic data from three normal distributions plus three outliers (anomalies).\nLet’s load needed packages, generate randomly some data, and show the first use case in the visualization:</p>\n<figure class=\"highlight\"><pre>library(data.table) # data handling\nlibrary(ggplot2) # visualisations\nlibrary(gridExtra) # visualisations\nlibrary(grid) # visualisations\nlibrary(cluster) # PAM - K-medoids\n \nset.seed(54321)\n \ndata_example &lt;- data.table(x = c(rnorm(10, 3.5, 0.1), rnorm(10, 2, 0.1),\n                                 rnorm(10, 4.5, 0.1), c(5, 1.9, 3.95)),\n                           y = c(rnorm(10, 3.5, 0.1), rnorm(10, 2, 0.1),\n                                 rnorm(10, 4.5, 0.1), c(1.65, 2.9, 4.2)))\n \ngg1 &lt;- ggplot(data_example, aes(x, y)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\n \nkmed_res &lt;- pam(data_example, 3)$clustering\n \ndata_example[, class := as.factor(kmed_res)]\n \ngg2 &lt;- ggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\n \ndefine_region &lt;- function(row, col){\n  viewport(layout.pos.row = row, layout.pos.col = col)\n}\n \ngrid.newpage()\n# Create layout : nrow = 2, ncol = 2\npushViewport(viewport(layout = grid.layout(1, 2)))\n# Arrange the plots\nprint(gg1, vp = define_region(1, 1))\nprint(gg2, vp = define_region(1, 2))</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-1\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-1-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-1\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-1-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see three nicely divided groups of data.</p>\n<h4 id=\"anomaly-detection\">Anomaly detection</h4>\n<p>Clustering can be also used as an anomaly detection technique, some methods of clustering can detect automatically outliers (anomalies). Let’s show visually what it looks like.</p>\n<figure class=\"highlight\"><pre>anom &lt;- c(rep(1, 30), rep(0, 3))\ndata_example[, class := as.factor(anom)]\nlevels(data_example$class) &lt;- c(\"Anomaly\", \"Normal\")\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-2\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-2-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-2\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-2-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h4 id=\"data-compression\">Data compression</h4>\n<p>In an era of a large amount of data (also many times used buzzword - big data), we have problems processing them in real time.\nHere clustering can help to reduce dimensionality by its compression feature.\nCreated clusters, that incorporate multiple points (data), can be replaced by their representatives (prototypes) - so one point. In this way, points were replaced by its cluster representative (“+”):</p>\n<figure class=\"highlight\"><pre>data_example[, class := as.factor(kmed_res)]\n \ncentroids &lt;- data_example[, .(x = mean(x), y = mean(y)), by = class]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  geom_point(data = centroids, aes(x, y), color = \"black\", shape = \"+\", size = 18) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-3\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-3-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-3\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-3-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h2 id=\"types-of-clustering-methods\">Types of clustering methods</h2>\n<p>Since cluster analysis has been here for more than 50 years, there are a large amount of available methods.\nThe basic classification of clustering methods is based on the objective to which they aim: <strong>hierarchical</strong>, <strong>non-hierarchical</strong>.</p>\n<p>The <strong>hierarchical clustering</strong> is a multi-level partition of a dataset that is a branch of classification (clustering). Hierarchical clustering has two types of access to data. The first one, divisive clustering, starts with one big cluster that is then divided into smaller clusters. The second one, agglomerative clustering, starts with individual objects that are single-element clusters, and then they are gradually merged. The whole process of hierarchical clustering can be expressed (visualized) as a dendrogram.</p>\n<p>The <strong>non-hierarchical clustering</strong> is dividing a dataset into a system of disjunctive subsets (clusters) so that an intersection of clusters would be an empty set.</p>\n<p>Clustering methods can be also divided in more detail based on the processes in the method (algorithm) itself:</p>\n<p><strong>Non-hierarchical</strong>:</p>\n<ul>\n<li>Centroid-based</li>\n<li>Model-based</li>\n<li>Density-based</li>\n<li>Grid-based</li>\n</ul>\n<p><strong>Hierarchical</strong>:</p>\n<ul>\n<li>Agglomerative</li>\n<li>Divisive</li>\n</ul>\n<p>But which to choose in your use case? Let’s dive deeper into the most known methods and discuss their advantages and disadvantages.</p>\n<h3 id=\"centroid-based\">Centroid-based</h3>\n<p>The most basic (maybe just for me) type of clustering method is centroid-based.\nThis type of clustering creates prototypes of clusters - centroids or medoids.</p>\n<p>The best well-known methods are:</p>\n<ul>\n<li>K-means</li>\n<li>K-medians</li>\n<li>K-medoids</li>\n<li>K-modes</li>\n</ul>\n<h4 id=\"k-means\">K-means</h4>\n<p>Steps:</p>\n<ul>\n<li>Create random K clusters (and compute centroids).</li>\n<li>Assign points to the nearest centroids.</li>\n<li>Update centroids.</li>\n<li>Go to step 2 while the centroids are changing.</li>\n</ul>\n<p>Pros and cons:</p>\n<ul>\n<li>[+] Fast to compute. Easy to understand.</li>\n<li>[-] Various initial clusters can lead to different final clustering.</li>\n<li>[-] Scale-dependent.</li>\n<li>[-] Creates only convex (spherical) shapes of clusters.</li>\n<li>[-] Sensitive to outliers.</li>\n</ul>\n<p><strong>K-means - example</strong></p>\n<p>It is very easy to try K-means in R (by the <code>kmeans</code> function), only needed parameter is a number of clusters.</p>\n<figure class=\"highlight\"><pre>km_res &lt;- kmeans(data_example, 3)$cluster\n \ndata_example[, class := as.factor(km_res)]\n \ncentroids &lt;- data_example[, .(x = mean(x), y = mean(y)), by = class]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  geom_point(data = centroids, aes(x, y), color = \"black\", shape = \"+\", size = 18) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-4\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-4-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-4\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-4-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see example, when K-means fails most often, so when there are outliers in the dataset.</p>\n<h4 id=\"k-medoids\">K-medoids</h4>\n<p>The problem with outliers solves K-medoids because prototypes are medoids - members of the dataset. So, not artificially created centroids, which helps to tackle outliers.</p>\n<p>Pros and cons:</p>\n<ul>\n<li>[+] Easy to understand.</li>\n<li>[+] Less sensitive to outliers.</li>\n<li>[+] Possibility to use any distance measure.</li>\n<li>[-] Various initial clusters can lead to different final clustering.</li>\n<li>[-] Scale-dependent.</li>\n<li>[-] Slower than K-means.</li>\n</ul>\n<p><strong>K-medoids - example</strong></p>\n<p>K-medoids problem can be solved by the Partition Around Medoids (PAM) algorithm (function <code>pam</code> in <code>cluster</code> package).</p>\n<figure class=\"highlight\"><pre>kmed_res &lt;- pam(data_example[, .(x, y)], 3)\ndata_example[, class := as.factor(kmed_res$clustering)]\n \nmedoids &lt;- data.table(kmed_res$medoids, class = as.factor(1:3))\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  geom_point(data = medoids, aes(x, y, shape = class),\n             color = \"black\", size = 11, alpha = 0.7) +\n  theme_bw() +\n  guides(shape = \"none\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-5\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-5-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-5\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-5-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see that medoids stayed nicely in the three main groups of data.</p>\n<h4 id=\"the-determination-of-the-number-of-clusters\">The determination of the number of clusters</h4>\n<p>The disadvantage of centroid-based methods is that the number of clusters needs to be known in advance (it is a parameter of the methods). However, we can determine the number of clusters by its Internal validation (index). Basic steps are based on that we compute some internal validation index with many ( K ) and we choose ( K ) with the best index value.</p>\n<p>Many indexes are there…</p>\n<ul>\n<li>Silhouette</li>\n<li>Davies-Bouldin index</li>\n<li>Dunn index</li>\n<li>etc.</li>\n</ul>\n<p>However, every index has similar characteristics:</p>\n\n\\[\\frac{within-cluster-similarity}{between-clusters-similarity} .\\]\n\n<p>so, it is the ratio of the average distances in clusters and between clusters.</p>\n<h4 id=\"elbow-diagram\">Elbow diagram</h4>\n<p>The Elbow diagram is a simple method (rule) how to determine the number of clusters - we compute the internal index with a set of K and choose K where positive change is largest.</p>\n<p>So for example, I chose the Davies-Bouldin index implemented in the <code>clusterCrit</code> package. For our simple dataset, I will generate clusterings with 2-6 number of clusters and compute the index.</p>\n<figure class=\"highlight\"><pre>library(clusterCrit)\n \nkm_res_k &lt;- lapply(2:6, function(i) kmeans(data_example[, .(x, y)], i)$cluster)\nkm_res_k</pre></figure>\n<figure class=\"highlight\"><pre>## [[1]]\n##  [1] 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2\n## \n## [[2]]\n##  [1] 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 3 2 1\n## \n## [[3]]\n##  [1] 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 2 4 3\n## \n## [[4]]\n##  [1] 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 1 3\n## \n## [[5]]\n##  [1] 1 1 1 1 1 1 1 5 5 1 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 2 6 4</pre></figure>\n<figure class=\"highlight\"><pre>db_km &lt;- lapply(km_res_k, function(j) intCriteria(data.matrix(data_example[, .(x, y)]),\n                                                  j,\n                                                  \"Davies_bouldin\")$davies_bouldin)\n \nggplot(data.table(K = 2:6, Dav_Boul = unlist(db_km)), aes(K, Dav_Boul)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-6\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-6-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-6\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-6-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<figure class=\"highlight\"><pre>data_example[, class := as.factor(km_res_k[[which.min(c(0,diff(unlist(db_km))))]])]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-7\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-7-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-7\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-7-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see that the Elbow diagram rule chose 4 clusters - makes sense to me actually…</p>\n<p>We can also try it with PAM - K-medoids.</p>\n<figure class=\"highlight\"><pre>kmed_res_k &lt;- lapply(2:6, function(i) pam(data_example[, .(x, y)], i)$clustering)\n \ndb_kmed &lt;- lapply(kmed_res_k, function(j) intCriteria(data.matrix(data_example[, .(x, y)]),\n                                                  j,\n                                                  \"Davies_bouldin\")$davies_bouldin)\n \nggplot(data.table(K = 2:6, Dav_Boul = unlist(db_kmed)), aes(K, Dav_Boul)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-8\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-8-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-8\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-8-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<figure class=\"highlight\"><pre>data_example[, class := as.factor(kmed_res_k[[which.min(c(0,diff(unlist(db_km))))]])]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-9\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-9-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-9\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-9-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>It is the same result.</p>\n<h3 id=\"model-based\">Model-based</h3>\n<p>Model-based clustering methods are based on some probabilistic distribution. It can be:</p>\n<ul>\n<li>Gaussian normal distribution</li>\n<li>Gamma distribution</li>\n<li>Student’s t-distribution</li>\n<li>Poisson distribution</li>\n<li>etc.</li>\n</ul>\n<p>Since we cluster multivariate data, model-based clustering uses Multivariate distributions and a so-called Mixture of models (Mixtures -&gt; clusters). When using clustering with Gaussian normal distribution, we are using the theory of <strong>Gaussian Mixture Models (GMM)</strong>.</p>\n<h4 id=\"gmm\">GMM</h4>\n<p>The target is to maximize likelihood:\n\\(L(\\boldsymbol{\\mu_1}, \\dots, \\boldsymbol{\\mu_k}, \\boldsymbol{\\Sigma_1}, \\dots, \\boldsymbol{\\Sigma_k} | \\boldsymbol{x_1}, \\dots, \\boldsymbol{x_n}).\\)\nHere, cluster is represented by mean (( \\mathbf{\\mu} )) and covariance matrix (( \\mathbf{\\Sigma} )). So not just centroid as in the case of K-means.</p>\n<p>This optimization problem is typically solved by the <strong>EM</strong> algorithm (Expectation Maximization).</p>\n<p>Pros and cons:</p>\n<ul>\n<li>[+] Ellipsoidal clusters,</li>\n<li>[+] Can be parameterized by covariance matrix,</li>\n<li>[+] Scale-independent,</li>\n<li>[-] Very slow for high-dimensional data,</li>\n<li>[-] Can be difficult to understand.</li>\n</ul>\n<p>EM algorithm with GMM is implemented in the <code>mclust</code> package. You can optimize various shapes of mixtures (clusters) by the <code>modelNames</code> parameter (check the <code>?mclustModelNames</code> function for more details).</p>\n<figure class=\"highlight\"><pre>library(mclust)\nres &lt;- Mclust(data_example[, .(x, y)], G = 3, modelNames = \"VVV\", verbose = FALSE)\n \nplot(res, what = \"classification\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-10\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-10-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-10\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-10-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Pretty interesting red ellipse that was created, but generally clustering is OK.</p>\n<h4 id=\"bic\">BIC</h4>\n<p>The Bayesian Information Criterion (BIC) for choosing the optimal number of clusters can be used with model-based clustering.\nIn the <code>mclust</code> package, you can just add multiple <code>modelNames</code> and it chooses by BIC the best one.\nWe can try also to vary the dependency of covariance matrix ( \\mathbf{\\Sigma} ).</p>\n<figure class=\"highlight\"><pre>res &lt;- Mclust(data_example[, .(x, y)], G = 2:6, modelNames = c(\"VVV\", \"EEE\", \"VII\", \"EII\"), verbose = FALSE)\nres</pre></figure>\n<figure class=\"highlight\"><pre>## 'Mclust' model object: (EII,6) \n## \n## Available components: \n##  [1] \"call\"           \"data\"           \"modelName\"      \"n\"              \"d\"              \"G\"              \"BIC\"            \"loglik\"         \"df\"             \"bic\"            \"icl\"           \n## [12] \"hypvol\"         \"parameters\"     \"z\"              \"classification\" \"uncertainty\"</pre></figure>\n<figure class=\"highlight\"><pre>plot(res, what = \"BIC\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-11\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-11-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-11\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-11-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>The result:</p>\n<figure class=\"highlight\"><pre>plot(res, what = \"classification\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-12\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-12-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-12\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-12-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>So, the methodology chose 6 clusters - 3 main groups of data and all 3 anomalies in separate clusters.</p>\n<h3 id=\"density-based\">Density-based</h3>\n<p>Density-based clusters are based on maximally connected components of the set of points that lie within some defined distance from some core object.</p>\n<p>Methods:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/DBSCAN\" rel=\"nofollow\" target=\"_blank\"><strong>DBSCAN</strong></a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/OPTICS_algorithm\" rel=\"nofollow\" target=\"_blank\"><strong>OPTICS</strong></a></li>\n<li>HDBSCAN</li>\n<li>Multiple densities (Multi-density) methods</li>\n</ul>\n<h4 id=\"dbscan\">DBSCAN</h4>\n<p>In the well-known method <a href=\"https://en.wikipedia.org/wiki/DBSCAN\" rel=\"nofollow\" target=\"_blank\"><strong>DBSCAN</strong></a>, density is defined as <em>neighborhood</em>, where points have to be reachable within a defined distance (( \\epsilon ) distance - first parameter of the method), however, clusters must have at least some number of minimal points (second parameter of the method). Points that weren’t connected with any cluster and did not pass the minimal points criterion are marked as noise (outliers).</p>\n<p>Pros and cons:</p>\n<ul>\n<li>[+] Extracts automatically outliers,</li>\n<li>[+] Fast to compute,</li>\n<li>[+] Can find clusters of arbitrary shapes,</li>\n<li>[+] The number of clusters is determined automatically based on data,</li>\n<li>[-] Parameters (( \\epsilon ), minPts) must be set by a practitioner,</li>\n<li>[-] Possible problem with neighborhoods - can be connected.</li>\n</ul>\n<p><strong>DBSCAN</strong> is implemented in the same named function and package, so let’s try it.</p>\n<figure class=\"highlight\"><pre>library(dbscan)\n \nres &lt;- dbscan(data_example[, .(x, y)], eps = 0.4, minPts = 5)\ntable(res$cluster)</pre></figure>\n<figure class=\"highlight\"><pre>## \n##  0  1  2  3 \n##  3 10 10 10</pre></figure>\n<figure class=\"highlight\"><pre>data_example[, class := as.factor(res$cluster)]\nlevels(data_example$class)[1] &lt;- c(\"Noise\")\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw() +\n  scale_shape_manual(values = c(3,16,17,18))</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-13\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-13-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-13\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-13-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see that DBSCAN found 3 clusters and 3 outliers correctly when parameters are wisely chosen.</p>\n<h4 id=\"bananas---dbscan-result\">Bananas - DBSCAN result</h4>\n<p>To demonstrate the strength of DBSCAN, researchers created many dummy artificial datasets, which are many times called bananas.</p>\n<figure class=\"highlight\"><pre>bananas &lt;- fread(\"_rmd/t7.10k.dat\")\ndb_res &lt;- dbscan(bananas, eps = 10, minPts = 15)\n \ndata_all &lt;- data.table(bananas, class = as.factor(db_res$cluster))\n \nlibrary(ggsci)\nggplot(data_all, aes(V1, V2, color = class, shape = class)) +\n  geom_point(alpha = 0.75) +\n  scale_color_d3() +\n  scale_shape_manual(values = c(3, rep(16, 9))) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-14\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-14-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-14\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-14-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h4 id=\"bananas---k-means-result\">Bananas - K-means result</h4>\n<figure class=\"highlight\"><pre>km_res &lt;- kmeans(bananas, 9)\ndata_all[, class := as.factor(km_res$cluster)]\n \nggplot(data_all, aes(V1, V2, color = class)) +\n  geom_point(alpha = 0.75) +\n  scale_color_d3() +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-15\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-15-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-15\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-15-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>K-means here is not a good choice obviously…but these datasets are far from real-world either.</p>\n<h3 id=\"spectral-clustering\">Spectral clustering</h3>\n<p>Spectral clustering methods are based on the spectral decomposition of data, so the creation of eigen vectors and eigen values.</p>\n<p>Steps:</p>\n<ol>\n<li>N = number of data, d = dimension of data,</li>\n<li>( \\mathbf{A} ) = affinity matrix, ( A_{ij} = \\exp(- (data_i - data_j)^2 / (2*\\sigma^2) ) ) - N by N matrix,</li>\n<li>( \\mathbf{D} ) = diagonal matrix whose (i,i)-element is the sum of ( \\mathbf{A} ) i-th row - N by N matrix,</li>\n<li>( \\mathbf{L} ) = ( \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2} ) - N by N matrix,</li>\n<li>( \\mathbf{X} ) = union of k largest eigenvectors of ( \\mathbf{L} ) - N by k matrix,</li>\n<li>Renormalising each of ( \\mathbf{X} ) rows to have unit length - N by k matrix,</li>\n<li>Run K-means algorithm on ( \\mathbf{X} ).</li>\n</ol>\n<h4 id=\"typical-use-case-for-spectral-clustering\">Typical use case for spectral clustering</h4>\n<p>We will try spectral clustering on the Spirals artificial dataset.</p>\n<figure class=\"highlight\"><pre>data_spiral &lt;- fread(\"_rmd/data_spiral.csv\")\n \nggplot(data_spiral, aes(x, y, color = as.factor(label), shape = as.factor(label))) +\n  geom_point(size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-16\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-16-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-16\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-16-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Spectral clustering is implemented in the <code>kernlab</code> package and <code>specc</code> function.</p>\n<figure class=\"highlight\"><pre>library(kernlab)\n \nres &lt;- specc(data.matrix(data_spiral[, .(x, y)]), centers = 3)\n \ndata_spiral[, class := as.factor(res)]\n \nggplot(data_spiral, aes(x, y, color = class, shape = class)) +\n  geom_point(size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-17\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-17-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-17\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-17-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Let’s it try on more advanced data - compound data.</p>\n<figure class=\"highlight\"><pre>data_compound &lt;- fread(\"_rmd/data_compound.csv\")\n \nggplot(data_compound, aes(x, y, color = as.factor(label), shape = as.factor(label))) +\n  geom_point(size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-18\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-18-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-18\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-18-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<figure class=\"highlight\"><pre>res &lt;- specc(data.matrix(data_compound[, .(x, y)]), centers = 6)\n \ndata_compound[, class := as.factor(res)]\n \nggplot(data_compound, aes(x, y, color = class, shape = class)) +\n  geom_point(size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-18\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-18-2.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-18\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-18-2.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>This is not a good result, let’s try DBSCAN.</p>\n<figure class=\"highlight\"><pre>db_res &lt;- dbscan(data.matrix(data_compound[, .(x, y)]), eps = 1.4, minPts = 5)\n# db_res\n \ndata_compound[, class := as.factor(db_res$cluster)]\n \nggplot(data_compound, aes(x, y, color = class, shape = class)) +\n  geom_point(size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-19\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-19-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-19\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-19-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Again, the nice result for DBSCAN on the artificial dataset.</p>\n<h3 id=\"hierarchical-clustering\">Hierarchical clustering</h3>\n<p>The result of a hierarchical clustering is a dendrogram. The dendrogram can be cut at any height to form a partition of the data into clusters.\nHow data points are connected in the dendrogram has multiple possible ways (linkages) and criteria:</p>\n<ul>\n<li>Single-linkage</li>\n<li>Complete-linkage</li>\n<li>Average-linkage</li>\n<li>Centroid-linkage</li>\n<li>Ward’s minimum variance method</li>\n<li>etc.</li>\n</ul>\n<p>Criteria:</p>\n<ul>\n<li>single-linkage: ( \\min { d(a,b):a\\in A, b\\in B } )</li>\n<li>complete-linkage: ( \\max { d(a,b):a\\in A, b\\in B } )</li>\n<li>\n<table>\n<tbody>\n<tr>\n<td>average-linkage: ( \\frac{1}{</td>\n<td>A</td>\n<td> </td>\n<td>B</td>\n<td>}\\sum_{a\\in A}\\sum_{b\\in B}d(a,b) )</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<table>\n<tbody>\n<tr>\n<td>centroid-linkage: (</td>\n<td> </td>\n<td>c_t - c_s</td>\n<td> </td>\n<td>), where ( c_s ) and ( c_t ) are the centroids of clusters ( s ) and ( t ).</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n<p>where ( d(a,b) ) is the distance between points ( a ) and ( b ).</p>\n<h4 id=\"iris-dataset-use-case\">IRIS dataset use case</h4>\n<p>Let’s try hierarchical clustering on the IRIS dataset.</p>\n<figure class=\"highlight\"><pre>ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point(alpha = 0.8, size = 5) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-20\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-20-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-20\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-20-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Single linkage:</p>\n<figure class=\"highlight\"><pre>library(ggdendro)\nlibrary(dendextend)\n \ndata_m &lt;- iris[,-5]\n \nhie_single &lt;- hclust(dist(data_m), method = \"single\")\ndend &lt;- as.dendrogram(hie_single)\ndend &lt;- dend %&gt;% set(\"branches_k_color\", k = 3) %&gt;%\n  set(\"branches_lwd\", 1.2) %&gt;% \n  set(\"labels\", rep(c(\"set\", \"ver\", \"vir\"), each = 50)) %&gt;%\n  set(\"labels_colors\", rep(c(\"red\", \"green\", \"blue\"), each = 50)) %&gt;%\n  set(\"labels_cex\", 0.6)\nggd1 &lt;- as.ggdend(dend)\nggplot(ggd1)</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-21\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-21-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-21\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-21-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Complete linkage:</p>\n<figure class=\"highlight\"><pre>hie_complete &lt;- hclust(dist(data_m), method = \"complete\")\ndend &lt;- as.dendrogram(hie_complete)\ndend &lt;- dend %&gt;% set(\"branches_k_color\", k = 3) %&gt;%\n  set(\"branches_lwd\", 1.2) %&gt;% \n  set(\"labels\", rep(c(\"set\", \"ver\", \"vir\"), each = 50)) %&gt;%\n  set(\"labels_colors\", rep(c(\"red\", \"green\", \"blue\"), each = 50)) %&gt;%\n  set(\"labels_cex\", 0.6)\nggd1 &lt;- as.ggdend(dend)\nggplot(ggd1)</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-22\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-22-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-22\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-22-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Average linkage:</p>\n<figure class=\"highlight\"><pre>hie_ave &lt;- hclust(dist(data_m), method = \"average\")\ndend &lt;- as.dendrogram(hie_ave)\ndend &lt;- dend %&gt;% set(\"branches_k_color\", k = 3) %&gt;% \n  set(\"branches_lwd\", 1.2) %&gt;% \n  set(\"labels\", rep(c(\"set\", \"ver\", \"vir\"), each = 50)) %&gt;%\n  set(\"labels_colors\", rep(c(\"red\", \"green\", \"blue\"), each = 50)) %&gt;%\n  set(\"labels_cex\", 0.6)\nggd1 &lt;- as.ggdend(dend)\nggplot(ggd1)</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-23\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-23-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-23\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-23-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Let’s compute the precision of these three clusterings with the <code>clusterCrit</code> package:</p>\n<figure class=\"highlight\"><pre>extCriteria(as.integer(iris[,5]), cutree(hie_single, 3), crit = \"Precision\")</pre></figure>\n<figure class=\"highlight\"><pre>## $precision\n## [1] 0.5985951</pre></figure>\n<figure class=\"highlight\"><pre>extCriteria(as.integer(iris[,5]), cutree(hie_complete, 3), crit = \"Precision\")</pre></figure>\n<figure class=\"highlight\"><pre>## $precision\n## [1] 0.7225295</pre></figure>\n<figure class=\"highlight\"><pre>extCriteria(as.integer(iris[,5]), cutree(hie_ave, 3), crit = \"Precision\")</pre></figure>\n<figure class=\"highlight\"><pre>## $precision\n## [1] 0.8191682</pre></figure>\n<p>The best results were obtained with average linkage with precision of 81.9%.</p>\n<h2 id=\"connected-data\">Connected data</h2>\n<p>I have prepared for you the last use case for most shown methods where data (and clusters) are closely connected, so the closest scenario of real data.</p>\n<figure class=\"highlight\"><pre>set.seed(5)\nlibrary(MASS)\n \ndata_connected &lt;- as.data.table(rbind(\n  mvrnorm(220, mu = c(3.48, 3.4), Sigma = matrix(c(0.005, -0.015, -0.01, 0.09), nrow = 2)),\n  mvrnorm(280, mu = c(3.8, 3.8), Sigma = matrix(c(0.05, 0, 0, 0.05), nrow = 2)),\n  mvrnorm(220, mu = c(3.85, 2.9), Sigma = matrix(c( 0.1, 0.03, 0.03, 0.017), nrow = 2))\n  ))\n \nsetnames(data_connected, c(\"V1\", \"V2\"), c(\"x\", \"y\"))\n \nggplot(data_connected, aes(x, y)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-25\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-25-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-25\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-25-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h4 id=\"dbscan---result-for-connected-data\">DBSCAN - result for connected data</h4>\n<p>Chosen parameters are ( \\epsilon = 0.08 ), ( minPts = 18 ).</p>\n<figure class=\"highlight\"><pre>db_res &lt;- dbscan(data_connected, eps = 0.08, minPts = 18)</pre></figure>\n<figure class=\"highlight\"><pre>data_all &lt;- data.table(data_connected, class = as.factor(db_res$cluster))\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-27\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-27-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-27\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-27-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>The result is quite good enough, where the main three core groups are identified. Let’s change minPts to 10.</p>\n<figure class=\"highlight\"><pre>db_res &lt;- dbscan(data_connected, eps = 0.08, minPts = 10)\n \ndata_all &lt;- data.table(data_connected, class = as.factor(db_res$cluster))\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-28\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-28-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-28\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_16/unnamed-chunk-28-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see a use case where DBSCAN is very sensitive to parameter settings, and you have to be very careful with some automatic settings of these parameters (in your use cases).</p>\n<h4 id=\"k-means---result-for-connected-data\">K-means - result for connected data</h4>\n<figure class=\"highlight\"><pre>km_res &lt;- kmeans(data_connected, 3)\n \ndata_all[, class := as.factor(km_res$cluster)]\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-29\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-29-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-29\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-29-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Nice result to be fair for this simple method.</p>\n<h4 id=\"gaussian-model-based-clustering-result\">Gaussian model-based clustering result</h4>\n<figure class=\"highlight\"><pre>m_res &lt;- Mclust(data_connected, G = 3, modelNames = \"VVV\", verbose = FALSE)\n \ndata_all[, class := as.factor(m_res$classification)]\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-30\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-30-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-30\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_16/unnamed-chunk-30-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Almost perfect result, but due to the normality of sampled data.</p>\n<h4 id=\"spectral-clustering-result\">Spectral clustering result</h4>\n<figure class=\"highlight\"><pre>res &lt;- specc(data.matrix(data_connected[, .(x, y)]), centers = 3)\n \ndata_all[, class := as.factor(res)]\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-31\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-31-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-31\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_16/unnamed-chunk-31-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Very nice result again!</p>\n<h3 id=\"other-types-of-clustering-methods\">Other types of clustering methods</h3>\n<p>Other types of clustering methods that were not covered in this blog post are:</p>\n<ul>\n<li>Grid-based</li>\n<li>Subspace clustering</li>\n<li>Multi-view clustering</li>\n<li>Based on artificial neural networks (e.g. SOM)</li>\n<li>Consensus (ensemble) clustering</li>\n<li>Data stream(s) clustering</li>\n<li>etc.</li>\n</ul>\n<h2 id=\"conclusions\">Conclusions</h2>\n<ul>\n<li>We have many types of clustering methods</li>\n<li>Different datasets need different clustering methods</li>\n<li>Automatic determination of a number of clusters can be tricky</li>\n<li>Real datasets are usually connected - density-based methods can fail</li>\n<li>Outliers (anomalies) can significantly influence clustering results - the solution is to preprocess the data or use density-based clustering</li>\n<li>Some methods are not suited for large (or high-dimensional) datasets - model-based or spectral clustering</li>\n<li>Some methods are not suited for non-convex clusters - K-means, basic hierarchical clustering</li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://petolau.github.io/Overview-clustering-methods-in-R/\"> Peter Laurinec</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
      "main_text": "Overview of clustering methods in R\nPosted on\nJanuary 10, 2024\nby\nPeter Laurinec\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nPeter Laurinec\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nClustering\nis a very popular technique in data science because of its unsupervised characteristic – we don’t need true labels of groups in data.\nIn this blog post, I will give you a “quick” survey of various clustering methods applied to synthetic but also real datasets.\nWhat is clustering?\nCluster analysis\nor clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\nIt is a technique of\nunsupervised learning\n, so clustering is used when no a priori information about data is available.\nThis makes clustering a very strong technique for gaining insights into data and making more accurate decisions.\nWhat is it good for?\nClustering is used for:\nTo gain insight into data, generate hypotheses, detect anomalies, and identify salient features,\nTo identify the degree of similarity among objects (i.e. organisms),\nAs a method for organizing the data and summarising it through cluster prototypes (compression).\nClassification to groups\nThe first use case is to group data, e.g. classify them into groups.\nFor explanation purposes, I will generate synthetic data from three normal distributions plus three outliers (anomalies).\nLet’s load needed packages, generate randomly some data, and show the first use case in the visualization:\nlibrary(data.table) # data handling\nlibrary(ggplot2) # visualisations\nlibrary(gridExtra) # visualisations\nlibrary(grid) # visualisations\nlibrary(cluster) # PAM - K-medoids\n \nset.seed(54321)\n \ndata_example <- data.table(x = c(rnorm(10, 3.5, 0.1), rnorm(10, 2, 0.1),\n                                 rnorm(10, 4.5, 0.1), c(5, 1.9, 3.95)),\n                           y = c(rnorm(10, 3.5, 0.1), rnorm(10, 2, 0.1),\n                                 rnorm(10, 4.5, 0.1), c(1.65, 2.9, 4.2)))\n \ngg1 <- ggplot(data_example, aes(x, y)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\n \nkmed_res <- pam(data_example, 3)$clustering\n \ndata_example[, class := as.factor(kmed_res)]\n \ngg2 <- ggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\n \ndefine_region <- function(row, col){\n  viewport(layout.pos.row = row, layout.pos.col = col)\n}\n \ngrid.newpage()\n# Create layout : nrow = 2, ncol = 2\npushViewport(viewport(layout = grid.layout(1, 2)))\n# Arrange the plots\nprint(gg1, vp = define_region(1, 1))\nprint(gg2, vp = define_region(1, 2))\nWe can see three nicely divided groups of data.\nAnomaly detection\nClustering can be also used as an anomaly detection technique, some methods of clustering can detect automatically outliers (anomalies). Let’s show visually what it looks like.\nanom <- c(rep(1, 30), rep(0, 3))\ndata_example[, class := as.factor(anom)]\nlevels(data_example$class) <- c(\"Anomaly\", \"Normal\")\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\nData compression\nIn an era of a large amount of data (also many times used buzzword - big data), we have problems processing them in real time.\nHere clustering can help to reduce dimensionality by its compression feature.\nCreated clusters, that incorporate multiple points (data), can be replaced by their representatives (prototypes) - so one point. In this way, points were replaced by its cluster representative (“+”):\ndata_example[, class := as.factor(kmed_res)]\n \ncentroids <- data_example[, .(x = mean(x), y = mean(y)), by = class]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  geom_point(data = centroids, aes(x, y), color = \"black\", shape = \"+\", size = 18) +\n  theme_bw()\nTypes of clustering methods\nSince cluster analysis has been here for more than 50 years, there are a large amount of available methods.\nThe basic classification of clustering methods is based on the objective to which they aim:\nhierarchical\n,\nnon-hierarchical\n.\nThe\nhierarchical clustering\nis a multi-level partition of a dataset that is a branch of classification (clustering). Hierarchical clustering has two types of access to data. The first one, divisive clustering, starts with one big cluster that is then divided into smaller clusters. The second one, agglomerative clustering, starts with individual objects that are single-element clusters, and then they are gradually merged. The whole process of hierarchical clustering can be expressed (visualized) as a dendrogram.\nThe\nnon-hierarchical clustering\nis dividing a dataset into a system of disjunctive subsets (clusters) so that an intersection of clusters would be an empty set.\nClustering methods can be also divided in more detail based on the processes in the method (algorithm) itself:\nNon-hierarchical\n:\nCentroid-based\nModel-based\nDensity-based\nGrid-based\nHierarchical\n:\nAgglomerative\nDivisive\nBut which to choose in your use case? Let’s dive deeper into the most known methods and discuss their advantages and disadvantages.\nCentroid-based\nThe most basic (maybe just for me) type of clustering method is centroid-based.\nThis type of clustering creates prototypes of clusters - centroids or medoids.\nThe best well-known methods are:\nK-means\nK-medians\nK-medoids\nK-modes\nK-means\nSteps:\nCreate random K clusters (and compute centroids).\nAssign points to the nearest centroids.\nUpdate centroids.\nGo to step 2 while the centroids are changing.\nPros and cons:\n[+] Fast to compute. Easy to understand.\n[-] Various initial clusters can lead to different final clustering.\n[-] Scale-dependent.\n[-] Creates only convex (spherical) shapes of clusters.\n[-] Sensitive to outliers.\nK-means - example\nIt is very easy to try K-means in R (by the\nkmeans\nfunction), only needed parameter is a number of clusters.\nkm_res <- kmeans(data_example, 3)$cluster\n \ndata_example[, class := as.factor(km_res)]\n \ncentroids <- data_example[, .(x = mean(x), y = mean(y)), by = class]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  geom_point(data = centroids, aes(x, y), color = \"black\", shape = \"+\", size = 18) +\n  theme_bw()\nWe can see example, when K-means fails most often, so when there are outliers in the dataset.\nK-medoids\nThe problem with outliers solves K-medoids because prototypes are medoids - members of the dataset. So, not artificially created centroids, which helps to tackle outliers.\nPros and cons:\n[+] Easy to understand.\n[+] Less sensitive to outliers.\n[+] Possibility to use any distance measure.\n[-] Various initial clusters can lead to different final clustering.\n[-] Scale-dependent.\n[-] Slower than K-means.\nK-medoids - example\nK-medoids problem can be solved by the Partition Around Medoids (PAM) algorithm (function\npam\nin\ncluster\npackage).\nkmed_res <- pam(data_example[, .(x, y)], 3)\ndata_example[, class := as.factor(kmed_res$clustering)]\n \nmedoids <- data.table(kmed_res$medoids, class = as.factor(1:3))\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  geom_point(data = medoids, aes(x, y, shape = class),\n             color = \"black\", size = 11, alpha = 0.7) +\n  theme_bw() +\n  guides(shape = \"none\")\nWe can see that medoids stayed nicely in the three main groups of data.\nThe determination of the number of clusters\nThe disadvantage of centroid-based methods is that the number of clusters needs to be known in advance (it is a parameter of the methods). However, we can determine the number of clusters by its Internal validation (index). Basic steps are based on that we compute some internal validation index with many ( K ) and we choose ( K ) with the best index value.\nMany indexes are there…\nSilhouette\nDavies-Bouldin index\nDunn index\netc.\nHowever, every index has similar characteristics:\n\\[\\frac{within-cluster-similarity}{between-clusters-similarity} .\\]\nso, it is the ratio of the average distances in clusters and between clusters.\nElbow diagram\nThe Elbow diagram is a simple method (rule) how to determine the number of clusters - we compute the internal index with a set of K and choose K where positive change is largest.\nSo for example, I chose the Davies-Bouldin index implemented in the\nclusterCrit\npackage. For our simple dataset, I will generate clusterings with 2-6 number of clusters and compute the index.\nlibrary(clusterCrit)\n \nkm_res_k <- lapply(2:6, function(i) kmeans(data_example[, .(x, y)], i)$cluster)\nkm_res_k\n## [[1]]\n##  [1] 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2\n## \n## [[2]]\n##  [1] 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 3 2 1\n## \n## [[3]]\n##  [1] 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 2 4 3\n## \n## [[4]]\n##  [1] 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 1 3\n## \n## [[5]]\n##  [1] 1 1 1 1 1 1 1 5 5 1 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 2 6 4\ndb_km <- lapply(km_res_k, function(j) intCriteria(data.matrix(data_example[, .(x, y)]),\n                                                  j,\n                                                  \"Davies_bouldin\")$davies_bouldin)\n \nggplot(data.table(K = 2:6, Dav_Boul = unlist(db_km)), aes(K, Dav_Boul)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\ndata_example[, class := as.factor(km_res_k[[which.min(c(0,diff(unlist(db_km))))]])]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\nWe can see that the Elbow diagram rule chose 4 clusters - makes sense to me actually…\nWe can also try it with PAM - K-medoids.\nkmed_res_k <- lapply(2:6, function(i) pam(data_example[, .(x, y)], i)$clustering)\n \ndb_kmed <- lapply(kmed_res_k, function(j) intCriteria(data.matrix(data_example[, .(x, y)]),\n                                                  j,\n                                                  \"Davies_bouldin\")$davies_bouldin)\n \nggplot(data.table(K = 2:6, Dav_Boul = unlist(db_kmed)), aes(K, Dav_Boul)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\ndata_example[, class := as.factor(kmed_res_k[[which.min(c(0,diff(unlist(db_km))))]])]\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw()\nIt is the same result.\nModel-based\nModel-based clustering methods are based on some probabilistic distribution. It can be:\nGaussian normal distribution\nGamma distribution\nStudent’s t-distribution\nPoisson distribution\netc.\nSince we cluster multivariate data, model-based clustering uses Multivariate distributions and a so-called Mixture of models (Mixtures -> clusters). When using clustering with Gaussian normal distribution, we are using the theory of\nGaussian Mixture Models (GMM)\n.\nGMM\nThe target is to maximize likelihood:\n\\(L(\\boldsymbol{\\mu_1}, \\dots, \\boldsymbol{\\mu_k}, \\boldsymbol{\\Sigma_1}, \\dots, \\boldsymbol{\\Sigma_k} | \\boldsymbol{x_1}, \\dots, \\boldsymbol{x_n}).\\)\nHere, cluster is represented by mean (( \\mathbf{\\mu} )) and covariance matrix (( \\mathbf{\\Sigma} )). So not just centroid as in the case of K-means.\nThis optimization problem is typically solved by the\nEM\nalgorithm (Expectation Maximization).\nPros and cons:\n[+] Ellipsoidal clusters,\n[+] Can be parameterized by covariance matrix,\n[+] Scale-independent,\n[-] Very slow for high-dimensional data,\n[-] Can be difficult to understand.\nEM algorithm with GMM is implemented in the\nmclust\npackage. You can optimize various shapes of mixtures (clusters) by the\nmodelNames\nparameter (check the\n?mclustModelNames\nfunction for more details).\nlibrary(mclust)\nres <- Mclust(data_example[, .(x, y)], G = 3, modelNames = \"VVV\", verbose = FALSE)\n \nplot(res, what = \"classification\")\nPretty interesting red ellipse that was created, but generally clustering is OK.\nBIC\nThe Bayesian Information Criterion (BIC) for choosing the optimal number of clusters can be used with model-based clustering.\nIn the\nmclust\npackage, you can just add multiple\nmodelNames\nand it chooses by BIC the best one.\nWe can try also to vary the dependency of covariance matrix ( \\mathbf{\\Sigma} ).\nres <- Mclust(data_example[, .(x, y)], G = 2:6, modelNames = c(\"VVV\", \"EEE\", \"VII\", \"EII\"), verbose = FALSE)\nres\n## 'Mclust' model object: (EII,6) \n## \n## Available components: \n##  [1] \"call\"           \"data\"           \"modelName\"      \"n\"              \"d\"              \"G\"              \"BIC\"            \"loglik\"         \"df\"             \"bic\"            \"icl\"           \n## [12] \"hypvol\"         \"parameters\"     \"z\"              \"classification\" \"uncertainty\"\nplot(res, what = \"BIC\")\nThe result:\nplot(res, what = \"classification\")\nSo, the methodology chose 6 clusters - 3 main groups of data and all 3 anomalies in separate clusters.\nDensity-based\nDensity-based clusters are based on maximally connected components of the set of points that lie within some defined distance from some core object.\nMethods:\nDBSCAN\nOPTICS\nHDBSCAN\nMultiple densities (Multi-density) methods\nDBSCAN\nIn the well-known method\nDBSCAN\n, density is defined as\nneighborhood\n, where points have to be reachable within a defined distance (( \\epsilon ) distance - first parameter of the method), however, clusters must have at least some number of minimal points (second parameter of the method). Points that weren’t connected with any cluster and did not pass the minimal points criterion are marked as noise (outliers).\nPros and cons:\n[+] Extracts automatically outliers,\n[+] Fast to compute,\n[+] Can find clusters of arbitrary shapes,\n[+] The number of clusters is determined automatically based on data,\n[-] Parameters (( \\epsilon ), minPts) must be set by a practitioner,\n[-] Possible problem with neighborhoods - can be connected.\nDBSCAN\nis implemented in the same named function and package, so let’s try it.\nlibrary(dbscan)\n \nres <- dbscan(data_example[, .(x, y)], eps = 0.4, minPts = 5)\ntable(res$cluster)\n## \n##  0  1  2  3 \n##  3 10 10 10\ndata_example[, class := as.factor(res$cluster)]\nlevels(data_example$class)[1] <- c(\"Noise\")\n \nggplot(data_example, aes(x, y, color = class, shape = class)) +\n  geom_point(alpha = 0.75, size = 8) +\n  theme_bw() +\n  scale_shape_manual(values = c(3,16,17,18))\nWe can see that DBSCAN found 3 clusters and 3 outliers correctly when parameters are wisely chosen.\nBananas - DBSCAN result\nTo demonstrate the strength of DBSCAN, researchers created many dummy artificial datasets, which are many times called bananas.\nbananas <- fread(\"_rmd/t7.10k.dat\")\ndb_res <- dbscan(bananas, eps = 10, minPts = 15)\n \ndata_all <- data.table(bananas, class = as.factor(db_res$cluster))\n \nlibrary(ggsci)\nggplot(data_all, aes(V1, V2, color = class, shape = class)) +\n  geom_point(alpha = 0.75) +\n  scale_color_d3() +\n  scale_shape_manual(values = c(3, rep(16, 9))) +\n  theme_bw()\nBananas - K-means result\nkm_res <- kmeans(bananas, 9)\ndata_all[, class := as.factor(km_res$cluster)]\n \nggplot(data_all, aes(V1, V2, color = class)) +\n  geom_point(alpha = 0.75) +\n  scale_color_d3() +\n  theme_bw()\nK-means here is not a good choice obviously…but these datasets are far from real-world either.\nSpectral clustering\nSpectral clustering methods are based on the spectral decomposition of data, so the creation of eigen vectors and eigen values.\nSteps:\nN = number of data, d = dimension of data,\n( \\mathbf{A} ) = affinity matrix, ( A_{ij} = \\exp(- (data_i - data_j)^2 / (2*\\sigma^2) ) ) - N by N matrix,\n( \\mathbf{D} ) = diagonal matrix whose (i,i)-element is the sum of ( \\mathbf{A} ) i-th row - N by N matrix,\n( \\mathbf{L} ) = ( \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2} ) - N by N matrix,\n( \\mathbf{X} ) = union of k largest eigenvectors of ( \\mathbf{L} ) - N by k matrix,\nRenormalising each of ( \\mathbf{X} ) rows to have unit length - N by k matrix,\nRun K-means algorithm on ( \\mathbf{X} ).\nTypical use case for spectral clustering\nWe will try spectral clustering on the Spirals artificial dataset.\ndata_spiral <- fread(\"_rmd/data_spiral.csv\")\n \nggplot(data_spiral, aes(x, y, color = as.factor(label), shape = as.factor(label))) +\n  geom_point(size = 2) +\n  theme_bw()\nSpectral clustering is implemented in the\nkernlab\npackage and\nspecc\nfunction.\nlibrary(kernlab)\n \nres <- specc(data.matrix(data_spiral[, .(x, y)]), centers = 3)\n \ndata_spiral[, class := as.factor(res)]\n \nggplot(data_spiral, aes(x, y, color = class, shape = class)) +\n  geom_point(size = 2) +\n  theme_bw()\nLet’s it try on more advanced data - compound data.\ndata_compound <- fread(\"_rmd/data_compound.csv\")\n \nggplot(data_compound, aes(x, y, color = as.factor(label), shape = as.factor(label))) +\n  geom_point(size = 2) +\n  theme_bw()\nres <- specc(data.matrix(data_compound[, .(x, y)]), centers = 6)\n \ndata_compound[, class := as.factor(res)]\n \nggplot(data_compound, aes(x, y, color = class, shape = class)) +\n  geom_point(size = 2) +\n  theme_bw()\nThis is not a good result, let’s try DBSCAN.\ndb_res <- dbscan(data.matrix(data_compound[, .(x, y)]), eps = 1.4, minPts = 5)\n# db_res\n \ndata_compound[, class := as.factor(db_res$cluster)]\n \nggplot(data_compound, aes(x, y, color = class, shape = class)) +\n  geom_point(size = 2) +\n  theme_bw()\nAgain, the nice result for DBSCAN on the artificial dataset.\nHierarchical clustering\nThe result of a hierarchical clustering is a dendrogram. The dendrogram can be cut at any height to form a partition of the data into clusters.\nHow data points are connected in the dendrogram has multiple possible ways (linkages) and criteria:\nSingle-linkage\nComplete-linkage\nAverage-linkage\nCentroid-linkage\nWard’s minimum variance method\netc.\nCriteria:\nsingle-linkage: ( \\min { d(a,b):a\\in A, b\\in B } )\ncomplete-linkage: ( \\max { d(a,b):a\\in A, b\\in B } )\naverage-linkage: ( \\frac{1}{\nA\nB\n}\\sum_{a\\in A}\\sum_{b\\in B}d(a,b) )\ncentroid-linkage: (\nc_t - c_s\n), where ( c_s ) and ( c_t ) are the centroids of clusters ( s ) and ( t ).\nwhere ( d(a,b) ) is the distance between points ( a ) and ( b ).\nIRIS dataset use case\nLet’s try hierarchical clustering on the IRIS dataset.\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point(alpha = 0.8, size = 5) +\n  theme_bw()\nSingle linkage:\nlibrary(ggdendro)\nlibrary(dendextend)\n \ndata_m <- iris[,-5]\n \nhie_single <- hclust(dist(data_m), method = \"single\")\ndend <- as.dendrogram(hie_single)\ndend <- dend %>% set(\"branches_k_color\", k = 3) %>%\n  set(\"branches_lwd\", 1.2) %>% \n  set(\"labels\", rep(c(\"set\", \"ver\", \"vir\"), each = 50)) %>%\n  set(\"labels_colors\", rep(c(\"red\", \"green\", \"blue\"), each = 50)) %>%\n  set(\"labels_cex\", 0.6)\nggd1 <- as.ggdend(dend)\nggplot(ggd1)\nComplete linkage:\nhie_complete <- hclust(dist(data_m), method = \"complete\")\ndend <- as.dendrogram(hie_complete)\ndend <- dend %>% set(\"branches_k_color\", k = 3) %>%\n  set(\"branches_lwd\", 1.2) %>% \n  set(\"labels\", rep(c(\"set\", \"ver\", \"vir\"), each = 50)) %>%\n  set(\"labels_colors\", rep(c(\"red\", \"green\", \"blue\"), each = 50)) %>%\n  set(\"labels_cex\", 0.6)\nggd1 <- as.ggdend(dend)\nggplot(ggd1)\nAverage linkage:\nhie_ave <- hclust(dist(data_m), method = \"average\")\ndend <- as.dendrogram(hie_ave)\ndend <- dend %>% set(\"branches_k_color\", k = 3) %>% \n  set(\"branches_lwd\", 1.2) %>% \n  set(\"labels\", rep(c(\"set\", \"ver\", \"vir\"), each = 50)) %>%\n  set(\"labels_colors\", rep(c(\"red\", \"green\", \"blue\"), each = 50)) %>%\n  set(\"labels_cex\", 0.6)\nggd1 <- as.ggdend(dend)\nggplot(ggd1)\nLet’s compute the precision of these three clusterings with the\nclusterCrit\npackage:\nextCriteria(as.integer(iris[,5]), cutree(hie_single, 3), crit = \"Precision\")\n## $precision\n## [1] 0.5985951\nextCriteria(as.integer(iris[,5]), cutree(hie_complete, 3), crit = \"Precision\")\n## $precision\n## [1] 0.7225295\nextCriteria(as.integer(iris[,5]), cutree(hie_ave, 3), crit = \"Precision\")\n## $precision\n## [1] 0.8191682\nThe best results were obtained with average linkage with precision of 81.9%.\nConnected data\nI have prepared for you the last use case for most shown methods where data (and clusters) are closely connected, so the closest scenario of real data.\nset.seed(5)\nlibrary(MASS)\n \ndata_connected <- as.data.table(rbind(\n  mvrnorm(220, mu = c(3.48, 3.4), Sigma = matrix(c(0.005, -0.015, -0.01, 0.09), nrow = 2)),\n  mvrnorm(280, mu = c(3.8, 3.8), Sigma = matrix(c(0.05, 0, 0, 0.05), nrow = 2)),\n  mvrnorm(220, mu = c(3.85, 2.9), Sigma = matrix(c( 0.1, 0.03, 0.03, 0.017), nrow = 2))\n  ))\n \nsetnames(data_connected, c(\"V1\", \"V2\"), c(\"x\", \"y\"))\n \nggplot(data_connected, aes(x, y)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()\nDBSCAN - result for connected data\nChosen parameters are ( \\epsilon = 0.08 ), ( minPts = 18 ).\ndb_res <- dbscan(data_connected, eps = 0.08, minPts = 18)\ndata_all <- data.table(data_connected, class = as.factor(db_res$cluster))\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()\nThe result is quite good enough, where the main three core groups are identified. Let’s change minPts to 10.\ndb_res <- dbscan(data_connected, eps = 0.08, minPts = 10)\n \ndata_all <- data.table(data_connected, class = as.factor(db_res$cluster))\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()\nWe can see a use case where DBSCAN is very sensitive to parameter settings, and you have to be very careful with some automatic settings of these parameters (in your use cases).\nK-means - result for connected data\nkm_res <- kmeans(data_connected, 3)\n \ndata_all[, class := as.factor(km_res$cluster)]\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()\nNice result to be fair for this simple method.\nGaussian model-based clustering result\nm_res <- Mclust(data_connected, G = 3, modelNames = \"VVV\", verbose = FALSE)\n \ndata_all[, class := as.factor(m_res$classification)]\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()\nAlmost perfect result, but due to the normality of sampled data.\nSpectral clustering result\nres <- specc(data.matrix(data_connected[, .(x, y)]), centers = 3)\n \ndata_all[, class := as.factor(res)]\n \nggplot(data_all, aes(x, y, color = class)) +\n  geom_point(alpha = 0.75, size = 2) +\n  theme_bw()\nVery nice result again!\nOther types of clustering methods\nOther types of clustering methods that were not covered in this blog post are:\nGrid-based\nSubspace clustering\nMulti-view clustering\nBased on artificial neural networks (e.g. SOM)\nConsensus (ensemble) clustering\nData stream(s) clustering\netc.\nConclusions\nWe have many types of clustering methods\nDifferent datasets need different clustering methods\nAutomatic determination of a number of clusters can be tricky\nReal datasets are usually connected - density-based methods can fail\nOutliers (anomalies) can significantly influence clustering results - the solution is to preprocess the data or use density-based clustering\nSome methods are not suited for large (or high-dimensional) datasets - model-based or spectral clustering\nSome methods are not suited for non-convex clusters - K-means, basic hierarchical clustering\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nPeter Laurinec\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Clustering is a very popular technique in data science because of its unsupervised characteristic - we don’t need true labels of groups in data. In this blog post, I will give you a “quick” survey of various clustering methods applied to synthetic but ...",
      "meta_keywords": null,
      "og_description": "Clustering is a very popular technique in data science because of its unsupervised characteristic - we don’t need true labels of groups in data. In this blog post, I will give you a “quick” survey of various clustering methods applied to synthetic but ...",
      "og_image": "https://petolau.github.io/images/post_16/unnamed-chunk-1-1.png",
      "og_title": "Overview of clustering methods in R | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 18.1,
      "sitemap_lastmod": "2024-01-11T00:00:00+00:00",
      "twitter_description": "Clustering is a very popular technique in data science because of its unsupervised characteristic - we don’t need true labels of groups in data. In this blog post, I will give you a “quick” survey of various clustering methods applied to synthetic but ...",
      "twitter_title": "Overview of clustering methods in R | R-bloggers",
      "url": "https://www.r-bloggers.com/2024/01/overview-of-clustering-methods-in-r/",
      "word_count": 3612
    }
  }
}