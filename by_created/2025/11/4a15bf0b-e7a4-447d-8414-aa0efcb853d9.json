{
  "uuid": "4a15bf0b-e7a4-447d-8414-aa0efcb853d9",
  "created_at": "2025-11-22 19:59:36",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2024/12/rethinking-how-i-do-supervised-topic-modeling-using-modernbert-and-gpt-4o-mini/",
    "crawled_at": "2025-11-22T10:56:39.664232",
    "external_links": [
      {
        "href": "https://www.markhw.com/blog/topicmodeling2",
        "text": "Mark H. White II, PhD"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://www.markhw.com/blog/supervisedtext",
        "text": "post"
      },
      {
        "href": "https://huggingface.co/blog/modernbert",
        "text": "ModernBERT"
      },
      {
        "href": "https://huggingface.co/blog/bert-101",
        "text": "BERT"
      },
      {
        "href": "https://arxiv.org/pdf/2412.13663",
        "text": "the arxiv paper"
      },
      {
        "href": "https://www.markhw.com/blog/supervisedtext",
        "text": "original post"
      },
      {
        "href": "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/",
        "text": "HuggingFace"
      },
      {
        "href": "https://www.wired.com/story/tiktok-platforms-cory-doctorow/",
        "text": "enshittification"
      },
      {
        "href": "https://github.com/markhwhiteii/blog/tree/master/topic_model_2",
        "text": "my GitHub"
      },
      {
        "href": "https://www.markhw.com/blog/topicmodeling2",
        "text": "Mark H. White II, PhD"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Rethinking How I Do Supervised Topic Modeling, Using ModernBERT and GPT-4o mini | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/mark-white/",
        "text": "Mark White"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-389403 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Rethinking How I Do Supervised Topic Modeling, Using ModernBERT and GPT-4o mini</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">December 21, 2024</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/mark-white/\">Mark White</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.markhw.com/blog/topicmodeling2\"> Mark H. White II, PhD</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>I wrote a <a href=\"https://www.markhw.com/blog/supervisedtext\" rel=\"nofollow\" target=\"_blank\">post</a> in July 2023\ndescribing my process for building a supervised text classification\npipeline. In short, the process first involves reading the text, writing\na thematic content coding guide, and having humans label text. Then, I\ndefine a variety of ways to pre-process text (e.g., word\nvs. word-and-bigram tokenizing, stemming vs. not, stop words vs. not,\nfiltering on the number of times a word had to appear in the corpus) in\na  workflowset. Then, I run these different\npre-processors through different standard models: elastic net, XGBoost,\nrandom forest, etc. Each class of text has its own model, so I would run\nthis pipeline five times if there were five topics in the text.\nImportantly, this is not natural language processing (NLP), as it was a\nbag-of-words approach.</p>\n<p>The idea was to leverage the domain knowledge of the experts on my\nteam through content coding, and then scaling it up using a machine\nlearning pipeline. In the post, I bemoaned how most of the “NLP” or\n“AI-driven” tools I had tested did not do very well. The tools I was\nthinking of were all web-based, point-and-click applications that I had\ntried out since about 2018, and they usually were unsupervised.</p>\n<p>We are in a wildly different environment now when it comes to\nanalyzing text than we were even a few years ago. I am revisiting that\npost to explore alternate routes to classifying text. I will use the\nsame data as I did in that post: 720 Letterboxd reviews of Wes\nAnderson’s film <em>Asteroid City.</em> There is only one code: Did the\nreview discuss Wes Anderson’s unique visual style (1) or not (0)? I\nhand-labeled all of these on one afternoon to give me a supervised\ndataset to play with.</p>\n<p>The use case I had in mind was from a previous job, where the goal\nwas not to predict if an <em>individual</em> discussed a topic or not.\nInstead, the focus was on, “What percentage of people mentioned this\ntopic this month?” in a tracking survey. The primary way I’ll judge\nthese approaches is by seeing how far away the predicted percent of\nreviews discussing visual style is from the actual percent of reviews\ndiscussing visual style. That is: I am interested in estimates of the\n<em>aggregate</em>.</p>\n<p>For a baseline, my bag-of-words supervised classification pipeline\npredicted 19% when it was actually 26% in the testing set, for an\nabsolute error of 7 points.</p>\n<h2>ModernBERT</h2>\n<p>Two days ago, researchers at Answer.AI and LightOn introduced <a href=\"https://huggingface.co/blog/modernbert\" rel=\"nofollow\" target=\"_blank\">ModernBERT</a>. I am\nexcited about this, as <a href=\"https://huggingface.co/blog/bert-101\" rel=\"nofollow\" target=\"_blank\">BERT</a> is one of my\nfavorite foundation large language models (LLMs). While most of the hype\nmachine focuses on <em>generative</em> AI, the tools like ChatGPT that\ncan create text, I think one of the most useful applications of LLMs are\nthe <em>encoder-only</em> models whose job it is to instead\n<em>represent</em> text. As a data scientist and survey researcher, I’ve\nfound this genre of model to be much more useful to the work I do. The\nupshot is that the transformer model architecture with attention allows\nthe model to transform text into a series of numeric columns that\nrepresent it. The text isn’t just fed into the model, but also the\nposition of each piece of text. Additionally, the “B” in “BERT” is\n“bidirectional,” allowing it to learn from context—it can look at words\n<em>before</em> and <em>after</em> each word to help in building a\nnumerical representation. What this all means is that it can figure out\nthings like synonyms and homonyms. Related to the current example of Wes\nAnderson’s visual style, consider a review that contains the phrase:\n“The film’s color palette is bright and warm.” Consider another that\nmight say: “The screenplay isn’t as bright as he thinks it is.” A\nbag-of-words approach is going to have a difficult time here with the\nword “bright”: It is used both in the context of discussing the visuals\nof the film and in a review that doesn’t discuss the visuals of the\nfilm. ModernBERT—the exciting “replacement” (we’ll see if that proves\ntrue) to the existing BERT model variants—<em>can</em> tell the\ndifference. That difference will be represented in different word\nembeddings.</p>\n<p>There are two ways I experimented with ModernBERT to see if it could\noutperform my previous approach: fill-masking and embeddings.</p>\n<h3>Fill-Masking</h3>\n<p>One of the steps of training BERT is through masking. I’m butchering\nthe process, but the idea is to take a sentence, mask a token (which\ncould be a word or sub-word), and train it to predict the correct token.\nFor example, in the code below, I load in ModernBERT and have it predict\na masked token from a very obvious example:</p>\n<pre>from transformers import pipeline\n\npipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base')\n\npipe('The dog DNA test said my dog is almost half American pit[MASK] terrier.')</pre>\n<p>This returns the token <code>' bull'</code> with 81% probability, to\nfinish the sentence as: “The dog DNA test said my dog is almost half\nAmerican pit bull terrier.” Right behind it, at about 19% probably, is\n<code>'bull'</code>, which would finish the sentence as: “The dog DNA\ntest said my dog is almost half American pitbull terrier.”</p>\n<p>We can use this to our advantage by creating a prompt that adds a\n<code>[MASK]</code> where we want the coding of the text to be. First,\nlet’s prep our script and define an empty column where we’re going to\nput the classifications in:</p>\n<pre># prep -------------------------------------------------------------------------\nfrom transformers import pipeline\nimport pandas as pd\n\ndat = pd.read_csv('ratings_coded.csv')\ndat = dat[dat['visual_style'] != 9]\ndat['masked_class'] = ''</pre>\n<p>Then we loop all of the reviews through a fill-mask prompt, get the\npredicted tokens, and save the result back out to a .csv (even though\nLLMs require Python scripting, I still very much prefer to do\ninteractive analysis in R):</p>\n<pre># fill mask --------------------------------------------------------------------\npipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base')\n\nfor i in dat.index:\n  review = dat['text'][i].lower()\n  \n  prompt = f'''this is a movie review: \n  question: did the review discuss the visual style of the film, yes or no?\n  answer:[MASK]'''\n  \n  out = pipe(prompt)\n  \n  tmp = out[0]['token_str'].strip().lower()\n  \n  if tmp not in ['yes', 'no']:\n      tmp = out[1]['token_str'].strip().lower()\n  \n  dat.loc[i, 'masked_class'] = tmp\n\ndat.to_csv('ratings_masked.csv')</pre>\n<p>I prompt it with a format of three lines, each defining something by\na colon. I give it the movie review, I ask if the movie review discussed\nthe visual style of the film, and then I ask for an answer and\n<code>[MASK]</code> after the last colon. (There’s a little\n<code>if</code> statement in there because one of the reviews did not\nreturn “yes” or “no” as the top predicted result.)</p>\n<p>The benefit here is that this requires no hand-coding of the data\nbeforehand. The downside is that we haven’t told it about what makes Wes\nAnderson’s style distinct, which would help improve classification.\n(We’ll see how to do that with a chat model below.)</p>\n<p>\n</p><h3>Embeddings</h3>\n<p>While we can hack around the encoding-only model’s inability to\ngenerate text by asking it to produce the next token in a mask, what is\nreally powerful here are the text embeddings. I am using the\n<code>base</code> version of the model, which will take a sentence and\ntransform it into 768 numbers (see Table 4 at <a href=\"https://arxiv.org/pdf/2412.13663\" rel=\"nofollow\" target=\"_blank\">the arxiv paper</a>). What we’re\ngoing to do is create a matrix where there are 720 rows (one for each\nreview) and 768 numbers (the dimensionality of the numeric embeddings,\nwhich will take into consideration how language is naturally\nstructured):</p>\n<pre># prep -------------------------------------------------------------------------\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\n\ndat = pd.read_csv('ratings_coded.csv')\ntext = dat['text']\n\n# get embeddings ---------------------------------------------------------------\nmodel = SentenceTransformer('answerdotai/ModernBERT-base')\nembeddings = model.encode(text)\n\ndat_embeddings = pd.DataFrame(embeddings).add_prefix('embed_')\ndat_out = pd.concat([dat, dat_embeddings], axis=1)\n\ndat_out.to_csv('ratings_embedded.csv', index=False)</pre>\n<p>If you go back to the bottom of my <a href=\"https://www.markhw.com/blog/supervisedtext\" rel=\"nofollow\" target=\"_blank\">original post</a>,\nthis is MUCH less code than the different pre-processors I had\ndefined.</p>\n<p>\n</p><h2>Generative Modeling</h2>\n<p>I sang the praises of encoding models above, but we could use a\ngenerative model here, too. One of the reasons I like ModernBERT is that\nyou can download it onto your own machine or server. You don’t have to\nsend a third-party your data, which could be proprietary or contain\nsensitive information. This is something that any organization should\nhave a policy for; the tools are easy enough to access now that it is\ntoo easy to mindlessly feed massive amounts of data into a server that\nis not yours. (You can certainly also download generative instruct or\nchat models from <a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/\" rel=\"nofollow\" target=\"_blank\">HuggingFace</a>,\nas well, but the state-of-the-art models tend to be not open source or\ntoo large or require too much computational power to run on a MacBook\nAir like I’m working on now.)</p>\n<p>In this case, the reviews are already public. I wanted to try\nzero-shot learning, where we do not have to label any cases ahead of\ntime or give the model any examples. I also wanted to be extra lazy and\nnot even come up with the coding criteria on my own, either. In my\noriginal post, I defined the coding scheme as such:</p>\n<blockquote>\n<p>This visual style, by my eye, was solidified by the time of The Grand\nBudapest Hotel. Symmetrical framing, meticulously organized sets, the\nuse of miniatures, a pastel color palette contrasted with saturated\nprimary colors, distinctive costumes, straight lines, lateral tracking\nshots, whip pans, and so on. However, I did not consider aspects of\nAnderson’s style that are unrelated to the visual sense. This is where\ndefining the themes with a clear line matters—and often there will be\nambiguities, but one must do their best, because the process we’re doing\nis fundamentally a simplification of the rich diversity of the text.\nThus, I did not consider the following to be in Anderson’s visual style:\nstories involving precocious children, fraught familial relations,\nuncommon friendships, dry humor, monotonous dialogue, soundtracks\nusually involving The Kinks, a fascination with stage productions,\nnesting doll narratives, or a decidedly twee yet bittersweet tone.</p>\n</blockquote>\n<p>After prepping my session, I ask GPT-4o mini to give me a brief\ndescription of Wes Anderson’s visual style:</p>\n<pre># prep -------------------------------------------------------------------------\nimport re\nimport pandas as pd\nfrom openai import OpenAI\n\nAPI_KEY='&lt;your_api_key&gt;'\nmodel='gpt-4o-mini'\nclient = OpenAI(api_key=API_KEY)\n\n# get description --------------------------------------------------------------\ndescription = client.chat.completions.create(\n  model=model, \n  messages=[\n    {\n      'role': 'user', \n      'content': 'Provide a brief description of director Wes Anderson\\'s visual style.'\n    }\n  ],\n  temperature=0\n)\n\ndescription = description.choices[0].message.content\n\nwith open('description.txt', 'w') as file:\n  file.write(description)\n\nwith open('description.txt', 'r') as file:\n  description = file.read()</pre>\n<p>Here is what <code>description.txt</code> says:</p>\n<blockquote>\n<p>Wes Anderson’s visual style is characterized by its meticulous\nsymmetry, vibrant color palettes, and whimsical, storybook-like\naesthetics. He often employs a distinctive use of wide-angle lenses,\nwhich creates a flat, two-dimensional look that enhances the surreal\nquality of his films. Anderson’s compositions are carefully arranged,\nwith a focus on geometric shapes and patterns, often featuring centered\nframing and balanced scenes. His sets are richly detailed, filled with\nquirky props and vintage elements that contribute to a nostalgic\natmosphere. Additionally, he frequently uses stop-motion animation and\nunique transitions, further emphasizing his playful and imaginative\nstorytelling approach. Overall, Anderson’s style is instantly\nrecognizable and evokes a sense of charm and artistry.</p>\n</blockquote>\n<p>It reads generic, but it’ll work for instructing GPT. Now, let’s add\nthat description into a prompt:</p>\n<pre># define coding fn -------------------------------------------------------------\njob = '''You are a helpful research assistant who is classifying movie reviews.\nYour job is to determine if a movie review discusses Wes Anderson's unique visual style.\n'''\n\ndirections = '''\nThe user will provide a review, and you are to respond with one number only.\nIf the review discusses Wes Anderson's visual style at all, reply 1. If it does not, reply 0.\nDo not give any commentary.'''\n\ndef get_code(review):\n  messages = [\n      {\n          'role': 'system',\n          'content': job + description + directions\n      },\n      {   \n          'role': 'user',\n          'content': review\n      },\n  ]\n  \n  response = client.chat.completions.create(\n    model=model, \n    messages=messages,\n    temperature=0\n  )\n    \n  return(response.choices[0].message.content)</pre>\n<p>Note that the <code>content</code> we’re giving as a static\ninstruction to the system is the job it is supposed to do, the\ndescription of Wes Anderson’s style, and then directions for formatting.\nWe now load the data in and again run all the pieces of text through the\ncoding function we just defined:</p>\n<pre># code text --------------------------------------------------------------------\ndat = pd.read_csv('ratings_coded.csv')\ndat['gen_class'] = ''\n\nfor i in dat.index:\n  dat.loc[i, 'gen_class'] = get_code(dat['text'][i])\n\ndat.to_csv('ratings_generative.csv')</pre>\n<p>Note that the use of GPT-4o mini via OpenAI’s API cost me $0.03. I do\nbelieve that <a href=\"https://www.wired.com/story/tiktok-platforms-cory-doctorow/\" rel=\"nofollow\" target=\"_blank\">enshittification</a>\ncomes for all platforms eventually, so I don’t expect it to always be\nthis cheap, regardless of what OpenAI says now. That is why open source\nmodels found on brilliant websites like HuggingFace are vital.</p>\n<p>\n</p><h2>Results</h2>\n<p>How did each approach do? Let’s bring the results into R and check it\nout. First, let’s prep the session:</p>\n<pre># prep -------------------------------------------------------------------------\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(glmnet)\nlibrary(tidyverse)\n\nms &lt;- metric_set(accuracy, sensitivity, specificity, precision)</pre>\n<p>And let’s take a look at fill-mask:</p>\n<pre># fill mask --------------------------------------------------------------------\ndat_mask &lt;- read_csv(\"ratings_masked.csv\") %&gt;% \n  select(-1) %&gt;% # I always forget to set index=False\n  filter(visual_style != 9) %&gt;% # not in English\n  mutate(\n    visual_style = factor(visual_style),\n    masked_class = factor(ifelse(masked_class == \"yes\", 1, 0))\n  )\n\nms(dat_mask, truth = visual_style, estimate = masked_class)\n## # A tibble: 4 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy    binary         0.499\n## 2 sensitivity binary         0.552\n## 3 specificity binary         0.348\n## 4 precision   binary         0.708\nwith(dat_mask, prop.table(table(visual_style)))\n## visual_style\n##         0         1 \n## 0.7414075 0.2585925\nwith(dat_mask, prop.table(table(masked_class)))\n## masked_class\n##         0         1 \n## 0.5777414 0.4222586</pre>\n<p>Not great, but we tried zero-shot (i.e., no examples or supervised\nlearning) prompting without giving it a definition of Wes Anderson’s\nstyle. What we really care about is how we predicted 42% of the reviews\nsaid something about Anderson’s visual style, when in reality only 26%\ndid. This is much worse than the 7-point error I found in my original\npost with bag-of-words supervised modeling.</p>\n<p>What I’m really excited about are the embeddings. Theoretically, I\ncould define an entire cross-validation pipeline to try a variety of\nmodels and hyper-parameters to find the best way to model the numeric\nembeddings from ModernBERT. Instead, I’m just going to do a LASSO using\n because that method and that package are both\nfantastic. Note, though, that this is a strictly additive model; no\ninteractions are specified.</p>\n<pre># embeddings -------------------------------------------------------------------\ndat_embed &lt;- read_csv(\"ratings_embedded.csv\") %&gt;% \n  filter(visual_style != 9) # not in English\n\nset.seed(1839)\ndat_embed &lt;- dat_embed %&gt;%\n  initial_split(.5, strata = visual_style)\n\nX &lt;- dat_embed %&gt;% \n  training() %&gt;% \n  select(starts_with(\"embed_\")) %&gt;% \n  as.matrix()\n\ny &lt;- dat_embed %&gt;% \n  training() %&gt;% \n  pull(visual_style)\n\nmod &lt;- cv.glmnet(X, y, family = \"binomial\")\n\nX_test &lt;- dat_embed %&gt;% \n  testing() %&gt;% \n  select(starts_with(\"embed_\")) %&gt;% \n  as.matrix()\n\npreds &lt;- predict(mod, X_test, s = mod$lambda.min, type = \"response\")\n\npreds &lt;- tibble(\n  y = factor(testing(dat_embed)$visual_style),\n  y_hat_prob = c(preds),\n  y_hat_class = factor(as.numeric(y_hat_prob &gt; .5))\n)\n\nms(preds, truth = y, estimate = y_hat_class)\n## # A tibble: 4 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy    binary         0.758\n## 2 sensitivity binary         0.877\n## 3 specificity binary         0.418\n## 4 precision   binary         0.812\nwith(preds, prop.table(table(y)))\n## y\n##         0         1 \n## 0.7418301 0.2581699\nmean(preds$y_hat_prob)\n## [1] 0.2476165</pre>\n<p>Since this is supervised learning, I had to cut it up into training\nand testing. I just did half: 50% reviews for training, 50% reviews for\ntesting. And I stratified on the outcome variable so that they’d be\nequally distributed across training and testing.</p>\n<p>A very simple model gave us incredible results. If we get the mean of\nthe probabilities, we’re at 25% predicted and 26% actual. This is a\none-point error, much better than a way more computationally intensive\nand way lengthier script from my original post. <strong>This is a\nsimplified version of how I would start to set up a text classification\npipeline, if I still managed one</strong>.</p>\n<p>What about the generative model, which didn’t require any people to\nhand-code data, and it didn’t even require any people to define the\ncoding criteria?</p>\n<pre># generative -------------------------------------------------------------------\ndat_gen &lt;- read_csv(\"ratings_generative.csv\") %&gt;% \n  select(-1) %&gt;% # I always forget to set index=False\n  filter(visual_style != 9) %&gt;% # not in English\n  mutate(\n    visual_style = factor(visual_style),\n    gen_class = factor(gen_class)\n  )\n\nms(dat_gen, truth = visual_style, estimate = gen_class)\n## # A tibble: 4 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy    binary         0.841\n## 2 sensitivity binary         0.870\n## 3 specificity binary         0.759\n## 4 precision   binary         0.912\nwith(dat_gen, prop.table(table(visual_style)))\n## visual_style\n##         0         1 \n## 0.7414075 0.2585925\nwith(dat_gen, prop.table(table(gen_class)))\n## gen_class\n##         0         1 \n## 0.7070376 0.2929624</pre>\n<p>This result is also impressive, with 29% predicted to be about the\nvisual style when it was 26% in reality, for a three-point error. Is the\nreduced human effort of no longer needing to hand-code text worth a\nlittle bit worse error than the embeddings in a LASSO? And is it worth\nthe money it could cost to do this at a large scale using OpenAI?</p>\n<p>\n</p><h2>Closing Thoughts</h2>\n<p>This was a very quick look at ModernBERT especially, because I’m\nexcited about the quality of these embeddings. And my thinking about how\nto process text has changed in the last year or two. There’s so much\nelse one could do here, such as fine-tuning ModernBERT or GPT-4o mini\nfor movie review classification. (I imagine there will be many\nfine-tuned ModernBERT models uploaded to HuggingFace shortly for a\nnumber of use-cases.)</p>\n<p>Classifying text without hand-coded labels is going to be more\nchallenging in environments where the domain is not as widely discussed\non the internet as movie reviews, cinematography, and Wes Anderson. Your\ncustomers or users could have specific concerns and use specific\nlanguage, and even providing a lengthy coding scheme to a generative\nmodel might not cut it. At any rate, you would want to hand-label a test\nset anyways. It’s important that you, as a researcher, actually read the\ntext and that there is always a human in the loop.</p>\n<p>What I’m most excited about here is being able to take survey\nresponses <em>plus text embeddings from open-ended questions</em> and\nusing them together in a machine learning model to predict attitudes or\nbehaviors.</p>\n<p class=\"\">See <a href=\"https://github.com/markhwhiteii/blog/tree/master/topic_model_2\" rel=\"nofollow\" target=\"_blank\">my GitHub</a> for code.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.markhw.com/blog/topicmodeling2\"> Mark H. White II, PhD</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Rethinking How I Do Supervised Topic Modeling, Using ModernBERT and GPT-4o mini\nPosted on\nDecember 21, 2024\nby\nMark White\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nMark H. White II, PhD\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nI wrote a\npost\nin July 2023\ndescribing my process for building a supervised text classification\npipeline. In short, the process first involves reading the text, writing\na thematic content coding guide, and having humans label text. Then, I\ndefine a variety of ways to pre-process text (e.g., word\nvs. word-and-bigram tokenizing, stemming vs. not, stop words vs. not,\nfiltering on the number of times a word had to appear in the corpus) in\na  workflowset. Then, I run these different\npre-processors through different standard models: elastic net, XGBoost,\nrandom forest, etc. Each class of text has its own model, so I would run\nthis pipeline five times if there were five topics in the text.\nImportantly, this is not natural language processing (NLP), as it was a\nbag-of-words approach.\nThe idea was to leverage the domain knowledge of the experts on my\nteam through content coding, and then scaling it up using a machine\nlearning pipeline. In the post, I bemoaned how most of the “NLP” or\n“AI-driven” tools I had tested did not do very well. The tools I was\nthinking of were all web-based, point-and-click applications that I had\ntried out since about 2018, and they usually were unsupervised.\nWe are in a wildly different environment now when it comes to\nanalyzing text than we were even a few years ago. I am revisiting that\npost to explore alternate routes to classifying text. I will use the\nsame data as I did in that post: 720 Letterboxd reviews of Wes\nAnderson’s film\nAsteroid City.\nThere is only one code: Did the\nreview discuss Wes Anderson’s unique visual style (1) or not (0)? I\nhand-labeled all of these on one afternoon to give me a supervised\ndataset to play with.\nThe use case I had in mind was from a previous job, where the goal\nwas not to predict if an\nindividual\ndiscussed a topic or not.\nInstead, the focus was on, “What percentage of people mentioned this\ntopic this month?” in a tracking survey. The primary way I’ll judge\nthese approaches is by seeing how far away the predicted percent of\nreviews discussing visual style is from the actual percent of reviews\ndiscussing visual style. That is: I am interested in estimates of the\naggregate\n.\nFor a baseline, my bag-of-words supervised classification pipeline\npredicted 19% when it was actually 26% in the testing set, for an\nabsolute error of 7 points.\nModernBERT\nTwo days ago, researchers at Answer.AI and LightOn introduced\nModernBERT\n. I am\nexcited about this, as\nBERT\nis one of my\nfavorite foundation large language models (LLMs). While most of the hype\nmachine focuses on\ngenerative\nAI, the tools like ChatGPT that\ncan create text, I think one of the most useful applications of LLMs are\nthe\nencoder-only\nmodels whose job it is to instead\nrepresent\ntext. As a data scientist and survey researcher, I’ve\nfound this genre of model to be much more useful to the work I do. The\nupshot is that the transformer model architecture with attention allows\nthe model to transform text into a series of numeric columns that\nrepresent it. The text isn’t just fed into the model, but also the\nposition of each piece of text. Additionally, the “B” in “BERT” is\n“bidirectional,” allowing it to learn from context—it can look at words\nbefore\nand\nafter\neach word to help in building a\nnumerical representation. What this all means is that it can figure out\nthings like synonyms and homonyms. Related to the current example of Wes\nAnderson’s visual style, consider a review that contains the phrase:\n“The film’s color palette is bright and warm.” Consider another that\nmight say: “The screenplay isn’t as bright as he thinks it is.” A\nbag-of-words approach is going to have a difficult time here with the\nword “bright”: It is used both in the context of discussing the visuals\nof the film and in a review that doesn’t discuss the visuals of the\nfilm. ModernBERT—the exciting “replacement” (we’ll see if that proves\ntrue) to the existing BERT model variants—\ncan\ntell the\ndifference. That difference will be represented in different word\nembeddings.\nThere are two ways I experimented with ModernBERT to see if it could\noutperform my previous approach: fill-masking and embeddings.\nFill-Masking\nOne of the steps of training BERT is through masking. I’m butchering\nthe process, but the idea is to take a sentence, mask a token (which\ncould be a word or sub-word), and train it to predict the correct token.\nFor example, in the code below, I load in ModernBERT and have it predict\na masked token from a very obvious example:\nfrom transformers import pipeline\n\npipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base')\n\npipe('The dog DNA test said my dog is almost half American pit[MASK] terrier.')\nThis returns the token\n' bull'\nwith 81% probability, to\nfinish the sentence as: “The dog DNA test said my dog is almost half\nAmerican pit bull terrier.” Right behind it, at about 19% probably, is\n'bull'\n, which would finish the sentence as: “The dog DNA\ntest said my dog is almost half American pitbull terrier.”\nWe can use this to our advantage by creating a prompt that adds a\n[MASK]\nwhere we want the coding of the text to be. First,\nlet’s prep our script and define an empty column where we’re going to\nput the classifications in:\n# prep -------------------------------------------------------------------------\nfrom transformers import pipeline\nimport pandas as pd\n\ndat = pd.read_csv('ratings_coded.csv')\ndat = dat[dat['visual_style'] != 9]\ndat['masked_class'] = ''\nThen we loop all of the reviews through a fill-mask prompt, get the\npredicted tokens, and save the result back out to a .csv (even though\nLLMs require Python scripting, I still very much prefer to do\ninteractive analysis in R):\n# fill mask --------------------------------------------------------------------\npipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base')\n\nfor i in dat.index:\n  review = dat['text'][i].lower()\n  \n  prompt = f'''this is a movie review: \n  question: did the review discuss the visual style of the film, yes or no?\n  answer:[MASK]'''\n  \n  out = pipe(prompt)\n  \n  tmp = out[0]['token_str'].strip().lower()\n  \n  if tmp not in ['yes', 'no']:\n      tmp = out[1]['token_str'].strip().lower()\n  \n  dat.loc[i, 'masked_class'] = tmp\n\ndat.to_csv('ratings_masked.csv')\nI prompt it with a format of three lines, each defining something by\na colon. I give it the movie review, I ask if the movie review discussed\nthe visual style of the film, and then I ask for an answer and\n[MASK]\nafter the last colon. (There’s a little\nif\nstatement in there because one of the reviews did not\nreturn “yes” or “no” as the top predicted result.)\nThe benefit here is that this requires no hand-coding of the data\nbeforehand. The downside is that we haven’t told it about what makes Wes\nAnderson’s style distinct, which would help improve classification.\n(We’ll see how to do that with a chat model below.)\nEmbeddings\nWhile we can hack around the encoding-only model’s inability to\ngenerate text by asking it to produce the next token in a mask, what is\nreally powerful here are the text embeddings. I am using the\nbase\nversion of the model, which will take a sentence and\ntransform it into 768 numbers (see Table 4 at\nthe arxiv paper\n). What we’re\ngoing to do is create a matrix where there are 720 rows (one for each\nreview) and 768 numbers (the dimensionality of the numeric embeddings,\nwhich will take into consideration how language is naturally\nstructured):\n# prep -------------------------------------------------------------------------\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\n\ndat = pd.read_csv('ratings_coded.csv')\ntext = dat['text']\n\n# get embeddings ---------------------------------------------------------------\nmodel = SentenceTransformer('answerdotai/ModernBERT-base')\nembeddings = model.encode(text)\n\ndat_embeddings = pd.DataFrame(embeddings).add_prefix('embed_')\ndat_out = pd.concat([dat, dat_embeddings], axis=1)\n\ndat_out.to_csv('ratings_embedded.csv', index=False)\nIf you go back to the bottom of my\noriginal post\n,\nthis is MUCH less code than the different pre-processors I had\ndefined.\nGenerative Modeling\nI sang the praises of encoding models above, but we could use a\ngenerative model here, too. One of the reasons I like ModernBERT is that\nyou can download it onto your own machine or server. You don’t have to\nsend a third-party your data, which could be proprietary or contain\nsensitive information. This is something that any organization should\nhave a policy for; the tools are easy enough to access now that it is\ntoo easy to mindlessly feed massive amounts of data into a server that\nis not yours. (You can certainly also download generative instruct or\nchat models from\nHuggingFace\n,\nas well, but the state-of-the-art models tend to be not open source or\ntoo large or require too much computational power to run on a MacBook\nAir like I’m working on now.)\nIn this case, the reviews are already public. I wanted to try\nzero-shot learning, where we do not have to label any cases ahead of\ntime or give the model any examples. I also wanted to be extra lazy and\nnot even come up with the coding criteria on my own, either. In my\noriginal post, I defined the coding scheme as such:\nThis visual style, by my eye, was solidified by the time of The Grand\nBudapest Hotel. Symmetrical framing, meticulously organized sets, the\nuse of miniatures, a pastel color palette contrasted with saturated\nprimary colors, distinctive costumes, straight lines, lateral tracking\nshots, whip pans, and so on. However, I did not consider aspects of\nAnderson’s style that are unrelated to the visual sense. This is where\ndefining the themes with a clear line matters—and often there will be\nambiguities, but one must do their best, because the process we’re doing\nis fundamentally a simplification of the rich diversity of the text.\nThus, I did not consider the following to be in Anderson’s visual style:\nstories involving precocious children, fraught familial relations,\nuncommon friendships, dry humor, monotonous dialogue, soundtracks\nusually involving The Kinks, a fascination with stage productions,\nnesting doll narratives, or a decidedly twee yet bittersweet tone.\nAfter prepping my session, I ask GPT-4o mini to give me a brief\ndescription of Wes Anderson’s visual style:\n# prep -------------------------------------------------------------------------\nimport re\nimport pandas as pd\nfrom openai import OpenAI\n\nAPI_KEY='<your_api_key>'\nmodel='gpt-4o-mini'\nclient = OpenAI(api_key=API_KEY)\n\n# get description --------------------------------------------------------------\ndescription = client.chat.completions.create(\n  model=model, \n  messages=[\n    {\n      'role': 'user', \n      'content': 'Provide a brief description of director Wes Anderson\\'s visual style.'\n    }\n  ],\n  temperature=0\n)\n\ndescription = description.choices[0].message.content\n\nwith open('description.txt', 'w') as file:\n  file.write(description)\n\nwith open('description.txt', 'r') as file:\n  description = file.read()\nHere is what\ndescription.txt\nsays:\nWes Anderson’s visual style is characterized by its meticulous\nsymmetry, vibrant color palettes, and whimsical, storybook-like\naesthetics. He often employs a distinctive use of wide-angle lenses,\nwhich creates a flat, two-dimensional look that enhances the surreal\nquality of his films. Anderson’s compositions are carefully arranged,\nwith a focus on geometric shapes and patterns, often featuring centered\nframing and balanced scenes. His sets are richly detailed, filled with\nquirky props and vintage elements that contribute to a nostalgic\natmosphere. Additionally, he frequently uses stop-motion animation and\nunique transitions, further emphasizing his playful and imaginative\nstorytelling approach. Overall, Anderson’s style is instantly\nrecognizable and evokes a sense of charm and artistry.\nIt reads generic, but it’ll work for instructing GPT. Now, let’s add\nthat description into a prompt:\n# define coding fn -------------------------------------------------------------\njob = '''You are a helpful research assistant who is classifying movie reviews.\nYour job is to determine if a movie review discusses Wes Anderson's unique visual style.\n'''\n\ndirections = '''\nThe user will provide a review, and you are to respond with one number only.\nIf the review discusses Wes Anderson's visual style at all, reply 1. If it does not, reply 0.\nDo not give any commentary.'''\n\ndef get_code(review):\n  messages = [\n      {\n          'role': 'system',\n          'content': job + description + directions\n      },\n      {   \n          'role': 'user',\n          'content': review\n      },\n  ]\n  \n  response = client.chat.completions.create(\n    model=model, \n    messages=messages,\n    temperature=0\n  )\n    \n  return(response.choices[0].message.content)\nNote that the\ncontent\nwe’re giving as a static\ninstruction to the system is the job it is supposed to do, the\ndescription of Wes Anderson’s style, and then directions for formatting.\nWe now load the data in and again run all the pieces of text through the\ncoding function we just defined:\n# code text --------------------------------------------------------------------\ndat = pd.read_csv('ratings_coded.csv')\ndat['gen_class'] = ''\n\nfor i in dat.index:\n  dat.loc[i, 'gen_class'] = get_code(dat['text'][i])\n\ndat.to_csv('ratings_generative.csv')\nNote that the use of GPT-4o mini via OpenAI’s API cost me $0.03. I do\nbelieve that\nenshittification\ncomes for all platforms eventually, so I don’t expect it to always be\nthis cheap, regardless of what OpenAI says now. That is why open source\nmodels found on brilliant websites like HuggingFace are vital.\nResults\nHow did each approach do? Let’s bring the results into R and check it\nout. First, let’s prep the session:\n# prep -------------------------------------------------------------------------\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(glmnet)\nlibrary(tidyverse)\n\nms <- metric_set(accuracy, sensitivity, specificity, precision)\nAnd let’s take a look at fill-mask:\n# fill mask --------------------------------------------------------------------\ndat_mask <- read_csv(\"ratings_masked.csv\") %>% \n  select(-1) %>% # I always forget to set index=False\n  filter(visual_style != 9) %>% # not in English\n  mutate(\n    visual_style = factor(visual_style),\n    masked_class = factor(ifelse(masked_class == \"yes\", 1, 0))\n  )\n\nms(dat_mask, truth = visual_style, estimate = masked_class)\n## # A tibble: 4 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 accuracy    binary         0.499\n## 2 sensitivity binary         0.552\n## 3 specificity binary         0.348\n## 4 precision   binary         0.708\nwith(dat_mask, prop.table(table(visual_style)))\n## visual_style\n##         0         1 \n## 0.7414075 0.2585925\nwith(dat_mask, prop.table(table(masked_class)))\n## masked_class\n##         0         1 \n## 0.5777414 0.4222586\nNot great, but we tried zero-shot (i.e., no examples or supervised\nlearning) prompting without giving it a definition of Wes Anderson’s\nstyle. What we really care about is how we predicted 42% of the reviews\nsaid something about Anderson’s visual style, when in reality only 26%\ndid. This is much worse than the 7-point error I found in my original\npost with bag-of-words supervised modeling.\nWhat I’m really excited about are the embeddings. Theoretically, I\ncould define an entire cross-validation pipeline to try a variety of\nmodels and hyper-parameters to find the best way to model the numeric\nembeddings from ModernBERT. Instead, I’m just going to do a LASSO using\n because that method and that package are both\nfantastic. Note, though, that this is a strictly additive model; no\ninteractions are specified.\n# embeddings -------------------------------------------------------------------\ndat_embed <- read_csv(\"ratings_embedded.csv\") %>% \n  filter(visual_style != 9) # not in English\n\nset.seed(1839)\ndat_embed <- dat_embed %>%\n  initial_split(.5, strata = visual_style)\n\nX <- dat_embed %>% \n  training() %>% \n  select(starts_with(\"embed_\")) %>% \n  as.matrix()\n\ny <- dat_embed %>% \n  training() %>% \n  pull(visual_style)\n\nmod <- cv.glmnet(X, y, family = \"binomial\")\n\nX_test <- dat_embed %>% \n  testing() %>% \n  select(starts_with(\"embed_\")) %>% \n  as.matrix()\n\npreds <- predict(mod, X_test, s = mod$lambda.min, type = \"response\")\n\npreds <- tibble(\n  y = factor(testing(dat_embed)$visual_style),\n  y_hat_prob = c(preds),\n  y_hat_class = factor(as.numeric(y_hat_prob > .5))\n)\n\nms(preds, truth = y, estimate = y_hat_class)\n## # A tibble: 4 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 accuracy    binary         0.758\n## 2 sensitivity binary         0.877\n## 3 specificity binary         0.418\n## 4 precision   binary         0.812\nwith(preds, prop.table(table(y)))\n## y\n##         0         1 \n## 0.7418301 0.2581699\nmean(preds$y_hat_prob)\n## [1] 0.2476165\nSince this is supervised learning, I had to cut it up into training\nand testing. I just did half: 50% reviews for training, 50% reviews for\ntesting. And I stratified on the outcome variable so that they’d be\nequally distributed across training and testing.\nA very simple model gave us incredible results. If we get the mean of\nthe probabilities, we’re at 25% predicted and 26% actual. This is a\none-point error, much better than a way more computationally intensive\nand way lengthier script from my original post.\nThis is a\nsimplified version of how I would start to set up a text classification\npipeline, if I still managed one\n.\nWhat about the generative model, which didn’t require any people to\nhand-code data, and it didn’t even require any people to define the\ncoding criteria?\n# generative -------------------------------------------------------------------\ndat_gen <- read_csv(\"ratings_generative.csv\") %>% \n  select(-1) %>% # I always forget to set index=False\n  filter(visual_style != 9) %>% # not in English\n  mutate(\n    visual_style = factor(visual_style),\n    gen_class = factor(gen_class)\n  )\n\nms(dat_gen, truth = visual_style, estimate = gen_class)\n## # A tibble: 4 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 accuracy    binary         0.841\n## 2 sensitivity binary         0.870\n## 3 specificity binary         0.759\n## 4 precision   binary         0.912\nwith(dat_gen, prop.table(table(visual_style)))\n## visual_style\n##         0         1 \n## 0.7414075 0.2585925\nwith(dat_gen, prop.table(table(gen_class)))\n## gen_class\n##         0         1 \n## 0.7070376 0.2929624\nThis result is also impressive, with 29% predicted to be about the\nvisual style when it was 26% in reality, for a three-point error. Is the\nreduced human effort of no longer needing to hand-code text worth a\nlittle bit worse error than the embeddings in a LASSO? And is it worth\nthe money it could cost to do this at a large scale using OpenAI?\nClosing Thoughts\nThis was a very quick look at ModernBERT especially, because I’m\nexcited about the quality of these embeddings. And my thinking about how\nto process text has changed in the last year or two. There’s so much\nelse one could do here, such as fine-tuning ModernBERT or GPT-4o mini\nfor movie review classification. (I imagine there will be many\nfine-tuned ModernBERT models uploaded to HuggingFace shortly for a\nnumber of use-cases.)\nClassifying text without hand-coded labels is going to be more\nchallenging in environments where the domain is not as widely discussed\non the internet as movie reviews, cinematography, and Wes Anderson. Your\ncustomers or users could have specific concerns and use specific\nlanguage, and even providing a lengthy coding scheme to a generative\nmodel might not cut it. At any rate, you would want to hand-label a test\nset anyways. It’s important that you, as a researcher, actually read the\ntext and that there is always a human in the loop.\nWhat I’m most excited about here is being able to take survey\nresponses\nplus text embeddings from open-ended questions\nand\nusing them together in a machine learning model to predict attitudes or\nbehaviors.\nSee\nmy GitHub\nfor code.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nMark H. White II, PhD\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "I wrote a post in July 2023 describing my process for building a supervised text classification pipeline. In short, the process first involves reading the text, writing a thematic content coding guide, and having humans label text. Then, I define a variety of ways to pre-process text (e.g., word vs. word-and-bigram tokenizing, stemming vs. not, stop words vs. not, filtering on the number of times a word had to appear in the corpus) in a workflowset. Then, I run these different pre-processors through different standard models: elastic net, XGBoost, random forest, etc. Each class of text has its own model, so I would run this pipeline five times if there were five topics in the text. Importantly, this is not natural language processing (NLP), as it was a bag-of-words approach. The idea was to leverage the domain knowledge of the experts on my team through content coding, and then scaling it up using a machine learning pipeline. In the post, I bemoaned how most of the “NLP” or “AI-driven” tools I had tested did not do very well. The tools I was thinking of were all web-based, point-and-click applications that I had tried out since about 2018, and they usually were unsupervised. We are in a wildly different environment now when it comes to analyzing text than we were even a few years ago. I am revisiting that post to explore alternate routes to classifying text. I will use the same data as I did in that post: 720 Letterboxd reviews of Wes Anderson’s film Asteroid City. There is only one code: Did the review discuss Wes Anderson’s unique visual style (1) or not (0)? I hand-labeled all of these on one afternoon to give me a supervised dataset to play with. The use case I had in mind was from a previous job, where the goal was not to predict if an individual discussed a topic or not. Instead, the focus was on, “What percentage of people mentioned this topic this month?” in a tracking survey. The primary way I’ll judge these approaches is by seeing how far away the predicted percent of reviews discussing visual style is from the actual percent of reviews discussing visual style. That is: I am interested in estimates of the aggregate. For a baseline, my bag-of-words supervised classification pipeline predicted 19% when it was actually 26% in the testing set, for an absolute error of 7 points. ModernBERT Two days ago, researchers at Answer.AI and LightOn introduced ModernBERT. I am excited about this, as BERT is one of my favorite foundation large language models (LLMs). While most of the hype machine focuses on generative AI, the tools like ChatGPT that can create text, I think one of the most useful applications of LLMs are the encoder-only models whose job it is to instead represent text. As a data scientist and survey researcher, I’ve found this genre of model to be much more useful to the work I do. The upshot is that the transformer model architecture with attention allows the model to transform text into a series of numeric columns that represent it. The text isn’t just fed into the model, but also the position of each piece of text. Additionally, the “B” in “BERT” is “bidirectional,” allowing it to learn from context—it can look at words before and after each word to help in building a numerical representation. What this all means is that it can figure out things like synonyms and homonyms. Related to the current example of Wes Anderson’s visual style, consider a review that contains the phrase: “The film’s color palette is bright and warm.” Consider another that might say: “The screenplay isn’t as bright as he thinks it is.” A bag-of-words approach is going to have a difficult time here with the word “bright”: It is used both in the context of discussing the visuals of the film and in a review that doesn’t discuss the visuals of the film. ModernBERT—the exciting “replacement” (we’ll see if that proves true) to the existing BERT model variants—can tell the difference. That difference will be represented in different word embeddings. There are two ways I experimented with ModernBERT to see if it could outperform my previous approach: fill-masking and embeddings. Fill-Masking One of the steps of training BERT is through masking. I’m butchering the process, but the idea is to take a sentence, mask a token (which could be a word or sub-word), and train it to predict the correct token. For example, in the code below, I load in ModernBERT and have it predict a masked token from a very obvious example: from transformers import pipeline pipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base') pipe('The dog DNA test said my dog is almost half American pit[MASK] terrier.') This returns the token ' bull' with 81% probability, to finish the sentence as: “The dog DNA test said my dog is almost half American pit bull terrier.” Right behind it, at about 19% probably, is 'bull', which would finish the sentence as: “The dog DNA test said my dog is almost half American pitbull terrier.” We can use this to our advantage by creating a prompt that adds a [MASK] where we want the coding of the text to be. First, let’s prep our script and define an empty column where we’re going to put the classifications in: # prep ------------------------------------------------------------------------- from transformers import pipeline import pandas as pd dat = pd.read_csv('ratings_coded.csv') dat = dat[dat['visual_style'] != 9] dat['masked_class'] = '' Then we loop all of the reviews through a fill-mask prompt, get the predicted tokens, and save the result back out to a .csv (even though LLMs require Python scripting, I still very much prefer to do interactive analysis in R): # fill mask -------------------------------------------------------------------- pipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base') for i in dat.index: review = dat['text'][i].lower() prompt = f'''this is a movie review: question: did the review discuss the visual style of the film, yes or no? answer:[MASK]''' out = pipe(prompt) tmp = out[0]['token_str'].strip().lower() if tmp not in ['yes', 'no']: tmp = out[1]['token_str'].strip().lower() dat.loc[i, 'masked_class'] = tmp dat.to_csv('ratings_masked.csv') I prompt it with a format of three lines, each defining something by a colon. I give it the movie review, I ask if the movie review discussed the visual style of the film, and then I ask for an answer and [MASK] after the last colon. (There’s a little if statement in there because one of the reviews did not return “yes” or “no” as the top predicted result.) The benefit here is that this requires no hand-coding of the data beforehand. The downside is that we haven’t told it about what makes Wes Anderson’s style distinct, which would help improve classification. (We’ll see how to do that with a chat model below.) Embeddings While we can hack around the encoding-only model’s inability to generate text by asking it to produce the next token in a mask, what is really powerful here are the text embeddings. I am using the base version of the model, which will take a sentence and transform it into 768 numbers (see Table 4 at the arxiv paper). What we’re going to do is create a matrix where there are 720 rows (one for each review) and 768 numbers (the dimensionality of the numeric embeddings, which will take into consideration how language is naturally structured): # prep ------------------------------------------------------------------------- import pandas as pd from sentence_transformers import SentenceTransformer dat = pd.read_csv('ratings_coded.csv') text = dat['text'] # get embeddings --------------------------------------------------------------- model = SentenceTransformer('answerdotai/ModernBERT-base') embeddings = model.encode(text) dat_embeddings = pd.DataFrame(embeddings).add_prefix('embed_') dat_out = pd.concat([dat, dat_embeddings], axis=1) dat_out.to_csv('ratings_embedded.csv', index=False) If you go back to the bottom of my original post, this is MUCH less code than the different pre-processors I had defined. Generative Modeling I sang the praises of encoding models above, but we could use a generative model here, too. One of the reasons I like ModernBERT is that you can download it onto your own machine or server. You don’t have to send a third-party your data, which could be proprietary or contain sensitive information. This is something that any organization should have a policy for; the tools are easy enough to access now that it is too easy to mindlessly feed massive amounts of data into a server that is not yours. (You can certainly also download generative instruct or chat models from HuggingFace, as well, but the state-of-the-art models tend to be not open source or too large or require too much computational power to run on a MacBook Air like I’m working on now.) In this case, the reviews are already public. I wanted to try zero-shot learning, where we do not have to label any cases ahead of time or give the model any examples. I also wanted to be extra lazy and not even come up with the coding criteria on my own, either. In my original post, I defined the coding scheme as such: This visual style, by my eye, was solidified by the time of The Grand Budapest Hotel. Symmetrical framing, meticulously organized sets, the use of miniatures, a pastel color palette contrasted with saturated primary colors, distinctive costumes, straight lines, lateral tracking shots, whip pans, and so on. However, I did not consider aspects of Anderson’s style that are unrelated to the visual sense. This is where defining the themes with a clear line matters—and often there will be ambiguities, but one must do their best, because the process we’re doing is fundamentally a simplification of the rich diversity of the text. Thus, I did not consider the following to be in Anderson’s visual style: stories involving precocious children, fraught familial relations, uncommon friendships, dry humor, monotonous dialogue, soundtracks usually involving The Kinks, a fascination with stage productions, nesting doll narratives, or a decidedly twee yet bittersweet tone. After prepping my session, I ask GPT-4o mini to give me a brief description of Wes Anderson’s visual style: # prep ------------------------------------------------------------------------- import re import pandas as pd from openai import OpenAI API_KEY='' model='gpt-4o-mini' client = OpenAI(api_key=API_KEY) # get description -------------------------------------------------------------- description = client.chat.completions.create( model=model, messages=[ { 'role': 'user', 'content': 'Provide a brief description of director Wes Anderson\\'s visual style.' } ], temperature=0 ) description = description.choices[0].message.content with open('description.txt', 'w') as file: file.write(description) with open('description.txt', 'r') as file: description = file.read() Here is what description.txt says: Wes Anderson’s visual style is characterized by its meticulous symmetry, vibrant color palettes, and whimsical, storybook-like aesthetics. He often employs a distinctive use of wide-angle lenses, which creates a flat, two-dimensional look that enhances the surreal quality of his films. Anderson’s compositions are carefully arranged, with a focus on geometric shapes and patterns, often featuring centered framing and balanced scenes. His sets are richly detailed, filled with quirky props and vintage elements that contribute to a nostalgic atmosphere. Additionally, he frequently uses stop-motion animation and unique transitions, further emphasizing his playful and imaginative storytelling approach. Overall, Anderson’s style is instantly recognizable and evokes a sense of charm and artistry. It reads generic, but it’ll work for instructing GPT. Now, let’s add that description into a prompt: # define coding fn ------------------------------------------------------------- job = '''You are a helpful research assistant who is classifying movie reviews. Your job is to determine if a movie review discusses Wes Anderson's unique visual style. ''' directions = ''' The user will provide a review, and you are to respond with one number only. If the review discusses Wes Anderson's visual style at all, reply 1. If it does not, reply 0. Do not give any commentary.''' def get_code(review): messages = [ { 'role': 'system', 'content': job + description + directions }, { 'role': 'user', 'content': review }, ] response = client.chat.completions.create( model=model, messages=messages, temperature=0 ) return(response.choices[0].message.content) Note that the content we’re giving as a static instruction to the system is the job it is supposed to do, the description of Wes Anderson’s style, and then directions for formatting. We now load the data in and again run all the pieces of text through the coding function we just defined: # code text -------------------------------------------------------------------- dat = pd.read_csv('ratings_coded.csv') dat['gen_class'] = '' for i in dat.index: dat.loc[i, 'gen_class'] = get_code(dat['text'][i]) dat.to_csv('ratings_generative.csv') Note that the use of GPT-4o mini via OpenAI’s API cost me $0.03. I do believe that enshittification comes for all platforms eventually, so I don’t expect it to always be this cheap, regardless of what OpenAI says now. That is why open source models found on brilliant websites like HuggingFace are vital. Results How did each approach do? Let’s bring the results into R and check it out. First, let’s prep the session: # prep ------------------------------------------------------------------------- library(rsample) library(yardstick) library(glmnet) library(tidyverse) ms % # I always forget to set index=False filter(visual_style != 9) %>% # not in English mutate( visual_style = factor(visual_style), masked_class = factor(ifelse(masked_class == \"yes\", 1, 0)) ) ms(dat_mask, truth = visual_style, estimate = masked_class) ## # A tibble: 4 × 3 ## .metric .estimator .estimate ## ## 1 accuracy binary 0.499 ## 2 sensitivity binary 0.552 ## 3 specificity binary 0.348 ## 4 precision binary 0.708 with(dat_mask, prop.table(table(visual_style))) ## visual_style ## 0 1 ## 0.7414075 0.2585925 with(dat_mask, prop.table(table(masked_class))) ## masked_class ## 0 1 ## 0.5777414 0.4222586 Not great, but we tried zero-shot (i.e., no examples or supervised learning) prompting without giving it a definition of Wes Anderson’s style. What we really care about is how we predicted 42% of the reviews said something about Anderson’s visual style, when in reality only 26% did. This is much worse than the 7-point error I found in my original post with bag-of-words supervised modeling. What I’m really excited about are the embeddings. Theoretically, I could define an entire cross-validation pipeline to try a variety of models and hyper-parameters to find the best way to model the numeric embeddings from ModernBERT. Instead, I’m just going to do a LASSO using because that method and that package are both fantastic. Note, though, that this is a strictly additive model; no interactions are specified. # embeddings ------------------------------------------------------------------- dat_embed % filter(visual_style != 9) # not in English set.seed(1839) dat_embed % initial_split(.5, strata = visual_style) X % training() %>% select(starts_with(\"embed_\")) %>% as.matrix() y % training() %>% pull(visual_style) mod % select(starts_with(\"embed_\")) %>% as.matrix() preds % # I always forget to set index=False filter(visual_style != 9) %>% # not in English mutate( visual_style = factor(visual_style), gen_class = factor(gen_class) ) ms(dat_gen, truth = visual_style, estimate = gen_class) ## # A tibble: 4 × 3 ## .metric .estimator .estimate ## ## 1 accuracy binary 0.841 ## 2 sensitivity binary 0.870 ## 3 specificity binary 0.759 ## 4 precision binary 0.912 with(dat_gen, prop.table(table(visual_style))) ## visual_style ## 0 1 ## 0.7414075 0.2585925 with(dat_gen, prop.table(table(gen_class))) ## gen_class ## 0 1 ## 0.7070376 0.2929624 This result is also impressive, with 29% predicted to be about the visual style when it was 26% in reality, for a three-point error. Is the reduced human effort of no longer needing to hand-code text worth a little bit worse error than the embeddings in a LASSO? And is it worth the money it could cost to do this at a large scale using OpenAI? Closing Thoughts This was a very quick look at ModernBERT especially, because I’m excited about the quality of these embeddings. And my thinking about how to process text has changed in the last year or two. There’s so much else one could do here, such as fine-tuning ModernBERT or GPT-4o mini for movie review classification. (I imagine there will be many fine-tuned ModernBERT models uploaded to HuggingFace shortly for a number of use-cases.) Classifying text without hand-coded labels is going to be more challenging in environments where the domain is not as widely discussed on the internet as movie reviews, cinematography, and Wes Anderson. Your customers or users could have specific concerns and use specific language, and even providing a lengthy coding scheme to a generative model might not cut it. At any rate, you would want to hand-label a test set anyways. It’s important that you, as a researcher, actually read the text and that there is always a human in the loop. What I’m most excited about here is being able to take survey responses plus text embeddings from open-ended questions and using them together in a machine learning model to predict attitudes or behaviors. See my GitHub for code.",
    "meta_keywords": null,
    "og_description": "I wrote a post in July 2023 describing my process for building a supervised text classification pipeline. In short, the process first involves reading the text, writing a thematic content coding guide, and having humans label text. Then, I define a variety of ways to pre-process text (e.g., word vs. word-and-bigram tokenizing, stemming vs. not, stop words vs. not, filtering on the number of times a word had to appear in the corpus) in a workflowset. Then, I run these different pre-processors through different standard models: elastic net, XGBoost, random forest, etc. Each class of text has its own model, so I would run this pipeline five times if there were five topics in the text. Importantly, this is not natural language processing (NLP), as it was a bag-of-words approach. The idea was to leverage the domain knowledge of the experts on my team through content coding, and then scaling it up using a machine learning pipeline. In the post, I bemoaned how most of the “NLP” or “AI-driven” tools I had tested did not do very well. The tools I was thinking of were all web-based, point-and-click applications that I had tried out since about 2018, and they usually were unsupervised. We are in a wildly different environment now when it comes to analyzing text than we were even a few years ago. I am revisiting that post to explore alternate routes to classifying text. I will use the same data as I did in that post: 720 Letterboxd reviews of Wes Anderson’s film Asteroid City. There is only one code: Did the review discuss Wes Anderson’s unique visual style (1) or not (0)? I hand-labeled all of these on one afternoon to give me a supervised dataset to play with. The use case I had in mind was from a previous job, where the goal was not to predict if an individual discussed a topic or not. Instead, the focus was on, “What percentage of people mentioned this topic this month?” in a tracking survey. The primary way I’ll judge these approaches is by seeing how far away the predicted percent of reviews discussing visual style is from the actual percent of reviews discussing visual style. That is: I am interested in estimates of the aggregate. For a baseline, my bag-of-words supervised classification pipeline predicted 19% when it was actually 26% in the testing set, for an absolute error of 7 points. ModernBERT Two days ago, researchers at Answer.AI and LightOn introduced ModernBERT. I am excited about this, as BERT is one of my favorite foundation large language models (LLMs). While most of the hype machine focuses on generative AI, the tools like ChatGPT that can create text, I think one of the most useful applications of LLMs are the encoder-only models whose job it is to instead represent text. As a data scientist and survey researcher, I’ve found this genre of model to be much more useful to the work I do. The upshot is that the transformer model architecture with attention allows the model to transform text into a series of numeric columns that represent it. The text isn’t just fed into the model, but also the position of each piece of text. Additionally, the “B” in “BERT” is “bidirectional,” allowing it to learn from context—it can look at words before and after each word to help in building a numerical representation. What this all means is that it can figure out things like synonyms and homonyms. Related to the current example of Wes Anderson’s visual style, consider a review that contains the phrase: “The film’s color palette is bright and warm.” Consider another that might say: “The screenplay isn’t as bright as he thinks it is.” A bag-of-words approach is going to have a difficult time here with the word “bright”: It is used both in the context of discussing the visuals of the film and in a review that doesn’t discuss the visuals of the film. ModernBERT—the exciting “replacement” (we’ll see if that proves true) to the existing BERT model variants—can tell the difference. That difference will be represented in different word embeddings. There are two ways I experimented with ModernBERT to see if it could outperform my previous approach: fill-masking and embeddings. Fill-Masking One of the steps of training BERT is through masking. I’m butchering the process, but the idea is to take a sentence, mask a token (which could be a word or sub-word), and train it to predict the correct token. For example, in the code below, I load in ModernBERT and have it predict a masked token from a very obvious example: from transformers import pipeline pipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base') pipe('The dog DNA test said my dog is almost half American pit[MASK] terrier.') This returns the token ' bull' with 81% probability, to finish the sentence as: “The dog DNA test said my dog is almost half American pit bull terrier.” Right behind it, at about 19% probably, is 'bull', which would finish the sentence as: “The dog DNA test said my dog is almost half American pitbull terrier.” We can use this to our advantage by creating a prompt that adds a [MASK] where we want the coding of the text to be. First, let’s prep our script and define an empty column where we’re going to put the classifications in: # prep ------------------------------------------------------------------------- from transformers import pipeline import pandas as pd dat = pd.read_csv('ratings_coded.csv') dat = dat[dat['visual_style'] != 9] dat['masked_class'] = '' Then we loop all of the reviews through a fill-mask prompt, get the predicted tokens, and save the result back out to a .csv (even though LLMs require Python scripting, I still very much prefer to do interactive analysis in R): # fill mask -------------------------------------------------------------------- pipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base') for i in dat.index: review = dat['text'][i].lower() prompt = f'''this is a movie review: question: did the review discuss the visual style of the film, yes or no? answer:[MASK]''' out = pipe(prompt) tmp = out[0]['token_str'].strip().lower() if tmp not in ['yes', 'no']: tmp = out[1]['token_str'].strip().lower() dat.loc[i, 'masked_class'] = tmp dat.to_csv('ratings_masked.csv') I prompt it with a format of three lines, each defining something by a colon. I give it the movie review, I ask if the movie review discussed the visual style of the film, and then I ask for an answer and [MASK] after the last colon. (There’s a little if statement in there because one of the reviews did not return “yes” or “no” as the top predicted result.) The benefit here is that this requires no hand-coding of the data beforehand. The downside is that we haven’t told it about what makes Wes Anderson’s style distinct, which would help improve classification. (We’ll see how to do that with a chat model below.) Embeddings While we can hack around the encoding-only model’s inability to generate text by asking it to produce the next token in a mask, what is really powerful here are the text embeddings. I am using the base version of the model, which will take a sentence and transform it into 768 numbers (see Table 4 at the arxiv paper). What we’re going to do is create a matrix where there are 720 rows (one for each review) and 768 numbers (the dimensionality of the numeric embeddings, which will take into consideration how language is naturally structured): # prep ------------------------------------------------------------------------- import pandas as pd from sentence_transformers import SentenceTransformer dat = pd.read_csv('ratings_coded.csv') text = dat['text'] # get embeddings --------------------------------------------------------------- model = SentenceTransformer('answerdotai/ModernBERT-base') embeddings = model.encode(text) dat_embeddings = pd.DataFrame(embeddings).add_prefix('embed_') dat_out = pd.concat([dat, dat_embeddings], axis=1) dat_out.to_csv('ratings_embedded.csv', index=False) If you go back to the bottom of my original post, this is MUCH less code than the different pre-processors I had defined. Generative Modeling I sang the praises of encoding models above, but we could use a generative model here, too. One of the reasons I like ModernBERT is that you can download it onto your own machine or server. You don’t have to send a third-party your data, which could be proprietary or contain sensitive information. This is something that any organization should have a policy for; the tools are easy enough to access now that it is too easy to mindlessly feed massive amounts of data into a server that is not yours. (You can certainly also download generative instruct or chat models from HuggingFace, as well, but the state-of-the-art models tend to be not open source or too large or require too much computational power to run on a MacBook Air like I’m working on now.) In this case, the reviews are already public. I wanted to try zero-shot learning, where we do not have to label any cases ahead of time or give the model any examples. I also wanted to be extra lazy and not even come up with the coding criteria on my own, either. In my original post, I defined the coding scheme as such: This visual style, by my eye, was solidified by the time of The Grand Budapest Hotel. Symmetrical framing, meticulously organized sets, the use of miniatures, a pastel color palette contrasted with saturated primary colors, distinctive costumes, straight lines, lateral tracking shots, whip pans, and so on. However, I did not consider aspects of Anderson’s style that are unrelated to the visual sense. This is where defining the themes with a clear line matters—and often there will be ambiguities, but one must do their best, because the process we’re doing is fundamentally a simplification of the rich diversity of the text. Thus, I did not consider the following to be in Anderson’s visual style: stories involving precocious children, fraught familial relations, uncommon friendships, dry humor, monotonous dialogue, soundtracks usually involving The Kinks, a fascination with stage productions, nesting doll narratives, or a decidedly twee yet bittersweet tone. After prepping my session, I ask GPT-4o mini to give me a brief description of Wes Anderson’s visual style: # prep ------------------------------------------------------------------------- import re import pandas as pd from openai import OpenAI API_KEY='' model='gpt-4o-mini' client = OpenAI(api_key=API_KEY) # get description -------------------------------------------------------------- description = client.chat.completions.create( model=model, messages=[ { 'role': 'user', 'content': 'Provide a brief description of director Wes Anderson\\'s visual style.' } ], temperature=0 ) description = description.choices[0].message.content with open('description.txt', 'w') as file: file.write(description) with open('description.txt', 'r') as file: description = file.read() Here is what description.txt says: Wes Anderson’s visual style is characterized by its meticulous symmetry, vibrant color palettes, and whimsical, storybook-like aesthetics. He often employs a distinctive use of wide-angle lenses, which creates a flat, two-dimensional look that enhances the surreal quality of his films. Anderson’s compositions are carefully arranged, with a focus on geometric shapes and patterns, often featuring centered framing and balanced scenes. His sets are richly detailed, filled with quirky props and vintage elements that contribute to a nostalgic atmosphere. Additionally, he frequently uses stop-motion animation and unique transitions, further emphasizing his playful and imaginative storytelling approach. Overall, Anderson’s style is instantly recognizable and evokes a sense of charm and artistry. It reads generic, but it’ll work for instructing GPT. Now, let’s add that description into a prompt: # define coding fn ------------------------------------------------------------- job = '''You are a helpful research assistant who is classifying movie reviews. Your job is to determine if a movie review discusses Wes Anderson's unique visual style. ''' directions = ''' The user will provide a review, and you are to respond with one number only. If the review discusses Wes Anderson's visual style at all, reply 1. If it does not, reply 0. Do not give any commentary.''' def get_code(review): messages = [ { 'role': 'system', 'content': job + description + directions }, { 'role': 'user', 'content': review }, ] response = client.chat.completions.create( model=model, messages=messages, temperature=0 ) return(response.choices[0].message.content) Note that the content we’re giving as a static instruction to the system is the job it is supposed to do, the description of Wes Anderson’s style, and then directions for formatting. We now load the data in and again run all the pieces of text through the coding function we just defined: # code text -------------------------------------------------------------------- dat = pd.read_csv('ratings_coded.csv') dat['gen_class'] = '' for i in dat.index: dat.loc[i, 'gen_class'] = get_code(dat['text'][i]) dat.to_csv('ratings_generative.csv') Note that the use of GPT-4o mini via OpenAI’s API cost me $0.03. I do believe that enshittification comes for all platforms eventually, so I don’t expect it to always be this cheap, regardless of what OpenAI says now. That is why open source models found on brilliant websites like HuggingFace are vital. Results How did each approach do? Let’s bring the results into R and check it out. First, let’s prep the session: # prep ------------------------------------------------------------------------- library(rsample) library(yardstick) library(glmnet) library(tidyverse) ms % # I always forget to set index=False filter(visual_style != 9) %>% # not in English mutate( visual_style = factor(visual_style), masked_class = factor(ifelse(masked_class == \"yes\", 1, 0)) ) ms(dat_mask, truth = visual_style, estimate = masked_class) ## # A tibble: 4 × 3 ## .metric .estimator .estimate ## ## 1 accuracy binary 0.499 ## 2 sensitivity binary 0.552 ## 3 specificity binary 0.348 ## 4 precision binary 0.708 with(dat_mask, prop.table(table(visual_style))) ## visual_style ## 0 1 ## 0.7414075 0.2585925 with(dat_mask, prop.table(table(masked_class))) ## masked_class ## 0 1 ## 0.5777414 0.4222586 Not great, but we tried zero-shot (i.e., no examples or supervised learning) prompting without giving it a definition of Wes Anderson’s style. What we really care about is how we predicted 42% of the reviews said something about Anderson’s visual style, when in reality only 26% did. This is much worse than the 7-point error I found in my original post with bag-of-words supervised modeling. What I’m really excited about are the embeddings. Theoretically, I could define an entire cross-validation pipeline to try a variety of models and hyper-parameters to find the best way to model the numeric embeddings from ModernBERT. Instead, I’m just going to do a LASSO using because that method and that package are both fantastic. Note, though, that this is a strictly additive model; no interactions are specified. # embeddings ------------------------------------------------------------------- dat_embed % filter(visual_style != 9) # not in English set.seed(1839) dat_embed % initial_split(.5, strata = visual_style) X % training() %>% select(starts_with(\"embed_\")) %>% as.matrix() y % training() %>% pull(visual_style) mod % select(starts_with(\"embed_\")) %>% as.matrix() preds % # I always forget to set index=False filter(visual_style != 9) %>% # not in English mutate( visual_style = factor(visual_style), gen_class = factor(gen_class) ) ms(dat_gen, truth = visual_style, estimate = gen_class) ## # A tibble: 4 × 3 ## .metric .estimator .estimate ## ## 1 accuracy binary 0.841 ## 2 sensitivity binary 0.870 ## 3 specificity binary 0.759 ## 4 precision binary 0.912 with(dat_gen, prop.table(table(visual_style))) ## visual_style ## 0 1 ## 0.7414075 0.2585925 with(dat_gen, prop.table(table(gen_class))) ## gen_class ## 0 1 ## 0.7070376 0.2929624 This result is also impressive, with 29% predicted to be about the visual style when it was 26% in reality, for a three-point error. Is the reduced human effort of no longer needing to hand-code text worth a little bit worse error than the embeddings in a LASSO? And is it worth the money it could cost to do this at a large scale using OpenAI? Closing Thoughts This was a very quick look at ModernBERT especially, because I’m excited about the quality of these embeddings. And my thinking about how to process text has changed in the last year or two. There’s so much else one could do here, such as fine-tuning ModernBERT or GPT-4o mini for movie review classification. (I imagine there will be many fine-tuned ModernBERT models uploaded to HuggingFace shortly for a number of use-cases.) Classifying text without hand-coded labels is going to be more challenging in environments where the domain is not as widely discussed on the internet as movie reviews, cinematography, and Wes Anderson. Your customers or users could have specific concerns and use specific language, and even providing a lengthy coding scheme to a generative model might not cut it. At any rate, you would want to hand-label a test set anyways. It’s important that you, as a researcher, actually read the text and that there is always a human in the loop. What I’m most excited about here is being able to take survey responses plus text embeddings from open-ended questions and using them together in a machine learning model to predict attitudes or behaviors. See my GitHub for code.",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "Rethinking How I Do Supervised Topic Modeling, Using ModernBERT and GPT-4o mini | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 16.3,
    "sitemap_lastmod": null,
    "twitter_description": "I wrote a post in July 2023 describing my process for building a supervised text classification pipeline. In short, the process first involves reading the text, writing a thematic content coding guide, and having humans label text. Then, I define a variety of ways to pre-process text (e.g., word vs. word-and-bigram tokenizing, stemming vs. not, stop words vs. not, filtering on the number of times a word had to appear in the corpus) in a workflowset. Then, I run these different pre-processors through different standard models: elastic net, XGBoost, random forest, etc. Each class of text has its own model, so I would run this pipeline five times if there were five topics in the text. Importantly, this is not natural language processing (NLP), as it was a bag-of-words approach. The idea was to leverage the domain knowledge of the experts on my team through content coding, and then scaling it up using a machine learning pipeline. In the post, I bemoaned how most of the “NLP” or “AI-driven” tools I had tested did not do very well. The tools I was thinking of were all web-based, point-and-click applications that I had tried out since about 2018, and they usually were unsupervised. We are in a wildly different environment now when it comes to analyzing text than we were even a few years ago. I am revisiting that post to explore alternate routes to classifying text. I will use the same data as I did in that post: 720 Letterboxd reviews of Wes Anderson’s film Asteroid City. There is only one code: Did the review discuss Wes Anderson’s unique visual style (1) or not (0)? I hand-labeled all of these on one afternoon to give me a supervised dataset to play with. The use case I had in mind was from a previous job, where the goal was not to predict if an individual discussed a topic or not. Instead, the focus was on, “What percentage of people mentioned this topic this month?” in a tracking survey. The primary way I’ll judge these approaches is by seeing how far away the predicted percent of reviews discussing visual style is from the actual percent of reviews discussing visual style. That is: I am interested in estimates of the aggregate. For a baseline, my bag-of-words supervised classification pipeline predicted 19% when it was actually 26% in the testing set, for an absolute error of 7 points. ModernBERT Two days ago, researchers at Answer.AI and LightOn introduced ModernBERT. I am excited about this, as BERT is one of my favorite foundation large language models (LLMs). While most of the hype machine focuses on generative AI, the tools like ChatGPT that can create text, I think one of the most useful applications of LLMs are the encoder-only models whose job it is to instead represent text. As a data scientist and survey researcher, I’ve found this genre of model to be much more useful to the work I do. The upshot is that the transformer model architecture with attention allows the model to transform text into a series of numeric columns that represent it. The text isn’t just fed into the model, but also the position of each piece of text. Additionally, the “B” in “BERT” is “bidirectional,” allowing it to learn from context—it can look at words before and after each word to help in building a numerical representation. What this all means is that it can figure out things like synonyms and homonyms. Related to the current example of Wes Anderson’s visual style, consider a review that contains the phrase: “The film’s color palette is bright and warm.” Consider another that might say: “The screenplay isn’t as bright as he thinks it is.” A bag-of-words approach is going to have a difficult time here with the word “bright”: It is used both in the context of discussing the visuals of the film and in a review that doesn’t discuss the visuals of the film. ModernBERT—the exciting “replacement” (we’ll see if that proves true) to the existing BERT model variants—can tell the difference. That difference will be represented in different word embeddings. There are two ways I experimented with ModernBERT to see if it could outperform my previous approach: fill-masking and embeddings. Fill-Masking One of the steps of training BERT is through masking. I’m butchering the process, but the idea is to take a sentence, mask a token (which could be a word or sub-word), and train it to predict the correct token. For example, in the code below, I load in ModernBERT and have it predict a masked token from a very obvious example: from transformers import pipeline pipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base') pipe('The dog DNA test said my dog is almost half American pit[MASK] terrier.') This returns the token ' bull' with 81% probability, to finish the sentence as: “The dog DNA test said my dog is almost half American pit bull terrier.” Right behind it, at about 19% probably, is 'bull', which would finish the sentence as: “The dog DNA test said my dog is almost half American pitbull terrier.” We can use this to our advantage by creating a prompt that adds a [MASK] where we want the coding of the text to be. First, let’s prep our script and define an empty column where we’re going to put the classifications in: # prep ------------------------------------------------------------------------- from transformers import pipeline import pandas as pd dat = pd.read_csv('ratings_coded.csv') dat = dat[dat['visual_style'] != 9] dat['masked_class'] = '' Then we loop all of the reviews through a fill-mask prompt, get the predicted tokens, and save the result back out to a .csv (even though LLMs require Python scripting, I still very much prefer to do interactive analysis in R): # fill mask -------------------------------------------------------------------- pipe = pipeline('fill-mask', model='answerdotai/ModernBERT-base') for i in dat.index: review = dat['text'][i].lower() prompt = f'''this is a movie review: question: did the review discuss the visual style of the film, yes or no? answer:[MASK]''' out = pipe(prompt) tmp = out[0]['token_str'].strip().lower() if tmp not in ['yes', 'no']: tmp = out[1]['token_str'].strip().lower() dat.loc[i, 'masked_class'] = tmp dat.to_csv('ratings_masked.csv') I prompt it with a format of three lines, each defining something by a colon. I give it the movie review, I ask if the movie review discussed the visual style of the film, and then I ask for an answer and [MASK] after the last colon. (There’s a little if statement in there because one of the reviews did not return “yes” or “no” as the top predicted result.) The benefit here is that this requires no hand-coding of the data beforehand. The downside is that we haven’t told it about what makes Wes Anderson’s style distinct, which would help improve classification. (We’ll see how to do that with a chat model below.) Embeddings While we can hack around the encoding-only model’s inability to generate text by asking it to produce the next token in a mask, what is really powerful here are the text embeddings. I am using the base version of the model, which will take a sentence and transform it into 768 numbers (see Table 4 at the arxiv paper). What we’re going to do is create a matrix where there are 720 rows (one for each review) and 768 numbers (the dimensionality of the numeric embeddings, which will take into consideration how language is naturally structured): # prep ------------------------------------------------------------------------- import pandas as pd from sentence_transformers import SentenceTransformer dat = pd.read_csv('ratings_coded.csv') text = dat['text'] # get embeddings --------------------------------------------------------------- model = SentenceTransformer('answerdotai/ModernBERT-base') embeddings = model.encode(text) dat_embeddings = pd.DataFrame(embeddings).add_prefix('embed_') dat_out = pd.concat([dat, dat_embeddings], axis=1) dat_out.to_csv('ratings_embedded.csv', index=False) If you go back to the bottom of my original post, this is MUCH less code than the different pre-processors I had defined. Generative Modeling I sang the praises of encoding models above, but we could use a generative model here, too. One of the reasons I like ModernBERT is that you can download it onto your own machine or server. You don’t have to send a third-party your data, which could be proprietary or contain sensitive information. This is something that any organization should have a policy for; the tools are easy enough to access now that it is too easy to mindlessly feed massive amounts of data into a server that is not yours. (You can certainly also download generative instruct or chat models from HuggingFace, as well, but the state-of-the-art models tend to be not open source or too large or require too much computational power to run on a MacBook Air like I’m working on now.) In this case, the reviews are already public. I wanted to try zero-shot learning, where we do not have to label any cases ahead of time or give the model any examples. I also wanted to be extra lazy and not even come up with the coding criteria on my own, either. In my original post, I defined the coding scheme as such: This visual style, by my eye, was solidified by the time of The Grand Budapest Hotel. Symmetrical framing, meticulously organized sets, the use of miniatures, a pastel color palette contrasted with saturated primary colors, distinctive costumes, straight lines, lateral tracking shots, whip pans, and so on. However, I did not consider aspects of Anderson’s style that are unrelated to the visual sense. This is where defining the themes with a clear line matters—and often there will be ambiguities, but one must do their best, because the process we’re doing is fundamentally a simplification of the rich diversity of the text. Thus, I did not consider the following to be in Anderson’s visual style: stories involving precocious children, fraught familial relations, uncommon friendships, dry humor, monotonous dialogue, soundtracks usually involving The Kinks, a fascination with stage productions, nesting doll narratives, or a decidedly twee yet bittersweet tone. After prepping my session, I ask GPT-4o mini to give me a brief description of Wes Anderson’s visual style: # prep ------------------------------------------------------------------------- import re import pandas as pd from openai import OpenAI API_KEY='' model='gpt-4o-mini' client = OpenAI(api_key=API_KEY) # get description -------------------------------------------------------------- description = client.chat.completions.create( model=model, messages=[ { 'role': 'user', 'content': 'Provide a brief description of director Wes Anderson\\'s visual style.' } ], temperature=0 ) description = description.choices[0].message.content with open('description.txt', 'w') as file: file.write(description) with open('description.txt', 'r') as file: description = file.read() Here is what description.txt says: Wes Anderson’s visual style is characterized by its meticulous symmetry, vibrant color palettes, and whimsical, storybook-like aesthetics. He often employs a distinctive use of wide-angle lenses, which creates a flat, two-dimensional look that enhances the surreal quality of his films. Anderson’s compositions are carefully arranged, with a focus on geometric shapes and patterns, often featuring centered framing and balanced scenes. His sets are richly detailed, filled with quirky props and vintage elements that contribute to a nostalgic atmosphere. Additionally, he frequently uses stop-motion animation and unique transitions, further emphasizing his playful and imaginative storytelling approach. Overall, Anderson’s style is instantly recognizable and evokes a sense of charm and artistry. It reads generic, but it’ll work for instructing GPT. Now, let’s add that description into a prompt: # define coding fn ------------------------------------------------------------- job = '''You are a helpful research assistant who is classifying movie reviews. Your job is to determine if a movie review discusses Wes Anderson's unique visual style. ''' directions = ''' The user will provide a review, and you are to respond with one number only. If the review discusses Wes Anderson's visual style at all, reply 1. If it does not, reply 0. Do not give any commentary.''' def get_code(review): messages = [ { 'role': 'system', 'content': job + description + directions }, { 'role': 'user', 'content': review }, ] response = client.chat.completions.create( model=model, messages=messages, temperature=0 ) return(response.choices[0].message.content) Note that the content we’re giving as a static instruction to the system is the job it is supposed to do, the description of Wes Anderson’s style, and then directions for formatting. We now load the data in and again run all the pieces of text through the coding function we just defined: # code text -------------------------------------------------------------------- dat = pd.read_csv('ratings_coded.csv') dat['gen_class'] = '' for i in dat.index: dat.loc[i, 'gen_class'] = get_code(dat['text'][i]) dat.to_csv('ratings_generative.csv') Note that the use of GPT-4o mini via OpenAI’s API cost me $0.03. I do believe that enshittification comes for all platforms eventually, so I don’t expect it to always be this cheap, regardless of what OpenAI says now. That is why open source models found on brilliant websites like HuggingFace are vital. Results How did each approach do? Let’s bring the results into R and check it out. First, let’s prep the session: # prep ------------------------------------------------------------------------- library(rsample) library(yardstick) library(glmnet) library(tidyverse) ms % # I always forget to set index=False filter(visual_style != 9) %>% # not in English mutate( visual_style = factor(visual_style), masked_class = factor(ifelse(masked_class == \"yes\", 1, 0)) ) ms(dat_mask, truth = visual_style, estimate = masked_class) ## # A tibble: 4 × 3 ## .metric .estimator .estimate ## ## 1 accuracy binary 0.499 ## 2 sensitivity binary 0.552 ## 3 specificity binary 0.348 ## 4 precision binary 0.708 with(dat_mask, prop.table(table(visual_style))) ## visual_style ## 0 1 ## 0.7414075 0.2585925 with(dat_mask, prop.table(table(masked_class))) ## masked_class ## 0 1 ## 0.5777414 0.4222586 Not great, but we tried zero-shot (i.e., no examples or supervised learning) prompting without giving it a definition of Wes Anderson’s style. What we really care about is how we predicted 42% of the reviews said something about Anderson’s visual style, when in reality only 26% did. This is much worse than the 7-point error I found in my original post with bag-of-words supervised modeling. What I’m really excited about are the embeddings. Theoretically, I could define an entire cross-validation pipeline to try a variety of models and hyper-parameters to find the best way to model the numeric embeddings from ModernBERT. Instead, I’m just going to do a LASSO using because that method and that package are both fantastic. Note, though, that this is a strictly additive model; no interactions are specified. # embeddings ------------------------------------------------------------------- dat_embed % filter(visual_style != 9) # not in English set.seed(1839) dat_embed % initial_split(.5, strata = visual_style) X % training() %>% select(starts_with(\"embed_\")) %>% as.matrix() y % training() %>% pull(visual_style) mod % select(starts_with(\"embed_\")) %>% as.matrix() preds % # I always forget to set index=False filter(visual_style != 9) %>% # not in English mutate( visual_style = factor(visual_style), gen_class = factor(gen_class) ) ms(dat_gen, truth = visual_style, estimate = gen_class) ## # A tibble: 4 × 3 ## .metric .estimator .estimate ## ## 1 accuracy binary 0.841 ## 2 sensitivity binary 0.870 ## 3 specificity binary 0.759 ## 4 precision binary 0.912 with(dat_gen, prop.table(table(visual_style))) ## visual_style ## 0 1 ## 0.7414075 0.2585925 with(dat_gen, prop.table(table(gen_class))) ## gen_class ## 0 1 ## 0.7070376 0.2929624 This result is also impressive, with 29% predicted to be about the visual style when it was 26% in reality, for a three-point error. Is the reduced human effort of no longer needing to hand-code text worth a little bit worse error than the embeddings in a LASSO? And is it worth the money it could cost to do this at a large scale using OpenAI? Closing Thoughts This was a very quick look at ModernBERT especially, because I’m excited about the quality of these embeddings. And my thinking about how to process text has changed in the last year or two. There’s so much else one could do here, such as fine-tuning ModernBERT or GPT-4o mini for movie review classification. (I imagine there will be many fine-tuned ModernBERT models uploaded to HuggingFace shortly for a number of use-cases.) Classifying text without hand-coded labels is going to be more challenging in environments where the domain is not as widely discussed on the internet as movie reviews, cinematography, and Wes Anderson. Your customers or users could have specific concerns and use specific language, and even providing a lengthy coding scheme to a generative model might not cut it. At any rate, you would want to hand-label a test set anyways. It’s important that you, as a researcher, actually read the text and that there is always a human in the loop. What I’m most excited about here is being able to take survey responses plus text embeddings from open-ended questions and using them together in a machine learning model to predict attitudes or behaviors. See my GitHub for code.",
    "twitter_title": "Rethinking How I Do Supervised Topic Modeling, Using ModernBERT and GPT-4o mini | R-bloggers",
    "url": "https://www.r-bloggers.com/2024/12/rethinking-how-i-do-supervised-topic-modeling-using-modernbert-and-gpt-4o-mini/",
    "word_count": 3269
  }
}