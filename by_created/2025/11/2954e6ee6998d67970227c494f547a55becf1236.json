{
  "id": "2954e6ee6998d67970227c494f547a55becf1236",
  "url": "https://www.r-bloggers.com/2025/05/multi-task-learning-with-torch-in-r/",
  "created_at_utc": "2025-11-22T19:58:38Z",
  "data": null,
  "raw_original": {
    "uuid": "7869c28e-c22b-478d-8149-52378052f52e",
    "created_at": "2025-11-22 19:58:38",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/05/multi-task-learning-with-torch-in-r/",
      "crawled_at": "2025-11-22T10:49:08.569559",
      "external_links": [
        {
          "href": "https://rtichoke.netlify.app/posts/multi-task-learning-with-torch.html",
          "text": "R'tichoke"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://rtichoke.netlify.app/posts/multi-task-learning-with-torch.html",
          "text": "R'tichoke"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Multi-Task Learning with torch in R | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/rtichoke.netlify.app/posts/multi-task-learning-with-torch_files/figure-html/unnamed-chunk-1-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/rtichoke/",
          "text": "R'tichoke"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-393300 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Multi-Task Learning with torch in R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 10, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/rtichoke/\">R'tichoke</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://rtichoke.netlify.app/posts/multi-task-learning-with-torch.html\"> R'tichoke</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>Multi-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information across tasks. This post explores how to implement a multi-task learning model using the <code>torch</code> package in R.</p>\n<section class=\"level2\" id=\"introduction\">\n<h2 class=\"anchored\" data-anchor-id=\"introduction\">Introduction</h2>\n<p>Multi-task learning operates by sharing representations between related tasks, enabling models to generalize more effectively. Instead of training separate models for each task, this approach develops a single model with:</p>\n<ul>\n<li><strong>Shared layers</strong> that learn common features across tasks</li>\n<li><strong>Task-specific layers</strong> that specialize for each individual task<br/>\n</li>\n<li><strong>Multiple loss functions</strong>, one for each task</li>\n</ul>\n<p>This approach is particularly valuable when dealing with related prediction problems that can benefit from shared feature representations.</p>\n</section>\n<section class=\"level2\" id=\"packages\">\n<h2 class=\"anchored\" data-anchor-id=\"packages\">Packages</h2>\n<div class=\"cell\">\n<pre># install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)</pre>\n</div>\n</section>\n<section class=\"level2\" id=\"creating-a-mtl-model\">\n<h2 class=\"anchored\" data-anchor-id=\"creating-a-mtl-model\">Creating a MTL Model</h2>\n<p>The implementation will construct a model that simultaneously performs two related tasks:</p>\n<ol type=\"1\">\n<li><strong>Regression</strong>: Predicting a continuous value</li>\n<li><strong>Classification</strong>: Predicting a binary outcome</li>\n</ol>\n<section class=\"level3\" id=\"sample-data\">\n<h3 class=\"anchored\" data-anchor-id=\"sample-data\">Sample Data</h3>\n<div class=\"cell\">\n<pre># Set seed for reproducibility\nset.seed(123)\n\n# Number of samples\nn &lt;- 1000\n\n# Create a dataset with 5 features\nx &lt;- torch_randn(n, 5)\n\n# Task 1 (Regression): Predict continuous value\n# Create a target that's a function of the input features plus some noise\ny_regression &lt;- x[, 1] * 0.7 + x[, 2] * 0.3 - x[, 3] * 0.5 + torch_randn(n) * 0.2\n\n# Task 2 (Classification): Predict binary outcome\n# Create a classification target based on a nonlinear combination of features\nlogits &lt;- x[, 1] * 0.8 - x[, 4] * 0.4 + x[, 5] * 0.6\ny_classification &lt;- (logits &gt; 0)$to(torch_float())\n\n# Split into training (70%) and testing (30%) sets\ntrain_idx &lt;- 1:round(0.7 * n)\ntest_idx &lt;- (round(0.7 * n) + 1):n\n\n# Training data\nx_train &lt;- x[train_idx, ]\ny_reg_train &lt;- y_regression[train_idx]\ny_cls_train &lt;- y_classification[train_idx]\n\n# Testing data\nx_test &lt;- x[test_idx, ]\ny_reg_test &lt;- y_regression[test_idx]\ny_cls_test &lt;- y_classification[test_idx]</pre>\n</div>\n</section>\n<section class=\"level3\" id=\"define-the-multi-task-neural-network\">\n<h3 class=\"anchored\" data-anchor-id=\"define-the-multi-task-neural-network\">Define the Multi-Task Neural Network</h3>\n<p>The architecture design creates a neural network with shared layers and task-specific branches:</p>\n<div class=\"cell\">\n<pre># Define the multi-task neural network\nmulti_task_net &lt;- nn_module(\n  \"MultiTaskNet\",\n  \n  initialize = function(input_size, \n                        hidden_size, \n                        reg_output_size = 1, \n                        cls_output_size = 1) {\n    \n    self$input_size &lt;- input_size\n    self$hidden_size &lt;- hidden_size\n    self$reg_output_size &lt;- reg_output_size\n    self$cls_output_size &lt;- cls_output_size\n    \n    # Shared layers - these learn representations useful for both tasks\n    self$shared_layer1 &lt;- nn_linear(input_size, hidden_size)\n    self$shared_layer2 &lt;- nn_linear(hidden_size, hidden_size)\n    \n    # Task-specific layers\n    # Regression branch\n    self$regression_layer &lt;- nn_linear(hidden_size, reg_output_size)\n    \n    # Classification branch\n    self$classification_layer &lt;- nn_linear(hidden_size, cls_output_size)\n  },\n  \n  forward = function(x) {\n    # Shared feature extraction\n    shared_features &lt;- x %&gt;%\n      self$shared_layer1() %&gt;%\n      nnf_relu() %&gt;%\n      self$shared_layer2() %&gt;%\n      nnf_relu()\n    \n    # Task-specific predictions\n    regression_output &lt;- self$regression_layer(shared_features)\n    classification_logits &lt;- self$classification_layer(shared_features)\n    \n    list(\n      regression = regression_output,\n      classification = classification_logits\n    )\n  }\n)\n\n# Create model instance\nmodel &lt;- multi_task_net(\n  input_size = 5,\n  hidden_size = 10\n)\n\n# Print model architecture\nprint(model)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>An `nn_module` containing 192 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• shared_layer1: &lt;nn_linear&gt; #60 parameters\n• shared_layer2: &lt;nn_linear&gt; #110 parameters\n• regression_layer: &lt;nn_linear&gt; #11 parameters\n• classification_layer: &lt;nn_linear&gt; #11 parameters</pre>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"define-loss-functions-and-optimizer\">\n<h3 class=\"anchored\" data-anchor-id=\"define-loss-functions-and-optimizer\">4. Define Loss Functions and Optimizer</h3>\n<p>Multi-task learning requires separate loss functions for each task.</p>\n<div class=\"cell\">\n<pre># Loss functions\nregression_loss_fn &lt;- nnf_mse_loss  # Mean squared error for regression\nclassification_loss_fn &lt;- nnf_binary_cross_entropy_with_logits  # Binary cross-entropy for classification\n\n# Optimizer with weight decay for L2 regularization\noptimizer &lt;- optim_adam(model$parameters, lr = 0.01)\n\n# Task weights - these control the relative importance of each task\ntask_weights &lt;- c(regression = 0.5, classification = 0.5)\n\n# Validation split from training data\nval_size &lt;- round(0.2 * length(train_idx))\nval_indices &lt;- sample(train_idx, val_size)\ntrain_indices &lt;- setdiff(train_idx, val_indices)\n\n# Create validation sets\nx_val &lt;- x[val_indices, ]\ny_reg_val &lt;- y_regression[val_indices]\ny_cls_val &lt;- y_classification[val_indices]\n\n# Update training sets\nx_train &lt;- x[train_indices, ]\ny_reg_train &lt;- y_regression[train_indices]\ny_cls_train &lt;- y_classification[train_indices]</pre>\n</div>\n</section>\n<section class=\"level3\" id=\"training-loop\">\n<h3 class=\"anchored\" data-anchor-id=\"training-loop\">Training Loop</h3>\n<div class=\"cell\">\n<pre># Hyperparameters\nepochs &lt;- 100  # Increased epochs since we have early stopping\n\n# Enhanced training history tracking\ntraining_history &lt;- data.frame(\n  epoch = integer(),\n  train_reg_loss = numeric(),\n  train_cls_loss = numeric(),\n  train_total_loss = numeric(),\n  val_reg_loss = numeric(),\n  val_cls_loss = numeric(),\n  val_total_loss = numeric(),\n  val_accuracy = numeric()\n)\n\nfor (epoch in 1:epochs) {\n  # Training phase\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass on training data\n  outputs &lt;- model(x_train)\n  \n  # Calculate training loss for each task\n  train_reg_loss &lt;- regression_loss_fn(\n    outputs$regression$squeeze(), \n    y_reg_train\n  )\n  \n  train_cls_loss &lt;- classification_loss_fn(\n    outputs$classification$squeeze(), \n    y_cls_train\n  )\n  \n  # Weighted combined training loss\n  train_total_loss &lt;- task_weights[\"regression\"] * train_reg_loss + \n    task_weights[\"classification\"] * train_cls_loss\n  \n  # Backward pass and optimize\n  train_total_loss$backward()\n  \n  # Gradient clipping to prevent exploding gradients\n  nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)\n  \n  optimizer$step()\n  \n  # Validation phase\n  model$eval()\n  \n  with_no_grad({\n    val_outputs &lt;- model(x_val)\n    \n    # Calculate validation losses\n    val_reg_loss &lt;- regression_loss_fn(\n      val_outputs$regression$squeeze(), \n      y_reg_val\n    )\n    \n    val_cls_loss &lt;- classification_loss_fn(\n      val_outputs$classification$squeeze(), \n      y_cls_val\n    )\n    \n    val_total_loss &lt;- task_weights[\"regression\"] * val_reg_loss + task_weights[\"classification\"] * val_cls_loss\n    \n    # Calculate validation accuracy\n    val_cls_probs &lt;- nnf_sigmoid(val_outputs$classification$squeeze())\n    val_cls_preds &lt;- (val_cls_probs &gt; 0.5)$to(torch_int())\n    val_accuracy &lt;- (val_cls_preds == y_cls_val$to(torch_int()))$sum()$item() / length(val_indices)\n  })\n  \n  # Record history\n  training_history &lt;- rbind(\n    training_history,\n    data.frame(\n      epoch = epoch,\n      train_reg_loss = as.numeric(train_reg_loss$item()),\n      train_cls_loss = as.numeric(train_cls_loss$item()),\n      train_total_loss = as.numeric(train_total_loss$item()),\n      val_reg_loss = as.numeric(val_reg_loss$item()),\n      val_cls_loss = as.numeric(val_cls_loss$item()),\n      val_total_loss = as.numeric(val_total_loss$item()),\n      val_accuracy = val_accuracy\n    )\n  )\n  \n  # Print progress every 25 epochs\n  if (epoch %% 25 == 0 || epoch == 1) {\n    cat(sprintf(\"Epoch %d - Train Loss: %.4f, Val Loss: %.4f, Val Acc: %.3f\\n\", \n                epoch, \n                train_total_loss$item(), \n                val_total_loss$item(), \n                val_accuracy))\n  }\n\n}</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Epoch 1 - Train Loss: 0.7958, Val Loss: 0.7369, Val Acc: 0.493\nEpoch 25 - Train Loss: 0.3267, Val Loss: 0.3035, Val Acc: 0.821\nEpoch 50 - Train Loss: 0.1548, Val Loss: 0.1350, Val Acc: 0.971\nEpoch 75 - Train Loss: 0.0599, Val Loss: 0.0479, Val Acc: 0.993\nEpoch 100 - Train Loss: 0.0381, Val Loss: 0.0356, Val Acc: 1.000</pre>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"model-evaluation\">\n<h3 class=\"anchored\" data-anchor-id=\"model-evaluation\">Model Evaluation</h3>\n<div class=\"cell\">\n<pre># Set model to evaluation mode\nmodel$eval()\n\n# Make predictions on test set\nwith_no_grad({\n  outputs &lt;- model(x_test)\n  \n  # Regression evaluation\n  reg_preds &lt;- outputs$regression$squeeze()\n  reg_test_loss &lt;- regression_loss_fn(reg_preds, y_reg_test)\n  \n  # Classification evaluation\n  cls_preds &lt;- outputs$classification$squeeze()\n  cls_probs &lt;- nnf_sigmoid(cls_preds)\n  cls_test_loss &lt;- classification_loss_fn(cls_preds, y_cls_test)\n  \n  # Convert predictions to binary (threshold = 0.5)\n  cls_pred_labels &lt;- (cls_probs &gt; 0.5)$to(torch_int())\n  \n  # Calculate accuracy\n  accuracy &lt;- (cls_pred_labels == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n})\n\n# Calculate additional metrics\nreg_preds_r &lt;- as.numeric(reg_preds)\ny_reg_test_r &lt;- as.numeric(y_reg_test)\ncls_probs_r &lt;- as.numeric(cls_probs)\ny_cls_test_r &lt;- as.numeric(y_cls_test)\n\n# Regression metrics\nrmse &lt;- sqrt(mean((reg_preds_r - y_reg_test_r)^2))\nmae &lt;- mean(abs(reg_preds_r - y_reg_test_r))\nr_squared &lt;- cor(reg_preds_r, y_reg_test_r)^2\n\n# Classification metrics\nauc &lt;- pROC::auc(pROC::roc(y_cls_test_r, cls_probs_r, quiet = TRUE))\n\n# Display results\nperformance_results &lt;- data.frame(\n  Task = c(\"Regression\", \"Regression\", \"Regression\", \"Classification\", \"Classification\", \"Classification\"),\n  Metric = c(\"Test Loss (MSE)\", \"RMSE\", \"R-squared\", \"Test Loss (BCE)\", \"Accuracy\", \"AUC\"),\n  Value = c(\n    round(reg_test_loss$item(), 4),\n    round(rmse, 4),\n    round(r_squared, 4),\n    round(cls_test_loss$item(), 4),\n    round(accuracy * 100, 2), \n    round(auc * 100, 2)\n  )\n)\n\nprint(performance_results)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>            Task          Metric   Value\n1     Regression Test Loss (MSE)  0.0533\n2     Regression            RMSE  0.2308\n3     Regression       R-squared  0.9449\n4 Classification Test Loss (BCE)  0.0492\n5 Classification        Accuracy 98.0000\n6 Classification             AUC 99.9400</pre>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"visualization-and-overfitting-analysis\">\n<h3 class=\"anchored\" data-anchor-id=\"visualization-and-overfitting-analysis\">Visualization and Overfitting Analysis</h3>\n<div class=\"cell\">\n<pre># Plot enhanced training history with overfitting detection\np1 &lt;- training_history %&gt;%\n  select(epoch, train_total_loss, val_total_loss) %&gt;%\n  pivot_longer(cols = c(train_total_loss, val_total_loss), \n               names_to = \"split\", values_to = \"loss\") %&gt;%\n  mutate(split = case_when(\n    split == \"train_total_loss\" ~ \"Training\",\n    split == \"val_total_loss\" ~ \"Validation\"\n  )) %&gt;%\n  ggplot(aes(x = epoch, y = loss, color = split)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = which.min(training_history$val_total_loss), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Training vs Validation Loss\",\n       subtitle = \"Red line shows optimal stopping point\",\n       x = \"Epoch\", y = \"Total Loss\", color = \"Dataset\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Separate task losses\np2 &lt;- training_history %&gt;%\n  select(epoch, train_reg_loss, val_reg_loss, train_cls_loss, val_cls_loss) %&gt;%\n  pivot_longer(cols = -epoch, names_to = \"metric\", values_to = \"loss\") %&gt;%\n  separate(metric, into = c(\"split\", \"task\", \"loss_type\"), sep = \"_\") %&gt;%\n  mutate(\n    split = ifelse(split == \"train\", \"Training\", \"Validation\"),\n    task = ifelse(task == \"reg\", \"Regression\", \"Classification\"),\n    metric_name = paste(split, task)\n  ) %&gt;%\n  ggplot(aes(x = epoch, y = loss, color = metric_name)) +\n  geom_line(size = 1) +\n  facet_wrap(~task, scales = \"free_y\") +\n  labs(title = \"Task-Specific Loss Curves\",\n       subtitle = \"Monitoring overfitting in individual tasks\",\n       x = \"Epoch\", y = \"Loss\", color = \"Split &amp; Task\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n# Validation accuracy progression\np3 &lt;- ggplot(training_history, aes(x = epoch, y = val_accuracy)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  geom_hline(yintercept = max(training_history$val_accuracy), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Validation Accuracy Progression\",\n       subtitle = paste(\"Peak accuracy:\", round(max(training_history$val_accuracy), 3)),\n       x = \"Epoch\", y = \"Validation Accuracy\") +\n  theme_minimal()\n\n# Overfitting analysis\ntraining_history$overfitting_gap &lt;- training_history$train_total_loss - training_history$val_total_loss\n\np4 &lt;- ggplot(training_history, aes(x = epoch, y = overfitting_gap)) +\n  geom_line(color = \"#e74c3c\", size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"Overfitting Gap Analysis\",\n       subtitle = \"Difference between training and validation loss\",\n       x = \"Epoch\", y = \"Training Loss - Validation Loss\") +\n  theme_minimal()\n\n# Regression predictions vs actual values\nregression_results &lt;- data.frame(\n  Actual = y_reg_test_r,\n  Predicted = reg_preds_r\n)\n\np5 &lt;- ggplot(regression_results, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.6, color = \"#2c3e50\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"#3498db\", se = TRUE) +\n  labs(title = \"Regression Task: Actual vs Predicted Values\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \", RMSE =\", round(rmse, 3)),\n       x = \"Actual Values\", y = \"Predicted Values\") +\n  theme_minimal()\n\n# Classification probability distribution\ncls_results &lt;- data.frame(\n  Probability = cls_probs_r,\n  Actual_Class = factor(y_cls_test_r, labels = c(\"Class 0\", \"Class 1\"))\n)\n\np6 &lt;- ggplot(cls_results, aes(x = Probability, fill = Actual_Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Classification Task: Predicted Probabilities\",\n       subtitle = paste(\"Accuracy =\", round(accuracy * 100, 1), \"%\"),\n       x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 | p3) / (p2) / (p4) / (p5 | p6)</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/rtichoke.netlify.app/posts/multi-task-learning-with-torch_files/figure-html/unnamed-chunk-1-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/rtichoke.netlify.app/posts/multi-task-learning-with-torch_files/figure-html/unnamed-chunk-1-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"key-takeaways\">\n<h2 class=\"anchored\" data-anchor-id=\"key-takeaways\">Key Takeaways</h2>\n<ol type=\"1\">\n<li><strong>Architecture Design</strong>: The shared-private paradigm enables models to learn both common and task-specific representations</li>\n<li><strong>Loss Combination</strong>: Properly weighting multiple loss functions proves crucial for balanced learning across tasks</li>\n<li><strong>Evaluation Strategy</strong>: Each task requires appropriate metrics, and overall model success depends on performance across all tasks</li>\n<li><strong>Parameter Efficiency</strong>: Multi-task models can achieve comparable performance with fewer total parameters when properly regularized</li>\n<li><strong>Knowledge Transfer</strong>: Related tasks can benefit from shared feature learning, especially when data is limited</li>\n</ol>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://rtichoke.netlify.app/posts/multi-task-learning-with-torch.html\"> R'tichoke</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Multi-Task Learning with torch in R\nPosted on\nMay 10, 2025\nby\nR'tichoke\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR'tichoke\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nMulti-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information across tasks. This post explores how to implement a multi-task learning model using the\ntorch\npackage in R.\nIntroduction\nMulti-task learning operates by sharing representations between related tasks, enabling models to generalize more effectively. Instead of training separate models for each task, this approach develops a single model with:\nShared layers\nthat learn common features across tasks\nTask-specific layers\nthat specialize for each individual task\nMultiple loss functions\n, one for each task\nThis approach is particularly valuable when dealing with related prediction problems that can benefit from shared feature representations.\nPackages\n# install.packages(c(\"torch\", \"tidyverse\", \"corrplot\"))\nlibrary(torch)\nlibrary(tidyverse)\nCreating a MTL Model\nThe implementation will construct a model that simultaneously performs two related tasks:\nRegression\n: Predicting a continuous value\nClassification\n: Predicting a binary outcome\nSample Data\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of samples\nn <- 1000\n\n# Create a dataset with 5 features\nx <- torch_randn(n, 5)\n\n# Task 1 (Regression): Predict continuous value\n# Create a target that's a function of the input features plus some noise\ny_regression <- x[, 1] * 0.7 + x[, 2] * 0.3 - x[, 3] * 0.5 + torch_randn(n) * 0.2\n\n# Task 2 (Classification): Predict binary outcome\n# Create a classification target based on a nonlinear combination of features\nlogits <- x[, 1] * 0.8 - x[, 4] * 0.4 + x[, 5] * 0.6\ny_classification <- (logits > 0)$to(torch_float())\n\n# Split into training (70%) and testing (30%) sets\ntrain_idx <- 1:round(0.7 * n)\ntest_idx <- (round(0.7 * n) + 1):n\n\n# Training data\nx_train <- x[train_idx, ]\ny_reg_train <- y_regression[train_idx]\ny_cls_train <- y_classification[train_idx]\n\n# Testing data\nx_test <- x[test_idx, ]\ny_reg_test <- y_regression[test_idx]\ny_cls_test <- y_classification[test_idx]\nDefine the Multi-Task Neural Network\nThe architecture design creates a neural network with shared layers and task-specific branches:\n# Define the multi-task neural network\nmulti_task_net <- nn_module(\n  \"MultiTaskNet\",\n  \n  initialize = function(input_size, \n                        hidden_size, \n                        reg_output_size = 1, \n                        cls_output_size = 1) {\n    \n    self$input_size <- input_size\n    self$hidden_size <- hidden_size\n    self$reg_output_size <- reg_output_size\n    self$cls_output_size <- cls_output_size\n    \n    # Shared layers - these learn representations useful for both tasks\n    self$shared_layer1 <- nn_linear(input_size, hidden_size)\n    self$shared_layer2 <- nn_linear(hidden_size, hidden_size)\n    \n    # Task-specific layers\n    # Regression branch\n    self$regression_layer <- nn_linear(hidden_size, reg_output_size)\n    \n    # Classification branch\n    self$classification_layer <- nn_linear(hidden_size, cls_output_size)\n  },\n  \n  forward = function(x) {\n    # Shared feature extraction\n    shared_features <- x %>%\n      self$shared_layer1() %>%\n      nnf_relu() %>%\n      self$shared_layer2() %>%\n      nnf_relu()\n    \n    # Task-specific predictions\n    regression_output <- self$regression_layer(shared_features)\n    classification_logits <- self$classification_layer(shared_features)\n    \n    list(\n      regression = regression_output,\n      classification = classification_logits\n    )\n  }\n)\n\n# Create model instance\nmodel <- multi_task_net(\n  input_size = 5,\n  hidden_size = 10\n)\n\n# Print model architecture\nprint(model)\nAn `nn_module` containing 192 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• shared_layer1: <nn_linear> #60 parameters\n• shared_layer2: <nn_linear> #110 parameters\n• regression_layer: <nn_linear> #11 parameters\n• classification_layer: <nn_linear> #11 parameters\n4. Define Loss Functions and Optimizer\nMulti-task learning requires separate loss functions for each task.\n# Loss functions\nregression_loss_fn <- nnf_mse_loss  # Mean squared error for regression\nclassification_loss_fn <- nnf_binary_cross_entropy_with_logits  # Binary cross-entropy for classification\n\n# Optimizer with weight decay for L2 regularization\noptimizer <- optim_adam(model$parameters, lr = 0.01)\n\n# Task weights - these control the relative importance of each task\ntask_weights <- c(regression = 0.5, classification = 0.5)\n\n# Validation split from training data\nval_size <- round(0.2 * length(train_idx))\nval_indices <- sample(train_idx, val_size)\ntrain_indices <- setdiff(train_idx, val_indices)\n\n# Create validation sets\nx_val <- x[val_indices, ]\ny_reg_val <- y_regression[val_indices]\ny_cls_val <- y_classification[val_indices]\n\n# Update training sets\nx_train <- x[train_indices, ]\ny_reg_train <- y_regression[train_indices]\ny_cls_train <- y_classification[train_indices]\nTraining Loop\n# Hyperparameters\nepochs <- 100  # Increased epochs since we have early stopping\n\n# Enhanced training history tracking\ntraining_history <- data.frame(\n  epoch = integer(),\n  train_reg_loss = numeric(),\n  train_cls_loss = numeric(),\n  train_total_loss = numeric(),\n  val_reg_loss = numeric(),\n  val_cls_loss = numeric(),\n  val_total_loss = numeric(),\n  val_accuracy = numeric()\n)\n\nfor (epoch in 1:epochs) {\n  # Training phase\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass on training data\n  outputs <- model(x_train)\n  \n  # Calculate training loss for each task\n  train_reg_loss <- regression_loss_fn(\n    outputs$regression$squeeze(), \n    y_reg_train\n  )\n  \n  train_cls_loss <- classification_loss_fn(\n    outputs$classification$squeeze(), \n    y_cls_train\n  )\n  \n  # Weighted combined training loss\n  train_total_loss <- task_weights[\"regression\"] * train_reg_loss + \n    task_weights[\"classification\"] * train_cls_loss\n  \n  # Backward pass and optimize\n  train_total_loss$backward()\n  \n  # Gradient clipping to prevent exploding gradients\n  nn_utils_clip_grad_norm_(model$parameters, max_norm = 1.0)\n  \n  optimizer$step()\n  \n  # Validation phase\n  model$eval()\n  \n  with_no_grad({\n    val_outputs <- model(x_val)\n    \n    # Calculate validation losses\n    val_reg_loss <- regression_loss_fn(\n      val_outputs$regression$squeeze(), \n      y_reg_val\n    )\n    \n    val_cls_loss <- classification_loss_fn(\n      val_outputs$classification$squeeze(), \n      y_cls_val\n    )\n    \n    val_total_loss <- task_weights[\"regression\"] * val_reg_loss + task_weights[\"classification\"] * val_cls_loss\n    \n    # Calculate validation accuracy\n    val_cls_probs <- nnf_sigmoid(val_outputs$classification$squeeze())\n    val_cls_preds <- (val_cls_probs > 0.5)$to(torch_int())\n    val_accuracy <- (val_cls_preds == y_cls_val$to(torch_int()))$sum()$item() / length(val_indices)\n  })\n  \n  # Record history\n  training_history <- rbind(\n    training_history,\n    data.frame(\n      epoch = epoch,\n      train_reg_loss = as.numeric(train_reg_loss$item()),\n      train_cls_loss = as.numeric(train_cls_loss$item()),\n      train_total_loss = as.numeric(train_total_loss$item()),\n      val_reg_loss = as.numeric(val_reg_loss$item()),\n      val_cls_loss = as.numeric(val_cls_loss$item()),\n      val_total_loss = as.numeric(val_total_loss$item()),\n      val_accuracy = val_accuracy\n    )\n  )\n  \n  # Print progress every 25 epochs\n  if (epoch %% 25 == 0 || epoch == 1) {\n    cat(sprintf(\"Epoch %d - Train Loss: %.4f, Val Loss: %.4f, Val Acc: %.3f\\n\", \n                epoch, \n                train_total_loss$item(), \n                val_total_loss$item(), \n                val_accuracy))\n  }\n\n}\nEpoch 1 - Train Loss: 0.7958, Val Loss: 0.7369, Val Acc: 0.493\nEpoch 25 - Train Loss: 0.3267, Val Loss: 0.3035, Val Acc: 0.821\nEpoch 50 - Train Loss: 0.1548, Val Loss: 0.1350, Val Acc: 0.971\nEpoch 75 - Train Loss: 0.0599, Val Loss: 0.0479, Val Acc: 0.993\nEpoch 100 - Train Loss: 0.0381, Val Loss: 0.0356, Val Acc: 1.000\nModel Evaluation\n# Set model to evaluation mode\nmodel$eval()\n\n# Make predictions on test set\nwith_no_grad({\n  outputs <- model(x_test)\n  \n  # Regression evaluation\n  reg_preds <- outputs$regression$squeeze()\n  reg_test_loss <- regression_loss_fn(reg_preds, y_reg_test)\n  \n  # Classification evaluation\n  cls_preds <- outputs$classification$squeeze()\n  cls_probs <- nnf_sigmoid(cls_preds)\n  cls_test_loss <- classification_loss_fn(cls_preds, y_cls_test)\n  \n  # Convert predictions to binary (threshold = 0.5)\n  cls_pred_labels <- (cls_probs > 0.5)$to(torch_int())\n  \n  # Calculate accuracy\n  accuracy <- (cls_pred_labels == y_cls_test$to(torch_int()))$sum()$item() / length(test_idx)\n})\n\n# Calculate additional metrics\nreg_preds_r <- as.numeric(reg_preds)\ny_reg_test_r <- as.numeric(y_reg_test)\ncls_probs_r <- as.numeric(cls_probs)\ny_cls_test_r <- as.numeric(y_cls_test)\n\n# Regression metrics\nrmse <- sqrt(mean((reg_preds_r - y_reg_test_r)^2))\nmae <- mean(abs(reg_preds_r - y_reg_test_r))\nr_squared <- cor(reg_preds_r, y_reg_test_r)^2\n\n# Classification metrics\nauc <- pROC::auc(pROC::roc(y_cls_test_r, cls_probs_r, quiet = TRUE))\n\n# Display results\nperformance_results <- data.frame(\n  Task = c(\"Regression\", \"Regression\", \"Regression\", \"Classification\", \"Classification\", \"Classification\"),\n  Metric = c(\"Test Loss (MSE)\", \"RMSE\", \"R-squared\", \"Test Loss (BCE)\", \"Accuracy\", \"AUC\"),\n  Value = c(\n    round(reg_test_loss$item(), 4),\n    round(rmse, 4),\n    round(r_squared, 4),\n    round(cls_test_loss$item(), 4),\n    round(accuracy * 100, 2), \n    round(auc * 100, 2)\n  )\n)\n\nprint(performance_results)\nTask          Metric   Value\n1     Regression Test Loss (MSE)  0.0533\n2     Regression            RMSE  0.2308\n3     Regression       R-squared  0.9449\n4 Classification Test Loss (BCE)  0.0492\n5 Classification        Accuracy 98.0000\n6 Classification             AUC 99.9400\nVisualization and Overfitting Analysis\n# Plot enhanced training history with overfitting detection\np1 <- training_history %>%\n  select(epoch, train_total_loss, val_total_loss) %>%\n  pivot_longer(cols = c(train_total_loss, val_total_loss), \n               names_to = \"split\", values_to = \"loss\") %>%\n  mutate(split = case_when(\n    split == \"train_total_loss\" ~ \"Training\",\n    split == \"val_total_loss\" ~ \"Validation\"\n  )) %>%\n  ggplot(aes(x = epoch, y = loss, color = split)) +\n  geom_line(size = 1) +\n  geom_vline(xintercept = which.min(training_history$val_total_loss), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Training vs Validation Loss\",\n       subtitle = \"Red line shows optimal stopping point\",\n       x = \"Epoch\", y = \"Total Loss\", color = \"Dataset\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Separate task losses\np2 <- training_history %>%\n  select(epoch, train_reg_loss, val_reg_loss, train_cls_loss, val_cls_loss) %>%\n  pivot_longer(cols = -epoch, names_to = \"metric\", values_to = \"loss\") %>%\n  separate(metric, into = c(\"split\", \"task\", \"loss_type\"), sep = \"_\") %>%\n  mutate(\n    split = ifelse(split == \"train\", \"Training\", \"Validation\"),\n    task = ifelse(task == \"reg\", \"Regression\", \"Classification\"),\n    metric_name = paste(split, task)\n  ) %>%\n  ggplot(aes(x = epoch, y = loss, color = metric_name)) +\n  geom_line(size = 1) +\n  facet_wrap(~task, scales = \"free_y\") +\n  labs(title = \"Task-Specific Loss Curves\",\n       subtitle = \"Monitoring overfitting in individual tasks\",\n       x = \"Epoch\", y = \"Loss\", color = \"Split & Task\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")\n\n# Validation accuracy progression\np3 <- ggplot(training_history, aes(x = epoch, y = val_accuracy)) +\n  geom_line(color = \"#2c3e50\", size = 1) +\n  geom_hline(yintercept = max(training_history$val_accuracy), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(title = \"Validation Accuracy Progression\",\n       subtitle = paste(\"Peak accuracy:\", round(max(training_history$val_accuracy), 3)),\n       x = \"Epoch\", y = \"Validation Accuracy\") +\n  theme_minimal()\n\n# Overfitting analysis\ntraining_history$overfitting_gap <- training_history$train_total_loss - training_history$val_total_loss\n\np4 <- ggplot(training_history, aes(x = epoch, y = overfitting_gap)) +\n  geom_line(color = \"#e74c3c\", size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"Overfitting Gap Analysis\",\n       subtitle = \"Difference between training and validation loss\",\n       x = \"Epoch\", y = \"Training Loss - Validation Loss\") +\n  theme_minimal()\n\n# Regression predictions vs actual values\nregression_results <- data.frame(\n  Actual = y_reg_test_r,\n  Predicted = reg_preds_r\n)\n\np5 <- ggplot(regression_results, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.6, color = \"#2c3e50\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", color = \"#3498db\", se = TRUE) +\n  labs(title = \"Regression Task: Actual vs Predicted Values\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \", RMSE =\", round(rmse, 3)),\n       x = \"Actual Values\", y = \"Predicted Values\") +\n  theme_minimal()\n\n# Classification probability distribution\ncls_results <- data.frame(\n  Probability = cls_probs_r,\n  Actual_Class = factor(y_cls_test_r, labels = c(\"Class 0\", \"Class 1\"))\n)\n\np6 <- ggplot(cls_results, aes(x = Probability, fill = Actual_Class)) +\n  geom_histogram(alpha = 0.7, bins = 20, position = \"identity\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Classification Task: Predicted Probabilities\",\n       subtitle = paste(\"Accuracy =\", round(accuracy * 100, 1), \"%\"),\n       x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 | p3) / (p2) / (p4) / (p5 | p6)\nKey Takeaways\nArchitecture Design\n: The shared-private paradigm enables models to learn both common and task-specific representations\nLoss Combination\n: Properly weighting multiple loss functions proves crucial for balanced learning across tasks\nEvaluation Strategy\n: Each task requires appropriate metrics, and overall model success depends on performance across all tasks\nParameter Efficiency\n: Multi-task models can achieve comparable performance with fewer total parameters when properly regularized\nKnowledge Transfer\n: Related tasks can benefit from shared feature learning, especially when data is limited\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR'tichoke\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Multi-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information acros...",
      "meta_keywords": null,
      "og_description": "Multi-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information acros...",
      "og_image": "https://rtichoke.netlify.app/posts/multi-task-learning-with-torch_files/figure-html/unnamed-chunk-1-1.png",
      "og_title": "Multi-Task Learning with torch in R | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 8.4,
      "sitemap_lastmod": null,
      "twitter_description": "Multi-task learning (MTL) is an approach where a single neural network model is trained to perform multiple related tasks simultaneously. This methodology can improve model generalization, reduce overfitting, and leverage shared information acros...",
      "twitter_title": "Multi-Task Learning with torch in R | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/05/multi-task-learning-with-torch-in-r/",
      "word_count": 1689
    }
  }
}