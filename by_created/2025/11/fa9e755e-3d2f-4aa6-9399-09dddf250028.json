{
  "uuid": "fa9e755e-3d2f-4aa6-9399-09dddf250028",
  "created_at": "2025-11-17 20:40:41",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2009/07/parsing-geo-soft-files-with-python-and-sqlite/",
    "crawled_at": "2025-11-17T10:32:19.872025",
    "external_links": [
      {
        "href": "https://digitheadslabnotebook.blogspot.com/2009/07/parsing-geo-soft-files-with-python-and.html",
        "text": "Digithead's Lab Notebook"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "http://www2.warwick.ac.uk/fac/sci/moac/currentstudents/peter_cock/r/geo/",
        "text": "easier"
      },
      {
        "href": "http://bioinformatics.oxfordjournals.org/cgi/content/abstract/btm254v1",
        "text": "way"
      },
      {
        "href": "http://gaggle.systemsbiology.net/docs/geese/genomebrowser/",
        "text": "genome browser"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "http://www.sqlite.org/",
        "text": "SQLite"
      },
      {
        "href": "https://www.python.org/",
        "text": "Python"
      },
      {
        "href": "http://www.artima.com/weblogs/viewpost.jsp?thread=4829",
        "text": "main function"
      },
      {
        "href": "https://docs.python.org/library/optparse.html#module-optparse",
        "text": "optparse"
      },
      {
        "href": "https://docs.python.org/library/getopt.html",
        "text": "getopt"
      },
      {
        "href": "https://digitheadslabnotebook.blogspot.com/2009/07/parsing-geo-soft-files-with-python-and.html",
        "text": "here"
      },
      {
        "href": "https://digitheadslabnotebook.blogspot.com/2009/07/parsing-geo-soft-files-with-python-and.html",
        "text": "Digithead's Lab Notebook"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Parsing GEO SOFT files with Python and Sqlite | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": null,
        "src": "https://blogger.googleusercontent.com/tracker/5964816804623588850-4663563734322412342?l=digitheadslabnotebook.blogspot.com"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://blogger.googleusercontent.com/tracker/5964816804623588850-4663563734322412342?l=digitheadslabnotebook.blogspot.com"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/chris/",
        "text": "Chris"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-498 post type-post status-publish format-standard hentry\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Parsing GEO SOFT files with Python and Sqlite</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">July 17, 2009</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/chris/\">Chris</a></span>  in Uncategorized | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<p class=\"syndicated-attribution\"><!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://digitheadslabnotebook.blogspot.com/2009/07/parsing-geo-soft-files-with-python-and.html\"> Digithead's Lab Notebook</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div></p>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.0--><p>NCBI’s GEO database of gene expression data is a great resource, but its records are very open ended. This lack of rigidity was perhaps necessary to accommodate the variety of measurement technologies, but makes getting data out a little tricky. But, all that flexibility is a curse from the point of view of extracting data. The scripts I end up with are not general parsers for GEO data, but will need to be adapted to the specifics of other datasets.</p><p><span style=\"font-weight: bold; color:red;\">Note:</span> It could be that I’m doing things the hard way. Maybe there’s an <a href=\"http://www2.warwick.ac.uk/fac/sci/moac/currentstudents/peter_cock/r/geo/\" rel=\"nofollow\" target=\"_blank\">easier</a> <a href=\"http://bioinformatics.oxfordjournals.org/cgi/content/abstract/btm254v1\" rel=\"nofollow\" target=\"_blank\">way</a>.</p><p>A GEO record consists of a platform, which describes (for example) a microarray and its probes, and series of samples. In this example, we need to do a join between the platform and the sample records to end up with a matrix of the form <i>(seq, strand, start, end, value1, value2, …, valueN)</i> where the value1 column holds measurements from the first sample and so on. If we do that, we’ll have coordinates on the genome and values for each measurement. My goal is to feed data into a <a href=\"http://gaggle.systemsbiology.net/docs/geese/genomebrowser/\" rel=\"nofollow\" target=\"_blank\">genome browser</a> known as HeebieGB with a stop-over in <a href=\"https://www.r-project.org/\" rel=\"nofollow\" target=\"_blank\">R</a> along the way.</p><p>Merging on a common key is only slightly complicated, but tiling arrays are big (~244,000 probes in this case). I hesitate to try merging several 244K row tables in memory. Database engines are made for this sort of thing, so I’ll use <a href=\"http://www.sqlite.org/\" rel=\"nofollow\" target=\"_blank\">SQLite</a> to get this done and <a href=\"https://www.python.org/\" rel=\"nofollow\" target=\"_blank\">Python</a> to script the whole process.</p><p>I like to start python scripts with a template similar to Guido’s <a href=\"http://www.artima.com/weblogs/viewpost.jsp?thread=4829\" rel=\"nofollow\" target=\"_blank\">main function</a>, except that I prefer <i><a href=\"https://docs.python.org/library/optparse.html#module-optparse\" rel=\"nofollow\" target=\"_blank\">optparse</a></i> to <i><a href=\"https://docs.python.org/library/getopt.html\" rel=\"nofollow\" target=\"_blank\">getopt</a></i>. An <i>–overwrite</i> option will force the user to be conscious of overwriting files.</p><pre>\nimport sys\nfrom optparse import OptionParser\n\ndef main():\n usage = \"%prog [options] input_file sqlite_db_file\"\n parser = OptionParser(usage=usage)\n parser.add_option(\"-o\", \"--overwrite\", dest=\"overwrite\", default=False, action=\"store_true\", \n  help=\"if output db file exists, overwrite it\")\n (options, args) = parser.parse_args()\n\n if len(args) &lt; 2:\n  parser.error(\"missing required arguments.\")\n  exit(2)\n\n input_filename = args[0]\n db_filename = args[1]\n\nif __name__ == \"__main__\":\n sys.exit(main())\n</pre><p>GEO records a bunch of descriptive data about each sample, some of which we want. I've read that storing arbitrary key-value pairs in a relational DB is considered bad by some. But, I'm going to do it anyway. The entity attributes will go in a table called <i>attributes</i> whose schema is <i>(entity_id, key, value)</i>.</p><p>The function <i>parse_platform_table</i> pulls the platform data from a tab-separated section in the SOFT file into a table with a schema something like this: <i>(id, sequence, strand, start, end)</i>. There's also a tab-separated section for each of the samples that refers back to its platform, so I extract that in a similar manner in <i>parse_sample_table</i>. It's easiest to start out with each sample in its own table, even though that's not really what we want.</p><p>The complete script -also available from SVN <a href=\"https://digitheadslabnotebook.blogspot.com/2009/07/parsing-geo-soft-files-with-python-and.html\" rel=\"nofollow\" target=\"_blank\">here</a>- ends up like this:</p><pre>\nimport sys\nfrom optparse import OptionParser\nimport re\nimport os\nimport os.path\nimport sqlite3\n\n# GEO SOFT format is documented here:\n# http://www.ncbi.nlm.nih.gov/projects/geo/info/soft2.html#SOFTformat\n\n# ID field in platform joins with ID_REF field in samples\n\nentity       = re.compile(r'\\^(\\S+) = (.+)')\nkvp          = re.compile(r'!(\\S+) = (.+)')\n\nSTATE_START = 0\nSTATE_IN_SERIES = 1001\nSTATE_IN_PLATFORM = 1002\nSTATE_IN_SAMPLE = 1003\n\n\ndef overwrite(name):\n if os.path.exists(name):\n  os.remove(name)\n  return True\n return False\n\ndef parse_series_file(file, conn):\n entity_id = None\n state = STATE_START\n\n # create an attributes table\n try:\n  cursor = conn.cursor()\n  cursor.execute('create table attributes (entity_id, key, value);')\n  conn.commit()\n  cursor.close()\n finally:\n  cursor.close()\n\n for line in file:\n  line = line.strip()\n\n  # read entity tags\n  if line.startswith('^'):\n   m = entity.match(line)\n   if m:\n    entity_type = m.group(1)\n    entity_id = m.group(2)\n    print(entity_id)\n    if entity_type == 'SERIES':\n     state = STATE_IN_SERIES\n    elif entity_type == 'PLATFORM':\n     state = STATE_IN_PLATFORM\n    elif entity_type == 'SAMPLE':\n     state = STATE_IN_SAMPLE\n\n  # read attribute key-value pairs and tab-separated tables\n  elif line.startswith('!'):\n   m = kvp.match(line)\n   if m:\n    key = m.group(1)\n    value = m.group(2)\n    handle_attribute(conn, entity_id, key, value)\n   elif state==STATE_IN_PLATFORM and line=='!platform_table_begin':\n    parse_platform_table(file, conn, entity_id)\n   elif state==STATE_IN_SAMPLE and line=='!sample_table_begin':\n    parse_sample_table(file, conn, entity_id)\n\ndef parse_platform_table(file, conn, platform_id):\n \"\"\"\n Read the tab-separated platform section of a SOFT file and store the ID,\n sequence, strand, start, and end columns in a SQLite database.\n\n file: a file object open for reading\n conn: a SQLite database connection\n platform_id: a string identifying a GEO platform\n \"\"\"\n cursor = conn.cursor()\n try:\n  # throw away line containing column headers\n  file.next()\n  # create platform table\n  cursor.execute('create table %s (id integer primary key not null, sequence text not null, strand not null, start integer not null, end integer not null, control_type integer);' % (platform_id))\n  conn.commit()\n  sql = 'insert into %s values(?,?,?,?,?,?)' % (platform_id)\n  for line in file:\n   line = line.strip('\\n')\n   if (line.strip() == '!platform_table_end'):\n    break\n   fields = line.split(\"\\t\")\n   cursor.execute(sql, (int(fields[0]), fields[6], fields[10], fields[7], fields[8], fields[4]))\n  conn.commit()\n finally:\n  cursor.close()\n\ndef parse_sample_table(file, conn, sample_id):\n \"\"\"\n Read a tab separated sample section from a SOFT file and store ID_REF and\n value in a SQLite DB.\n\n file: a file object open for reading\n conn: a SQLite database connection\n sample_id: a string identifying a GEO sample\n \"\"\"\n cursor = conn.cursor()\n try:\n  # throw away line containing column headers\n  file.next()\n  # create sample table\n  cursor.execute('create table %s (id_ref integer not null, value numeric not null);' % (sample_id))\n  conn.commit()\n  sql = 'insert into %s values(?,?)' % (sample_id)\n  for line in file:\n   line = line.strip('\\n')\n   if (line.strip() == '!sample_table_end'):\n    break\n   fields = line.split(\"\\t\")\n   cursor.execute(sql, (int(fields[0]), float(fields[1])))\n  conn.commit()\n finally:\n  cursor.close()\n\ndef handle_attribute(conn, entity_id, key, value):\n \"\"\"\n Store an entity attribute in the attributes table\n \"\"\"\n cursor = None\n try:\n  cursor = conn.cursor()\n  cursor.execute(\"insert into attributes values(?,?,?);\", (entity_id, key, value))\n  conn.commit()\n finally:\n  if cursor:\n   cursor.close()\n\n\ndef main():\n usage = \"%prog [options] input_file\"\n parser = OptionParser(usage=usage)\n parser.add_option(\"-o\", \"--overwrite\", dest=\"overwrite\", default=False, action=\"store_true\", \n  help=\"if output db file exists, overwrite it\")\n (options, args) = parser.parse_args()\n\n if len(args) &lt; 2:\n  parser.error(\"missing required arguments.\")\n  exit(2)\n\n input_filename = args[0]\n db_filename = args[1]\n\n if options.overwrite:\n  overwrite(db_filename)\n\n input_file = None\n conn = None\n try:\n  conn = sqlite3.connect(db_filename)\n  input_file = open(input_filename, 'r')\n  parse_series_file(input_file, conn)\n finally:\n  if input_file:\n   input_file.close()\n  if conn:\n   conn.close()\n\n\nif __name__ == \"__main__\":\n sys.exit(main())\n</pre><p>The specific series I'm interested in (GSE12923) has 53 samples. The platform (GPL7255) is a custom array on Agilent's 244k feature microarrays or just short of 13 million individual features. The SOFT file is 708 MB and the script takes a good 5 or 6 minutes to ingest all that data. The next step is merging all the data into a single matrix.</p><p>This turned out to be harder than I thought. At first, I naively tried to do a big 54 way join between the platform table and all the sample tables, with an order-by to sort by chromosomal location. I let this run for a couple hours, then gave up. Sure, a big join on unindexed tables was bound to be ugly, but it only had to run once. I'm still surprised that this choked, after all, it's not <i>that</i> much data.</p><p>There are two ways around it. One is to index the sample tables by ID_REF and the platform table by (sequence, strand, start, end). The other is to do the big join then sort into a second table. Either takes several minutes, but it's just a one-off, so that's OK.</p><pre>\ninsert into matrix\nselect GPL7255.sequence, GPL7255.strand, GPL7255.start, GPL7255.end,\nGSM320660.VALUE as GSM320660,\nGSM320661.VALUE as GSM320661,\n...GSM320712.VALUE as GSM320712\nfrom GPL7255\njoin GSM320660 on GPL7255.ID = GSM320660.ID_REF\njoin GSM320661 on GPL7255.ID = GSM320661.ID_REF\n...join GSM320712 on GPL7255.ID = GSM320712.ID_REF\nwhere GPL7255.control_type==0 and sequence!='NA';\norder by sequence, strand, start, end;\n</pre><p>Now that we've done that, do you ever find data that doesn't need to be cleaned up a little bit?</p><pre>\n-- swap mislabeled + and - strands (how embarrassing!)\nupdate matrix set strand='Z' where strand='-';\nupdate matrix set strand='-' where strand='+';\nupdate matrix set strand='+' where strand='Z';\n\n-- fix up sequence names\nupdate matrix set sequence='chromosome' where sequence='chr1';\nupdate matrix set sequence='pNRC200' where sequence='chr2';\nupdate matrix set sequence='pNRC100' where sequence='chr3';\n\n-- fix probes crossing the \"zero\" point\nupdate matrix set start=end, end=start where end-start &gt; 60;\n</pre><p>That's about all the data munging I can stand for now. The rest, I'll leave for Part 2.</p><div class=\"blogger-post-footer\"><img alt=\"\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://blogger.googleusercontent.com/tracker/5964816804623588850-4663563734322412342?l=digitheadslabnotebook.blogspot.com&amp;is-pending-load=1\" height=\"1\" src=\"https://blogger.googleusercontent.com/tracker/5964816804623588850-4663563734322412342?l=digitheadslabnotebook.blogspot.com\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" width=\"1\"/><noscript><img alt=\"\" height=\"1\" src=\"https://blogger.googleusercontent.com/tracker/5964816804623588850-4663563734322412342?l=digitheadslabnotebook.blogspot.com\" width=\"1\"/></noscript></div>\n\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.0-->\n<p class=\"syndicated-attribution\"><div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://digitheadslabnotebook.blogspot.com/2009/07/parsing-geo-soft-files-with-python-and.html\"> Digithead's Lab Notebook</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div></p> </div>\n</article>",
    "main_text": "Parsing GEO SOFT files with Python and Sqlite\nPosted on\nJuly 17, 2009\nby\nChris\nin Uncategorized | 0 Comments\n[This article was first published on\nDigithead's Lab Notebook\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nNCBI’s GEO database of gene expression data is a great resource, but its records are very open ended. This lack of rigidity was perhaps necessary to accommodate the variety of measurement technologies, but makes getting data out a little tricky. But, all that flexibility is a curse from the point of view of extracting data. The scripts I end up with are not general parsers for GEO data, but will need to be adapted to the specifics of other datasets.\nNote:\nIt could be that I’m doing things the hard way. Maybe there’s an\neasier\nway\n.\nA GEO record consists of a platform, which describes (for example) a microarray and its probes, and series of samples. In this example, we need to do a join between the platform and the sample records to end up with a matrix of the form\n(seq, strand, start, end, value1, value2, …, valueN)\nwhere the value1 column holds measurements from the first sample and so on. If we do that, we’ll have coordinates on the genome and values for each measurement. My goal is to feed data into a\ngenome browser\nknown as HeebieGB with a stop-over in\nR\nalong the way.\nMerging on a common key is only slightly complicated, but tiling arrays are big (~244,000 probes in this case). I hesitate to try merging several 244K row tables in memory. Database engines are made for this sort of thing, so I’ll use\nSQLite\nto get this done and\nPython\nto script the whole process.\nI like to start python scripts with a template similar to Guido’s\nmain function\n, except that I prefer\noptparse\nto\ngetopt\n. An\n–overwrite\noption will force the user to be conscious of overwriting files.\nimport sys\nfrom optparse import OptionParser\n\ndef main():\n usage = \"%prog [options] input_file sqlite_db_file\"\n parser = OptionParser(usage=usage)\n parser.add_option(\"-o\", \"--overwrite\", dest=\"overwrite\", default=False, action=\"store_true\", \n  help=\"if output db file exists, overwrite it\")\n (options, args) = parser.parse_args()\n\n if len(args) < 2:\n  parser.error(\"missing required arguments.\")\n  exit(2)\n\n input_filename = args[0]\n db_filename = args[1]\n\nif __name__ == \"__main__\":\n sys.exit(main())\nGEO records a bunch of descriptive data about each sample, some of which we want. I've read that storing arbitrary key-value pairs in a relational DB is considered bad by some. But, I'm going to do it anyway. The entity attributes will go in a table called\nattributes\nwhose schema is\n(entity_id, key, value)\n.\nThe function\nparse_platform_table\npulls the platform data from a tab-separated section in the SOFT file into a table with a schema something like this:\n(id, sequence, strand, start, end)\n. There's also a tab-separated section for each of the samples that refers back to its platform, so I extract that in a similar manner in\nparse_sample_table\n. It's easiest to start out with each sample in its own table, even though that's not really what we want.\nThe complete script -also available from SVN\nhere\n- ends up like this:\nimport sys\nfrom optparse import OptionParser\nimport re\nimport os\nimport os.path\nimport sqlite3\n\n# GEO SOFT format is documented here:\n# http://www.ncbi.nlm.nih.gov/projects/geo/info/soft2.html#SOFTformat\n\n# ID field in platform joins with ID_REF field in samples\n\nentity       = re.compile(r'\\^(\\S+) = (.+)')\nkvp          = re.compile(r'!(\\S+) = (.+)')\n\nSTATE_START = 0\nSTATE_IN_SERIES = 1001\nSTATE_IN_PLATFORM = 1002\nSTATE_IN_SAMPLE = 1003\n\ndef overwrite(name):\n if os.path.exists(name):\n  os.remove(name)\n  return True\n return False\n\ndef parse_series_file(file, conn):\n entity_id = None\n state = STATE_START\n\n # create an attributes table\n try:\n  cursor = conn.cursor()\n  cursor.execute('create table attributes (entity_id, key, value);')\n  conn.commit()\n  cursor.close()\n finally:\n  cursor.close()\n\n for line in file:\n  line = line.strip()\n\n  # read entity tags\n  if line.startswith('^'):\n   m = entity.match(line)\n   if m:\n    entity_type = m.group(1)\n    entity_id = m.group(2)\n    print(entity_id)\n    if entity_type == 'SERIES':\n     state = STATE_IN_SERIES\n    elif entity_type == 'PLATFORM':\n     state = STATE_IN_PLATFORM\n    elif entity_type == 'SAMPLE':\n     state = STATE_IN_SAMPLE\n\n  # read attribute key-value pairs and tab-separated tables\n  elif line.startswith('!'):\n   m = kvp.match(line)\n   if m:\n    key = m.group(1)\n    value = m.group(2)\n    handle_attribute(conn, entity_id, key, value)\n   elif state==STATE_IN_PLATFORM and line=='!platform_table_begin':\n    parse_platform_table(file, conn, entity_id)\n   elif state==STATE_IN_SAMPLE and line=='!sample_table_begin':\n    parse_sample_table(file, conn, entity_id)\n\ndef parse_platform_table(file, conn, platform_id):\n \"\"\"\n Read the tab-separated platform section of a SOFT file and store the ID,\n sequence, strand, start, and end columns in a SQLite database.\n\n file: a file object open for reading\n conn: a SQLite database connection\n platform_id: a string identifying a GEO platform\n \"\"\"\n cursor = conn.cursor()\n try:\n  # throw away line containing column headers\n  file.next()\n  # create platform table\n  cursor.execute('create table %s (id integer primary key not null, sequence text not null, strand not null, start integer not null, end integer not null, control_type integer);' % (platform_id))\n  conn.commit()\n  sql = 'insert into %s values(?,?,?,?,?,?)' % (platform_id)\n  for line in file:\n   line = line.strip('\\n')\n   if (line.strip() == '!platform_table_end'):\n    break\n   fields = line.split(\"\\t\")\n   cursor.execute(sql, (int(fields[0]), fields[6], fields[10], fields[7], fields[8], fields[4]))\n  conn.commit()\n finally:\n  cursor.close()\n\ndef parse_sample_table(file, conn, sample_id):\n \"\"\"\n Read a tab separated sample section from a SOFT file and store ID_REF and\n value in a SQLite DB.\n\n file: a file object open for reading\n conn: a SQLite database connection\n sample_id: a string identifying a GEO sample\n \"\"\"\n cursor = conn.cursor()\n try:\n  # throw away line containing column headers\n  file.next()\n  # create sample table\n  cursor.execute('create table %s (id_ref integer not null, value numeric not null);' % (sample_id))\n  conn.commit()\n  sql = 'insert into %s values(?,?)' % (sample_id)\n  for line in file:\n   line = line.strip('\\n')\n   if (line.strip() == '!sample_table_end'):\n    break\n   fields = line.split(\"\\t\")\n   cursor.execute(sql, (int(fields[0]), float(fields[1])))\n  conn.commit()\n finally:\n  cursor.close()\n\ndef handle_attribute(conn, entity_id, key, value):\n \"\"\"\n Store an entity attribute in the attributes table\n \"\"\"\n cursor = None\n try:\n  cursor = conn.cursor()\n  cursor.execute(\"insert into attributes values(?,?,?);\", (entity_id, key, value))\n  conn.commit()\n finally:\n  if cursor:\n   cursor.close()\n\ndef main():\n usage = \"%prog [options] input_file\"\n parser = OptionParser(usage=usage)\n parser.add_option(\"-o\", \"--overwrite\", dest=\"overwrite\", default=False, action=\"store_true\", \n  help=\"if output db file exists, overwrite it\")\n (options, args) = parser.parse_args()\n\n if len(args) < 2:\n  parser.error(\"missing required arguments.\")\n  exit(2)\n\n input_filename = args[0]\n db_filename = args[1]\n\n if options.overwrite:\n  overwrite(db_filename)\n\n input_file = None\n conn = None\n try:\n  conn = sqlite3.connect(db_filename)\n  input_file = open(input_filename, 'r')\n  parse_series_file(input_file, conn)\n finally:\n  if input_file:\n   input_file.close()\n  if conn:\n   conn.close()\n\nif __name__ == \"__main__\":\n sys.exit(main())\nThe specific series I'm interested in (GSE12923) has 53 samples. The platform (GPL7255) is a custom array on Agilent's 244k feature microarrays or just short of 13 million individual features. The SOFT file is 708 MB and the script takes a good 5 or 6 minutes to ingest all that data. The next step is merging all the data into a single matrix.\nThis turned out to be harder than I thought. At first, I naively tried to do a big 54 way join between the platform table and all the sample tables, with an order-by to sort by chromosomal location. I let this run for a couple hours, then gave up. Sure, a big join on unindexed tables was bound to be ugly, but it only had to run once. I'm still surprised that this choked, after all, it's not\nthat\nmuch data.\nThere are two ways around it. One is to index the sample tables by ID_REF and the platform table by (sequence, strand, start, end). The other is to do the big join then sort into a second table. Either takes several minutes, but it's just a one-off, so that's OK.\ninsert into matrix\nselect GPL7255.sequence, GPL7255.strand, GPL7255.start, GPL7255.end,\nGSM320660.VALUE as GSM320660,\nGSM320661.VALUE as GSM320661,\n...GSM320712.VALUE as GSM320712\nfrom GPL7255\njoin GSM320660 on GPL7255.ID = GSM320660.ID_REF\njoin GSM320661 on GPL7255.ID = GSM320661.ID_REF\n...join GSM320712 on GPL7255.ID = GSM320712.ID_REF\nwhere GPL7255.control_type==0 and sequence!='NA';\norder by sequence, strand, start, end;\nNow that we've done that, do you ever find data that doesn't need to be cleaned up a little bit?\n-- swap mislabeled + and - strands (how embarrassing!)\nupdate matrix set strand='Z' where strand='-';\nupdate matrix set strand='-' where strand='+';\nupdate matrix set strand='+' where strand='Z';\n\n-- fix up sequence names\nupdate matrix set sequence='chromosome' where sequence='chr1';\nupdate matrix set sequence='pNRC200' where sequence='chr2';\nupdate matrix set sequence='pNRC100' where sequence='chr3';\n\n-- fix probes crossing the \"zero\" point\nupdate matrix set start=end, end=start where end-start > 60;\nThat's about all the data munging I can stand for now. The rest, I'll leave for Part 2.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nDigithead's Lab Notebook\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "NCBI's GEO database of gene expression data is a great resource, but its records are very open ended. This lack of rigidity was perhaps necessary to accommodate the variety of measurement technologies, but makes getting data out a little tricky. But, a...",
    "meta_keywords": null,
    "og_description": "NCBI's GEO database of gene expression data is a great resource, but its records are very open ended. This lack of rigidity was perhaps necessary to accommodate the variety of measurement technologies, but makes getting data out a little tricky. But, a...",
    "og_image": "https://blogger.googleusercontent.com/tracker/5964816804623588850-4663563734322412342?l=digitheadslabnotebook.blogspot.com",
    "og_title": "Parsing GEO SOFT files with Python and Sqlite | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 8,
    "sitemap_lastmod": "2010-09-06T20:59:57+00:00",
    "twitter_description": "NCBI's GEO database of gene expression data is a great resource, but its records are very open ended. This lack of rigidity was perhaps necessary to accommodate the variety of measurement technologies, but makes getting data out a little tricky. But, a...",
    "twitter_title": "Parsing GEO SOFT files with Python and Sqlite | R-bloggers",
    "url": "https://www.r-bloggers.com/2009/07/parsing-geo-soft-files-with-python-and-sqlite/",
    "word_count": 1595
  }
}