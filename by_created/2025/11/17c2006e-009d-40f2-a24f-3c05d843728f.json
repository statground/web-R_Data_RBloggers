{
  "uuid": "17c2006e-009d-40f2-a24f-3c05d843728f",
  "created_at": "2025-11-17 20:39:06",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2023/11/multivariate-regression-ensemble-models-for-errors-prediction/",
    "crawled_at": "2025-11-17T09:45:14.515395",
    "external_links": [
      {
        "href": "https://petolau.github.io/Multivariate-Regression-Ensemble-Models-for-Errors-Prediction/",
        "text": "Peter Laurinec"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://petolau.github.io/Multistep-loss-optimized-forecasting-with-ADAM/",
        "text": "Multistep forecasting losses"
      },
      {
        "href": "https://predictivehacks.com/how-to-build-stacked-ensemble-models-in-r/",
        "text": "stacking"
      },
      {
        "href": "https://github.com/PetoLau/petolau.github.io/blob/master/_rmd/data_household_predictions.Rdata",
        "text": "GitHub repository"
      },
      {
        "href": "https://petolau.github.io/Multivariate-Regression-Ensemble-Models-for-Errors-Prediction/",
        "text": "Peter Laurinec"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Multivariate Regression Ensemble Models for Errors Prediction | R-bloggers",
    "images": [
      {
        "alt": "plot of chunk unnamed-chunk-7",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-7",
        "base64": null,
        "src": "https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-7-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-10",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-10",
        "base64": null,
        "src": "https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-10-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-14",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-14",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-14-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-18",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-18",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-18-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-20",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-20",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-20-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-21",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-21",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-21-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-22",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-22",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-22-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-23",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-23",
        "base64": null,
        "src": "https://i0.wp.com/petolau.github.io/images/post_15/unnamed-chunk-23-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-24",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-24",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-24-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-25",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-25",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-25-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-28",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-28",
        "base64": null,
        "src": "https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-28-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-33",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-33",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-33-1.png?w=578&ssl=1"
      },
      {
        "alt": "plot of chunk unnamed-chunk-34",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "plot of chunk unnamed-chunk-34",
        "base64": null,
        "src": "https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-34-1.png?w=578&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/peter-laurinec/",
        "text": "Peter Laurinec"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-380406 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Multivariate Regression Ensemble Models for Errors Prediction</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 28, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/peter-laurinec/\">Peter Laurinec</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://petolau.github.io/Multivariate-Regression-Ensemble-Models-for-Errors-Prediction/\"> Peter Laurinec</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9--><p>In the last blog post about <a href=\"https://petolau.github.io/Multistep-loss-optimized-forecasting-with-ADAM/\" rel=\"nofollow\" target=\"_blank\"><strong>Multistep forecasting losses</strong></a>, I showed the usage of the fantastic method <code>adam</code> from the <code>smooth</code> R package on household electricity consumption data, and compared it with benchmarks.</p>\n<p>Since I computed predictions from 10 methods/models for a long period of time, it would be nice to create some ensemble models for precise prediction for our household consumption data. For that purpose, it would be great to predict for example future errors of these methods. It is used in some known ensemble methods, which are not direct about <a href=\"https://predictivehacks.com/how-to-build-stacked-ensemble-models-in-r/\" rel=\"nofollow\" target=\"_blank\"><em>stacking</em></a>.\nPredicting errors can be beneficial for prediction weighting or for predicting the rank of methods (i.e. best one prediction).\nFor the sake of learning something new, I will try multivariate regression models, so learning from multiple targets at once. At least, it has the benefit of simplicity, that we need only one model for all base prediction models.</p>\n<p>What I will show you in this post:</p>\n<ul>\n<li>Prediction of forecasting errors (absolute residuals) of simple exponential smoothing methods.</li>\n<li>Usage of multivariate multiple linear regression benchmarks.</li>\n<li>Check of right rank prediction of forecasting methods.</li>\n<li>Catboost <em>MultiRMSE</em> capability and evaluation if it is more powerful than linear models.</li>\n<li>Computation of two types of ensemble predictions from predicted errors.</li>\n</ul>\n<h3 id=\"simple-forecasting-methods-error-prediction\">Simple forecasting methods error prediction</h3>\n<p>In the beginning, we need to load the required packages, be aware that <code>catboost</code> has to be installed by the <code>devtools</code> package in R.</p>\n<figure class=\"highlight\"><pre>library(data.table)\nlibrary(lubridate)\nlibrary(TSrepr)\nlibrary(ggplot2)\nlibrary(dygraphs)\nlibrary(glmnet)\n# devtools::install_url('https://github.com/catboost/catboost/releases/download/v1.2.2/catboost-R-Windows-1.2.2.tgz')\nlibrary(catboost)</pre></figure>\n<p>I will use predictions and errors from a previous blog post, you can download them from my <a href=\"https://github.com/PetoLau/petolau.github.io/blob/master/_rmd/data_household_predictions.Rdata\" rel=\"nofollow\" target=\"_blank\">GitHub repository</a>.</p>\n<figure class=\"highlight\"><pre>load(\"_rmd/data_household_predictions.Rdata\")\ndata_predictions_all</pre></figure>\n<figure class=\"highlight\"><pre>##                   Date_Time Load_real Load_predicted    model\n##      1: 2023-01-30 00:00:00     0.274     -0.7679345  STL+ETS\n##      2: 2023-01-30 00:15:00     0.363     -0.7959601  STL+ETS\n##      3: 2023-01-30 00:30:00     0.342     -0.7443988  STL+ETS\n##      4: 2023-01-30 00:45:00     0.391     -0.8461708  STL+ETS\n##      5: 2023-01-30 01:00:00     0.230     -0.8708582  STL+ETS\n##     ---                                                      \n## 205436: 2023-08-31 22:45:00     0.080      0.4030999 ADAM-GPL\n## 205437: 2023-08-31 23:00:00     0.039      0.2060178 ADAM-GPL\n## 205438: 2023-08-31 23:15:00     0.087      0.2489654 ADAM-GPL\n## 205439: 2023-08-31 23:30:00     0.080      0.1719251 ADAM-GPL\n## 205440: 2023-08-31 23:45:00     0.040      0.4710469 ADAM-GPL</pre></figure>\n<p>We will model absolute errors of predictions:</p>\n<figure class=\"highlight\"><pre>data_predictions_all[, Error := abs(Load_real - Load_predicted)]</pre></figure>\n<p>To be able to model all method’s predictions at once, we need to cast <strong>predictions and errors</strong> to wide form.</p>\n<figure class=\"highlight\"><pre>data_predictions_all_casted &lt;- dcast(data_predictions_all,\n                                     formula = Date_Time ~ model,\n                                     value.var = c(\"Load_predicted\", \"Error\"))\n \nsetnames(data_predictions_all_casted,\n         colnames(data_predictions_all_casted),\n         gsub(\"-\", \"_\", colnames(data_predictions_all_casted)))\nsetnames(data_predictions_all_casted,\n         colnames(data_predictions_all_casted),\n         gsub(\"\\\\+\", \"_\", colnames(data_predictions_all_casted)))\n \nstr(data_predictions_all_casted)</pre></figure>\n<figure class=\"highlight\"><pre>## Classes 'data.table' and 'data.frame':\t20544 obs. of  21 variables:\n##  $ Date_Time                : POSIXct, format: \"2023-01-30 00:00:00\" \"2023-01-30 00:15:00\" ...\n##  $ Load_predicted_ADAM_GPL  : num  0.658 0.679 0.447 0.366 0.288 ...\n##  $ Load_predicted_ADAM_GTMSE: num  0.663 0.68 0.449 0.373 0.296 ...\n##  $ Load_predicted_ADAM_MAE  : num  0.324 0.462 0.34 0.349 0.29 ...\n##  $ Load_predicted_ADAM_MAEh : num  0.545 0.566 0.334 0.253 0.175 ...\n##  $ Load_predicted_ADAM_MSCE : num  0.676 0.761 0.603 0.597 0.466 ...\n##  $ Load_predicted_ADAM_MSE  : num  0.276 0.394 0.257 0.262 0.246 ...\n##  $ Load_predicted_ADAM_MSEh : num  0.662 0.681 0.452 0.377 0.299 ...\n##  $ Load_predicted_ADAM_TMSE : num  0.663 0.68 0.449 0.373 0.296 ...\n##  $ Load_predicted_STL_ARIMA : num  -0.386 -0.412 -0.406 -0.482 -0.529 ...\n##  $ Load_predicted_STL_ETS   : num  -0.768 -0.796 -0.744 -0.846 -0.871 ...\n##  $ Error_ADAM_GPL           : num  0.3839 0.316 0.1048 0.0248 0.0576 ...\n##  $ Error_ADAM_GTMSE         : num  0.389 0.3168 0.1067 0.0183 0.0659 ...\n##  $ Error_ADAM_MAE           : num  0.0504 0.09894 0.00155 0.04186 0.06019 ...\n##  $ Error_ADAM_MAEh          : num  0.27104 0.20306 0.00819 0.1378 0.0554 ...\n##  $ Error_ADAM_MSCE          : num  0.402 0.398 0.261 0.206 0.236 ...\n##  $ Error_ADAM_MSE           : num  0.00248 0.03124 0.08472 0.12901 0.01597 ...\n##  $ Error_ADAM_MSEh          : num  0.3884 0.3179 0.1097 0.0137 0.0692 ...\n##  $ Error_ADAM_TMSE          : num  0.389 0.3168 0.1067 0.0183 0.0659 ...\n##  $ Error_STL_ARIMA          : num  0.66 0.775 0.748 0.873 0.759 ...\n##  $ Error_STL_ETS            : num  1.04 1.16 1.09 1.24 1.1 ...\n##  - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n##  - attr(*, \"sorted\")= chr \"Date_Time\"</pre></figure>\n<p>Since we will train the regression model, there can be added also other helpful <strong>features</strong>.\nLet’s define helping <strong>daily and weekly seasonality</strong> features in the form of Sinus and Cosines.\nYou can add also other features like holidays and weather parameters if it makes sense for your case.</p>\n<figure class=\"highlight\"><pre>period &lt;- 96\ndata_predictions_all_casted[, Day_period := 1:.N, by = .(Date = lubridate::date(Date_Time))]\ndata_predictions_all_casted[, S_day := (sin(2*pi*(Day_period/period)) + 1)/2]\ndata_predictions_all_casted[, C_day := (cos(2*pi*(Day_period/period)) + 1)/2]\ndata_predictions_all_casted[, Day_period := NULL]\n \ndata_predictions_all_casted[, Wday := lubridate::wday(Date_Time, label = F,\n                                                      week_start = getOption(\"lubridate.week.start\", 1))]\n  \ndata_predictions_all_casted[, S_week := (sin(2*pi*(Wday/7)) + 1)/2]\ndata_predictions_all_casted[, C_week := (cos(2*pi*(Wday/7)) + 1)/2]\ndata_predictions_all_casted[, Wday := NULL]</pre></figure>\n<p>Let’s split the train and test set: the first 170 days of predictions will go to the train part, rest of the 44 days for the testing (evaluation) part.</p>\n<figure class=\"highlight\"><pre>data_predictions_all_casted_train &lt;- copy(data_predictions_all_casted[1:(96*170)])\ndata_predictions_all_casted_test &lt;- copy(data_predictions_all_casted[((96*170)+1):.N])\nall_cols &lt;- colnames(data_predictions_all_casted)\ntargets &lt;- all_cols[grepl(\"Error\", all_cols)]\ntargets</pre></figure>\n<figure class=\"highlight\"><pre>##  [1] \"Error_ADAM_GPL\"   \"Error_ADAM_GTMSE\" \"Error_ADAM_MAE\"   \"Error_ADAM_MAEh\" \n##  [5] \"Error_ADAM_MSCE\"  \"Error_ADAM_MSE\"   \"Error_ADAM_MSEh\"  \"Error_ADAM_TMSE\" \n##  [9] \"Error_STL_ARIMA\"  \"Error_STL_ETS\"</pre></figure>\n<figure class=\"highlight\"><pre>features &lt;- all_cols[!all_cols %in% c(\"Date_Time\", targets)]\nfeatures</pre></figure>\n<figure class=\"highlight\"><pre>##  [1] \"Load_predicted_ADAM_GPL\"   \"Load_predicted_ADAM_GTMSE\"\n##  [3] \"Load_predicted_ADAM_MAE\"   \"Load_predicted_ADAM_MAEh\" \n##  [5] \"Load_predicted_ADAM_MSCE\"  \"Load_predicted_ADAM_MSE\"  \n##  [7] \"Load_predicted_ADAM_MSEh\"  \"Load_predicted_ADAM_TMSE\" \n##  [9] \"Load_predicted_STL_ARIMA\"  \"Load_predicted_STL_ETS\"   \n## [11] \"S_day\"                     \"C_day\"                    \n## [13] \"S_week\"                    \"C_week\"</pre></figure>\n<p>Let’s see our error targets:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all,\n       aes(Date_Time, Error)) +\n  facet_wrap(~model, ncol = 2) +\n  geom_line() +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-7\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-7-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-7\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-7-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h3 id=\"multivariate-linear-regression-model\">Multivariate Linear Regression Model</h3>\n<p>As a benchmark, as opposed to catboost, we will try a simple multivariate multiple linear regression model by the base <code>lm</code> function.\nYou just need to use the <code>cbind</code> function inside the formula to incorporate multiple targets in the model.</p>\n<figure class=\"highlight\"><pre>mmlm &lt;- lm(as.formula(paste0(\"cbind(\", paste(targets, collapse = \", \"), \") ~ .\")),\n           data = data_predictions_all_casted_train[, .SD, .SDcols = c(targets, features)])\n \n# summary(mmlm)\n# manova(mmlm)\n# confint(mmlm)\n# coef(mmlm)</pre></figure>\n<p>Summaries of the model would be very large to print here, so I commented on the most useful functions to do your analysis of the trained model with <code>summary</code>, <code>manova</code>, <code>coef</code>, and <code>confint</code>.</p>\n<p>Now, let’s predict errors on our selected test set and compute directly prediction errors and Rank accuracy.</p>\n<figure class=\"highlight\"><pre>mmlm_predicted &lt;- predict(mmlm, data_predictions_all_casted_test[, .SD, .SDcols = c(features)])\ndt_mmlm_predicted &lt;- copy(data_predictions_all_casted_test[, .SD, .SDcols = c(\"Date_Time\", targets)])\ndt_mmlm_predicted[, (paste0(targets, \"_prediction\")) := as.data.table(mmlm_predicted)]\n \ndt_mmlm_predicted_melt &lt;- copy(melt(dt_mmlm_predicted,\n                                    id.vars = \"Date_Time\",\n                                    measure.vars = list(real = targets,\n                                                        predicted = paste0(targets, \"_prediction\")),\n                                    variable.name = \"model\",\n                                    variable.factor = F))\n \ndt_mmlm_predicted_melt[data.table(model = as.character(1:10),\n                                  model_name = targets),\n                       on = .(model),\n                       model := i.model_name]\n \nsetorder(dt_mmlm_predicted_melt, Date_Time, real)\ndt_mmlm_predicted_melt[, Rank := 1:.N, by = .(Date_Time)]\nsetorder(dt_mmlm_predicted_melt, Date_Time, predicted)\ndt_mmlm_predicted_melt[, Rank_pred := 1:.N, by = .(Date_Time)]\n \ndt_mmlm_predicted_melt[, .(RMSE = round(rmse(real, predicted), 3),\n                           MAE = round(mae(real, predicted), 3),\n                           MdAE = round(mdae(real, predicted), 3),\n                           Rank_error = round(mae(Rank, Rank_pred), 3),\n                           Rank_1_accuracy = round(.SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N], 3),\n                           Rank_1_cases = .SD[Rank == 1, .N]),\n                       by  = .(model)][order(-Rank_1_cases)]</pre></figure>\n<figure class=\"highlight\"><pre>##                model  RMSE   MAE  MdAE Rank_error Rank_1_accuracy Rank_1_cases\n##  1:    Error_STL_ETS 0.476 0.324 0.239      3.149           0.314          892\n##  2:  Error_ADAM_MSCE 0.429 0.276 0.191      2.766           0.279          634\n##  3:  Error_ADAM_MAEh 0.443 0.289 0.207      2.768           0.367          621\n##  4:  Error_STL_ARIMA 0.459 0.308 0.228      2.976           0.223          588\n##  5:   Error_ADAM_MAE 0.453 0.307 0.226      3.474           0.466          584\n##  6:   Error_ADAM_MSE 0.448 0.305 0.224      3.303           0.011          379\n##  7:  Error_ADAM_MSEh 0.420 0.270 0.192      1.985           0.009          212\n##  8:   Error_ADAM_GPL 0.415 0.266 0.190      2.106           0.097          185\n##  9: Error_ADAM_GTMSE 0.419 0.267 0.188      2.045           0.012           82\n## 10:  Error_ADAM_TMSE 0.420 0.269 0.191      2.060           0.000           47</pre></figure>\n<figure class=\"highlight\"><pre>dt_mmlm_predicted_melt[, .(RMSE = rmse(real, predicted),\n                           MAE = mae(real, predicted),\n                           MdAE = mdae(real, predicted),\n                           Rank_error = mae(Rank, Rank_pred),\n                           Rank_1_accuracy = .SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N])]</pre></figure>\n<figure class=\"highlight\"><pre>##         RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n## 1: 0.4387276 0.2880984 0.2051204   2.662926       0.2634943</pre></figure>\n<p>We can see that the best MAEs have GTMSE, TMSE, and GPL losses. On the other hand, best Rank estimation has MSEh loss and best first Rank estimation accuracy has simple MAE loss method.\nIn general, we miss Rank on average by 2.6 from 10 possible ranks.</p>\n<p>Let’s see error predictions from four of our methods.</p>\n<figure class=\"highlight\"><pre>ggplot(dt_mmlm_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n       aes(Date_Time, predicted, color = \"predicted error\")) +\n  facet_wrap(~model) +\n  geom_line(data = dt_mmlm_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n            aes(Date_Time, real, color = \"real error\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-10\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-10-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-10\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-10-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>At first sight, it can be seen that we can nicely hit low and medium magnitudes of errors. Again, as shown in my previous post, higher values can’t be predicted.</p>\n<h3 id=\"shrinkage---lasso-with-full-interactions\">Shrinkage - LASSO with full interactions</h3>\n<p>As the second multivariate model benchmark, I will try <strong>LASSO</strong> (<code>glmnet</code>) <strong>shrinkage</strong> capability and use all possible interactions to model (by using <code>.^2</code> in the formula).</p>\n<figure class=\"highlight\"><pre>lambdas &lt;- 10^seq(3, -2, by = -.1)\n \ndata_with_interactions &lt;- model.matrix(~ . + .^2,\n                                       data = data_predictions_all_casted_train[, .SD,\n                                                                                .SDcols = c(features)])\n \nmlassom &lt;- cv.glmnet(x = data_with_interactions,\n                     y = as.matrix(data_predictions_all_casted_train[, .SD, .SDcols = c(targets)]),\n                     family = \"mgaussian\",\n                     alpha = 1,\n                     lambda = lambdas,\n                     intercept = T,\n                     standardize = T)</pre></figure>\n<p>To check estimated coefficients, we can use the <code>predict</code> method.</p>\n<figure class=\"highlight\"><pre>coefs &lt;- predict(mlassom, type = \"coefficients\",\n                 s = c(mlassom$lambda.min))\n \nmlassom$lambda.min</pre></figure>\n<figure class=\"highlight\"><pre>## [1] 0.01</pre></figure>\n<p>Let’s predict errors on the test set and compute prediction errors.</p>\n<figure class=\"highlight\"><pre>mlassom_prediction &lt;- predict(mlassom,\n                              type = \"response\",\n                              s = mlassom$lambda.min,\n                              newx = model.matrix(~ . + .^2,\n                                                  data = data_predictions_all_casted_test[, .SD,\n                                                                                          .SDcols = c(features)]))\n \ndt_mlassom_predicted &lt;- copy(data_predictions_all_casted_test[, .SD, .SDcols = c(\"Date_Time\", targets)])\ndt_mlassom_predicted[, (paste0(targets, \"_prediction\")) := as.data.table(mlassom_prediction[,,1])]\n \ndt_mlassom_predicted_melt &lt;- copy(melt(dt_mlassom_predicted,\n                                       id.vars = \"Date_Time\",\n                                       measure.vars = list(real = targets,\n                                                           predicted = paste0(targets, \"_prediction\")),\n                                       variable.name = \"model\",\n                                       variable.factor = F))\n \ndt_mlassom_predicted_melt[data.table(model = as.character(1:10),\n                                     model_name = targets),\n                          on = .(model),\n                          model := i.model_name]\n \nsetorder(dt_mlassom_predicted_melt, Date_Time, real)\ndt_mlassom_predicted_melt[, Rank := 1:.N, by = .(Date_Time)]\nsetorder(dt_mlassom_predicted_melt, Date_Time, predicted)\ndt_mlassom_predicted_melt[, Rank_pred := 1:.N, by = .(Date_Time)]\n \ndt_mlassom_predicted_melt[, .(RMSE = round(rmse(real, predicted), 3),\n                              MAE = round(mae(real, predicted), 3),\n                              MdAE = round(mdae(real, predicted), 3),\n                              Rank_error = round(mae(Rank, Rank_pred), 3),\n                              Rank_1_accuracy = round(.SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N], 3),\n                              Rank_1_cases = .SD[Rank == 1, .N]),\n                          by  = .(model)][order(-Rank_1_cases)]</pre></figure>\n<figure class=\"highlight\"><pre>##                model  RMSE   MAE  MdAE Rank_error Rank_1_accuracy Rank_1_cases\n##  1:    Error_STL_ETS 0.466 0.325 0.257      3.640           0.151          892\n##  2:  Error_ADAM_MSCE 0.427 0.277 0.190      2.956           0.341          634\n##  3:  Error_ADAM_MAEh 0.439 0.290 0.211      2.689           0.243          621\n##  4:  Error_STL_ARIMA 0.455 0.313 0.239      3.153           0.105          588\n##  5:   Error_ADAM_MAE 0.447 0.315 0.240      3.208           0.158          584\n##  6:   Error_ADAM_MSE 0.439 0.306 0.229      3.313           0.003          379\n##  7:  Error_ADAM_MSEh 0.414 0.263 0.185      2.528           0.019          212\n##  8:   Error_ADAM_GPL 0.410 0.261 0.182      2.708           0.103          185\n##  9: Error_ADAM_GTMSE 0.414 0.261 0.179      2.502           0.134           82\n## 10:  Error_ADAM_TMSE 0.415 0.264 0.184      2.608           0.064           47</pre></figure>\n<figure class=\"highlight\"><pre>dt_mlassom_predicted_melt[, .(RMSE = rmse(real, predicted),\n                              MAE = mae(real, predicted),\n                              MdAE = mdae(real, predicted),\n                              Rank_error = mae(Rank, Rank_pred),\n                              Rank_1_accuracy = .SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N])]</pre></figure>\n<figure class=\"highlight\"><pre>##         RMSE      MAE      MdAE Rank_error Rank_1_accuracy\n## 1: 0.4328982 0.287333 0.2033996    2.93054       0.1642992</pre></figure>\n<p>We can see that the best MAEs have the same methods as with MMLM. On the other hand, best Rank estimation has GTMSE loss and best first Rank estimation accuracy has MSCE multistep loss method.\nIn general, we miss Rank on average by 2.9 from 10 possible ranks, so worse by 0.3 as opposed to MMLM. We can say that LASSO didn’t much help to our error modeling.</p>\n<p>But, let’s see LASSO error predictions from four of our methods.</p>\n<figure class=\"highlight\"><pre>ggplot(dt_mlassom_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n       aes(Date_Time, predicted, color = \"predicted error\")) +\n  facet_wrap(~model) +\n  geom_line(data = dt_mlassom_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n            aes(Date_Time, real, color = \"real error\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-14\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-14-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-14\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-14-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h3 id=\"catboost-multirmse\">Catboost MultiRMSE</h3>\n<p>The final multivariate regression method used will be <strong>Catboost</strong> gradient boosting trees with Multi-target regression loss - <em>MultiRMSE</em>, you can check documentation for this loss here: https://catboost.ai/en/docs/concepts/loss-functions-multiregression.</p>\n<p>Let’s define the training method with early stopping based on the validation set and directly train on our dataset.</p>\n<figure class=\"highlight\"><pre>train_catboost_multirmse &lt;- function(data, target, ncores, valid_ratio) {\n  \n  data &lt;- na.omit(data)\n  \n  n_rows &lt;- nrow(data)\n  idx &lt;- 1:n_rows\n  \n  end_ratio &lt;- 0.02\n  train_idx &lt;- c(idx[1:floor(n_rows*(1 - (end_ratio + valid_ratio)))],\n                 idx[(floor(n_rows*(1-end_ratio)) + 1):n_rows])\n  valid_idx &lt;- c(idx[(floor(n_rows*(1 - (end_ratio + valid_ratio)))+1):floor(n_rows*(1-end_ratio))])\n  \n  train_pool &lt;- catboost::catboost.load_pool(as.data.frame(data[train_idx, !target, with = F]),\n                                             label = as.matrix(data[train_idx, mget(target)]))\n  \n  valid_pool &lt;- catboost::catboost.load_pool(as.data.frame(data[valid_idx, !target, with = F]),\n                                             label = as.matrix(data[valid_idx, mget(target)]))\n  \n  fit_params &lt;- list(iterations = 3000,\n                     learning_rate = 0.015,\n                     depth = 12,\n                     loss_function = 'MultiRMSE',\n                     langevin = TRUE,\n                     min_data_in_leaf = 5,\n                     eval_metric = \"MultiRMSE\",\n                     metric_period = 50,\n                     early_stopping_rounds = 60,\n                     rsm = 0.75,\n                     l2_leaf_reg = 1,\n                     train_dir = \"train_dir\",\n                     logging_level = \"Verbose\",\n                     boosting_type = \"Plain\",\n                     grow_policy = \"Lossguide\",\n                     score_function = \"L2\",\n                     bootstrap_type = \"Bayesian\",\n                     bagging_temperature = 1,\n                     use_best_model = T,\n                     allow_writing_files = F,\n                     thread_count = ncores)\n  \n  m_cat &lt;- catboost::catboost.train(learn_pool = train_pool,\n                                    test_pool = valid_pool,\n                                    params = fit_params)\n  \n  return(m_cat)\n  \n}\n \nm_catboost_multirmse &lt;- train_catboost_multirmse(data = data_predictions_all_casted_train[, .SD, .SDcols = c(targets, features)],\n                                                 target = targets,\n                                                 ncores = 4,\n                                                 valid_ratio = 0.05)</pre></figure>\n<figure class=\"highlight\"><pre>## Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n## 0:\tlearn: 1.8651752\ttest: 1.6317912\tbest: 1.6317912 (0)\ttotal: 7.09ms\tremaining: 21.3s\n## 50:\tlearn: 1.7805225\ttest: 1.4486824\tbest: 1.4486824 (50)\ttotal: 422ms\tremaining: 24.4s\n## 100:\tlearn: 1.7326169\ttest: 1.3486589\tbest: 1.3486589 (100)\ttotal: 860ms\tremaining: 24.7s\n## 150:\tlearn: 1.7030793\ttest: 1.2922393\tbest: 1.2922393 (150)\ttotal: 1.35s\tremaining: 25.5s\n## 200:\tlearn: 1.6875372\ttest: 1.2686592\tbest: 1.2686592 (200)\ttotal: 1.82s\tremaining: 25.3s\n## 250:\tlearn: 1.6747041\ttest: 1.2518136\tbest: 1.2518136 (250)\ttotal: 2.35s\tremaining: 25.8s\n## 300:\tlearn: 1.6645811\ttest: 1.2407228\tbest: 1.2407228 (300)\ttotal: 2.9s\tremaining: 26s\n## 350:\tlearn: 1.6557448\ttest: 1.2312354\tbest: 1.2312354 (350)\ttotal: 3.5s\tremaining: 26.4s\n## 400:\tlearn: 1.6464796\ttest: 1.2225415\tbest: 1.2225415 (400)\ttotal: 4.17s\tremaining: 27s\n## 450:\tlearn: 1.6404967\ttest: 1.2173237\tbest: 1.2173036 (449)\ttotal: 4.79s\tremaining: 27s\n## 500:\tlearn: 1.6353707\ttest: 1.2138953\tbest: 1.2138953 (500)\ttotal: 5.42s\tremaining: 27s\n## 550:\tlearn: 1.6321053\ttest: 1.2113879\tbest: 1.2113879 (550)\ttotal: 5.96s\tremaining: 26.5s\n## 600:\tlearn: 1.6273276\ttest: 1.2084719\tbest: 1.2083969 (599)\ttotal: 6.6s\tremaining: 26.4s\n## 650:\tlearn: 1.6235998\ttest: 1.2067629\tbest: 1.2067526 (648)\ttotal: 7.22s\tremaining: 26s\n## 700:\tlearn: 1.6207368\ttest: 1.2056776\tbest: 1.2056776 (700)\ttotal: 7.78s\tremaining: 25.5s\n## 750:\tlearn: 1.6178637\ttest: 1.2046009\tbest: 1.2045980 (747)\ttotal: 8.38s\tremaining: 25.1s\n## 800:\tlearn: 1.6151604\ttest: 1.2039642\tbest: 1.2039435 (796)\ttotal: 8.99s\tremaining: 24.7s\n## 850:\tlearn: 1.6122595\ttest: 1.2028463\tbest: 1.2028463 (850)\ttotal: 9.6s\tremaining: 24.2s\n## 900:\tlearn: 1.6101196\ttest: 1.2020336\tbest: 1.2019819 (890)\ttotal: 10.2s\tremaining: 23.7s\n## 950:\tlearn: 1.6073753\ttest: 1.2011880\tbest: 1.2011371 (948)\ttotal: 10.8s\tremaining: 23.3s\n## 1000:\tlearn: 1.6052842\ttest: 1.2006492\tbest: 1.2006411 (999)\ttotal: 11.4s\tremaining: 22.8s\n## 1050:\tlearn: 1.6039241\ttest: 1.2001930\tbest: 1.2001853 (1049)\ttotal: 12s\tremaining: 22.2s\n## 1100:\tlearn: 1.6028500\ttest: 1.1999358\tbest: 1.1999358 (1100)\ttotal: 12.5s\tremaining: 21.5s\n## 1150:\tlearn: 1.6012678\ttest: 1.1992978\tbest: 1.1992978 (1150)\ttotal: 13.2s\tremaining: 21.2s\n## 1200:\tlearn: 1.5997241\ttest: 1.1989927\tbest: 1.1989927 (1200)\ttotal: 13.8s\tremaining: 20.7s\n## 1250:\tlearn: 1.5984334\ttest: 1.1989688\tbest: 1.1989617 (1248)\ttotal: 14.5s\tremaining: 20.3s\n## 1300:\tlearn: 1.5969307\ttest: 1.1986201\tbest: 1.1985152 (1278)\ttotal: 15.2s\tremaining: 19.9s\n## 1350:\tlearn: 1.5948609\ttest: 1.1986032\tbest: 1.1985116 (1314)\ttotal: 15.9s\tremaining: 19.4s\n## Stopped by overfitting detector  (60 iterations wait)\n## \n## bestTest = 1.198511627\n## bestIteration = 1314\n## \n## Shrink model to first 1315 iterations.</pre></figure>\n<figure class=\"highlight\"><pre>m_catboost_multirmse</pre></figure>\n<figure class=\"highlight\"><pre>## CatBoost model (1315 trees)\n## Loss function: MultiRMSE\n## Fit to 14 feature(s)</pre></figure>\n<p>We can check the Feature Importance of the trained model:</p>\n<figure class=\"highlight\"><pre>fea_imp_catboost &lt;- catboost::catboost.get_feature_importance(m_catboost_multirmse)\nfea_imp_catboost &lt;- data.table::data.table(Gain = fea_imp_catboost[,1],\n                                           Feature = features)\nsetorder(fea_imp_catboost, -Gain)\nfea_imp_catboost</pre></figure>\n<figure class=\"highlight\"><pre>##           Gain                   Feature\n##  1: 26.2825537    Load_predicted_STL_ETS\n##  2: 10.2856293   Load_predicted_ADAM_MAE\n##  3:  8.9606314   Load_predicted_ADAM_GPL\n##  4:  8.5230838  Load_predicted_ADAM_MSCE\n##  5:  8.4006334 Load_predicted_ADAM_GTMSE\n##  6:  6.5706169  Load_predicted_ADAM_TMSE\n##  7:  6.2375988  Load_predicted_STL_ARIMA\n##  8:  5.0254951  Load_predicted_ADAM_MSEh\n##  9:  4.8948488                     S_day\n## 10:  4.7176922   Load_predicted_ADAM_MSE\n## 11:  3.8361990  Load_predicted_ADAM_MAEh\n## 12:  3.3433404                    C_week\n## 13:  2.2428758                     C_day\n## 14:  0.6788015                    S_week</pre></figure>\n<p>STL_ETS and MAE simple loss predictions have the highest importance to the multivariate model.</p>\n<p>Let’s predict the test set and compute prediction errors.</p>\n<figure class=\"highlight\"><pre>predict_multirmse_catboost &lt;- function(data, target, cat_model) {\n  \n  pred &lt;- catboost::catboost.predict(cat_model,\n                                     catboost::catboost.load_pool(as.data.frame(data),\n                                                                  label = as.matrix(rep(0, nrow(data)), ncol = length(target))\n                                     ),\n                                     thread_count = 1)\n  \n  return(pred)\n  \n}\n \ncatboost_prediction &lt;- predict_multirmse_catboost(data = data_predictions_all_casted_test[, .SD, .SDcols = c(features)],\n                                                  target = targets,\n                                                  cat_model = m_catboost_multirmse)\n \ndt_catboost_predicted &lt;- copy(data_predictions_all_casted_test[, .SD, .SDcols = c(\"Date_Time\", targets)])\ndt_catboost_predicted[, (paste0(targets, \"_prediction\")) := as.data.table(catboost_prediction)]\n \ndt_catboost_predicted_melt &lt;- copy(melt(dt_catboost_predicted,\n                                        id.vars = \"Date_Time\",\n                                        measure.vars = list(real = targets,\n                                                            predicted = paste0(targets, \"_prediction\")),\n                                        variable.name = \"model\",\n                                        variable.factor = F))\n \ndt_catboost_predicted_melt[data.table(model = as.character(1:10),\n                                      model_name = targets),\n                           on = .(model),\n                           model := i.model_name]\n \nsetorder(dt_catboost_predicted_melt, Date_Time, real)\ndt_catboost_predicted_melt[, Rank := 1:.N, by = .(Date_Time)]\nsetorder(dt_catboost_predicted_melt, Date_Time, predicted)\ndt_catboost_predicted_melt[, Rank_pred := 1:.N, by = .(Date_Time)]\n \ndt_catboost_predicted_melt[, .(RMSE = round(rmse(real, predicted), 3),\n                               MAE = round(mae(real, predicted), 3),\n                               MdAE = round(mdae(real, predicted), 3),\n                               Rank_error = round(mae(Rank, Rank_pred), 3),\n                               Rank_1_accuracy = round(.SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N], 3),\n                               Rank_1_cases = .SD[Rank == 1, .N]),\n                           by  = .(model)][order(-Rank_1_cases)]</pre></figure>\n<figure class=\"highlight\"><pre>##                model  RMSE   MAE  MdAE Rank_error Rank_1_accuracy Rank_1_cases\n##  1:    Error_STL_ETS 0.467 0.307 0.213      3.428           0.367          892\n##  2:  Error_ADAM_MSCE 0.430 0.273 0.178      2.996           0.181          634\n##  3:  Error_ADAM_MAEh 0.440 0.280 0.187      2.920           0.072          621\n##  4:  Error_STL_ARIMA 0.456 0.291 0.195      3.262           0.315          588\n##  5:   Error_ADAM_MAE 0.449 0.305 0.218      3.229           0.166          584\n##  6:   Error_ADAM_MSE 0.440 0.293 0.204      3.209           0.021          379\n##  7:  Error_ADAM_MSEh 0.415 0.252 0.158      2.458           0.042          212\n##  8:   Error_ADAM_GPL 0.411 0.249 0.155      2.626           0.049          185\n##  9: Error_ADAM_GTMSE 0.415 0.248 0.153      2.417           0.024           82\n## 10:  Error_ADAM_TMSE 0.416 0.251 0.157      2.008           0.000           47</pre></figure>\n<figure class=\"highlight\"><pre>dt_catboost_predicted_melt[, .(RMSE = rmse(real, predicted),\n                               MAE = mae(real, predicted),\n                               MdAE = mdae(real, predicted),\n                               Rank_error = mae(Rank, Rank_pred),\n                               Rank_1_accuracy = .SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N])]</pre></figure>\n<figure class=\"highlight\"><pre>##         RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n## 1: 0.4343794 0.2748845 0.1785648   2.855114       0.1886837</pre></figure>\n<p>We can see that the MAE and MdAE metrics were improved by catboost. On the other hand, average Rank estimation and first Rank estimation are not better than with MMLM.</p>\n<p>Let’s plot our predicted errors.</p>\n<figure class=\"highlight\"><pre>ggplot(dt_catboost_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n       aes(Date_Time, predicted, color = \"predicted error\")) +\n  facet_wrap(~model) +\n  geom_line(data = dt_catboost_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n            aes(Date_Time, real, color = \"real error\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-18\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-18-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-18\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-18-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<h3 id=\"models-analysis\">Models analysis</h3>\n<p>To have it nicely together, let’s bind all three models’ predictions and compute prediction errors for models and methods.</p>\n<figure class=\"highlight\"><pre>dt_all_predicted &lt;- rbindlist(list(dt_mmlm_predicted_melt[, method := \"mmlm\"],\n                                   dt_mlassom_predicted_melt[, method := \"mlasso\"],\n                                   dt_catboost_predicted_melt[, method := \"catboost\"]),\n                              use.names = T, fill = T)\n \ndt_all_predicted[, Error := real - predicted]\ndt_all_predicted[, AE := abs(Error)]\n \ndt_all_predicted[, .(RMSE = rmse(real, predicted),\n                     MAE = mae(real, predicted),\n                     MdAE = mdae(real, predicted),\n                     Rank_error = mae(Rank, Rank_pred),\n                     Rank_1_accuracy = .SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N]),\n                 by = .(method)][order(MAE)]</pre></figure>\n<figure class=\"highlight\"><pre>##      method      RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n## 1: catboost 0.4343794 0.2748845 0.1785648   2.855114       0.1886837\n## 2:   mlasso 0.4328982 0.2873330 0.2033996   2.930540       0.1642992\n## 3:     mmlm 0.4387276 0.2880984 0.2051204   2.662926       0.2634943</pre></figure>\n<figure class=\"highlight\"><pre>dt_all_predicted[, .(RMSE = rmse(real, predicted),\n                     MAE = mae(real, predicted),\n                     MdAE = mdae(real, predicted),\n                     Rank_error = mae(Rank, Rank_pred),\n                     Rank_1_accuracy = .SD[Rank_pred == 1 &amp; Rank_pred == Rank, .N] / .SD[Rank == 1, .N]),\n                 by = .(model)][order(Rank_error)]</pre></figure>\n<figure class=\"highlight\"><pre>##                model      RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n##  1:  Error_ADAM_TMSE 0.4172927 0.2610935 0.1773354   2.225221      0.02127660\n##  2: Error_ADAM_GTMSE 0.4161971 0.2587184 0.1728826   2.321023      0.05691057\n##  3:  Error_ADAM_MSEh 0.4161620 0.2616851 0.1787479   2.323469      0.02358491\n##  4:   Error_ADAM_GPL 0.4119315 0.2586888 0.1757232   2.479640      0.08288288\n##  5:  Error_ADAM_MAEh 0.4407660 0.2861382 0.2026681   2.792456      0.22758991\n##  6:  Error_ADAM_MSCE 0.4286877 0.2752749 0.1868000   2.906013      0.26708728\n##  7:  Error_STL_ARIMA 0.4566267 0.3042826 0.2207566   3.130051      0.21428571\n##  8:   Error_ADAM_MSE 0.4423139 0.3010806 0.2195689   3.274779      0.01143360\n##  9:   Error_ADAM_MAE 0.4496008 0.3087655 0.2274286   3.303741      0.26312785\n## 10:    Error_STL_ETS 0.4697423 0.3186589 0.2340111   3.405540      0.27727952</pre></figure>\n<p>As already mentioned, the best <strong>multivariate regression model</strong> based on classical prediction metrics is <strong>catboosts MultiRMSE</strong>.\nOn the other hand, the best model based on right Rank estimation is simple <strong>multivariate linear regression</strong>.</p>\n<p>Let’s do some basic model analysis and see a boxplot of absolute errors.</p>\n<figure class=\"highlight\"><pre>ggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(method, AE, fill = method)) +\n  geom_boxplot() +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-20\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-20-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-20\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-20-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Predicted vs. real values scatter plot:</p>\n<figure class=\"highlight\"><pre>ggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(predicted, real)) +\n  facet_wrap(~method, scales = \"fixed\", ncol = 3) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", \n              linetype = \"dashed\", linewidth = 1) +\n  geom_smooth(alpha = 0.4) +\n  labs(title = \"Predicted vs Real values\") +\n  theme_bw()</pre></figure>\n<figure class=\"highlight\"><pre>## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-21\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-21-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-21\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-21-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Errors vs. real values scatter plot:</p>\n<figure class=\"highlight\"><pre>ggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(Error, real)) +\n  facet_wrap(~method, scales = \"fixed\", ncol = 3) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  xlab(\"resid (predicted_value - real_value)\") +\n  labs(title = \"Residual vs Real values\") +\n  theme_bw()</pre></figure>\n<figure class=\"highlight\"><pre>## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-22\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-22-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-22\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-22-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>And heteroscedasticity check, so absolute errors vs. real values:</p>\n<figure class=\"highlight\"><pre>ggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(real, AE)) +\n  facet_wrap(~method, scales = \"free\") +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  ylab(\"abs(resid)\") +\n  labs(title = \"Heteroscedasticity check\") + \n  theme_bw()</pre></figure>\n<figure class=\"highlight\"><pre>## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-23\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_15/unnamed-chunk-23-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-23\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_15/unnamed-chunk-23-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>From all the above graphs, we can see that low and medium magnitudes of errors can be nicely predicted, but again prediction of high values of errors is a very difficult task.</p>\n<p>We can use nonparametric multiple comparisons test (Nemenyi test) also to decide which error model is best (by Absolute Errors - AE).</p>\n<figure class=\"highlight\"><pre>tsutils::nemenyi(data.matrix(dcast(dt_all_predicted,\n                                   Date_Time + model ~ method,\n                                   value.var = \"AE\")[, !c(\"Date_Time\", \"model\"), with = F]),\n                 plottype = \"vmcb\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-24\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-24-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-24\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-24-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<figure class=\"highlight\"><pre>## Friedman and Nemenyi Tests\n## The confidence level is 5%\n## Number of observations is 42240 and number of methods is 3\n## Friedman test p-value: 0.0000 - Ha: Different\n## Critical distance: 0.0161</pre></figure>\n<p><strong>Catboost</strong> is best all the way.</p>\n<p>We can also use the Nemenyi test on a question that which of the 10 model errors can be predicted the best.</p>\n<figure class=\"highlight\"><pre>tsutils::nemenyi(data.matrix(dcast(dt_all_predicted,\n                                   Date_Time + method ~ model,\n                                   value.var = \"AE\")[, !c(\"Date_Time\", \"method\"), with = F]),\n                 plottype = \"vmcb\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-25\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-25-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-25\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-25-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<figure class=\"highlight\"><pre>## Friedman and Nemenyi Tests\n## The confidence level is 5%\n## Number of observations is 12672 and number of methods is 10\n## Friedman test p-value: 0.0000 - Ha: Different\n## Critical distance: 0.1203</pre></figure>\n<p>We see that GTMSE and TMSE have the most predictable errors, for non-multistep losses methods (benchmarks) it is more difficult.</p>\n<h3 id=\"ensemble-predictions\">Ensemble predictions</h3>\n<p>Finally, we can now construct some ensemble predictions by our multivariate error model predictions.</p>\n<h4 id=\"best-rank-prediction-ensemble\">Best Rank prediction ensemble</h4>\n<p>The first method will be very simple, we will use as final ensemble prediction the prediction with the lowest estimated error.</p>\n<figure class=\"highlight\"><pre>dt_all_predicted[, model := gsub(\"Error_\", \"\", model)]\ndata_best_rank_prediction &lt;- copy(dt_all_predicted[, .SD[Rank_pred == 1], by = .(method, Date_Time)])\ndata_best_rank_prediction[, table(model)] # distribution of used methods</pre></figure>\n<figure class=\"highlight\"><pre>## model\n##   ADAM_GPL ADAM_GTMSE   ADAM_MAE  ADAM_MAEh  ADAM_MSCE   ADAM_MSE  ADAM_MSEh \n##        899        880       1513       1613       2302        204        322 \n##  ADAM_TMSE  STL_ARIMA    STL_ETS \n##        279       1713       2947</pre></figure>\n<p>Let’s evaluate ensemble predictions by our error models and also compute metrics for our base predictions for comparison.</p>\n<figure class=\"highlight\"><pre>data_predictions_all[, model := gsub(\"-\", \"_\", model)]\ndata_predictions_all[, model := gsub(\"\\\\+\", \"_\", model)]\n \ndata_best_rank_prediction[data_predictions_all,\n                          on = .(model, Date_Time),\n                          Load_predicted := i.Load_predicted]\n \ndata_best_rank_prediction[data_predictions_all,\n                          on = .(Date_Time),\n                          Load_real := i.Load_real]\n \n# original\ndata_predictions_all[Date_Time %in% data_best_rank_prediction[, unique(Date_Time)],\n                     .(RMSE = rmse(Load_real, Load_predicted),\n                       MAE = mae(Load_real, Load_predicted),\n                       MAAPE = maape(Load_real, Load_predicted)),\n                     by = .(model)][order(MAE)]</pre></figure>\n<figure class=\"highlight\"><pre>##          model      RMSE       MAE    MAAPE\n##  1:  ADAM_MAEh 0.6153573 0.3915900 79.73346\n##  2:  ADAM_MSEh 0.6060004 0.4032142 84.61694\n##  3:  ADAM_TMSE 0.6080947 0.4053491 84.76779\n##  4: ADAM_GTMSE 0.6094993 0.4085337 85.40032\n##  5:   ADAM_GPL 0.6082200 0.4106951 85.66970\n##  6:  ADAM_MSCE 0.6251760 0.4153277 85.63160\n##  7:  STL_ARIMA 0.6709538 0.4348279 87.92342\n##  8:   ADAM_MSE 0.6487472 0.4378956 87.45296\n##  9:   ADAM_MAE 0.6550635 0.4386036 87.33271\n## 10:    STL_ETS 0.6887292 0.4388718 80.82078</pre></figure>\n<figure class=\"highlight\"><pre># ensemble\ndata_best_rank_prediction[, .(RMSE = rmse(Load_real, Load_predicted),\n                              MAE = mae(Load_real, Load_predicted),\n                              MAAPE = maape(Load_real, Load_predicted)),\n                          by = .(method)][order(MAE)]</pre></figure>\n<figure class=\"highlight\"><pre>##      method      RMSE       MAE    MAAPE\n## 1: catboost 0.6176298 0.3765020 78.38730\n## 2:     mmlm 0.6368562 0.3777519 76.25634\n## 3:   mlasso 0.6090476 0.3789960 81.54793</pre></figure>\n<p>All three ensembles are better than any base prediction based on MAE. Well done!</p>\n<p>Let’s plot a graph of the final ensemble predictions:</p>\n<figure class=\"highlight\"><pre>ggplot(data_best_rank_prediction,\n       aes(Date_Time, Load_predicted, color = \"predicted load\")) +\n  facet_wrap(~method, ncol = 1) +\n  geom_line(data = data_best_rank_prediction,\n            aes(Date_Time, Load_real, color = \"real load\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-28\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-28-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-28\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_15/unnamed-chunk-28-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Catboost doesn’t go that much below zero than the other two.</p>\n<h4 id=\"weighted-average-ensemble\">Weighted average Ensemble</h4>\n<p>The second ensemble method will be based on weighting the estimated errors.\nFirstly, we need to normalize all weights (errors) by the Min-max method and compute the sum ratio.</p>\n<figure class=\"highlight\"><pre>weis &lt;- copy(dt_all_predicted[, .(Wei_m = TSrepr::norm_min_max(-abs(predicted)),\n                                  model),\n                             by = .(method, Date_Time)])\n \nweis_sums &lt;- copy(dt_all_predicted[, .(Wei_sum = sum(TSrepr::norm_min_max(-abs(predicted)))),\n                                   by = .(method, Date_Time)])\n \nweis[weis_sums,\n     Wei_m := Wei_m / i.Wei_sum,\n     on = .(method, Date_Time)]</pre></figure>\n<p>We can use all predictions, but a better way is to remove the bad half of the predictions from the ensemble:</p>\n<figure class=\"highlight\"><pre>ratio_forec &lt;- 0.5 # 1 to use all 10 predictions\nsetorder(weis, method, Date_Time, Wei_m)\nweis[, Wei_m := replace(Wei_m, 1:floor(.N * ratio_forec), 0),\n     by = .(method, Date_Time)]</pre></figure>\n<p>Now, let’s join weights and predictions and compute final ensemble predictions.</p>\n<figure class=\"highlight\"><pre>dt_all_predicted_wei_ave &lt;- copy(dt_all_predicted)\ndt_all_predicted_wei_ave[weis,\n                         Wei_m := i.Wei_m,\n                         on = .(model, method, Date_Time)]\n \ndt_all_predicted_wei_ave[data_predictions_all,\n                         on = .(model, Date_Time),\n                         Load_predicted := i.Load_predicted]\n \ndata_predicted_wei_ave &lt;- copy(dt_all_predicted_wei_ave[,\n                                                        .(Load_predicted = weighted.mean(Load_predicted, Wei_m)),\n                                                        by = .(method, Date_Time)])</pre></figure>\n<p>Let’s evaluate them:</p>\n<figure class=\"highlight\"><pre>data_predicted_wei_ave[data_predictions_all,\n                       on = .(Date_Time),\n                       Load_real := i.Load_real]\n \ndata_predicted_wei_ave[, .(RMSE = rmse(Load_real, Load_predicted),\n                           MAE = mae(Load_real, Load_predicted),\n                           MAAPE = maape(Load_real, Load_predicted)),\n                       by = .(method)][order(MAE)]</pre></figure>\n<figure class=\"highlight\"><pre>##      method      RMSE       MAE    MAAPE\n## 1:     mmlm 0.6080235 0.3721450 78.36590\n## 2: catboost 0.6027960 0.3814034 82.43628\n## 3:   mlasso 0.5981348 0.3824002 81.68486</pre></figure>\n<p>Even better with weighting than the best rank way!</p>\n<p>Let’s plot a graph of ensemble predictions:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predicted_wei_ave,\n       aes(Date_Time, Load_predicted, color = \"predicted load\")) +\n  facet_wrap(~method, ncol = 1) +\n  geom_line(data = data_predicted_wei_ave,\n            aes(Date_Time, Load_real, color = \"real load\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-33\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-33-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-33\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-33-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Ultimately, let’s decide which model is best for household electricity consumption prediction:</p>\n<figure class=\"highlight\"><pre>data_predictions_all_ensembles &lt;- rbindlist(list(data_predictions_all[Date_Time %in% data_best_rank_prediction[, unique(Date_Time)], .(Date_Time, Load_predicted, Load_real, model)],\n                                                 data_predicted_wei_ave[, .(Date_Time, Load_predicted, Load_real, model = paste0(\"ensemble_wei_ave_\", method))],\n                                                 data_best_rank_prediction[, .(Date_Time, Load_predicted, Load_real, model = paste0(\"ensemble_best_rank_\", method))]))\n \ndata_predictions_all_ensembles[, AE := abs(Load_real - Load_predicted)]\n \ntsutils::nemenyi(data.matrix(dcast(data_predictions_all_ensembles,\n                                   Date_Time ~ model,\n                                   value.var = \"AE\")[, !c(\"Date_Time\"), with = F]),\n                 plottype = \"vmcb\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-34\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-34-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-34\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_15/unnamed-chunk-34-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<figure class=\"highlight\"><pre>## Friedman and Nemenyi Tests\n## The confidence level is 5%\n## Number of observations is 4224 and number of methods is 16\n## Friedman test p-value: 0.0000 - Ha: Different\n## Critical distance: 0.3549</pre></figure>\n<p>Weighted average ensemble prediction using the MMLM method is best!</p>\n<h3 id=\"summary\">Summary</h3>\n<ul>\n<li>I showed how easily can be modeled multiple targets with three methods: MMLM, LASSO, and Catboost in R</li>\n<li>I trained a model for predicting absolute errors of mostly simple exponential smoothing methods</li>\n<li>The multivariate models can nicely predict errors of low-medium magnitude</li>\n<li>Again the high errors can not be predicted with those simple models</li>\n<li>Catboost showed the best results in regard of absolute error measure</li>\n<li>I used predicted errors to create two types of ensemble predictions of electricity consumption</li>\n<li>Both ensemble methods showed better results than any original prediction</li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://petolau.github.io/Multivariate-Regression-Ensemble-Models-for-Errors-Prediction/\"> Peter Laurinec</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
    "main_text": "Multivariate Regression Ensemble Models for Errors Prediction\nPosted on\nNovember 28, 2023\nby\nPeter Laurinec\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nPeter Laurinec\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIn the last blog post about\nMultistep forecasting losses\n, I showed the usage of the fantastic method\nadam\nfrom the\nsmooth\nR package on household electricity consumption data, and compared it with benchmarks.\nSince I computed predictions from 10 methods/models for a long period of time, it would be nice to create some ensemble models for precise prediction for our household consumption data. For that purpose, it would be great to predict for example future errors of these methods. It is used in some known ensemble methods, which are not direct about\nstacking\n.\nPredicting errors can be beneficial for prediction weighting or for predicting the rank of methods (i.e. best one prediction).\nFor the sake of learning something new, I will try multivariate regression models, so learning from multiple targets at once. At least, it has the benefit of simplicity, that we need only one model for all base prediction models.\nWhat I will show you in this post:\nPrediction of forecasting errors (absolute residuals) of simple exponential smoothing methods.\nUsage of multivariate multiple linear regression benchmarks.\nCheck of right rank prediction of forecasting methods.\nCatboost\nMultiRMSE\ncapability and evaluation if it is more powerful than linear models.\nComputation of two types of ensemble predictions from predicted errors.\nSimple forecasting methods error prediction\nIn the beginning, we need to load the required packages, be aware that\ncatboost\nhas to be installed by the\ndevtools\npackage in R.\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(TSrepr)\nlibrary(ggplot2)\nlibrary(dygraphs)\nlibrary(glmnet)\n# devtools::install_url('https://github.com/catboost/catboost/releases/download/v1.2.2/catboost-R-Windows-1.2.2.tgz')\nlibrary(catboost)\nI will use predictions and errors from a previous blog post, you can download them from my\nGitHub repository\n.\nload(\"_rmd/data_household_predictions.Rdata\")\ndata_predictions_all\n##                   Date_Time Load_real Load_predicted    model\n##      1: 2023-01-30 00:00:00     0.274     -0.7679345  STL+ETS\n##      2: 2023-01-30 00:15:00     0.363     -0.7959601  STL+ETS\n##      3: 2023-01-30 00:30:00     0.342     -0.7443988  STL+ETS\n##      4: 2023-01-30 00:45:00     0.391     -0.8461708  STL+ETS\n##      5: 2023-01-30 01:00:00     0.230     -0.8708582  STL+ETS\n##     ---                                                      \n## 205436: 2023-08-31 22:45:00     0.080      0.4030999 ADAM-GPL\n## 205437: 2023-08-31 23:00:00     0.039      0.2060178 ADAM-GPL\n## 205438: 2023-08-31 23:15:00     0.087      0.2489654 ADAM-GPL\n## 205439: 2023-08-31 23:30:00     0.080      0.1719251 ADAM-GPL\n## 205440: 2023-08-31 23:45:00     0.040      0.4710469 ADAM-GPL\nWe will model absolute errors of predictions:\ndata_predictions_all[, Error := abs(Load_real - Load_predicted)]\nTo be able to model all method’s predictions at once, we need to cast\npredictions and errors\nto wide form.\ndata_predictions_all_casted <- dcast(data_predictions_all,\n                                     formula = Date_Time ~ model,\n                                     value.var = c(\"Load_predicted\", \"Error\"))\n \nsetnames(data_predictions_all_casted,\n         colnames(data_predictions_all_casted),\n         gsub(\"-\", \"_\", colnames(data_predictions_all_casted)))\nsetnames(data_predictions_all_casted,\n         colnames(data_predictions_all_casted),\n         gsub(\"\\\\+\", \"_\", colnames(data_predictions_all_casted)))\n \nstr(data_predictions_all_casted)\n## Classes 'data.table' and 'data.frame':\t20544 obs. of  21 variables:\n##  $ Date_Time                : POSIXct, format: \"2023-01-30 00:00:00\" \"2023-01-30 00:15:00\" ...\n##  $ Load_predicted_ADAM_GPL  : num  0.658 0.679 0.447 0.366 0.288 ...\n##  $ Load_predicted_ADAM_GTMSE: num  0.663 0.68 0.449 0.373 0.296 ...\n##  $ Load_predicted_ADAM_MAE  : num  0.324 0.462 0.34 0.349 0.29 ...\n##  $ Load_predicted_ADAM_MAEh : num  0.545 0.566 0.334 0.253 0.175 ...\n##  $ Load_predicted_ADAM_MSCE : num  0.676 0.761 0.603 0.597 0.466 ...\n##  $ Load_predicted_ADAM_MSE  : num  0.276 0.394 0.257 0.262 0.246 ...\n##  $ Load_predicted_ADAM_MSEh : num  0.662 0.681 0.452 0.377 0.299 ...\n##  $ Load_predicted_ADAM_TMSE : num  0.663 0.68 0.449 0.373 0.296 ...\n##  $ Load_predicted_STL_ARIMA : num  -0.386 -0.412 -0.406 -0.482 -0.529 ...\n##  $ Load_predicted_STL_ETS   : num  -0.768 -0.796 -0.744 -0.846 -0.871 ...\n##  $ Error_ADAM_GPL           : num  0.3839 0.316 0.1048 0.0248 0.0576 ...\n##  $ Error_ADAM_GTMSE         : num  0.389 0.3168 0.1067 0.0183 0.0659 ...\n##  $ Error_ADAM_MAE           : num  0.0504 0.09894 0.00155 0.04186 0.06019 ...\n##  $ Error_ADAM_MAEh          : num  0.27104 0.20306 0.00819 0.1378 0.0554 ...\n##  $ Error_ADAM_MSCE          : num  0.402 0.398 0.261 0.206 0.236 ...\n##  $ Error_ADAM_MSE           : num  0.00248 0.03124 0.08472 0.12901 0.01597 ...\n##  $ Error_ADAM_MSEh          : num  0.3884 0.3179 0.1097 0.0137 0.0692 ...\n##  $ Error_ADAM_TMSE          : num  0.389 0.3168 0.1067 0.0183 0.0659 ...\n##  $ Error_STL_ARIMA          : num  0.66 0.775 0.748 0.873 0.759 ...\n##  $ Error_STL_ETS            : num  1.04 1.16 1.09 1.24 1.1 ...\n##  - attr(*, \".internal.selfref\")=<externalptr> \n##  - attr(*, \"sorted\")= chr \"Date_Time\"\nSince we will train the regression model, there can be added also other helpful\nfeatures\n.\nLet’s define helping\ndaily and weekly seasonality\nfeatures in the form of Sinus and Cosines.\nYou can add also other features like holidays and weather parameters if it makes sense for your case.\nperiod <- 96\ndata_predictions_all_casted[, Day_period := 1:.N, by = .(Date = lubridate::date(Date_Time))]\ndata_predictions_all_casted[, S_day := (sin(2*pi*(Day_period/period)) + 1)/2]\ndata_predictions_all_casted[, C_day := (cos(2*pi*(Day_period/period)) + 1)/2]\ndata_predictions_all_casted[, Day_period := NULL]\n \ndata_predictions_all_casted[, Wday := lubridate::wday(Date_Time, label = F,\n                                                      week_start = getOption(\"lubridate.week.start\", 1))]\n  \ndata_predictions_all_casted[, S_week := (sin(2*pi*(Wday/7)) + 1)/2]\ndata_predictions_all_casted[, C_week := (cos(2*pi*(Wday/7)) + 1)/2]\ndata_predictions_all_casted[, Wday := NULL]\nLet’s split the train and test set: the first 170 days of predictions will go to the train part, rest of the 44 days for the testing (evaluation) part.\ndata_predictions_all_casted_train <- copy(data_predictions_all_casted[1:(96*170)])\ndata_predictions_all_casted_test <- copy(data_predictions_all_casted[((96*170)+1):.N])\nall_cols <- colnames(data_predictions_all_casted)\ntargets <- all_cols[grepl(\"Error\", all_cols)]\ntargets\n##  [1] \"Error_ADAM_GPL\"   \"Error_ADAM_GTMSE\" \"Error_ADAM_MAE\"   \"Error_ADAM_MAEh\" \n##  [5] \"Error_ADAM_MSCE\"  \"Error_ADAM_MSE\"   \"Error_ADAM_MSEh\"  \"Error_ADAM_TMSE\" \n##  [9] \"Error_STL_ARIMA\"  \"Error_STL_ETS\"\nfeatures <- all_cols[!all_cols %in% c(\"Date_Time\", targets)]\nfeatures\n##  [1] \"Load_predicted_ADAM_GPL\"   \"Load_predicted_ADAM_GTMSE\"\n##  [3] \"Load_predicted_ADAM_MAE\"   \"Load_predicted_ADAM_MAEh\" \n##  [5] \"Load_predicted_ADAM_MSCE\"  \"Load_predicted_ADAM_MSE\"  \n##  [7] \"Load_predicted_ADAM_MSEh\"  \"Load_predicted_ADAM_TMSE\" \n##  [9] \"Load_predicted_STL_ARIMA\"  \"Load_predicted_STL_ETS\"   \n## [11] \"S_day\"                     \"C_day\"                    \n## [13] \"S_week\"                    \"C_week\"\nLet’s see our error targets:\nggplot(data_predictions_all,\n       aes(Date_Time, Error)) +\n  facet_wrap(~model, ncol = 2) +\n  geom_line() +\n  theme_bw()\nMultivariate Linear Regression Model\nAs a benchmark, as opposed to catboost, we will try a simple multivariate multiple linear regression model by the base\nlm\nfunction.\nYou just need to use the\ncbind\nfunction inside the formula to incorporate multiple targets in the model.\nmmlm <- lm(as.formula(paste0(\"cbind(\", paste(targets, collapse = \", \"), \") ~ .\")),\n           data = data_predictions_all_casted_train[, .SD, .SDcols = c(targets, features)])\n \n# summary(mmlm)\n# manova(mmlm)\n# confint(mmlm)\n# coef(mmlm)\nSummaries of the model would be very large to print here, so I commented on the most useful functions to do your analysis of the trained model with\nsummary\n,\nmanova\n,\ncoef\n, and\nconfint\n.\nNow, let’s predict errors on our selected test set and compute directly prediction errors and Rank accuracy.\nmmlm_predicted <- predict(mmlm, data_predictions_all_casted_test[, .SD, .SDcols = c(features)])\ndt_mmlm_predicted <- copy(data_predictions_all_casted_test[, .SD, .SDcols = c(\"Date_Time\", targets)])\ndt_mmlm_predicted[, (paste0(targets, \"_prediction\")) := as.data.table(mmlm_predicted)]\n \ndt_mmlm_predicted_melt <- copy(melt(dt_mmlm_predicted,\n                                    id.vars = \"Date_Time\",\n                                    measure.vars = list(real = targets,\n                                                        predicted = paste0(targets, \"_prediction\")),\n                                    variable.name = \"model\",\n                                    variable.factor = F))\n \ndt_mmlm_predicted_melt[data.table(model = as.character(1:10),\n                                  model_name = targets),\n                       on = .(model),\n                       model := i.model_name]\n \nsetorder(dt_mmlm_predicted_melt, Date_Time, real)\ndt_mmlm_predicted_melt[, Rank := 1:.N, by = .(Date_Time)]\nsetorder(dt_mmlm_predicted_melt, Date_Time, predicted)\ndt_mmlm_predicted_melt[, Rank_pred := 1:.N, by = .(Date_Time)]\n \ndt_mmlm_predicted_melt[, .(RMSE = round(rmse(real, predicted), 3),\n                           MAE = round(mae(real, predicted), 3),\n                           MdAE = round(mdae(real, predicted), 3),\n                           Rank_error = round(mae(Rank, Rank_pred), 3),\n                           Rank_1_accuracy = round(.SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N], 3),\n                           Rank_1_cases = .SD[Rank == 1, .N]),\n                       by  = .(model)][order(-Rank_1_cases)]\n##                model  RMSE   MAE  MdAE Rank_error Rank_1_accuracy Rank_1_cases\n##  1:    Error_STL_ETS 0.476 0.324 0.239      3.149           0.314          892\n##  2:  Error_ADAM_MSCE 0.429 0.276 0.191      2.766           0.279          634\n##  3:  Error_ADAM_MAEh 0.443 0.289 0.207      2.768           0.367          621\n##  4:  Error_STL_ARIMA 0.459 0.308 0.228      2.976           0.223          588\n##  5:   Error_ADAM_MAE 0.453 0.307 0.226      3.474           0.466          584\n##  6:   Error_ADAM_MSE 0.448 0.305 0.224      3.303           0.011          379\n##  7:  Error_ADAM_MSEh 0.420 0.270 0.192      1.985           0.009          212\n##  8:   Error_ADAM_GPL 0.415 0.266 0.190      2.106           0.097          185\n##  9: Error_ADAM_GTMSE 0.419 0.267 0.188      2.045           0.012           82\n## 10:  Error_ADAM_TMSE 0.420 0.269 0.191      2.060           0.000           47\ndt_mmlm_predicted_melt[, .(RMSE = rmse(real, predicted),\n                           MAE = mae(real, predicted),\n                           MdAE = mdae(real, predicted),\n                           Rank_error = mae(Rank, Rank_pred),\n                           Rank_1_accuracy = .SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N])]\n##         RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n## 1: 0.4387276 0.2880984 0.2051204   2.662926       0.2634943\nWe can see that the best MAEs have GTMSE, TMSE, and GPL losses. On the other hand, best Rank estimation has MSEh loss and best first Rank estimation accuracy has simple MAE loss method.\nIn general, we miss Rank on average by 2.6 from 10 possible ranks.\nLet’s see error predictions from four of our methods.\nggplot(dt_mmlm_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n       aes(Date_Time, predicted, color = \"predicted error\")) +\n  facet_wrap(~model) +\n  geom_line(data = dt_mmlm_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n            aes(Date_Time, real, color = \"real error\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\nAt first sight, it can be seen that we can nicely hit low and medium magnitudes of errors. Again, as shown in my previous post, higher values can’t be predicted.\nShrinkage - LASSO with full interactions\nAs the second multivariate model benchmark, I will try\nLASSO\n(\nglmnet\n)\nshrinkage\ncapability and use all possible interactions to model (by using\n.^2\nin the formula).\nlambdas <- 10^seq(3, -2, by = -.1)\n \ndata_with_interactions <- model.matrix(~ . + .^2,\n                                       data = data_predictions_all_casted_train[, .SD,\n                                                                                .SDcols = c(features)])\n \nmlassom <- cv.glmnet(x = data_with_interactions,\n                     y = as.matrix(data_predictions_all_casted_train[, .SD, .SDcols = c(targets)]),\n                     family = \"mgaussian\",\n                     alpha = 1,\n                     lambda = lambdas,\n                     intercept = T,\n                     standardize = T)\nTo check estimated coefficients, we can use the\npredict\nmethod.\ncoefs <- predict(mlassom, type = \"coefficients\",\n                 s = c(mlassom$lambda.min))\n \nmlassom$lambda.min\n## [1] 0.01\nLet’s predict errors on the test set and compute prediction errors.\nmlassom_prediction <- predict(mlassom,\n                              type = \"response\",\n                              s = mlassom$lambda.min,\n                              newx = model.matrix(~ . + .^2,\n                                                  data = data_predictions_all_casted_test[, .SD,\n                                                                                          .SDcols = c(features)]))\n \ndt_mlassom_predicted <- copy(data_predictions_all_casted_test[, .SD, .SDcols = c(\"Date_Time\", targets)])\ndt_mlassom_predicted[, (paste0(targets, \"_prediction\")) := as.data.table(mlassom_prediction[,,1])]\n \ndt_mlassom_predicted_melt <- copy(melt(dt_mlassom_predicted,\n                                       id.vars = \"Date_Time\",\n                                       measure.vars = list(real = targets,\n                                                           predicted = paste0(targets, \"_prediction\")),\n                                       variable.name = \"model\",\n                                       variable.factor = F))\n \ndt_mlassom_predicted_melt[data.table(model = as.character(1:10),\n                                     model_name = targets),\n                          on = .(model),\n                          model := i.model_name]\n \nsetorder(dt_mlassom_predicted_melt, Date_Time, real)\ndt_mlassom_predicted_melt[, Rank := 1:.N, by = .(Date_Time)]\nsetorder(dt_mlassom_predicted_melt, Date_Time, predicted)\ndt_mlassom_predicted_melt[, Rank_pred := 1:.N, by = .(Date_Time)]\n \ndt_mlassom_predicted_melt[, .(RMSE = round(rmse(real, predicted), 3),\n                              MAE = round(mae(real, predicted), 3),\n                              MdAE = round(mdae(real, predicted), 3),\n                              Rank_error = round(mae(Rank, Rank_pred), 3),\n                              Rank_1_accuracy = round(.SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N], 3),\n                              Rank_1_cases = .SD[Rank == 1, .N]),\n                          by  = .(model)][order(-Rank_1_cases)]\n##                model  RMSE   MAE  MdAE Rank_error Rank_1_accuracy Rank_1_cases\n##  1:    Error_STL_ETS 0.466 0.325 0.257      3.640           0.151          892\n##  2:  Error_ADAM_MSCE 0.427 0.277 0.190      2.956           0.341          634\n##  3:  Error_ADAM_MAEh 0.439 0.290 0.211      2.689           0.243          621\n##  4:  Error_STL_ARIMA 0.455 0.313 0.239      3.153           0.105          588\n##  5:   Error_ADAM_MAE 0.447 0.315 0.240      3.208           0.158          584\n##  6:   Error_ADAM_MSE 0.439 0.306 0.229      3.313           0.003          379\n##  7:  Error_ADAM_MSEh 0.414 0.263 0.185      2.528           0.019          212\n##  8:   Error_ADAM_GPL 0.410 0.261 0.182      2.708           0.103          185\n##  9: Error_ADAM_GTMSE 0.414 0.261 0.179      2.502           0.134           82\n## 10:  Error_ADAM_TMSE 0.415 0.264 0.184      2.608           0.064           47\ndt_mlassom_predicted_melt[, .(RMSE = rmse(real, predicted),\n                              MAE = mae(real, predicted),\n                              MdAE = mdae(real, predicted),\n                              Rank_error = mae(Rank, Rank_pred),\n                              Rank_1_accuracy = .SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N])]\n##         RMSE      MAE      MdAE Rank_error Rank_1_accuracy\n## 1: 0.4328982 0.287333 0.2033996    2.93054       0.1642992\nWe can see that the best MAEs have the same methods as with MMLM. On the other hand, best Rank estimation has GTMSE loss and best first Rank estimation accuracy has MSCE multistep loss method.\nIn general, we miss Rank on average by 2.9 from 10 possible ranks, so worse by 0.3 as opposed to MMLM. We can say that LASSO didn’t much help to our error modeling.\nBut, let’s see LASSO error predictions from four of our methods.\nggplot(dt_mlassom_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n       aes(Date_Time, predicted, color = \"predicted error\")) +\n  facet_wrap(~model) +\n  geom_line(data = dt_mlassom_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n            aes(Date_Time, real, color = \"real error\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\nCatboost MultiRMSE\nThe final multivariate regression method used will be\nCatboost\ngradient boosting trees with Multi-target regression loss -\nMultiRMSE\n, you can check documentation for this loss here: https://catboost.ai/en/docs/concepts/loss-functions-multiregression.\nLet’s define the training method with early stopping based on the validation set and directly train on our dataset.\ntrain_catboost_multirmse <- function(data, target, ncores, valid_ratio) {\n  \n  data <- na.omit(data)\n  \n  n_rows <- nrow(data)\n  idx <- 1:n_rows\n  \n  end_ratio <- 0.02\n  train_idx <- c(idx[1:floor(n_rows*(1 - (end_ratio + valid_ratio)))],\n                 idx[(floor(n_rows*(1-end_ratio)) + 1):n_rows])\n  valid_idx <- c(idx[(floor(n_rows*(1 - (end_ratio + valid_ratio)))+1):floor(n_rows*(1-end_ratio))])\n  \n  train_pool <- catboost::catboost.load_pool(as.data.frame(data[train_idx, !target, with = F]),\n                                             label = as.matrix(data[train_idx, mget(target)]))\n  \n  valid_pool <- catboost::catboost.load_pool(as.data.frame(data[valid_idx, !target, with = F]),\n                                             label = as.matrix(data[valid_idx, mget(target)]))\n  \n  fit_params <- list(iterations = 3000,\n                     learning_rate = 0.015,\n                     depth = 12,\n                     loss_function = 'MultiRMSE',\n                     langevin = TRUE,\n                     min_data_in_leaf = 5,\n                     eval_metric = \"MultiRMSE\",\n                     metric_period = 50,\n                     early_stopping_rounds = 60,\n                     rsm = 0.75,\n                     l2_leaf_reg = 1,\n                     train_dir = \"train_dir\",\n                     logging_level = \"Verbose\",\n                     boosting_type = \"Plain\",\n                     grow_policy = \"Lossguide\",\n                     score_function = \"L2\",\n                     bootstrap_type = \"Bayesian\",\n                     bagging_temperature = 1,\n                     use_best_model = T,\n                     allow_writing_files = F,\n                     thread_count = ncores)\n  \n  m_cat <- catboost::catboost.train(learn_pool = train_pool,\n                                    test_pool = valid_pool,\n                                    params = fit_params)\n  \n  return(m_cat)\n  \n}\n \nm_catboost_multirmse <- train_catboost_multirmse(data = data_predictions_all_casted_train[, .SD, .SDcols = c(targets, features)],\n                                                 target = targets,\n                                                 ncores = 4,\n                                                 valid_ratio = 0.05)\n## Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n## 0:\tlearn: 1.8651752\ttest: 1.6317912\tbest: 1.6317912 (0)\ttotal: 7.09ms\tremaining: 21.3s\n## 50:\tlearn: 1.7805225\ttest: 1.4486824\tbest: 1.4486824 (50)\ttotal: 422ms\tremaining: 24.4s\n## 100:\tlearn: 1.7326169\ttest: 1.3486589\tbest: 1.3486589 (100)\ttotal: 860ms\tremaining: 24.7s\n## 150:\tlearn: 1.7030793\ttest: 1.2922393\tbest: 1.2922393 (150)\ttotal: 1.35s\tremaining: 25.5s\n## 200:\tlearn: 1.6875372\ttest: 1.2686592\tbest: 1.2686592 (200)\ttotal: 1.82s\tremaining: 25.3s\n## 250:\tlearn: 1.6747041\ttest: 1.2518136\tbest: 1.2518136 (250)\ttotal: 2.35s\tremaining: 25.8s\n## 300:\tlearn: 1.6645811\ttest: 1.2407228\tbest: 1.2407228 (300)\ttotal: 2.9s\tremaining: 26s\n## 350:\tlearn: 1.6557448\ttest: 1.2312354\tbest: 1.2312354 (350)\ttotal: 3.5s\tremaining: 26.4s\n## 400:\tlearn: 1.6464796\ttest: 1.2225415\tbest: 1.2225415 (400)\ttotal: 4.17s\tremaining: 27s\n## 450:\tlearn: 1.6404967\ttest: 1.2173237\tbest: 1.2173036 (449)\ttotal: 4.79s\tremaining: 27s\n## 500:\tlearn: 1.6353707\ttest: 1.2138953\tbest: 1.2138953 (500)\ttotal: 5.42s\tremaining: 27s\n## 550:\tlearn: 1.6321053\ttest: 1.2113879\tbest: 1.2113879 (550)\ttotal: 5.96s\tremaining: 26.5s\n## 600:\tlearn: 1.6273276\ttest: 1.2084719\tbest: 1.2083969 (599)\ttotal: 6.6s\tremaining: 26.4s\n## 650:\tlearn: 1.6235998\ttest: 1.2067629\tbest: 1.2067526 (648)\ttotal: 7.22s\tremaining: 26s\n## 700:\tlearn: 1.6207368\ttest: 1.2056776\tbest: 1.2056776 (700)\ttotal: 7.78s\tremaining: 25.5s\n## 750:\tlearn: 1.6178637\ttest: 1.2046009\tbest: 1.2045980 (747)\ttotal: 8.38s\tremaining: 25.1s\n## 800:\tlearn: 1.6151604\ttest: 1.2039642\tbest: 1.2039435 (796)\ttotal: 8.99s\tremaining: 24.7s\n## 850:\tlearn: 1.6122595\ttest: 1.2028463\tbest: 1.2028463 (850)\ttotal: 9.6s\tremaining: 24.2s\n## 900:\tlearn: 1.6101196\ttest: 1.2020336\tbest: 1.2019819 (890)\ttotal: 10.2s\tremaining: 23.7s\n## 950:\tlearn: 1.6073753\ttest: 1.2011880\tbest: 1.2011371 (948)\ttotal: 10.8s\tremaining: 23.3s\n## 1000:\tlearn: 1.6052842\ttest: 1.2006492\tbest: 1.2006411 (999)\ttotal: 11.4s\tremaining: 22.8s\n## 1050:\tlearn: 1.6039241\ttest: 1.2001930\tbest: 1.2001853 (1049)\ttotal: 12s\tremaining: 22.2s\n## 1100:\tlearn: 1.6028500\ttest: 1.1999358\tbest: 1.1999358 (1100)\ttotal: 12.5s\tremaining: 21.5s\n## 1150:\tlearn: 1.6012678\ttest: 1.1992978\tbest: 1.1992978 (1150)\ttotal: 13.2s\tremaining: 21.2s\n## 1200:\tlearn: 1.5997241\ttest: 1.1989927\tbest: 1.1989927 (1200)\ttotal: 13.8s\tremaining: 20.7s\n## 1250:\tlearn: 1.5984334\ttest: 1.1989688\tbest: 1.1989617 (1248)\ttotal: 14.5s\tremaining: 20.3s\n## 1300:\tlearn: 1.5969307\ttest: 1.1986201\tbest: 1.1985152 (1278)\ttotal: 15.2s\tremaining: 19.9s\n## 1350:\tlearn: 1.5948609\ttest: 1.1986032\tbest: 1.1985116 (1314)\ttotal: 15.9s\tremaining: 19.4s\n## Stopped by overfitting detector  (60 iterations wait)\n## \n## bestTest = 1.198511627\n## bestIteration = 1314\n## \n## Shrink model to first 1315 iterations.\nm_catboost_multirmse\n## CatBoost model (1315 trees)\n## Loss function: MultiRMSE\n## Fit to 14 feature(s)\nWe can check the Feature Importance of the trained model:\nfea_imp_catboost <- catboost::catboost.get_feature_importance(m_catboost_multirmse)\nfea_imp_catboost <- data.table::data.table(Gain = fea_imp_catboost[,1],\n                                           Feature = features)\nsetorder(fea_imp_catboost, -Gain)\nfea_imp_catboost\n##           Gain                   Feature\n##  1: 26.2825537    Load_predicted_STL_ETS\n##  2: 10.2856293   Load_predicted_ADAM_MAE\n##  3:  8.9606314   Load_predicted_ADAM_GPL\n##  4:  8.5230838  Load_predicted_ADAM_MSCE\n##  5:  8.4006334 Load_predicted_ADAM_GTMSE\n##  6:  6.5706169  Load_predicted_ADAM_TMSE\n##  7:  6.2375988  Load_predicted_STL_ARIMA\n##  8:  5.0254951  Load_predicted_ADAM_MSEh\n##  9:  4.8948488                     S_day\n## 10:  4.7176922   Load_predicted_ADAM_MSE\n## 11:  3.8361990  Load_predicted_ADAM_MAEh\n## 12:  3.3433404                    C_week\n## 13:  2.2428758                     C_day\n## 14:  0.6788015                    S_week\nSTL_ETS and MAE simple loss predictions have the highest importance to the multivariate model.\nLet’s predict the test set and compute prediction errors.\npredict_multirmse_catboost <- function(data, target, cat_model) {\n  \n  pred <- catboost::catboost.predict(cat_model,\n                                     catboost::catboost.load_pool(as.data.frame(data),\n                                                                  label = as.matrix(rep(0, nrow(data)), ncol = length(target))\n                                     ),\n                                     thread_count = 1)\n  \n  return(pred)\n  \n}\n \ncatboost_prediction <- predict_multirmse_catboost(data = data_predictions_all_casted_test[, .SD, .SDcols = c(features)],\n                                                  target = targets,\n                                                  cat_model = m_catboost_multirmse)\n \ndt_catboost_predicted <- copy(data_predictions_all_casted_test[, .SD, .SDcols = c(\"Date_Time\", targets)])\ndt_catboost_predicted[, (paste0(targets, \"_prediction\")) := as.data.table(catboost_prediction)]\n \ndt_catboost_predicted_melt <- copy(melt(dt_catboost_predicted,\n                                        id.vars = \"Date_Time\",\n                                        measure.vars = list(real = targets,\n                                                            predicted = paste0(targets, \"_prediction\")),\n                                        variable.name = \"model\",\n                                        variable.factor = F))\n \ndt_catboost_predicted_melt[data.table(model = as.character(1:10),\n                                      model_name = targets),\n                           on = .(model),\n                           model := i.model_name]\n \nsetorder(dt_catboost_predicted_melt, Date_Time, real)\ndt_catboost_predicted_melt[, Rank := 1:.N, by = .(Date_Time)]\nsetorder(dt_catboost_predicted_melt, Date_Time, predicted)\ndt_catboost_predicted_melt[, Rank_pred := 1:.N, by = .(Date_Time)]\n \ndt_catboost_predicted_melt[, .(RMSE = round(rmse(real, predicted), 3),\n                               MAE = round(mae(real, predicted), 3),\n                               MdAE = round(mdae(real, predicted), 3),\n                               Rank_error = round(mae(Rank, Rank_pred), 3),\n                               Rank_1_accuracy = round(.SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N], 3),\n                               Rank_1_cases = .SD[Rank == 1, .N]),\n                           by  = .(model)][order(-Rank_1_cases)]\n##                model  RMSE   MAE  MdAE Rank_error Rank_1_accuracy Rank_1_cases\n##  1:    Error_STL_ETS 0.467 0.307 0.213      3.428           0.367          892\n##  2:  Error_ADAM_MSCE 0.430 0.273 0.178      2.996           0.181          634\n##  3:  Error_ADAM_MAEh 0.440 0.280 0.187      2.920           0.072          621\n##  4:  Error_STL_ARIMA 0.456 0.291 0.195      3.262           0.315          588\n##  5:   Error_ADAM_MAE 0.449 0.305 0.218      3.229           0.166          584\n##  6:   Error_ADAM_MSE 0.440 0.293 0.204      3.209           0.021          379\n##  7:  Error_ADAM_MSEh 0.415 0.252 0.158      2.458           0.042          212\n##  8:   Error_ADAM_GPL 0.411 0.249 0.155      2.626           0.049          185\n##  9: Error_ADAM_GTMSE 0.415 0.248 0.153      2.417           0.024           82\n## 10:  Error_ADAM_TMSE 0.416 0.251 0.157      2.008           0.000           47\ndt_catboost_predicted_melt[, .(RMSE = rmse(real, predicted),\n                               MAE = mae(real, predicted),\n                               MdAE = mdae(real, predicted),\n                               Rank_error = mae(Rank, Rank_pred),\n                               Rank_1_accuracy = .SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N])]\n##         RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n## 1: 0.4343794 0.2748845 0.1785648   2.855114       0.1886837\nWe can see that the MAE and MdAE metrics were improved by catboost. On the other hand, average Rank estimation and first Rank estimation are not better than with MMLM.\nLet’s plot our predicted errors.\nggplot(dt_catboost_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n       aes(Date_Time, predicted, color = \"predicted error\")) +\n  facet_wrap(~model) +\n  geom_line(data = dt_catboost_predicted_melt[.(targets[c(1,3,5,9)]), on = .(model)],\n            aes(Date_Time, real, color = \"real error\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\nModels analysis\nTo have it nicely together, let’s bind all three models’ predictions and compute prediction errors for models and methods.\ndt_all_predicted <- rbindlist(list(dt_mmlm_predicted_melt[, method := \"mmlm\"],\n                                   dt_mlassom_predicted_melt[, method := \"mlasso\"],\n                                   dt_catboost_predicted_melt[, method := \"catboost\"]),\n                              use.names = T, fill = T)\n \ndt_all_predicted[, Error := real - predicted]\ndt_all_predicted[, AE := abs(Error)]\n \ndt_all_predicted[, .(RMSE = rmse(real, predicted),\n                     MAE = mae(real, predicted),\n                     MdAE = mdae(real, predicted),\n                     Rank_error = mae(Rank, Rank_pred),\n                     Rank_1_accuracy = .SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N]),\n                 by = .(method)][order(MAE)]\n##      method      RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n## 1: catboost 0.4343794 0.2748845 0.1785648   2.855114       0.1886837\n## 2:   mlasso 0.4328982 0.2873330 0.2033996   2.930540       0.1642992\n## 3:     mmlm 0.4387276 0.2880984 0.2051204   2.662926       0.2634943\ndt_all_predicted[, .(RMSE = rmse(real, predicted),\n                     MAE = mae(real, predicted),\n                     MdAE = mdae(real, predicted),\n                     Rank_error = mae(Rank, Rank_pred),\n                     Rank_1_accuracy = .SD[Rank_pred == 1 & Rank_pred == Rank, .N] / .SD[Rank == 1, .N]),\n                 by = .(model)][order(Rank_error)]\n##                model      RMSE       MAE      MdAE Rank_error Rank_1_accuracy\n##  1:  Error_ADAM_TMSE 0.4172927 0.2610935 0.1773354   2.225221      0.02127660\n##  2: Error_ADAM_GTMSE 0.4161971 0.2587184 0.1728826   2.321023      0.05691057\n##  3:  Error_ADAM_MSEh 0.4161620 0.2616851 0.1787479   2.323469      0.02358491\n##  4:   Error_ADAM_GPL 0.4119315 0.2586888 0.1757232   2.479640      0.08288288\n##  5:  Error_ADAM_MAEh 0.4407660 0.2861382 0.2026681   2.792456      0.22758991\n##  6:  Error_ADAM_MSCE 0.4286877 0.2752749 0.1868000   2.906013      0.26708728\n##  7:  Error_STL_ARIMA 0.4566267 0.3042826 0.2207566   3.130051      0.21428571\n##  8:   Error_ADAM_MSE 0.4423139 0.3010806 0.2195689   3.274779      0.01143360\n##  9:   Error_ADAM_MAE 0.4496008 0.3087655 0.2274286   3.303741      0.26312785\n## 10:    Error_STL_ETS 0.4697423 0.3186589 0.2340111   3.405540      0.27727952\nAs already mentioned, the best\nmultivariate regression model\nbased on classical prediction metrics is\ncatboosts MultiRMSE\n.\nOn the other hand, the best model based on right Rank estimation is simple\nmultivariate linear regression\n.\nLet’s do some basic model analysis and see a boxplot of absolute errors.\nggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(method, AE, fill = method)) +\n  geom_boxplot() +\n  theme_bw()\nPredicted vs. real values scatter plot:\nggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(predicted, real)) +\n  facet_wrap(~method, scales = \"fixed\", ncol = 3) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", \n              linetype = \"dashed\", linewidth = 1) +\n  geom_smooth(alpha = 0.4) +\n  labs(title = \"Predicted vs Real values\") +\n  theme_bw()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nErrors vs. real values scatter plot:\nggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(Error, real)) +\n  facet_wrap(~method, scales = \"fixed\", ncol = 3) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  xlab(\"resid (predicted_value - real_value)\") +\n  labs(title = \"Residual vs Real values\") +\n  theme_bw()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nAnd heteroscedasticity check, so absolute errors vs. real values:\nggplot(dt_all_predicted[.(targets[c(1, 3, 5, 9)]), on = .(model)],\n       aes(real, AE)) +\n  facet_wrap(~method, scales = \"free\") +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  ylab(\"abs(resid)\") +\n  labs(title = \"Heteroscedasticity check\") + \n  theme_bw()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nFrom all the above graphs, we can see that low and medium magnitudes of errors can be nicely predicted, but again prediction of high values of errors is a very difficult task.\nWe can use nonparametric multiple comparisons test (Nemenyi test) also to decide which error model is best (by Absolute Errors - AE).\ntsutils::nemenyi(data.matrix(dcast(dt_all_predicted,\n                                   Date_Time + model ~ method,\n                                   value.var = \"AE\")[, !c(\"Date_Time\", \"model\"), with = F]),\n                 plottype = \"vmcb\")\n## Friedman and Nemenyi Tests\n## The confidence level is 5%\n## Number of observations is 42240 and number of methods is 3\n## Friedman test p-value: 0.0000 - Ha: Different\n## Critical distance: 0.0161\nCatboost\nis best all the way.\nWe can also use the Nemenyi test on a question that which of the 10 model errors can be predicted the best.\ntsutils::nemenyi(data.matrix(dcast(dt_all_predicted,\n                                   Date_Time + method ~ model,\n                                   value.var = \"AE\")[, !c(\"Date_Time\", \"method\"), with = F]),\n                 plottype = \"vmcb\")\n## Friedman and Nemenyi Tests\n## The confidence level is 5%\n## Number of observations is 12672 and number of methods is 10\n## Friedman test p-value: 0.0000 - Ha: Different\n## Critical distance: 0.1203\nWe see that GTMSE and TMSE have the most predictable errors, for non-multistep losses methods (benchmarks) it is more difficult.\nEnsemble predictions\nFinally, we can now construct some ensemble predictions by our multivariate error model predictions.\nBest Rank prediction ensemble\nThe first method will be very simple, we will use as final ensemble prediction the prediction with the lowest estimated error.\ndt_all_predicted[, model := gsub(\"Error_\", \"\", model)]\ndata_best_rank_prediction <- copy(dt_all_predicted[, .SD[Rank_pred == 1], by = .(method, Date_Time)])\ndata_best_rank_prediction[, table(model)] # distribution of used methods\n## model\n##   ADAM_GPL ADAM_GTMSE   ADAM_MAE  ADAM_MAEh  ADAM_MSCE   ADAM_MSE  ADAM_MSEh \n##        899        880       1513       1613       2302        204        322 \n##  ADAM_TMSE  STL_ARIMA    STL_ETS \n##        279       1713       2947\nLet’s evaluate ensemble predictions by our error models and also compute metrics for our base predictions for comparison.\ndata_predictions_all[, model := gsub(\"-\", \"_\", model)]\ndata_predictions_all[, model := gsub(\"\\\\+\", \"_\", model)]\n \ndata_best_rank_prediction[data_predictions_all,\n                          on = .(model, Date_Time),\n                          Load_predicted := i.Load_predicted]\n \ndata_best_rank_prediction[data_predictions_all,\n                          on = .(Date_Time),\n                          Load_real := i.Load_real]\n \n# original\ndata_predictions_all[Date_Time %in% data_best_rank_prediction[, unique(Date_Time)],\n                     .(RMSE = rmse(Load_real, Load_predicted),\n                       MAE = mae(Load_real, Load_predicted),\n                       MAAPE = maape(Load_real, Load_predicted)),\n                     by = .(model)][order(MAE)]\n##          model      RMSE       MAE    MAAPE\n##  1:  ADAM_MAEh 0.6153573 0.3915900 79.73346\n##  2:  ADAM_MSEh 0.6060004 0.4032142 84.61694\n##  3:  ADAM_TMSE 0.6080947 0.4053491 84.76779\n##  4: ADAM_GTMSE 0.6094993 0.4085337 85.40032\n##  5:   ADAM_GPL 0.6082200 0.4106951 85.66970\n##  6:  ADAM_MSCE 0.6251760 0.4153277 85.63160\n##  7:  STL_ARIMA 0.6709538 0.4348279 87.92342\n##  8:   ADAM_MSE 0.6487472 0.4378956 87.45296\n##  9:   ADAM_MAE 0.6550635 0.4386036 87.33271\n## 10:    STL_ETS 0.6887292 0.4388718 80.82078\n# ensemble\ndata_best_rank_prediction[, .(RMSE = rmse(Load_real, Load_predicted),\n                              MAE = mae(Load_real, Load_predicted),\n                              MAAPE = maape(Load_real, Load_predicted)),\n                          by = .(method)][order(MAE)]\n##      method      RMSE       MAE    MAAPE\n## 1: catboost 0.6176298 0.3765020 78.38730\n## 2:     mmlm 0.6368562 0.3777519 76.25634\n## 3:   mlasso 0.6090476 0.3789960 81.54793\nAll three ensembles are better than any base prediction based on MAE. Well done!\nLet’s plot a graph of the final ensemble predictions:\nggplot(data_best_rank_prediction,\n       aes(Date_Time, Load_predicted, color = \"predicted load\")) +\n  facet_wrap(~method, ncol = 1) +\n  geom_line(data = data_best_rank_prediction,\n            aes(Date_Time, Load_real, color = \"real load\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\nCatboost doesn’t go that much below zero than the other two.\nWeighted average Ensemble\nThe second ensemble method will be based on weighting the estimated errors.\nFirstly, we need to normalize all weights (errors) by the Min-max method and compute the sum ratio.\nweis <- copy(dt_all_predicted[, .(Wei_m = TSrepr::norm_min_max(-abs(predicted)),\n                                  model),\n                             by = .(method, Date_Time)])\n \nweis_sums <- copy(dt_all_predicted[, .(Wei_sum = sum(TSrepr::norm_min_max(-abs(predicted)))),\n                                   by = .(method, Date_Time)])\n \nweis[weis_sums,\n     Wei_m := Wei_m / i.Wei_sum,\n     on = .(method, Date_Time)]\nWe can use all predictions, but a better way is to remove the bad half of the predictions from the ensemble:\nratio_forec <- 0.5 # 1 to use all 10 predictions\nsetorder(weis, method, Date_Time, Wei_m)\nweis[, Wei_m := replace(Wei_m, 1:floor(.N * ratio_forec), 0),\n     by = .(method, Date_Time)]\nNow, let’s join weights and predictions and compute final ensemble predictions.\ndt_all_predicted_wei_ave <- copy(dt_all_predicted)\ndt_all_predicted_wei_ave[weis,\n                         Wei_m := i.Wei_m,\n                         on = .(model, method, Date_Time)]\n \ndt_all_predicted_wei_ave[data_predictions_all,\n                         on = .(model, Date_Time),\n                         Load_predicted := i.Load_predicted]\n \ndata_predicted_wei_ave <- copy(dt_all_predicted_wei_ave[,\n                                                        .(Load_predicted = weighted.mean(Load_predicted, Wei_m)),\n                                                        by = .(method, Date_Time)])\nLet’s evaluate them:\ndata_predicted_wei_ave[data_predictions_all,\n                       on = .(Date_Time),\n                       Load_real := i.Load_real]\n \ndata_predicted_wei_ave[, .(RMSE = rmse(Load_real, Load_predicted),\n                           MAE = mae(Load_real, Load_predicted),\n                           MAAPE = maape(Load_real, Load_predicted)),\n                       by = .(method)][order(MAE)]\n##      method      RMSE       MAE    MAAPE\n## 1:     mmlm 0.6080235 0.3721450 78.36590\n## 2: catboost 0.6027960 0.3814034 82.43628\n## 3:   mlasso 0.5981348 0.3824002 81.68486\nEven better with weighting than the best rank way!\nLet’s plot a graph of ensemble predictions:\nggplot(data_predicted_wei_ave,\n       aes(Date_Time, Load_predicted, color = \"predicted load\")) +\n  facet_wrap(~method, ncol = 1) +\n  geom_line(data = data_predicted_wei_ave,\n            aes(Date_Time, Load_real, color = \"real load\")) +\n  geom_line(alpha = 0.7, linewidth = 0.6) +\n  scale_color_manual(values = c(\"red\", \"black\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\nUltimately, let’s decide which model is best for household electricity consumption prediction:\ndata_predictions_all_ensembles <- rbindlist(list(data_predictions_all[Date_Time %in% data_best_rank_prediction[, unique(Date_Time)], .(Date_Time, Load_predicted, Load_real, model)],\n                                                 data_predicted_wei_ave[, .(Date_Time, Load_predicted, Load_real, model = paste0(\"ensemble_wei_ave_\", method))],\n                                                 data_best_rank_prediction[, .(Date_Time, Load_predicted, Load_real, model = paste0(\"ensemble_best_rank_\", method))]))\n \ndata_predictions_all_ensembles[, AE := abs(Load_real - Load_predicted)]\n \ntsutils::nemenyi(data.matrix(dcast(data_predictions_all_ensembles,\n                                   Date_Time ~ model,\n                                   value.var = \"AE\")[, !c(\"Date_Time\"), with = F]),\n                 plottype = \"vmcb\")\n## Friedman and Nemenyi Tests\n## The confidence level is 5%\n## Number of observations is 4224 and number of methods is 16\n## Friedman test p-value: 0.0000 - Ha: Different\n## Critical distance: 0.3549\nWeighted average ensemble prediction using the MMLM method is best!\nSummary\nI showed how easily can be modeled multiple targets with three methods: MMLM, LASSO, and Catboost in R\nI trained a model for predicting absolute errors of mostly simple exponential smoothing methods\nThe multivariate models can nicely predict errors of low-medium magnitude\nAgain the high errors can not be predicted with those simple models\nCatboost showed the best results in regard of absolute error measure\nI used predicted errors to create two types of ensemble predictions of electricity consumption\nBoth ensemble methods showed better results than any original prediction\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nPeter Laurinec\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "In the last blog post about Multistep forecasting losses, I showed the usage of the fantastic method adam from the smooth R package on household electricity consumption data, and compared it with benchmarks. Since I computed predictions from 10 method...",
    "meta_keywords": null,
    "og_description": "In the last blog post about Multistep forecasting losses, I showed the usage of the fantastic method adam from the smooth R package on household electricity consumption data, and compared it with benchmarks. Since I computed predictions from 10 method...",
    "og_image": "https://petolau.github.io/images/post_15/unnamed-chunk-7-1.png",
    "og_title": "Multivariate Regression Ensemble Models for Errors Prediction | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 25.4,
    "sitemap_lastmod": "2023-11-29T00:00:00+00:00",
    "twitter_description": "In the last blog post about Multistep forecasting losses, I showed the usage of the fantastic method adam from the smooth R package on household electricity consumption data, and compared it with benchmarks. Since I computed predictions from 10 method...",
    "twitter_title": "Multivariate Regression Ensemble Models for Errors Prediction | R-bloggers",
    "url": "https://www.r-bloggers.com/2023/11/multivariate-regression-ensemble-models-for-errors-prediction/",
    "word_count": 5081
  }
}