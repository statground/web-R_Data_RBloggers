{
  "id": "a7eb2788e347dc67fe8bb372867ff319dc1cec2a",
  "url": "https://www.r-bloggers.com/2025/05/taylor-series-approximation-to-newton-raphson-algorithm-a-note-for-myself-of-the-proof/",
  "created_at_utc": "2025-11-22T19:58:32Z",
  "data": null,
  "raw_original": {
    "uuid": "45cbee1f-ed3b-49fd-b158-ad77cb6b4445",
    "created_at": "2025-11-22 19:58:32",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/05/taylor-series-approximation-to-newton-raphson-algorithm-a-note-for-myself-of-the-proof/",
      "crawled_at": "2025-11-22T10:48:27.994826",
      "external_links": [
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#objectives",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#proof",
          "text": "The Proof From Taylor Series Approximation To Newton Raphson Algorithm"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#code",
          "text": "Let’s Code"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#glm",
          "text": "Compare with glm"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#lessons",
          "text": "Lessons Learnt"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#proof",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#what-is-taylor-series",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#what-is-newton-raphson-algorithm",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#how-do-we-get-from-taylor-series-to-newton-raphson---proof",
          "text": null
        },
        {
          "href": "https://sites.stat.washington.edu/adobra/classes/536/Files/week1/newtonfull.pdf",
          "text": "Dive deeper"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/",
          "text": "Previously"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#lets-put-it-all-together",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#beta0",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#beta1",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#code",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#glm",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#fisher-information-matrix",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#hessian",
          "text": "As previously stated"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#lets-put-it-all-together-1",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#fisher-information-iteration",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/#lessons",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/",
          "text": "comment or visit my other blogs"
        },
        {
          "href": "https://bsky.app/profile/kenkoonwong.bsky.social",
          "text": "BlueSky"
        },
        {
          "href": "https://twitter.com/kenkoonwong/",
          "text": "twitter"
        },
        {
          "href": "https://github.com/kenkoonwong/",
          "text": "GitHub"
        },
        {
          "href": "https://med-mastodon.com/@kenkoonwong",
          "text": "Mastodon"
        },
        {
          "href": "https://www.kenkoonwong.com/contact/",
          "text": "contact me"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/newton-raphson/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Taylor Series Approximation To Newton Raphson Algorithm – A note for myself of the proof | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/www.kenkoonwong.com/blog/newton-raphson/math.jpg?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392618 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Taylor Series Approximation To Newton Raphson Algorithm – A note for myself of the proof</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 24, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/\">r on Everyday Is A School Day</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.kenkoonwong.com/blog/newton-raphson/\"> r on Everyday Is A School Day</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><blockquote>\n<p>We learnt to derive the Newton-Raphson algorithm from Taylor series approximation and implements it for logistic regression in R. We’ll show how the second-order Taylor expansion leads to the Newton-Raphson update formula, then compare individual parameter updates versus using the full Fisher Information matrix for faster convergence.</p>\n</blockquote>\n<p><img alt=\"\" data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/newton-raphson/math.jpg?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/newton-raphson/math.jpg?w=578&amp;ssl=1\"/></noscript></p>\n<h2 id=\"objectives\">Objectives\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#objectives\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#proof\" rel=\"nofollow\" target=\"_blank\">The Proof From Taylor Series Approximation To Newton Raphson Algorithm</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#code\" rel=\"nofollow\" target=\"_blank\">Let’s Code</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#glm\" rel=\"nofollow\" target=\"_blank\">Compare with glm</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#lessons\" rel=\"nofollow\" target=\"_blank\">Lessons Learnt</a></li>\n</ul>\n<h2 id=\"proof\">The Proof From Taylor Series Approximation To Newton Raphson Algorithm\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#proof\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<h4 id=\"what-is-taylor-series\">What is Taylor Series?\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#what-is-taylor-series\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>A Taylor series approximation is a mathematical technique that represents a smooth function as an infinite sum of terms calculated from the function’s derivatives at a single point. Named after mathematician Brook Taylor, this method expresses a function f(x) around a point ‘a’ as <code>f(a) + f'(a)(x-a) + f''(a)(x-a)²/2! + f'''(a)(x-a)³/3! + ...</code>, where each term involves higher-order derivatives and powers of (x-a). The beauty of Taylor series lies in their ability to approximate complex functions using simple polynomial terms – the more terms you include, the more accurate your approximation becomes within a certain radius of convergence around the expansion point. This makes Taylor series invaluable in calculus, physics, and engineering for solving differential equations, analyzing oscillations, and performing numerical computations where exact solutions are difficult to obtain.</p>\n<h4 id=\"what-is-newton-raphson-algorithm\">What is Newton Raphson Algorithm?\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#what-is-newton-raphson-algorithm\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>The Newton-Raphson algorithm is an iterative numerical method used to find successively better approximations of the roots (or zeroes) of a real-valued function. It is based on the idea that if you have a function f(x) and its derivative f’(x), you can use the tangent line at a point x₀ to find a better approximation of the root. The formula for the next approximation x₁ is given by <code>x₁ = x₀ - f(x₀)/f'(x₀)</code>. By repeating this process, you can converge to the actual root of the function. The method is particularly effective for functions that are continuous and differentiable, and it converges rapidly when the initial guess is close to the true root.</p>\n<h4 id=\"how-do-we-get-from-taylor-series-to-newton-raphson---proof\">How Do We Get From Taylor Series to Newton Raphson – Proof\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#how-do-we-get-from-taylor-series-to-newton-raphson---proof\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>Theorem 1: Taylor Theorem</p>\n<p>$$\nf(x+h) = f(x) + f’(x)h + \\frac{1}{2}f’’(x)h^2 + \\frac{1}{3!}f’’’(x)h^3 + \\ldots + \\frac{1}{k!}f^{(k)}(x)h^k + \\frac{1}{(k+1)!}f^{(k+1)}(w)h^{k+1}\n$$\nIt can be shown that as h goes to 0 the higher order terms in our Taylor theorem go to 0 much\nfaster than h goes to 0.</p>\n<p>If we were to use the second-Order Taylor Approximation\nWe have:\n$$\nf(x + h) ≈ f(x) + f’(x)h + \\frac{1}{2}f’’(x)h^2\n$$\nSo we have a <code>f(x)</code> and we want to add a value <code>h</code> to <code>x</code>. The Taylor series expansion gives us an approximation of the <code>function f(x + h)</code></p>\n<p>Let’s call this approximation <code>g(h)</code></p>\n<p>$$\ng(h) = f(x) + f’(x)h + \\frac{1}{2}f’’(x)h^2\n$$\nWe want to find the value of <code>h</code> that maximizes <code>g(h)</code>. That means we can take the derivative of g(h) with respect to h and set it equal to 0.</p>\n<p>$$\n\\begin{gather}\n\\frac{\\partial}{\\partial h} g(h) = 0 + f’(x) + f’’(x)h \\\\\n0 = f’(x) + f’’(x)h \\\\\n-f’(x) = f’’(x)h \\\\\nh = -\\frac{f’(x)}{f’’(x)}\n\\end{gather}\n$$\nThis gives us the value of <code>h</code> that maximizes <code>g(h)</code>.</p>\n<p>If we want to estimate the new value of <code>x</code>, we can add <code>h</code> to <code>x</code>:\n$$\nx_{new} = x_{old} + h \\\\ = x_{old} – \\frac{f’(x_{old})}{f’’(x_{old})}\n$$\nEt viola! From Taylor series to Newton Raphson Alogorithm! ❤️</p>\n<p>\n<a href=\"https://sites.stat.washington.edu/adobra/classes/536/Files/week1/newtonfull.pdf\" rel=\"nofollow\" target=\"_blank\">Dive deeper</a></p>\n<p>Now, let’s continue our logistic regression journey by implementing the Newton-Raphson algorithm to estimate coefficients using MLE.</p>\n<p>\n<a href=\"https://www.kenkoonwong.com/blog/mle/\" rel=\"nofollow\" target=\"_blank\">Previously</a>, we have established the likelihood function for logistic regression and derived the log-likelihood function for beta0 (intercept) and beta1 (coefficient for 1 predictor).</p>\n<p>$$\n\\ln L(\\boldsymbol{\\beta_0, \\beta_1}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n$$\nFor beta0, to find the derivate of the log-likelihood function, we need to take the first derivative of the log-likelihood function with respect to beta0, like so.</p>\n<p>$$\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( y_i – p_i \\right)\n$$\nFor beta1, we need to take the first derivative of the log-likelihood function with respect to beta1, like so.\n$$\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\left( y_i – p_i \\right) x_{1i}\n$$\nThese will be our <code>f'(x)</code></p>\n<br/>\n<p>The second derivative of the log-likelihood function with respect to beta0 is given by:\n$$\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i (1 – p_i)\n$$\nFor beta1, we need to take the second derivative of the log-likelihood function with respect to beta1, like so.\n$$\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i (1 – p_i) x_{1i}^2\n$$</p>\n<p>These will be our <code>f''(x)</code></p>\n<br/>\n<h4 id=\"lets-put-it-all-together\">Let’s put it all together\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#lets-put-it-all-together\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>We can now use the Newton-Raphson algorithm to estimate the coefficients of the logistic regression model. The algorithm will iteratively update the coefficients until convergence is achieved.</p>\n<h4 id=\"beta0\">Beta0\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#beta0\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\begin{gather}\n\\beta_{new} = \\beta_{old} + h \\\\\n= \\beta_{old} – \\frac{f’(\\beta_{old})}{f’’(\\beta_{old})} \\\\\n= \\beta_{old} – (\\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right)}{-\\sum_{i=1}^{n} p_i (1 – p_i)}) \\\\\n= \\beta_{old} + \\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right)}{\\sum_{i=1}^{n} p_i (1 – p_i)}\n\\end{gather}\n$$</p>\n<h4 id=\"beta1\">Beta1\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#beta1\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\begin{gather}\n\\beta_{new} = \\beta_{old} + h \\\\\n= \\beta_{old} – \\frac{f’(\\beta_{old})}{f’’(\\beta_{old})} \\\\\n= \\beta_{old} – (\\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right) x_{1i}}{-\\sum_{i=1}^{n} p_i (1 – p_i) x_{1i}^2}) \\\\\n= \\beta_{old} + \\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right) x_{1i}}{\\sum_{i=1}^{n} p_i (1 – p_i) x_{1i}^2}\n\\end{gather}\n$$\nWow, amazing! We did it! Now, let’s put these formulae into code and see how it works.</p>\n<h2 id=\"code\">Let’s Code\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#code\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<pre>library(tidyverse)\n\nset.seed(100)\nn &lt;- 100\nx &lt;- rbinom(n,1,0.5)\ny &lt;- rbinom(n,1,plogis(-2+2*x))\n\nbeta &lt;- c(0,0)\niter &lt;- 100\nhistory &lt;- matrix(0,nrow = iter, ncol = 2)\ntolerance &lt;- 10^-8\n\nfor (i in 1:iter) {\n\nz &lt;- beta[1] + beta[2]*x\np &lt;- 1 / (1+exp(-z))\nd_b0 &lt;- sum(y-p)\nd_b1 &lt;- sum((y-p)*x)\nd2_b0 &lt;- -sum(p*(1-p))\nd2_b1 &lt;- -sum(p*(1-p)*x^2)\n\nbeta_new &lt;- beta - c(d_b0/d2_b0, d_b1/d2_b1)\nhistory[i,] &lt;- beta_new\n\nif (abs(beta_new[1] - beta[1]) &lt; tolerance &amp;&amp; abs(beta_new[2] - beta[2]) &lt; tolerance) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\"\\nbeta1: \",beta_new[2]))\n  break\n}\nbeta &lt;- beta_new\n\nif (i==iter) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\"\\nbeta1: \",beta_new[2]))\n}\n}\n\n## converged on iter 100\n## beta0: -2.75149081264213\n## beta1: 2.99265005420773\n</pre><p>Alright! We have successfully implemented the Newton-Raphson algorithm to estimate the coefficients of the logistic regression model. The algorithm iteratively updates the coefficients until convergence is achieved, and we can see the final estimates of beta0 and beta1.</p>\n<p>It’s kind of odd that it took so many more iterations to converge than glm as the max iter set for glm is 25.</p>\n<pre>glm.control\n\n## function (epsilon = 1e-08, maxit = 25, trace = FALSE) \n## {\n##     if (!is.numeric(epsilon) || epsilon &lt;= 0) \n##         stop(\"value of 'epsilon' must be &gt; 0\")\n##     if (!is.numeric(maxit) || maxit &lt;= 0) \n##         stop(\"maximum number of iterations must be &gt; 0\")\n##     list(epsilon = epsilon, maxit = maxit, trace = trace)\n## }\n## &lt;bytecode: 0x10544c4a8&gt;\n## &lt;environment: namespace:stats&gt;\n</pre>\n<h2 id=\"glm\">Compare with glm\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#glm\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<pre>(summary(glm(y~x,family = binomial(link = \"logit\"))))\n\n## \n## Call:\n## glm(formula = y ~ x, family = binomial(link = \"logit\"))\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -2.7515     0.5955  -4.621 3.83e-06 ***\n## x             2.9927     0.6601   4.533 5.80e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 123.82  on 99  degrees of freedom\n## Residual deviance:  91.29  on 98  degrees of freedom\n## AIC: 95.29\n## \n## Number of Fisher Scoring iterations: 5\n</pre><p>OK, the point estimates look very close to each other. But why is our iteration so much more than glm? That’s because we are not using the full Fisher Information matrix to update our coefficients.</p>\n<h4 id=\"fisher-information-matrix\">Fisher Information Matrix\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#fisher-information-matrix\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#hessian\" rel=\"nofollow\" target=\"_blank\">As previously stated</a>, the Fisher Information matrix is a square matrix that contains the second-order partial derivatives of the log-likelihood function with respect to the parameters. It provides information about the curvature of the log-likelihood function and is used to estimate the variance-covariance matrix of the maximum likelihood estimates. Not only that, using as a whole, further optimizes the convergence of the algorithm.</p>\n<p>$$\n\\begin{gather}\nI(\\boldsymbol{\\beta}) = \\begin{bmatrix}\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0^2} &amp; \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0 \\partial \\beta_1} \\\\\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1 \\partial \\beta_0} &amp; \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1^2}\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n-\\sum_{i=1}^{n} p_i (1 - p_i) &amp; -\\sum_{i=1}^{n} p_i (1 - p_i) x_{1i} \\\\\n-\\sum_{i=1}^{n} p_i (1 - p_i) x_{1i} &amp; -\\sum_{i=1}^{n} p_i (1 - p_i) x_{1i}^2\n\\end{bmatrix}\n\\end{gather}\n$$</p>\n<h4 id=\"lets-put-it-all-together-1\">Let’s put it all together\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#lets-put-it-all-together-1\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>We can now use the Fisher Information matrix to update the coefficients of the logistic regression model. The algorithm will iteratively update the coefficients until convergence is achieved.</p>\n<p>Let’s expand our previous formulae to include the Fisher Information matrix and score vector (aka gradient of the vectors) like so.</p>\n<p>$$\n\\beta_{new} = \\beta_{old} + h \\\\\n\\begin{bmatrix}\n\\beta_{0_\\text{new}} \\\\\n\\beta_{1_\\text{new}}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - I(\\boldsymbol{\\beta_{old}})^{-1} \\cdot \\nabla f(\\beta_{old}) \\\\\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\begin{bmatrix}\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0^2} &amp; \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0 \\partial \\beta_1} \\\\\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1 \\partial \\beta_0} &amp; \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1^2}\n\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix}\n$$\nLet’s try to make it a bit less messy\n$$\n\\text{Let a} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0^2} , \\text{b} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0 \\partial \\beta_1} , \\text{c} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1 \\partial \\beta_0} , \\text{d} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1^2} \\\\\n\\begin{bmatrix}\n\\beta_{0_\\text{new}} \\\\\n\\beta_{1_\\text{new}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\begin{bmatrix}\na &amp; b \\\\\nc &amp; d\n\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\frac{1}{ad - bc} \\begin{bmatrix}\nd &amp; -b \\\\\n-c &amp; a\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\begin{bmatrix}\n\\frac{d}{ad - bc} &amp; \\frac{-b}{ad - bc} \\\\\n\\frac{-c}{ad - bc} &amp; \\frac{a}{ad - bc}\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix}\n$$\nAs we can see here, we have a matrix multiplication of the inverse of the Fisher Information matrix and the score vector. In our previous simple example, we only included the diagonal individual elements of the Fisher Information matrix. By including the off-diagonal elements, we can achieve a more accurate estimate of the coefficients and probably a faster convergence like in glm. Let’s code and see if that’s true!</p>\n<h4 id=\"fisher-information-iteration\">Fisher Information Iteration\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#fisher-information-iteration\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<pre>beta &lt;- c(0,0)\niter &lt;- 25\nhistory &lt;- matrix(0,nrow = iter, ncol = 2)\ntolerance &lt;- 10^-8\n\nfor (i in 1:iter) {\n\nz &lt;- beta[1] + beta[2]*x\np &lt;- 1 / (1+exp(-z))\nd_b0 &lt;- sum(y-p)\nd_b1 &lt;- sum((y-p)*x)\nscore_vec &lt;- c(d_b0, d_b1)\ni_11 &lt;- sum(p*(1-p))\ni_10 &lt;- sum(x*p*(1-p))\ni_01 &lt;- i_10\ni_22 &lt;- sum(x^2*p*(1-p))\ni_mat &lt;- matrix(c(i_11,i_01,i_10,i_22),nrow = 2, ncol = 2)\ni_mat_inv &lt;- solve(i_mat)\n\nbeta_new &lt;- beta + i_mat_inv %*% score_vec\nhistory[i,] &lt;- beta_new\n\n## se\nse_beta0 &lt;- sqrt(diag(i_mat_inv)[1])\nse_beta1 &lt;- sqrt(diag(i_mat_inv)[2])\n\nif (abs(beta_new[1] - beta[1]) &lt; tolerance &amp;&amp; abs(beta_new[2] - beta[2]) &lt; tolerance) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\" (\",se_beta0,\") \",\"\\nbeta1: \",beta_new[2],\" (\",se_beta1,\") \"))\n  break\n}\nbeta &lt;- beta_new\n\nif (i==iter) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\" (\",se_beta0,\") \",\"\\nbeta1: \",beta_new[2],\" (\",se_beta1,\") \"))\n}\n}\n\n## converged on iter 7\n## beta0: -2.75153531304195 (0.595491334175413) \n## beta1: 2.99269736985884 (0.660135410538508)\n</pre><p>Wow, not too shabby! Point estimates and SE are very close to glm. Iterations were definitely shorter than the previous example.</p>\n<p>There we have it! It’s fascinating to look under the hood how all these works. The Newton-Raphson algorithm is a powerful tool for estimating coefficients in logistic regression, and understanding its connection to Taylor series approximation provides valuable insight into its convergence properties.</p>\n<h2 id=\"lessons\">Lessons Learnt\n  <a href=\"https://www.kenkoonwong.com/blog/newton-raphson/#lessons\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>Learnt the surface understanding of what Taylor series approximation does.</li>\n<li>Learnt Newton-Raphson algorithm. Such an elegant formula!</li>\n<li>The proof of the Newton-Raphson algorithm using Taylor series approximation</li>\n<li>How including the Fisher Information matrix can improve convergence, previously I’ve always thought the coefficients are independent of each other and only the individual diagonal elements are needed. But it turns out the off-diagonal elements are also important for convergence.</li>\n<li>learnt the <code>\\(\\nabla\\)</code> symbol is called the nabla symbol, which is used to denote the gradient of a function.</li>\n<li>learnt <code>glm.control</code> and how glm default maxiter is 25 and epsilon (tolerance threshold) is 10^-8.</li>\n</ul>\n<p>If you like this article:</p>\n<ul>\n<li>please feel free to send me a \n<a href=\"https://www.kenkoonwong.com/blog/\" rel=\"nofollow\" target=\"_blank\">comment or visit my other blogs</a></li>\n<li>please feel free to follow me on \n<a href=\"https://bsky.app/profile/kenkoonwong.bsky.social\" rel=\"nofollow\" target=\"_blank\">BlueSky</a>, \n<a href=\"https://twitter.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">twitter</a>, \n<a href=\"https://github.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">GitHub</a> or \n<a href=\"https://med-mastodon.com/@kenkoonwong\" rel=\"nofollow\" target=\"_blank\">Mastodon</a></li>\n<li>if you would like collaborate please feel free to \n<a href=\"https://www.kenkoonwong.com/contact/\" rel=\"nofollow\" target=\"_blank\">contact me</a></li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.kenkoonwong.com/blog/newton-raphson/\"> r on Everyday Is A School Day</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Taylor Series Approximation To Newton Raphson Algorithm – A note for myself of the proof\nPosted on\nMay 24, 2025\nby\nr on Everyday Is A School Day\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nr on Everyday Is A School Day\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nWe learnt to derive the Newton-Raphson algorithm from Taylor series approximation and implements it for logistic regression in R. We’ll show how the second-order Taylor expansion leads to the Newton-Raphson update formula, then compare individual parameter updates versus using the full Fisher Information matrix for faster convergence.\nObjectives\nThe Proof From Taylor Series Approximation To Newton Raphson Algorithm\nLet’s Code\nCompare with glm\nLessons Learnt\nThe Proof From Taylor Series Approximation To Newton Raphson Algorithm\nWhat is Taylor Series?\nA Taylor series approximation is a mathematical technique that represents a smooth function as an infinite sum of terms calculated from the function’s derivatives at a single point. Named after mathematician Brook Taylor, this method expresses a function f(x) around a point ‘a’ as\nf(a) + f'(a)(x-a) + f''(a)(x-a)²/2! + f'''(a)(x-a)³/3! + ...\n, where each term involves higher-order derivatives and powers of (x-a). The beauty of Taylor series lies in their ability to approximate complex functions using simple polynomial terms – the more terms you include, the more accurate your approximation becomes within a certain radius of convergence around the expansion point. This makes Taylor series invaluable in calculus, physics, and engineering for solving differential equations, analyzing oscillations, and performing numerical computations where exact solutions are difficult to obtain.\nWhat is Newton Raphson Algorithm?\nThe Newton-Raphson algorithm is an iterative numerical method used to find successively better approximations of the roots (or zeroes) of a real-valued function. It is based on the idea that if you have a function f(x) and its derivative f’(x), you can use the tangent line at a point x₀ to find a better approximation of the root. The formula for the next approximation x₁ is given by\nx₁ = x₀ - f(x₀)/f'(x₀)\n. By repeating this process, you can converge to the actual root of the function. The method is particularly effective for functions that are continuous and differentiable, and it converges rapidly when the initial guess is close to the true root.\nHow Do We Get From Taylor Series to Newton Raphson – Proof\nTheorem 1: Taylor Theorem\n$$\nf(x+h) = f(x) + f’(x)h + \\frac{1}{2}f’’(x)h^2 + \\frac{1}{3!}f’’’(x)h^3 + \\ldots + \\frac{1}{k!}f^{(k)}(x)h^k + \\frac{1}{(k+1)!}f^{(k+1)}(w)h^{k+1}\n$$\nIt can be shown that as h goes to 0 the higher order terms in our Taylor theorem go to 0 much\nfaster than h goes to 0.\nIf we were to use the second-Order Taylor Approximation\nWe have:\n$$\nf(x + h) ≈ f(x) + f’(x)h + \\frac{1}{2}f’’(x)h^2\n$$\nSo we have a\nf(x)\nand we want to add a value\nh\nto\nx\n. The Taylor series expansion gives us an approximation of the\nfunction f(x + h)\nLet’s call this approximation\ng(h)\n$$\ng(h) = f(x) + f’(x)h + \\frac{1}{2}f’’(x)h^2\n$$\nWe want to find the value of\nh\nthat maximizes\ng(h)\n. That means we can take the derivative of g(h) with respect to h and set it equal to 0.\n$$\n\\begin{gather}\n\\frac{\\partial}{\\partial h} g(h) = 0 + f’(x) + f’’(x)h \\\\\n0 = f’(x) + f’’(x)h \\\\\n-f’(x) = f’’(x)h \\\\\nh = -\\frac{f’(x)}{f’’(x)}\n\\end{gather}\n$$\nThis gives us the value of\nh\nthat maximizes\ng(h)\n.\nIf we want to estimate the new value of\nx\n, we can add\nh\nto\nx\n:\n$$\nx_{new} = x_{old} + h \\\\ = x_{old} – \\frac{f’(x_{old})}{f’’(x_{old})}\n$$\nEt viola! From Taylor series to Newton Raphson Alogorithm! ❤️\nDive deeper\nNow, let’s continue our logistic regression journey by implementing the Newton-Raphson algorithm to estimate coefficients using MLE.\nPreviously\n, we have established the likelihood function for logistic regression and derived the log-likelihood function for beta0 (intercept) and beta1 (coefficient for 1 predictor).\n$$\n\\ln L(\\boldsymbol{\\beta_0, \\beta_1}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n$$\nFor beta0, to find the derivate of the log-likelihood function, we need to take the first derivative of the log-likelihood function with respect to beta0, like so.\n$$\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( y_i – p_i \\right)\n$$\nFor beta1, we need to take the first derivative of the log-likelihood function with respect to beta1, like so.\n$$\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\left( y_i – p_i \\right) x_{1i}\n$$\nThese will be our\nf'(x)\nThe second derivative of the log-likelihood function with respect to beta0 is given by:\n$$\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i (1 – p_i)\n$$\nFor beta1, we need to take the second derivative of the log-likelihood function with respect to beta1, like so.\n$$\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i (1 – p_i) x_{1i}^2\n$$\nThese will be our\nf''(x)\nLet’s put it all together\nWe can now use the Newton-Raphson algorithm to estimate the coefficients of the logistic regression model. The algorithm will iteratively update the coefficients until convergence is achieved.\nBeta0\n$$\n\\begin{gather}\n\\beta_{new} = \\beta_{old} + h \\\\\n= \\beta_{old} – \\frac{f’(\\beta_{old})}{f’’(\\beta_{old})} \\\\\n= \\beta_{old} – (\\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right)}{-\\sum_{i=1}^{n} p_i (1 – p_i)}) \\\\\n= \\beta_{old} + \\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right)}{\\sum_{i=1}^{n} p_i (1 – p_i)}\n\\end{gather}\n$$\nBeta1\n$$\n\\begin{gather}\n\\beta_{new} = \\beta_{old} + h \\\\\n= \\beta_{old} – \\frac{f’(\\beta_{old})}{f’’(\\beta_{old})} \\\\\n= \\beta_{old} – (\\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right) x_{1i}}{-\\sum_{i=1}^{n} p_i (1 – p_i) x_{1i}^2}) \\\\\n= \\beta_{old} + \\frac{\\sum_{i=1}^{n} \\left( y_i – p_i \\right) x_{1i}}{\\sum_{i=1}^{n} p_i (1 – p_i) x_{1i}^2}\n\\end{gather}\n$$\nWow, amazing! We did it! Now, let’s put these formulae into code and see how it works.\nLet’s Code\nlibrary(tidyverse)\n\nset.seed(100)\nn <- 100\nx <- rbinom(n,1,0.5)\ny <- rbinom(n,1,plogis(-2+2*x))\n\nbeta <- c(0,0)\niter <- 100\nhistory <- matrix(0,nrow = iter, ncol = 2)\ntolerance <- 10^-8\n\nfor (i in 1:iter) {\n\nz <- beta[1] + beta[2]*x\np <- 1 / (1+exp(-z))\nd_b0 <- sum(y-p)\nd_b1 <- sum((y-p)*x)\nd2_b0 <- -sum(p*(1-p))\nd2_b1 <- -sum(p*(1-p)*x^2)\n\nbeta_new <- beta - c(d_b0/d2_b0, d_b1/d2_b1)\nhistory[i,] <- beta_new\n\nif (abs(beta_new[1] - beta[1]) < tolerance && abs(beta_new[2] - beta[2]) < tolerance) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\"\\nbeta1: \",beta_new[2]))\n  break\n}\nbeta <- beta_new\n\nif (i==iter) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\"\\nbeta1: \",beta_new[2]))\n}\n}\n\n## converged on iter 100\n## beta0: -2.75149081264213\n## beta1: 2.99265005420773\nAlright! We have successfully implemented the Newton-Raphson algorithm to estimate the coefficients of the logistic regression model. The algorithm iteratively updates the coefficients until convergence is achieved, and we can see the final estimates of beta0 and beta1.\nIt’s kind of odd that it took so many more iterations to converge than glm as the max iter set for glm is 25.\nglm.control\n\n## function (epsilon = 1e-08, maxit = 25, trace = FALSE) \n## {\n##     if (!is.numeric(epsilon) || epsilon <= 0) \n##         stop(\"value of 'epsilon' must be > 0\")\n##     if (!is.numeric(maxit) || maxit <= 0) \n##         stop(\"maximum number of iterations must be > 0\")\n##     list(epsilon = epsilon, maxit = maxit, trace = trace)\n## }\n## <bytecode: 0x10544c4a8>\n## <environment: namespace:stats>\nCompare with glm\n(summary(glm(y~x,family = binomial(link = \"logit\"))))\n\n## \n## Call:\n## glm(formula = y ~ x, family = binomial(link = \"logit\"))\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -2.7515     0.5955  -4.621 3.83e-06 ***\n## x             2.9927     0.6601   4.533 5.80e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 123.82  on 99  degrees of freedom\n## Residual deviance:  91.29  on 98  degrees of freedom\n## AIC: 95.29\n## \n## Number of Fisher Scoring iterations: 5\nOK, the point estimates look very close to each other. But why is our iteration so much more than glm? That’s because we are not using the full Fisher Information matrix to update our coefficients.\nFisher Information Matrix\nAs previously stated\n, the Fisher Information matrix is a square matrix that contains the second-order partial derivatives of the log-likelihood function with respect to the parameters. It provides information about the curvature of the log-likelihood function and is used to estimate the variance-covariance matrix of the maximum likelihood estimates. Not only that, using as a whole, further optimizes the convergence of the algorithm.\n$$\n\\begin{gather}\nI(\\boldsymbol{\\beta}) = \\begin{bmatrix}\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0^2} & \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0 \\partial \\beta_1} \\\\\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1 \\partial \\beta_0} & \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1^2}\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n-\\sum_{i=1}^{n} p_i (1 - p_i) & -\\sum_{i=1}^{n} p_i (1 - p_i) x_{1i} \\\\\n-\\sum_{i=1}^{n} p_i (1 - p_i) x_{1i} & -\\sum_{i=1}^{n} p_i (1 - p_i) x_{1i}^2\n\\end{bmatrix}\n\\end{gather}\n$$\nLet’s put it all together\nWe can now use the Fisher Information matrix to update the coefficients of the logistic regression model. The algorithm will iteratively update the coefficients until convergence is achieved.\nLet’s expand our previous formulae to include the Fisher Information matrix and score vector (aka gradient of the vectors) like so.\n$$\n\\beta_{new} = \\beta_{old} + h \\\\\n\\begin{bmatrix}\n\\beta_{0_\\text{new}} \\\\\n\\beta_{1_\\text{new}}\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - I(\\boldsymbol{\\beta_{old}})^{-1} \\cdot \\nabla f(\\beta_{old}) \\\\\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\begin{bmatrix}\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0^2} & \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0 \\partial \\beta_1} \\\\\n\\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1 \\partial \\beta_0} & \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1^2}\n\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix}\n$$\nLet’s try to make it a bit less messy\n$$\n\\text{Let a} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0^2} , \\text{b} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0 \\partial \\beta_1} , \\text{c} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1 \\partial \\beta_0} , \\text{d} = \\frac{\\partial^2 \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1^2} \\\\\n\\begin{bmatrix}\n\\beta_{0_\\text{new}} \\\\\n\\beta_{1_\\text{new}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix} \\\\\n= \\begin{bmatrix}\n\\beta_{0_\\text{old}} \\\\\n\\beta_{1_\\text{old}}\n\\end{bmatrix} - \\begin{bmatrix}\n\\frac{d}{ad - bc} & \\frac{-b}{ad - bc} \\\\\n\\frac{-c}{ad - bc} & \\frac{a}{ad - bc}\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_0} \\\\\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta})}{\\partial \\beta_1}\n\\end{bmatrix}\n$$\nAs we can see here, we have a matrix multiplication of the inverse of the Fisher Information matrix and the score vector. In our previous simple example, we only included the diagonal individual elements of the Fisher Information matrix. By including the off-diagonal elements, we can achieve a more accurate estimate of the coefficients and probably a faster convergence like in glm. Let’s code and see if that’s true!\nFisher Information Iteration\nbeta <- c(0,0)\niter <- 25\nhistory <- matrix(0,nrow = iter, ncol = 2)\ntolerance <- 10^-8\n\nfor (i in 1:iter) {\n\nz <- beta[1] + beta[2]*x\np <- 1 / (1+exp(-z))\nd_b0 <- sum(y-p)\nd_b1 <- sum((y-p)*x)\nscore_vec <- c(d_b0, d_b1)\ni_11 <- sum(p*(1-p))\ni_10 <- sum(x*p*(1-p))\ni_01 <- i_10\ni_22 <- sum(x^2*p*(1-p))\ni_mat <- matrix(c(i_11,i_01,i_10,i_22),nrow = 2, ncol = 2)\ni_mat_inv <- solve(i_mat)\n\nbeta_new <- beta + i_mat_inv %*% score_vec\nhistory[i,] <- beta_new\n\n## se\nse_beta0 <- sqrt(diag(i_mat_inv)[1])\nse_beta1 <- sqrt(diag(i_mat_inv)[2])\n\nif (abs(beta_new[1] - beta[1]) < tolerance && abs(beta_new[2] - beta[2]) < tolerance) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\" (\",se_beta0,\") \",\"\\nbeta1: \",beta_new[2],\" (\",se_beta1,\") \"))\n  break\n}\nbeta <- beta_new\n\nif (i==iter) {\n  cat(paste0(\"converged on iter \", i,\"\\nbeta0: \",beta_new[1],\" (\",se_beta0,\") \",\"\\nbeta1: \",beta_new[2],\" (\",se_beta1,\") \"))\n}\n}\n\n## converged on iter 7\n## beta0: -2.75153531304195 (0.595491334175413) \n## beta1: 2.99269736985884 (0.660135410538508)\nWow, not too shabby! Point estimates and SE are very close to glm. Iterations were definitely shorter than the previous example.\nThere we have it! It’s fascinating to look under the hood how all these works. The Newton-Raphson algorithm is a powerful tool for estimating coefficients in logistic regression, and understanding its connection to Taylor series approximation provides valuable insight into its convergence properties.\nLessons Learnt\nLearnt the surface understanding of what Taylor series approximation does.\nLearnt Newton-Raphson algorithm. Such an elegant formula!\nThe proof of the Newton-Raphson algorithm using Taylor series approximation\nHow including the Fisher Information matrix can improve convergence, previously I’ve always thought the coefficients are independent of each other and only the individual diagonal elements are needed. But it turns out the off-diagonal elements are also important for convergence.\nlearnt the\n\\(\\nabla\\)\nsymbol is called the nabla symbol, which is used to denote the gradient of a function.\nlearnt\nglm.control\nand how glm default maxiter is 25 and epsilon (tolerance threshold) is 10^-8.\nIf you like this article:\nplease feel free to send me a\ncomment or visit my other blogs\nplease feel free to follow me on\nBlueSky\n,\ntwitter\n,\nGitHub\nor\nMastodon\nif you would like collaborate please feel free to\ncontact me\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nr on Everyday Is A School Day\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "We learnt to derive the Newton-Raphson algorithm from Taylor series approximation and implements it for logistic regression in R. We’ll show how the second-order Taylor expansion leads to the Newton-Raphson update formula, then compare individua...",
      "meta_keywords": null,
      "og_description": "We learnt to derive the Newton-Raphson algorithm from Taylor series approximation and implements it for logistic regression in R. We’ll show how the second-order Taylor expansion leads to the Newton-Raphson update formula, then compare individua...",
      "og_image": "https://www.kenkoonwong.com/blog/newton-raphson/math.jpg",
      "og_title": "Taylor Series Approximation To Newton Raphson Algorithm – A note for myself of the proof | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 12.9,
      "sitemap_lastmod": null,
      "twitter_description": "We learnt to derive the Newton-Raphson algorithm from Taylor series approximation and implements it for logistic regression in R. We’ll show how the second-order Taylor expansion leads to the Newton-Raphson update formula, then compare individua...",
      "twitter_title": "Taylor Series Approximation To Newton Raphson Algorithm – A note for myself of the proof | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/05/taylor-series-approximation-to-newton-raphson-algorithm-a-note-for-myself-of-the-proof/",
      "word_count": 2581
    }
  }
}