{
  "id": "8e9f4a777b5e3e38997fc9798d7f4df2f4415792",
  "url": "https://www.r-bloggers.com/2025/01/wrapper-based-ensemble-feature-selection/",
  "created_at_utc": "2025-11-22T19:59:32Z",
  "data": null,
  "raw_original": {
    "uuid": "ee96b814-022a-41d6-974c-168662db53ce",
    "created_at": "2025-11-22 19:59:32",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/01/wrapper-based-ensemble-feature-selection/",
      "crawled_at": "2025-11-22T10:56:15.690224",
      "external_links": [
        {
          "href": "https://mlr-org.com/gallery/technical/2025-01-12-efs/",
          "text": "mlr-org"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://mlr3fselect.mlr-org.com/reference/embedded_ensemble_fselect.html",
          "text": "mlr3fselect::embedded_ensemble_fselect()"
        },
        {
          "href": "https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html",
          "text": "mlr3fselect::ensemble_fselect()"
        },
        {
          "href": "https://mlr3fselect.mlr-org.com/reference/auto_fselector.html",
          "text": "mlr3fselect::auto_fselector"
        },
        {
          "href": "https://mlr-org.com/gallery/optimization/2023-02-07-recursive-feature-elimination/",
          "text": "this gallery post"
        },
        {
          "href": "https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html",
          "text": "mlr3fselect::ensemble_fselect()"
        },
        {
          "href": "https://mlr3fselect.mlr-org.com/reference/ensemble_fs_result.html",
          "text": "EnsembleFSResult"
        },
        {
          "href": "https://bblodfon.github.io/fastVoteR/",
          "text": "fastVoteR"
        },
        {
          "href": "https://doi.org/10.21105/JOSS.03010",
          "text": "https://doi.org/10.21105/JOSS.03010"
        },
        {
          "href": "https://doi.org/10.1007/BF01195985/METRICS",
          "text": "https://doi.org/10.1007/BF01195985/METRICS"
        },
        {
          "href": "https://doi.org/10.1007/978-3-031-09016-5",
          "text": "https://doi.org/10.1007/978-3-031-09016-5"
        },
        {
          "href": "https://doi.org/10.1111/J.1467-9868.2010.00740.X",
          "text": "https://doi.org/10.1111/J.1467-9868.2010.00740.X"
        },
        {
          "href": "http://jmlr.org/papers/v18/17-514.html",
          "text": "http://jmlr.org/papers/v18/17-514.html"
        },
        {
          "href": "https://doi.org/10.1007/978-3-540-87481-2_21",
          "text": "https://doi.org/10.1007/978-3-540-87481-2_21"
        },
        {
          "href": "https://mlr-org.com/gallery/technical/2025-01-12-efs/",
          "text": "mlr-org"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Wrapper-based Ensemble Feature Selection | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAANBAMAAABr8kJMAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid2Zu0SrMu9mIlQQds0LZxKKAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAZUlEQVQIHWNgYBD6LGgSwMDAwJLAwPodSHNMYGD4CKTXL2Dg+g2kZwClHIB0GsNRXyDF0Lx7ohWQ4vrFwBC/gIGBs4GBgf8CAwOzA1ALUCsTkC1xgIHhfAEDZysDA+N7Q+HkAwwAMVwV8OSeZpAAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?B"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAANBAMAAAC5okgUAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid0yzZlEdu9UECKrZrsQi3lRAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAfElEQVQIHWNgYBD6zMDAwPI1AUiydwKJKQFAgoHdnYGBKz8BxNx2lIGB034BiHlBjoHBqgjEYrhwiYFhwlMIM5+BkwGklYFrwv4Fm7l/gZicC/gNJvAqgJi8DBzTFvAcgDBZJBk4LoCYtgw8Dgz7NwBZjEoX2A0Y/YMZGABTJhoTVgOLEwAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?M"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAANBAMAAAAOH7AzAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid2Zu0SrMu9mIlQQds0LZxKKAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABA0lEQVQYGVWOsUoDQRCGv9PEZPcImthYybVWRtDG6l5AuMpKMCBYyj2CIBZis5rGMm+Qq7TQ4ppUIkYsYiWID5BIkICmcGYPIZli/3++2d35gca4vp0wW8FmKu1wR45Si/JkdgZvGYSfyqoOvuaH733ofSvrdrC/c0O734S7qbIb+TkWtUJqiRITxFSeIrXH9PZUuYUXb4yJCEtyF87ur3YL1qytFKY8Iqtm4u0PHHY8bL964dFObdrNpTGnsNz3dCnywgOTkGv1i7EEdp62i5U4Lg8kitSCvFrL1Zn/nY6PjAtFzynmXI2mratWHCc+CsFwa/UoV2ZbECRiGhsMWB+n/AEXYjpTMEvWgQAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?B%20%5Ctimes%20M"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAANBAMAAABr8kJMAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid2Zu0SrMu9mIlQQds0LZxKKAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAZUlEQVQIHWNgYBD6LGgSwMDAwJLAwPodSHNMYGD4CKTXL2Dg+g2kZwClHIB0GsNRXyDF0Lx7ohWQ4vrFwBC/gIGBs4GBgf8CAwOzA1ALUCsTkC1xgIHhfAEDZysDA+N7Q+HkAwwAMVwV8OSeZpAAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?B"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAADkAAAANBAMAAADh3dsNAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid2Zu0SrMu9mIlQQds0LZxKKAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA5klEQVQYGWNgYBD6LGgSwIALsCQwsH5HkVwneWUBQ+xEsBaOCQwMH1FkF/3IYmB/wPAKJLh+AQPXbxADDlYDWSwbGC6DBGYAmQ4gBhyAZPkLGCJAAmkMR33hEmDG2pmiDPULGOoPAHnNuydagQSZ00BAAMg6y8BUEA+UXcDAwPWLgQHIRAXMDkApkChnA9CSC6iSDLwN6wsY9h8AGugAdDbQU0hAm4H3A38Aww6gEBNQn8QBIANh7yugFpYJDNeAgucLGDhbgTQSmMNwYgGrAyg0GN8bCicfQJICMjkvWjEw7N0YwAAAfI88ka+ePrYAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?B%20=%2050"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAADQAAAANBAMAAAAUI1C9AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid0yzZlEdu9UECKrZrsQi3lRAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAuElEQVQYGWNgYBD6zMDAwPI1AUiiA/ZOoMiUADRhQxCf3Z2BgSs/AU1KFcTfdpSBgdN+AaoUL1CQgeGCHAODVRGqDINxKFjqEgPDhKdoUhcgUvkMnAwgpzCwngEBASCLlwEkxTVh/4LN3L9AUghgDJbiXMBvMIFXASEMYl0AS/EycExbwHMARYo3UFBFGGQsiyQDxwWQFMIuBgaQu2wZeBwY9m9A0QXkAKUYlS6wGzD6B6NJGXZIAwCjFSpdluYHJAAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?M%20=%204"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAAMBAMAAAB2C0uMAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMARM0yInYQiatmu93vVJnF6s/xAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAUUlEQVQIHWNgEDJgYFGfuoSBwdUeyNp/woGBgYEfyLoApNFZkdMFIGJsAQybISwg+QrOioGwuC8wxENYTA4QWQEGzgS2IwwM6f3qBgxT6xwYAPv+EXHLiQHFAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?15"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAALQAAAANBAMAAADyJlm8AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid0yzZlEdu9UECKrZrsQi3lRAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACQElEQVQ4EZ1TPYjUQBT+stkkF7PZvV245USUQyu7FF4jyK2VxSEXsVQklT+N6wkWgspaKFiIEcQfEAwIKlhc8OTEKmtxlVvMCQsiHmwjcoVwspyoW5zvTZJNcqWvmHzzve99M3kzA6AxBFDe8mjMorFVP+Nl0/9Exh0qfOAWq6td6LxkLtTJ3ARv994IcW7Ny3MwpjYgSf16kxPGMWBXu6jBhAB+FspQKqz+5vdHqC3MFzR7sCQkeQjlkDIrq4A5FxQ0iEJYf4rU14L1e0pWfVwpaA5Am5HkBgxWi33A4csssWhqeIzwmQoHEqWDOsvisYStaw7Op2n5vQtlU5JD6B1ixCfA/yZTT4FFCdDDi+8xSkdbk7tOJe/Wm5gLsfNvqy0mn4+g/qVC0YYJPkrAdo1JCXDrydopRqd7FB8Yidg6lbxEyWmTSwhorOnFhZEkV8h6RCfoR8GyxWtQ9JPeWZSgwlyoTmw9lkAbkG9RBBznxdrLI+jkYAY1x7dnYpdK8rU71MlLMRePNhLrVAKjEzl4FuRFUASYfD2ESZu1MfEwqHRjRT9ptTYAIpGv6tcbRx0mEslBGJs1D4/zGmCddsTkL5gdti7vlteYRGkjUaIdTwfEZL1G1aX5WDJPDakKXCMq67XuQ0jyB4wucASVFl9jDjr+ugRLDuzbEmVDbJ1KvuBVqA92PJnZsxdcSV7kJ6PsF4ajLJxkC6sLKB4BZeHE1GpAIBfq9E1afyyxF+kGPbrv5RTAve1tV5Lm1Sb+AZhVqak6OmMeAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?M%20%5Ctimes%20B%20=%204%20%5Ctimes%2050%20=%20200"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-015-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAAMBAMAAACdPPCPAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAIs2JZna73e+rVEQQMpmfRCcIAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAc0lEQVQIHWNgVHZNZOAOsWZgYKzvFGC4xMAkwMC4gIGBIYuB9QGE9ZeBu4GBMcR5A+9HBt4/DDwBDLXcQNZHoCKGZ0AKyAGCCMa/DFx/GNgXMMQz/GbgagAZ9IwhhYF1AgPLAZ5Ghm0gAQZXOwEGrihrBgA3MRqgsgcMGAAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?50"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-016-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-017-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABsAAAATBAMAAACTqWsLAAAALVBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAOrOgAAAADnRSTlMARM0yInYQiauZu2ZU3c+kpY0AAAAJcEhZcwAADsQAAA7EAZUrDhsAAACVSURBVBgZY2CAAE4oDaXcYVwhAxCrBMp1tQNzF0C5DHwgLnMACpfTAYXLxcDAeNJxZwIDRLE0A4MN82peASi3jYEhgPkZN1AH2KgGIIMVRIC5PAeADCYQAeZyAGkBvwRWEBeoP4OBgfvC3oQtDAzpfeoGDBsZGFhqHGtADgKBBWASRrBMgLHANCtMFUQU6ERkwALnAACvHxVxnpwNFwAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?1/x"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-018-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-022-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-023-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/john-zobolas/",
          "text": "John Zobolas"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-390409 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Wrapper-based Ensemble Feature Selection</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 11, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/john-zobolas/\">John Zobolas</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mlr-org.com/gallery/technical/2025-01-12-efs/\"> mlr-org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<section class=\"level2\" id=\"intro\">\n<h2 class=\"anchored\" data-anchor-id=\"intro\">Intro</h2>\n<p>In this post we will show how we can use the <code>mlr3fselect</code> R package to perform <em>wrapped</em>-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resampling of the data) to create robust feature subsets by leveraging multiple ML models in wrapper-based feature selection strategies.</p>\n<p>Some papers from which we draw the ideas for this tutorial:</p>\n<ul>\n<li><strong>Stability selection</strong>, i.e. drawing multiple subsamples from a dataset and performing feature selection on each <span class=\"citation\" data-cites=\"Meinshausen2010\">(Meinshausen and Bühlmann 2010)</span>. Stability selection helps ensure that the selected features are robust to variations in the training data, increasing the reliability of the feature selection process.</li>\n<li>The <strong>ensemble idea</strong> for feature selection, i.e. using multiple methods or models to perform feature selection on a dataset <span class=\"citation\" data-cites=\"Saeys2008\">(Saeys, Abeel, and Van De Peer 2008)</span>. This combines the strengths of different approaches to achieve more comprehensive results and alleviates biases that may arise from each individual approach for feature selection.</li>\n</ul>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nNote\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>We also support <em>embedded-based</em> ensemble feature selection, see the function <a href=\"https://mlr3fselect.mlr-org.com/reference/embedded_ensemble_fselect.html\" rel=\"nofollow\" target=\"_blank\"><code>mlr3fselect::embedded_ensemble_fselect()</code></a> for more details.</p>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"libraries\">\n<h2 class=\"anchored\" data-anchor-id=\"libraries\">Libraries</h2>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3verse)\nlibrary(fastVoteR) # for feature ranking\nlibrary(ggplot2)\nlibrary(future)\nlibrary(progressr)</pre>\n</div>\n</section>\n<section class=\"level2\" id=\"dataset\">\n<h2 class=\"anchored\" data-anchor-id=\"dataset\">Dataset</h2>\n<p>We will use the <code>sonar</code> dataset, which is a binary classification task:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>task = tsk(\"sonar\")\ntask</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;TaskClassif:sonar&gt; (208 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26,\n    V27, V28, V29, V3, V30, V31, V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43, V44, V45,\n    V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55, V56, V57, V58, V59, V6, V60, V7, V8, V9</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"efs-workflow\">\n<h2 class=\"anchored\" data-anchor-id=\"efs-workflow\">EFS Workflow</h2>\n<p>The <strong>ensemble feature selection (EFS)</strong> workflow is the following (in parentheses we provide the arguments for the <a href=\"https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html\" rel=\"nofollow\" target=\"_blank\"><code>mlr3fselect::ensemble_fselect()</code></a> function that implements this process):</p>\n<ol type=\"1\">\n<li>Repeatedly split a dataset to <strong>train/test sets</strong> (<code>init_resampling</code>), e.g. by subsampling <img data-lazy-src=\"https://latex.codecogs.com/png.latex?B\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?B\"/></noscript> times.</li>\n<li>Choose <img data-lazy-src=\"https://latex.codecogs.com/png.latex?M\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?M\"/></noscript> <strong>learners</strong> (<code>learners</code>).<br/>\n</li>\n<li>Perform <strong>wrapped-based feature selection</strong> on each train set from (1) using each of the models from (2). This process results in a ‘best’ feature (sub)set and a final trained model using these best features, for each combination of train set and learner (<img data-lazy-src=\"https://latex.codecogs.com/png.latex?B%20%5Ctimes%20M\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?B%20%5Ctimes%20M\"/></noscript> combinations in total).</li>\n<li>Score the final models on the respective test sets.</li>\n</ol>\n<p>To guide the feature selection process (3) we need to choose:</p>\n<ul>\n<li>An optimization algorithm (<code>fselector</code>), e.g. Recursive Feature Elimination (RFE)</li>\n<li>An inner resampling technique (<code>inner_resampling</code>), e.g. 5-fold cross-validation (CV)</li>\n<li>An inner measure (<code>inner_measure</code>), e.g. classification error</li>\n<li>A stopping criterion for the feature selection (<code>terminator</code>), i.e. how many iterations should the optimization algorithm run</li>\n</ul>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nNote\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>The <code>inner_measure</code> (used for finding the best feature subset in each train set) and <code>measure</code> (assesses performance on each test set) can be different.</p>\n</div>\n</div>\n<section class=\"level3\" id=\"parallelization\">\n<h3 class=\"anchored\" data-anchor-id=\"parallelization\">Parallelization</h3>\n<p>Internally, <code>ensemble_fselect()</code> performs a full <code>mlr3::benchmark()</code>, the results of which can be stored with the argument <code>store_benchmark_result</code>. The process is fully parallelizable, where <strong>every job is a (init resampling iteration, learner) combination</strong>. So it’s better to make sure that each RFE optimization (done via <a href=\"https://mlr3fselect.mlr-org.com/reference/auto_fselector.html\" rel=\"nofollow\" target=\"_blank\">mlr3fselect::auto_fselector</a>) is single-threaded.</p>\n<p>Below we show the code that setups the configuration for the parallelization:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre># Parallelization for EFS: use 10 cores\nplan(\"multisession\", workers = 10)</pre>\n</div>\n</section>\n<section class=\"level3\" id=\"rfe\">\n<h3 class=\"anchored\" data-anchor-id=\"rfe\">RFE</h3>\n<p>For each (train set, learner) combination we will run a Recursive Feature Elimination (RFE) optimization algorithm. We configure the algorithm to start with all features of the <code>task</code>, remove the 80% less important features in each iteration, and stop when 2 features are reached. In each RFE iteration, a 5-fold CV resampling of the given <code>task</code> takes place and a <code>learner</code> is trained and used for prediction on the test folds. The outcome of each RFE iteration is the average CV error (performance estimate) and the feature importances (by default the average of the feature ranks from each fold). Practically, for the <code>sonar</code> dataset, we will have <strong>15 RFE iterations</strong>, with the following feature subset sizes:</p>\n<p><code>60 48 38 30 24 19 15 12 10 8  6  5  4  3  2</code></p>\n<p>The best feature set will be chosen as the one with the <strong>lowest 5-fold CV error</strong>. i.e. the best performance estimate in the inner resampling.</p>\n<p>In <code>mlr3</code> code, we specify the RFE <code>fselector</code> as:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>rfe = fs(\"rfe\", n_features = 2, feature_fraction = 0.8)</pre>\n</div>\n<p>See <a href=\"https://mlr-org.com/gallery/optimization/2023-02-07-recursive-feature-elimination/\" rel=\"nofollow\" target=\"_blank\">this gallery post</a> for more details on RFE optimization.</p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nNote\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<ul>\n<li>Using RFE as the feature selection optimization algorithm means that all <code>learners</code> need to have the <code>\"importance\"</code> property.</li>\n</ul>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"learners\">\n<h3 class=\"anchored\" data-anchor-id=\"learners\">Learners</h3>\n<p>We define a <code>list()</code> with the following classification <code>learners</code> (parameters are set at default values):</p>\n<ol type=\"1\">\n<li>XGBoost with early stopping</li>\n<li>A tree</li>\n<li>A random forest (RF)</li>\n<li>A Support Vector Machine (SVM)</li>\n</ol>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>max_nrounds = 500\n\nlearners = list(\n  lrn(\"classif.xgboost\", id = \"xgb\", nrounds = max_nrounds,\n      early_stopping_rounds = 20, validate = \"test\"),\n  lrn(\"classif.rpart\", id = \"tree\"),\n  lrn(\"classif.ranger\", id = \"rf\", importance = \"permutation\"),\n  lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\", kernel = \"linear\")\n)</pre>\n</div>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nNote\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>It is possible to perform tuning while also performing wrapper-based feature selection. This practically means that we would use an <code>AutoTuner</code> learner with its own inner resampling scheme and tuning space in the above list. The whole process would then be a double (nested) cross-validation with outer loop the <img data-lazy-src=\"https://latex.codecogs.com/png.latex?B\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?B\"/></noscript> subsample iterations, which is computationally taxing. Models that need minimum to no tuning (e.g. like Random Forests) are therefore ideal candidates for wrapper-based ensemble feature selection.</p>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"callbacks\">\n<h3 class=\"anchored\" data-anchor-id=\"callbacks\">Callbacks</h3>\n<p>Since SVM doesn’t support <code>importance</code> scores by itself, we convert the coefficients of the trained linear SVM model to importance scores via a callback:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>svm_rfe = clbk(\"mlr3fselect.svm_rfe\")\nsvm_rfe</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;CallbackBatchFSelect:mlr3fselect.svm_rfe&gt;: SVM-RFE Callback\n* Active Stages: on_optimization_begin</pre>\n</div>\n</div>\n<hr/>\n<p>Also, since the XGBoost learner performs <strong>internal tuning via early stopping</strong>, where the test folds in the inner cross-validation resampling scheme act as validation sets, we need to define the following callback:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>internal_ss = ps(\n  nrounds = p_int(upper = max_nrounds, aggr = function(x) as.integer(mean(unlist(x))))\n)\n\nxgb_clbk = clbk(\"mlr3fselect.internal_tuning\", internal_search_space = internal_ss)\nxgb_clbk</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;CallbackBatchFSelect:mlr3fselect.internal_tuning&gt;: Internal Tuning\n* Active Stages: on_auto_fselector_after_final_model, on_auto_fselector_before_final_model,\n  on_eval_before_archive, on_optimization_end</pre>\n</div>\n</div>\n<p>This practically sets the boosting rounds of the final XGBoost model (after the RFE optimization is finished) as the average boosting rounds from each subsequent training fold (corresponding to the model trained with the ‘best’ feature subset). For example, since we’re performing a 5-fold inner CV, we would have 5 different early-stopped boosting <code>nrounds</code>, from which we will use the average value to train the final XGBoost model using the whole train set.</p>\n<hr/>\n<p>For all learners we will prefer <strong>sparser models during the RFE optimization process</strong>. This means that across all RFE iterations, we will choose as ‘best’ feature subset the one that has the minimum number of features and its performance is <strong>within one standard error</strong> of the feature set with the best performance (e.g. the lowest classification error). This can be achieved with the following callback:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>one_se_clbk = clbk(\"mlr3fselect.one_se_rule\")\none_se_clbk</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;CallbackBatchFSelect:mlr3fselect.one_se_rule&gt;: One Standard Error Rule Callback\n* Active Stages: on_optimization_end</pre>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"execute-efs\">\n<h2 class=\"anchored\" data-anchor-id=\"execute-efs\">Execute EFS</h2>\n<p>Using the <a href=\"https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html\" rel=\"nofollow\" target=\"_blank\"><code>mlr3fselect::ensemble_fselect()</code></a> function, we split the <code>sonar</code> task to <img data-lazy-src=\"https://latex.codecogs.com/png.latex?B%20=%2050\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?B%20=%2050\"/></noscript> subsamples (each corresponding to a 80%/20% train/test set split) and perform RFE in each train set using each of the <img data-lazy-src=\"https://latex.codecogs.com/png.latex?M%20=%204\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?M%20=%204\"/></noscript> learners.</p>\n<p>For a particular (train set, learner) combination, the RFE process will evaluate the <img data-lazy-src=\"https://latex.codecogs.com/png.latex?15\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?15\"/></noscript> feature subsets mentioned above. Using the inner 5-fold CV resampling scheme, the average CV classification error will be used to find the best feature subset. Using only features from this best feature set, a final model will be trained using all the observations from each trained set. Lastly, the performance of this final model will be assessed on the corresponding test set using the classification accuracy metric.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>set.seed(42)\nefs = ensemble_fselect(\n  fselector = rfe,\n  task = task,\n  learners = learners,\n  init_resampling = rsmp(\"subsampling\", repeats = 50, ratio = 0.8),\n  inner_resampling = rsmp(\"cv\", folds = 5),\n  inner_measure = msr(\"classif.ce\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"none\"),\n  # following list must be named with the learners' ids\n  callbacks = list(\n    xgb  = list(one_se_clbk, xgb_clbk),\n    tree = list(one_se_clbk),\n    rf   = list(one_se_clbk),\n    svm  = list(one_se_clbk, svm_rfe)\n  ),\n  store_benchmark_result = FALSE\n)</pre>\n</div>\n<p>The result is stored in an <a href=\"https://mlr3fselect.mlr-org.com/reference/ensemble_fs_result.html\" rel=\"nofollow\" target=\"_blank\"><code>EnsembleFSResult</code></a> object, which can use to visualize the results, rank the features and assess the stability of the ensemble feature selection process, among others.</p>\n</section>\n<section class=\"level1\" id=\"analyze-efs-results\">\n<h1>Analyze EFS Results</h1>\n<section class=\"level2\" id=\"result-object\">\n<h2 class=\"anchored\" data-anchor-id=\"result-object\">Result Object</h2>\n<p>Printing the result object provides some initial information:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>print(efs)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;EnsembleFSResult&gt; with 4 learners and 50 initial resamplings\n     resampling_iteration    learner_id n_features\n                    &lt;int&gt;        &lt;char&gt;      &lt;int&gt;\n  1:                    1 xgb.fselector          8\n  2:                    2 xgb.fselector         30\n  3:                    3 xgb.fselector         60\n  4:                    4 xgb.fselector         19\n  5:                    5 xgb.fselector          6\n ---                                              \n196:                   46 svm.fselector         24\n197:                   47 svm.fselector         19\n198:                   48 svm.fselector         15\n199:                   49 svm.fselector         19\n200:                   50 svm.fselector          8</pre>\n</div>\n</div>\n<p>As we can see, we have <img data-lazy-src=\"https://latex.codecogs.com/png.latex?M%20%5Ctimes%20B%20=%204%20%5Ctimes%2050%20=%20200\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?M%20%5Ctimes%20B%20=%204%20%5Ctimes%2050%20=%20200\"/></noscript> (init resampling, learner) combinations. We can inspect the actual <code>data.table</code> result:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$result</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>        learner_id resampling_iteration classif.acc                    features n_features classif.ce_inner\n            &lt;char&gt;                &lt;int&gt;       &lt;num&gt;                      &lt;list&gt;      &lt;int&gt;            &lt;num&gt;\n  1: xgb.fselector                    1   0.6904762  V11,V12,V16,V21,V36,V4,...          8        0.1081996\n  2: xgb.fselector                    2   0.9285714  V1,V10,V11,V12,V13,V15,...         30        0.1627451\n  3: xgb.fselector                    3   0.8333333  V1,V10,V11,V12,V13,V14,...         60        0.1203209\n  4: xgb.fselector                    4   0.8571429 V11,V12,V15,V16,V21,V27,...         19        0.1447415\n  5: xgb.fselector                    5   0.7142857     V10,V11,V16,V31,V36,V45          6        0.1319073\n ---                                                                                                       \n196: svm.fselector                   46   0.7619048  V1,V11,V12,V14,V23,V25,...         24        0.1440285\n197: svm.fselector                   47   0.7380952 V11,V15,V23,V30,V31,V33,...         19        0.1331551\n198: svm.fselector                   48   0.7619048 V12,V17,V31,V32,V36,V37,...         15        0.1809269\n199: svm.fselector                   49   0.7142857 V11,V12,V14,V17,V20,V24,...         19        0.1864528\n200: svm.fselector                   50   0.7619048 V11,V14,V23,V36,V39,V40,...          8        0.1629234\n                            importance\n                                &lt;list&gt;\n  1:       6.8,6.8,6.4,3.8,3.6,3.4,...\n  2: 27.2,26.8,25.6,22.0,22.0,21.6,...\n  3: 60.0,57.6,53.8,52.0,51.4,49.6,...\n  4: 19.0,13.4,13.0,12.8,12.2,11.0,...\n  5:           4.4,4.2,4.2,3.8,2.2,2.2\n ---                                  \n196: 24.0,21.6,20.2,18.4,17.6,16.8,...\n197: 16.6,16.2,16.2,15.0,14.8,14.4,...\n198: 14.0,13.6,13.0,10.4, 9.8, 9.2,...\n199: 17.6,16.0,15.6,14.4,13.4,12.8,...\n200:       8.0,5.8,5.6,5.2,4.6,3.2,...</pre>\n</div>\n</div>\n<p>For each learner (<code>\"learner_id\"</code>) and dataset subsample (<code>\"resampling_iteration\"</code>) we get:</p>\n<ul>\n<li>The ‘best’ feature subsets (<code>\"features\"</code>)</li>\n<li>The number of ‘best’ features (<code>\"nfeatures\"</code>)</li>\n<li>The importances for these ‘best’ features (<code>\"importance\"</code>) – this output column we get only because RFE optimization was used</li>\n<li>The inner optimization performance scores on the train sets (<code>\"classif.ce_inner\"</code>)</li>\n<li>The performance scores on the test sets (<code>\"classif.acc\"</code>)</li>\n</ul>\n<p>Since there are two ways in this process to evaluate performance, we can always check which is the <strong>active measure</strong>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$active_measure</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] \"outer\"</pre>\n</div>\n</div>\n<p>By default the active measure is the <code>\"outer\"</code>, i.e. the measure used to evaluate each learner’s performance in the test sets. In our case that was the classification accuracy:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$measure</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;MeasureClassifSimple:classif.acc&gt;: Classification Accuracy\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response</pre>\n</div>\n</div>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nNote\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>In the following sections we can use the inner optimization scores (i.e. <code>\"classif.ce_inner\"</code>) by executing <code>efs$set_active_measure(\"inner\")</code>. This affects all methods and plots that use performance scores.</p>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"performance\">\n<h2 class=\"anchored\" data-anchor-id=\"performance\">Performance</h2>\n<p>We can view the <strong>performance scores of the different learners</strong> used in the ensemble feature selection process. Each box represents the distribution of scores across different resampling iterations for a particular learner.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>autoplot(efs, type = \"performance\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")</pre>\n<div class=\"cell-output-display\">\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-015-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-015-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p>We observe that RF has better classification accuracy on the test sets of the <img data-lazy-src=\"https://latex.codecogs.com/png.latex?50\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?50\"/></noscript> subsamples, followed by XGBoost, then the SVM and last the tree model.</p>\n</section>\n<section class=\"level2\" id=\"number-of-selected-features\">\n<h2 class=\"anchored\" data-anchor-id=\"number-of-selected-features\">Number of Selected Features</h2>\n<p>Continuing, we can plot <strong>the number of features selected by each learner</strong> in the different resampling iterations:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>autoplot(efs, type = \"n_features\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(breaks = seq(0, 60, 10))</pre>\n<div class=\"cell-output-display\">\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i0.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-016-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-016-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p>We observe that RF needed more features to achieve the best average performance, followed by SVM, then XGBoost and the tree model was the model using the least features (but with worst performance).</p>\n</section>\n<section class=\"level2\" id=\"pareto-plot\">\n<h2 class=\"anchored\" data-anchor-id=\"pareto-plot\">Pareto Plot</h2>\n<p>Both performance scores and number of features selected by the RFE optimization process can be visualized jointly in the Pareto plot. Here we also draw the <strong>Pareto front</strong>, i.e. the set of points that represent the trade-off between the number of features and performance (classification accuracy). As we see below, these points are derived from multiple learners and resamplings:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>autoplot(efs, type = \"pareto\", theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Empirical Pareto front\")</pre>\n<div class=\"cell-output-display\">\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-017-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-017-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p>We can also draw an <strong>estimated Pareto front curve</strong> by fitting a linear model with the inverse of the number of selected features (<img data-lazy-src=\"https://latex.codecogs.com/png.latex?1/x\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?1/x\"/></noscript>) of the empirical Pareto front as input, and the associated performance scores as output:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>autoplot(efs, type = \"pareto\", pareto_front = \"estimated\", \n         theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Estimated Pareto front\")</pre>\n<div class=\"cell-output-display\">\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-018-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-018-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"knee-point-identification\">\n<h2 class=\"anchored\" data-anchor-id=\"knee-point-identification\">Knee Point Identification</h2>\n<p>No matter the type of Pareto front that we chose, specialized methods are available to identify <strong>knee points</strong>, i.e. points of the Pareto front with an <strong>optimal trade-off between performance and number of selected features</strong>.</p>\n<p>By default, we use the geometry-based <em>Normal-Boundary Intersection</em> (NBI) method. This approach calculates the perpendicular distance of each point from the line connecting the first (worst performance, minimum number of features) and last (best performance, maximum number of features) point of the Pareto front. The knee point is then identified as the point with the maximum distance from this line <span class=\"citation\" data-cites=\"Das1999\">(Das 1999)</span>.</p>\n<p>Using the empirical and estimated Pareto fronts, we observe that the optimal knee points correspond to different numbers of features:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$knee_points()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   n_features classif.acc\n        &lt;num&gt;       &lt;num&gt;\n1:         10   0.9047619</pre>\n</div>\n<pre>efs$knee_points(type = \"estimated\")</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   n_features classif.acc\n        &lt;int&gt;       &lt;num&gt;\n1:          8   0.8597253</pre>\n</div>\n</div>\n<div class=\"callout callout-style-default callout-tip callout-titled\" title=\"Number of features cutoff\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nNumber of features cutoff\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>The number of features at the identified knee point provides a cutoff for prioritizing features when working with a ranked feature list (see “Feature Ranking” section).</p>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"stability\">\n<h2 class=\"anchored\" data-anchor-id=\"stability\">Stability</h2>\n<p>The <code>stabm</code> R package <span class=\"citation\" data-cites=\"Bommert2021\">(Bommert and Lang 2021)</span> implements many measures for the assessment of the <strong>stability of feature selection</strong>, i.e. the similarity between the selected feature sets (<code>\"features\"</code> column in the <code>EnsembleFSResult</code> object). We can use these measures to assess and visualize the stability across all resampling iterations and learners (<code>global = \"TRUE\"</code>) or per each learner separately (<code>global = \"FALSE\"</code>).</p>\n<p>The default stability measure is the <strong>Jaccard Index</strong>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$stability(stability_measure = \"jaccard\", global = TRUE)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 0.2640504</pre>\n</div>\n</div>\n<p>Stability per learner:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$stability(stability_measure = \"jaccard\", global = FALSE)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre> xgb.fselector tree.fselector   rf.fselector  svm.fselector \n     0.3657964      0.3554681      0.4716744      0.3119381 </pre>\n</div>\n</div>\n<p>We observe that the RF model was the most stable in identifying similar predictive features across the different subsamples of the dataset, while the SVM model the least stable.</p>\n<p>To visualize stability, the following code generates a stability barplot:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>autoplot(efs, type = \"stability\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")</pre>\n<div class=\"cell-output-display\">\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-022-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-022-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p>Alternatively, the <strong>Nogueira</strong> stability measure can be used, which unlike the Jaccard Index, it’s a chance-corrected similarity measure <span class=\"citation\" data-cites=\"Nogueira2018\">(Nogueira, Sechidis, and Brown 2018)</span>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>autoplot(efs, type = \"stability\", stability_measure = \"nogueira\", \n         stability_args = list(p = task$n_features), \n         theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")</pre>\n<div class=\"cell-output-display\">\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-023-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mlr-org.com/gallery/technical/2025-01-12-efs/index_files/figure-html/efs-023-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"feature-ranking\">\n<h2 class=\"anchored\" data-anchor-id=\"feature-ranking\">Feature Ranking</h2>\n<p>Using the Pareto method, we demonstrated how we can identify a reasonable cutoff for the number of selected features. Now we will focus on how to create a consensus ranked feature list based on the results of the ensemble feature selection.</p>\n<p>The most straightforward ranking is obtained by counting how often each feature appears in the ‘best’ feature subsets (<code>\"features\"</code>). Below we show the top 8 features, i.e. up to the cutoff derived from the knee point of the estimated Pareto front. The column <code>\"score\"</code> represents these counts, while the column <code>\"norm_score\"</code> is the <strong>feature selection frequency</strong> or also known as <strong>selection probability</strong> <span class=\"citation\" data-cites=\"Meinshausen2010\">(Meinshausen and Bühlmann 2010)</span>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$feature_ranking(method = \"av\", use_weights = FALSE, committee_size = 8)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   feature score norm_score borda_score\n    &lt;char&gt; &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     V12   179      0.895   1.0000000\n2:     V11   170      0.850   0.9830508\n3:      V9   123      0.615   0.9661017\n4:     V45   121      0.605   0.9491525\n5:     V16   118      0.590   0.9322034\n6:     V36   113      0.565   0.9152542\n7:     V49   104      0.520   0.8983051\n8:      V4    99      0.495   0.8813559</pre>\n</div>\n</div>\n<p>In the language of Voting Theory, we call the method that generates these counts <em>approval voting</em> (<code>method = \"av\"</code>) <span class=\"citation\" data-cites=\"Lackner2023\">(Lackner and Skowron 2023)</span>. Using this framework, learners act as <em>voters</em>, features act as <em>candidates</em> and voters select certain candidates (features). The primary objective is to compile these selections into a consensus ranked list of features (a committee). The <code>committee_size</code> specifies how many (top-ranked) features to return.</p>\n<p>Internally, <code>$feature_ranking()</code> uses the <a href=\"https://bblodfon.github.io/fastVoteR/\" rel=\"nofollow\" target=\"_blank\"><code>fastVoteR</code></a> R package, which supports more advanced ranking methods. For example, we can perform <strong>weighted ranking</strong>, by considering the varying performance (accuracy) of each learner. This results in the same top 8 features but with slightly different ordering:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$feature_ranking(method = \"av\", use_weights = TRUE, committee_size = 8)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   feature     score norm_score borda_score\n    &lt;char&gt;     &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     V12 134.78571  0.8995710   1.0000000\n2:     V11 127.33333  0.8498331   0.9830508\n3:     V45  94.45238  0.6303830   0.9661017\n4:      V9  93.71429  0.6254569   0.9491525\n5:     V16  89.76190  0.5990783   0.9322034\n6:     V36  87.97619  0.5871603   0.9152542\n7:     V49  80.64286  0.5382171   0.8983051\n8:      V4  76.64286  0.5115207   0.8813559</pre>\n</div>\n</div>\n<p>Additionally, alternative ranking methods are supported. Below, we use <em>satisfaction approval voting</em> (SAV), which ranks features by normalizing approval scores based on the number of features a model has selected. Specifically, models that select more features distribute their “approval” across a larger set, reducing the contribution to each selected feature. Conversely, <strong>features chosen by models with fewer selected features receive higher weights</strong>, as their selection reflects stronger individual importance. This approach ensures that sparsely selected features are prioritized in the ranking, leading to a different set of top-ranked features compared to standard approval voting. For instance, in the example above, the <code>\"V10\"</code> feature enters the top 8 features, replacing <code>\"V4\"</code>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>efs$feature_ranking(method = \"sav\", committee_size = 8)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   feature     score norm_score borda_score\n    &lt;char&gt;     &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     V11 15.353545  0.9100050   1.0000000\n2:     V12 14.107691  0.8361632   0.9830508\n3:     V16  7.698460  0.4562879   0.9661017\n4:     V45  6.811607  0.4037241   0.9491525\n5:      V9  6.443311  0.3818952   0.9322034\n6:     V36  6.060615  0.3592128   0.9152542\n7:     V10  5.955446  0.3529794   0.8983051\n8:     V49  4.741014  0.2810000   0.8813559</pre>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"efs-based-feature-selection\">\n<h1>EFS-based Feature Selection</h1>\n<p>The ultimate goal of the ensemble feature selection process is to identify predictive and stable features. By combining the ranked feature list with the Pareto-derived cutoff, we can select the final set of features for further modeling:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>n_features = efs$knee_points(type = \"estimated\")$n_features\nres = efs$feature_ranking(method = \"sav\", committee_size = n_features)\nres$feature</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] \"V11\" \"V12\" \"V16\" \"V45\" \"V9\"  \"V36\" \"V10\" \"V49\"</pre>\n</div>\n</div>\n</section>\n<section class=\"level1\" id=\"session-information\">\n<h1>Session Information</h1>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>sessioninfo::session_info(info = \"packages\")</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package          * version     date (UTC) lib source\n P backports          1.5.0       2024-05-23 [?] CRAN (R 4.4.2)\n P bbotk              1.5.0       2024-12-17 [?] CRAN (R 4.4.2)\n P checkmate          2.3.2       2024-07-29 [?] CRAN (R 4.4.2)\n P class              7.3-22      2023-05-03 [?] CRAN (R 4.4.2)\n P cli                3.6.3       2024-06-21 [?] CRAN (R 4.4.2)\n P clue               0.3-66      2024-11-13 [?] CRAN (R 4.4.2)\n P cluster            2.1.6       2023-12-01 [?] CRAN (R 4.4.2)\n P codetools          0.2-20      2024-03-31 [?] CRAN (R 4.4.2)\n P colorspace         2.1-1       2024-07-26 [?] CRAN (R 4.4.2)\n P crayon             1.5.3       2024-06-20 [?] CRAN (R 4.4.2)\n P data.table       * 1.16.4      2024-12-06 [?] CRAN (R 4.4.2)\n P DEoptimR           1.1-3-1     2024-11-23 [?] CRAN (R 4.4.2)\n P digest             0.6.37      2024-08-19 [?] CRAN (R 4.4.2)\n P diptest            0.77-1      2024-04-10 [?] CRAN (R 4.4.2)\n P evaluate           1.0.1       2024-10-10 [?] CRAN (R 4.4.2)\n P farver             2.1.2       2024-05-13 [?] CRAN (R 4.4.2)\n P fastmap            1.2.0       2024-05-15 [?] CRAN (R 4.4.2)\n P fastVoteR        * 0.0.1       2024-11-27 [?] RSPM\n P flexmix            2.3-19      2023-03-16 [?] CRAN (R 4.4.2)\n P fpc                2.2-13      2024-09-24 [?] CRAN (R 4.4.2)\n P future           * 1.34.0      2024-07-29 [?] CRAN (R 4.4.2)\n P ggplot2          * 3.5.1       2024-04-23 [?] CRAN (R 4.4.2)\n P globals            0.16.3      2024-03-08 [?] CRAN (R 4.4.2)\n P glue               1.8.0       2024-09-30 [?] CRAN (R 4.4.2)\n P gtable             0.3.6       2024-10-25 [?] CRAN (R 4.4.2)\n P htmltools          0.5.8.1     2024-04-04 [?] CRAN (R 4.4.2)\n P jsonlite           1.8.9       2024-09-20 [?] CRAN (R 4.4.2)\n P kernlab            0.9-33      2024-08-13 [?] CRAN (R 4.4.2)\n P knitr              1.49        2024-11-08 [?] CRAN (R 4.4.2)\n P labeling           0.4.3       2023-08-29 [?] CRAN (R 4.4.2)\n P lattice            0.22-6      2024-03-20 [?] CRAN (R 4.4.2)\n P lgr                0.4.4       2022-09-05 [?] CRAN (R 4.4.2)\n P lifecycle          1.0.4       2023-11-07 [?] CRAN (R 4.4.2)\n P listenv            0.9.1       2024-01-29 [?] CRAN (R 4.4.2)\n P magrittr           2.0.3       2022-03-30 [?] CRAN (R 4.4.2)\n P MASS               7.3-61      2024-06-13 [?] CRAN (R 4.4.2)\n P Matrix             1.7-1       2024-10-18 [?] CRAN (R 4.4.2)\n P mclust             6.1.1       2024-04-29 [?] CRAN (R 4.4.2)\n P mlr3             * 0.22.1      2024-11-27 [?] CRAN (R 4.4.2)\n P mlr3cluster        0.1.10      2024-10-03 [?] CRAN (R 4.4.2)\n P mlr3data           0.9.0       2024-11-08 [?] CRAN (R 4.4.2)\n P mlr3filters        0.8.1       2024-11-08 [?] CRAN (R 4.4.2)\n P mlr3fselect        1.2.1.9000  2024-12-16 [?] Github (mlr-org/mlr3fselect@ab6360a)\n P mlr3hyperband      0.6.0       2024-06-29 [?] CRAN (R 4.4.2)\n P mlr3learners       0.9.0       2024-11-23 [?] CRAN (R 4.4.2)\n P mlr3mbo            0.2.8       2024-11-21 [?] CRAN (R 4.4.2)\n P mlr3measures       1.0.0       2024-09-11 [?] CRAN (R 4.4.2)\n P mlr3misc           0.16.0      2024-11-28 [?] CRAN (R 4.4.2)\n P mlr3pipelines      0.7.1       2024-11-14 [?] CRAN (R 4.4.2)\n P mlr3tuning         1.3.0       2024-12-17 [?] CRAN (R 4.4.2)\n P mlr3tuningspaces   0.5.2       2024-11-22 [?] CRAN (R 4.4.2)\n P mlr3verse        * 0.3.0       2024-06-30 [?] CRAN (R 4.4.2)\n P mlr3viz            0.10.0.9000 2024-12-30 [?] Github (mlr-org/mlr3viz@b96b886)\n P modeltools         0.2-23      2020-03-05 [?] CRAN (R 4.4.2)\n P munsell            0.5.1       2024-04-01 [?] CRAN (R 4.4.2)\n P nnet               7.3-19      2023-05-03 [?] CRAN (R 4.4.2)\n P palmerpenguins     0.1.1       2022-08-15 [?] CRAN (R 4.4.2)\n P paradox            1.0.1       2024-07-09 [?] CRAN (R 4.4.2)\n P parallelly         1.41.0      2024-12-18 [?] CRAN (R 4.4.2)\n P pillar             1.10.0      2024-12-17 [?] CRAN (R 4.4.2)\n P pkgconfig          2.0.3       2019-09-22 [?] CRAN (R 4.4.2)\n P prabclus           2.3-4       2024-09-24 [?] CRAN (R 4.4.2)\n P progressr        * 0.15.1      2024-11-22 [?] CRAN (R 4.4.2)\n P R6                 2.5.1       2021-08-19 [?] CRAN (R 4.4.2)\n P RColorBrewer       1.1-3       2022-04-03 [?] CRAN (R 4.4.2)\n P Rcpp               1.0.13-1    2024-11-02 [?] CRAN (R 4.4.2)\n   renv               1.0.11      2024-10-12 [1] CRAN (R 4.4.2)\n P rlang              1.1.4       2024-06-04 [?] CRAN (R 4.4.2)\n P rmarkdown          2.29        2024-11-04 [?] CRAN (R 4.4.2)\n P robustbase         0.99-4-1    2024-09-27 [?] CRAN (R 4.4.2)\n P scales             1.3.0       2023-11-28 [?] CRAN (R 4.4.2)\n P sessioninfo        1.2.2       2021-12-06 [?] CRAN (R 4.4.2)\n P spacefillr         0.3.3       2024-05-22 [?] CRAN (R 4.4.2)\n P stabm              1.2.2       2023-04-04 [?] CRAN (R 4.4.2)\n P tibble             3.2.1       2023-03-20 [?] CRAN (R 4.4.2)\n P uuid               1.2-1       2024-07-29 [?] CRAN (R 4.4.2)\n P vctrs              0.6.5       2023-12-01 [?] CRAN (R 4.4.2)\n P withr              3.0.2       2024-10-28 [?] CRAN (R 4.4.2)\n P xfun               0.49        2024-10-31 [?] CRAN (R 4.4.2)\n P yaml               2.3.10      2024-07-26 [?] RSPM\n\n [1] /home/john/repos/mlr3-packages/mlr3website/mlr-org/renv/library/linux-ubuntu-focal/R-4.4/x86_64-pc-linux-gnu\n [2] /home/john/.cache/R/renv/sandbox/linux-ubuntu-focal/R-4.4/x86_64-pc-linux-gnu/db5e602d\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────</pre>\n</div>\n</div>\n<section class=\"level2\" id=\"references\">\n</section>\n</section>\n<div class=\"default\" id=\"quarto-appendix\"><section class=\"quarto-appendix-contents\" id=\"quarto-bibliography\"><h2 class=\"anchored quarto-appendix-heading\">References</h2><div class=\"references csl-bib-body hanging-indent\" data-entry-spacing=\"0\" id=\"refs\">\n<div class=\"csl-entry\" id=\"ref-Bommert2021\">\nBommert, Andrea, and Michel Lang. 2021. <span>“<span class=\"nocase\">stabm: Stability Measures for Feature Selection</span>.”</span> <em>Journal of Open Source Software</em> 6 (59): 3010. <a href=\"https://doi.org/10.21105/JOSS.03010\" rel=\"nofollow\" target=\"_blank\">https://doi.org/10.21105/JOSS.03010</a>.\n</div>\n<div class=\"csl-entry\" id=\"ref-Das1999\">\nDas, I. 1999. <span>“<span class=\"nocase\">On characterizing the “knee” of the Pareto curve based on normal-boundary intersection</span>.”</span> <em>Structural Optimization</em> 18 (2-3): 107–15. <a href=\"https://doi.org/10.1007/BF01195985/METRICS\" rel=\"nofollow\" target=\"_blank\">https://doi.org/10.1007/BF01195985/METRICS</a>.\n</div>\n<div class=\"csl-entry\" id=\"ref-Lackner2023\">\nLackner, Martin, and Piotr Skowron. 2023. <em><span class=\"nocase\">Multi-Winner Voting with Approval Preferences</span></em>. Springer Nature. <a href=\"https://doi.org/10.1007/978-3-031-09016-5\" rel=\"nofollow\" target=\"_blank\">https://doi.org/10.1007/978-3-031-09016-5</a>.\n</div>\n<div class=\"csl-entry\" id=\"ref-Meinshausen2010\">\nMeinshausen, Nicolai, and Peter Bühlmann. 2010. <span>“<span>Stability Selection</span>.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 72 (4): 417–73. <a href=\"https://doi.org/10.1111/J.1467-9868.2010.00740.X\" rel=\"nofollow\" target=\"_blank\">https://doi.org/10.1111/J.1467-9868.2010.00740.X</a>.\n</div>\n<div class=\"csl-entry\" id=\"ref-Nogueira2018\">\nNogueira, Sarah, Konstantinos Sechidis, and Gavin Brown. 2018. <span>“<span class=\"nocase\">On the Stability of Feature Selection Algorithms</span>.”</span> <em>Journal of Machine Learning Research</em> 18 (174): 1–54. <a href=\"http://jmlr.org/papers/v18/17-514.html\" rel=\"nofollow\" target=\"_blank\">http://jmlr.org/papers/v18/17-514.html</a>.\n</div>\n<div class=\"csl-entry\" id=\"ref-Saeys2008\">\nSaeys, Yvan, Thomas Abeel, and Yves Van De Peer. 2008. <span>“<span class=\"nocase\">Robust feature selection using ensemble feature selection techniques</span>.”</span> <em>Machine Learning and Knowledge Discovery in Databases</em> 5212 LNAI: 313–25. <a href=\"https://doi.org/10.1007/978-3-540-87481-2_21\" rel=\"nofollow\" target=\"_blank\">https://doi.org/10.1007/978-3-540-87481-2_21</a>.\n</div>\n</div></section></div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mlr-org.com/gallery/technical/2025-01-12-efs/\"> mlr-org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Wrapper-based Ensemble Feature Selection\nPosted on\nJanuary 11, 2025\nby\nJohn Zobolas\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nmlr-org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIntro\nIn this post we will show how we can use the\nmlr3fselect\nR package to perform\nwrapped\n-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resampling of the data) to create robust feature subsets by leveraging multiple ML models in wrapper-based feature selection strategies.\nSome papers from which we draw the ideas for this tutorial:\nStability selection\n, i.e. drawing multiple subsamples from a dataset and performing feature selection on each\n(Meinshausen and Bühlmann 2010)\n. Stability selection helps ensure that the selected features are robust to variations in the training data, increasing the reliability of the feature selection process.\nThe\nensemble idea\nfor feature selection, i.e. using multiple methods or models to perform feature selection on a dataset\n(Saeys, Abeel, and Van De Peer 2008)\n. This combines the strengths of different approaches to achieve more comprehensive results and alleviates biases that may arise from each individual approach for feature selection.\nNote\nWe also support\nembedded-based\nensemble feature selection, see the function\nmlr3fselect::embedded_ensemble_fselect()\nfor more details.\nLibraries\nlibrary(mlr3verse)\nlibrary(fastVoteR) # for feature ranking\nlibrary(ggplot2)\nlibrary(future)\nlibrary(progressr)\nDataset\nWe will use the\nsonar\ndataset, which is a binary classification task:\ntask = tsk(\"sonar\")\ntask\n<TaskClassif:sonar> (208 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26,\n    V27, V28, V29, V3, V30, V31, V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43, V44, V45,\n    V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55, V56, V57, V58, V59, V6, V60, V7, V8, V9\nEFS Workflow\nThe\nensemble feature selection (EFS)\nworkflow is the following (in parentheses we provide the arguments for the\nmlr3fselect::ensemble_fselect()\nfunction that implements this process):\nRepeatedly split a dataset to\ntrain/test sets\n(\ninit_resampling\n), e.g. by subsampling\ntimes.\nChoose\nlearners\n(\nlearners\n).\nPerform\nwrapped-based feature selection\non each train set from (1) using each of the models from (2). This process results in a ‘best’ feature (sub)set and a final trained model using these best features, for each combination of train set and learner (\ncombinations in total).\nScore the final models on the respective test sets.\nTo guide the feature selection process (3) we need to choose:\nAn optimization algorithm (\nfselector\n), e.g. Recursive Feature Elimination (RFE)\nAn inner resampling technique (\ninner_resampling\n), e.g. 5-fold cross-validation (CV)\nAn inner measure (\ninner_measure\n), e.g. classification error\nA stopping criterion for the feature selection (\nterminator\n), i.e. how many iterations should the optimization algorithm run\nNote\nThe\ninner_measure\n(used for finding the best feature subset in each train set) and\nmeasure\n(assesses performance on each test set) can be different.\nParallelization\nInternally,\nensemble_fselect()\nperforms a full\nmlr3::benchmark()\n, the results of which can be stored with the argument\nstore_benchmark_result\n. The process is fully parallelizable, where\nevery job is a (init resampling iteration, learner) combination\n. So it’s better to make sure that each RFE optimization (done via\nmlr3fselect::auto_fselector\n) is single-threaded.\nBelow we show the code that setups the configuration for the parallelization:\n# Parallelization for EFS: use 10 cores\nplan(\"multisession\", workers = 10)\nRFE\nFor each (train set, learner) combination we will run a Recursive Feature Elimination (RFE) optimization algorithm. We configure the algorithm to start with all features of the\ntask\n, remove the 80% less important features in each iteration, and stop when 2 features are reached. In each RFE iteration, a 5-fold CV resampling of the given\ntask\ntakes place and a\nlearner\nis trained and used for prediction on the test folds. The outcome of each RFE iteration is the average CV error (performance estimate) and the feature importances (by default the average of the feature ranks from each fold). Practically, for the\nsonar\ndataset, we will have\n15 RFE iterations\n, with the following feature subset sizes:\n60 48 38 30 24 19 15 12 10 8  6  5  4  3  2\nThe best feature set will be chosen as the one with the\nlowest 5-fold CV error\n. i.e. the best performance estimate in the inner resampling.\nIn\nmlr3\ncode, we specify the RFE\nfselector\nas:\nrfe = fs(\"rfe\", n_features = 2, feature_fraction = 0.8)\nSee\nthis gallery post\nfor more details on RFE optimization.\nNote\nUsing RFE as the feature selection optimization algorithm means that all\nlearners\nneed to have the\n\"importance\"\nproperty.\nLearners\nWe define a\nlist()\nwith the following classification\nlearners\n(parameters are set at default values):\nXGBoost with early stopping\nA tree\nA random forest (RF)\nA Support Vector Machine (SVM)\nmax_nrounds = 500\n\nlearners = list(\n  lrn(\"classif.xgboost\", id = \"xgb\", nrounds = max_nrounds,\n      early_stopping_rounds = 20, validate = \"test\"),\n  lrn(\"classif.rpart\", id = \"tree\"),\n  lrn(\"classif.ranger\", id = \"rf\", importance = \"permutation\"),\n  lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\", kernel = \"linear\")\n)\nNote\nIt is possible to perform tuning while also performing wrapper-based feature selection. This practically means that we would use an\nAutoTuner\nlearner with its own inner resampling scheme and tuning space in the above list. The whole process would then be a double (nested) cross-validation with outer loop the\nsubsample iterations, which is computationally taxing. Models that need minimum to no tuning (e.g. like Random Forests) are therefore ideal candidates for wrapper-based ensemble feature selection.\nCallbacks\nSince SVM doesn’t support\nimportance\nscores by itself, we convert the coefficients of the trained linear SVM model to importance scores via a callback:\nsvm_rfe = clbk(\"mlr3fselect.svm_rfe\")\nsvm_rfe\n<CallbackBatchFSelect:mlr3fselect.svm_rfe>: SVM-RFE Callback\n* Active Stages: on_optimization_begin\nAlso, since the XGBoost learner performs\ninternal tuning via early stopping\n, where the test folds in the inner cross-validation resampling scheme act as validation sets, we need to define the following callback:\ninternal_ss = ps(\n  nrounds = p_int(upper = max_nrounds, aggr = function(x) as.integer(mean(unlist(x))))\n)\n\nxgb_clbk = clbk(\"mlr3fselect.internal_tuning\", internal_search_space = internal_ss)\nxgb_clbk\n<CallbackBatchFSelect:mlr3fselect.internal_tuning>: Internal Tuning\n* Active Stages: on_auto_fselector_after_final_model, on_auto_fselector_before_final_model,\n  on_eval_before_archive, on_optimization_end\nThis practically sets the boosting rounds of the final XGBoost model (after the RFE optimization is finished) as the average boosting rounds from each subsequent training fold (corresponding to the model trained with the ‘best’ feature subset). For example, since we’re performing a 5-fold inner CV, we would have 5 different early-stopped boosting\nnrounds\n, from which we will use the average value to train the final XGBoost model using the whole train set.\nFor all learners we will prefer\nsparser models during the RFE optimization process\n. This means that across all RFE iterations, we will choose as ‘best’ feature subset the one that has the minimum number of features and its performance is\nwithin one standard error\nof the feature set with the best performance (e.g. the lowest classification error). This can be achieved with the following callback:\none_se_clbk = clbk(\"mlr3fselect.one_se_rule\")\none_se_clbk\n<CallbackBatchFSelect:mlr3fselect.one_se_rule>: One Standard Error Rule Callback\n* Active Stages: on_optimization_end\nExecute EFS\nUsing the\nmlr3fselect::ensemble_fselect()\nfunction, we split the\nsonar\ntask to\nsubsamples (each corresponding to a 80%/20% train/test set split) and perform RFE in each train set using each of the\nlearners.\nFor a particular (train set, learner) combination, the RFE process will evaluate the\nfeature subsets mentioned above. Using the inner 5-fold CV resampling scheme, the average CV classification error will be used to find the best feature subset. Using only features from this best feature set, a final model will be trained using all the observations from each trained set. Lastly, the performance of this final model will be assessed on the corresponding test set using the classification accuracy metric.\nset.seed(42)\nefs = ensemble_fselect(\n  fselector = rfe,\n  task = task,\n  learners = learners,\n  init_resampling = rsmp(\"subsampling\", repeats = 50, ratio = 0.8),\n  inner_resampling = rsmp(\"cv\", folds = 5),\n  inner_measure = msr(\"classif.ce\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"none\"),\n  # following list must be named with the learners' ids\n  callbacks = list(\n    xgb  = list(one_se_clbk, xgb_clbk),\n    tree = list(one_se_clbk),\n    rf   = list(one_se_clbk),\n    svm  = list(one_se_clbk, svm_rfe)\n  ),\n  store_benchmark_result = FALSE\n)\nThe result is stored in an\nEnsembleFSResult\nobject, which can use to visualize the results, rank the features and assess the stability of the ensemble feature selection process, among others.\nAnalyze EFS Results\nResult Object\nPrinting the result object provides some initial information:\nprint(efs)\n<EnsembleFSResult> with 4 learners and 50 initial resamplings\n     resampling_iteration    learner_id n_features\n                    <int>        <char>      <int>\n  1:                    1 xgb.fselector          8\n  2:                    2 xgb.fselector         30\n  3:                    3 xgb.fselector         60\n  4:                    4 xgb.fselector         19\n  5:                    5 xgb.fselector          6\n ---                                              \n196:                   46 svm.fselector         24\n197:                   47 svm.fselector         19\n198:                   48 svm.fselector         15\n199:                   49 svm.fselector         19\n200:                   50 svm.fselector          8\nAs we can see, we have\n(init resampling, learner) combinations. We can inspect the actual\ndata.table\nresult:\nefs$result\nlearner_id resampling_iteration classif.acc                    features n_features classif.ce_inner\n            <char>                <int>       <num>                      <list>      <int>            <num>\n  1: xgb.fselector                    1   0.6904762  V11,V12,V16,V21,V36,V4,...          8        0.1081996\n  2: xgb.fselector                    2   0.9285714  V1,V10,V11,V12,V13,V15,...         30        0.1627451\n  3: xgb.fselector                    3   0.8333333  V1,V10,V11,V12,V13,V14,...         60        0.1203209\n  4: xgb.fselector                    4   0.8571429 V11,V12,V15,V16,V21,V27,...         19        0.1447415\n  5: xgb.fselector                    5   0.7142857     V10,V11,V16,V31,V36,V45          6        0.1319073\n ---                                                                                                       \n196: svm.fselector                   46   0.7619048  V1,V11,V12,V14,V23,V25,...         24        0.1440285\n197: svm.fselector                   47   0.7380952 V11,V15,V23,V30,V31,V33,...         19        0.1331551\n198: svm.fselector                   48   0.7619048 V12,V17,V31,V32,V36,V37,...         15        0.1809269\n199: svm.fselector                   49   0.7142857 V11,V12,V14,V17,V20,V24,...         19        0.1864528\n200: svm.fselector                   50   0.7619048 V11,V14,V23,V36,V39,V40,...          8        0.1629234\n                            importance\n                                <list>\n  1:       6.8,6.8,6.4,3.8,3.6,3.4,...\n  2: 27.2,26.8,25.6,22.0,22.0,21.6,...\n  3: 60.0,57.6,53.8,52.0,51.4,49.6,...\n  4: 19.0,13.4,13.0,12.8,12.2,11.0,...\n  5:           4.4,4.2,4.2,3.8,2.2,2.2\n ---                                  \n196: 24.0,21.6,20.2,18.4,17.6,16.8,...\n197: 16.6,16.2,16.2,15.0,14.8,14.4,...\n198: 14.0,13.6,13.0,10.4, 9.8, 9.2,...\n199: 17.6,16.0,15.6,14.4,13.4,12.8,...\n200:       8.0,5.8,5.6,5.2,4.6,3.2,...\nFor each learner (\n\"learner_id\"\n) and dataset subsample (\n\"resampling_iteration\"\n) we get:\nThe ‘best’ feature subsets (\n\"features\"\n)\nThe number of ‘best’ features (\n\"nfeatures\"\n)\nThe importances for these ‘best’ features (\n\"importance\"\n) – this output column we get only because RFE optimization was used\nThe inner optimization performance scores on the train sets (\n\"classif.ce_inner\"\n)\nThe performance scores on the test sets (\n\"classif.acc\"\n)\nSince there are two ways in this process to evaluate performance, we can always check which is the\nactive measure\n:\nefs$active_measure\n[1] \"outer\"\nBy default the active measure is the\n\"outer\"\n, i.e. the measure used to evaluate each learner’s performance in the test sets. In our case that was the classification accuracy:\nefs$measure\n<MeasureClassifSimple:classif.acc>: Classification Accuracy\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\nNote\nIn the following sections we can use the inner optimization scores (i.e.\n\"classif.ce_inner\"\n) by executing\nefs$set_active_measure(\"inner\")\n. This affects all methods and plots that use performance scores.\nPerformance\nWe can view the\nperformance scores of the different learners\nused in the ensemble feature selection process. Each box represents the distribution of scores across different resampling iterations for a particular learner.\nautoplot(efs, type = \"performance\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\nWe observe that RF has better classification accuracy on the test sets of the\nsubsamples, followed by XGBoost, then the SVM and last the tree model.\nNumber of Selected Features\nContinuing, we can plot\nthe number of features selected by each learner\nin the different resampling iterations:\nautoplot(efs, type = \"n_features\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(breaks = seq(0, 60, 10))\nWe observe that RF needed more features to achieve the best average performance, followed by SVM, then XGBoost and the tree model was the model using the least features (but with worst performance).\nPareto Plot\nBoth performance scores and number of features selected by the RFE optimization process can be visualized jointly in the Pareto plot. Here we also draw the\nPareto front\n, i.e. the set of points that represent the trade-off between the number of features and performance (classification accuracy). As we see below, these points are derived from multiple learners and resamplings:\nautoplot(efs, type = \"pareto\", theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Empirical Pareto front\")\nWe can also draw an\nestimated Pareto front curve\nby fitting a linear model with the inverse of the number of selected features (\n) of the empirical Pareto front as input, and the associated performance scores as output:\nautoplot(efs, type = \"pareto\", pareto_front = \"estimated\", \n         theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Estimated Pareto front\")\nKnee Point Identification\nNo matter the type of Pareto front that we chose, specialized methods are available to identify\nknee points\n, i.e. points of the Pareto front with an\noptimal trade-off between performance and number of selected features\n.\nBy default, we use the geometry-based\nNormal-Boundary Intersection\n(NBI) method. This approach calculates the perpendicular distance of each point from the line connecting the first (worst performance, minimum number of features) and last (best performance, maximum number of features) point of the Pareto front. The knee point is then identified as the point with the maximum distance from this line\n(Das 1999)\n.\nUsing the empirical and estimated Pareto fronts, we observe that the optimal knee points correspond to different numbers of features:\nefs$knee_points()\nn_features classif.acc\n        <num>       <num>\n1:         10   0.9047619\nefs$knee_points(type = \"estimated\")\nn_features classif.acc\n        <int>       <num>\n1:          8   0.8597253\nNumber of features cutoff\nThe number of features at the identified knee point provides a cutoff for prioritizing features when working with a ranked feature list (see “Feature Ranking” section).\nStability\nThe\nstabm\nR package\n(Bommert and Lang 2021)\nimplements many measures for the assessment of the\nstability of feature selection\n, i.e. the similarity between the selected feature sets (\n\"features\"\ncolumn in the\nEnsembleFSResult\nobject). We can use these measures to assess and visualize the stability across all resampling iterations and learners (\nglobal = \"TRUE\"\n) or per each learner separately (\nglobal = \"FALSE\"\n).\nThe default stability measure is the\nJaccard Index\n:\nefs$stability(stability_measure = \"jaccard\", global = TRUE)\n[1] 0.2640504\nStability per learner:\nefs$stability(stability_measure = \"jaccard\", global = FALSE)\nxgb.fselector tree.fselector   rf.fselector  svm.fselector \n     0.3657964      0.3554681      0.4716744      0.3119381\nWe observe that the RF model was the most stable in identifying similar predictive features across the different subsamples of the dataset, while the SVM model the least stable.\nTo visualize stability, the following code generates a stability barplot:\nautoplot(efs, type = \"stability\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\nAlternatively, the\nNogueira\nstability measure can be used, which unlike the Jaccard Index, it’s a chance-corrected similarity measure\n(Nogueira, Sechidis, and Brown 2018)\n:\nautoplot(efs, type = \"stability\", stability_measure = \"nogueira\", \n         stability_args = list(p = task$n_features), \n         theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\nFeature Ranking\nUsing the Pareto method, we demonstrated how we can identify a reasonable cutoff for the number of selected features. Now we will focus on how to create a consensus ranked feature list based on the results of the ensemble feature selection.\nThe most straightforward ranking is obtained by counting how often each feature appears in the ‘best’ feature subsets (\n\"features\"\n). Below we show the top 8 features, i.e. up to the cutoff derived from the knee point of the estimated Pareto front. The column\n\"score\"\nrepresents these counts, while the column\n\"norm_score\"\nis the\nfeature selection frequency\nor also known as\nselection probability\n(Meinshausen and Bühlmann 2010)\n:\nefs$feature_ranking(method = \"av\", use_weights = FALSE, committee_size = 8)\nfeature score norm_score borda_score\n    <char> <num>      <num>       <num>\n1:     V12   179      0.895   1.0000000\n2:     V11   170      0.850   0.9830508\n3:      V9   123      0.615   0.9661017\n4:     V45   121      0.605   0.9491525\n5:     V16   118      0.590   0.9322034\n6:     V36   113      0.565   0.9152542\n7:     V49   104      0.520   0.8983051\n8:      V4    99      0.495   0.8813559\nIn the language of Voting Theory, we call the method that generates these counts\napproval voting\n(\nmethod = \"av\"\n)\n(Lackner and Skowron 2023)\n. Using this framework, learners act as\nvoters\n, features act as\ncandidates\nand voters select certain candidates (features). The primary objective is to compile these selections into a consensus ranked list of features (a committee). The\ncommittee_size\nspecifies how many (top-ranked) features to return.\nInternally,\n$feature_ranking()\nuses the\nfastVoteR\nR package, which supports more advanced ranking methods. For example, we can perform\nweighted ranking\n, by considering the varying performance (accuracy) of each learner. This results in the same top 8 features but with slightly different ordering:\nefs$feature_ranking(method = \"av\", use_weights = TRUE, committee_size = 8)\nfeature     score norm_score borda_score\n    <char>     <num>      <num>       <num>\n1:     V12 134.78571  0.8995710   1.0000000\n2:     V11 127.33333  0.8498331   0.9830508\n3:     V45  94.45238  0.6303830   0.9661017\n4:      V9  93.71429  0.6254569   0.9491525\n5:     V16  89.76190  0.5990783   0.9322034\n6:     V36  87.97619  0.5871603   0.9152542\n7:     V49  80.64286  0.5382171   0.8983051\n8:      V4  76.64286  0.5115207   0.8813559\nAdditionally, alternative ranking methods are supported. Below, we use\nsatisfaction approval voting\n(SAV), which ranks features by normalizing approval scores based on the number of features a model has selected. Specifically, models that select more features distribute their “approval” across a larger set, reducing the contribution to each selected feature. Conversely,\nfeatures chosen by models with fewer selected features receive higher weights\n, as their selection reflects stronger individual importance. This approach ensures that sparsely selected features are prioritized in the ranking, leading to a different set of top-ranked features compared to standard approval voting. For instance, in the example above, the\n\"V10\"\nfeature enters the top 8 features, replacing\n\"V4\"\n:\nefs$feature_ranking(method = \"sav\", committee_size = 8)\nfeature     score norm_score borda_score\n    <char>     <num>      <num>       <num>\n1:     V11 15.353545  0.9100050   1.0000000\n2:     V12 14.107691  0.8361632   0.9830508\n3:     V16  7.698460  0.4562879   0.9661017\n4:     V45  6.811607  0.4037241   0.9491525\n5:      V9  6.443311  0.3818952   0.9322034\n6:     V36  6.060615  0.3592128   0.9152542\n7:     V10  5.955446  0.3529794   0.8983051\n8:     V49  4.741014  0.2810000   0.8813559\nEFS-based Feature Selection\nThe ultimate goal of the ensemble feature selection process is to identify predictive and stable features. By combining the ranked feature list with the Pareto-derived cutoff, we can select the final set of features for further modeling:\nn_features = efs$knee_points(type = \"estimated\")$n_features\nres = efs$feature_ranking(method = \"sav\", committee_size = n_features)\nres$feature\n[1] \"V11\" \"V12\" \"V16\" \"V45\" \"V9\"  \"V36\" \"V10\" \"V49\"\nSession Information\nsessioninfo::session_info(info = \"packages\")\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package          * version     date (UTC) lib source\n P backports          1.5.0       2024-05-23 [?] CRAN (R 4.4.2)\n P bbotk              1.5.0       2024-12-17 [?] CRAN (R 4.4.2)\n P checkmate          2.3.2       2024-07-29 [?] CRAN (R 4.4.2)\n P class              7.3-22      2023-05-03 [?] CRAN (R 4.4.2)\n P cli                3.6.3       2024-06-21 [?] CRAN (R 4.4.2)\n P clue               0.3-66      2024-11-13 [?] CRAN (R 4.4.2)\n P cluster            2.1.6       2023-12-01 [?] CRAN (R 4.4.2)\n P codetools          0.2-20      2024-03-31 [?] CRAN (R 4.4.2)\n P colorspace         2.1-1       2024-07-26 [?] CRAN (R 4.4.2)\n P crayon             1.5.3       2024-06-20 [?] CRAN (R 4.4.2)\n P data.table       * 1.16.4      2024-12-06 [?] CRAN (R 4.4.2)\n P DEoptimR           1.1-3-1     2024-11-23 [?] CRAN (R 4.4.2)\n P digest             0.6.37      2024-08-19 [?] CRAN (R 4.4.2)\n P diptest            0.77-1      2024-04-10 [?] CRAN (R 4.4.2)\n P evaluate           1.0.1       2024-10-10 [?] CRAN (R 4.4.2)\n P farver             2.1.2       2024-05-13 [?] CRAN (R 4.4.2)\n P fastmap            1.2.0       2024-05-15 [?] CRAN (R 4.4.2)\n P fastVoteR        * 0.0.1       2024-11-27 [?] RSPM\n P flexmix            2.3-19      2023-03-16 [?] CRAN (R 4.4.2)\n P fpc                2.2-13      2024-09-24 [?] CRAN (R 4.4.2)\n P future           * 1.34.0      2024-07-29 [?] CRAN (R 4.4.2)\n P ggplot2          * 3.5.1       2024-04-23 [?] CRAN (R 4.4.2)\n P globals            0.16.3      2024-03-08 [?] CRAN (R 4.4.2)\n P glue               1.8.0       2024-09-30 [?] CRAN (R 4.4.2)\n P gtable             0.3.6       2024-10-25 [?] CRAN (R 4.4.2)\n P htmltools          0.5.8.1     2024-04-04 [?] CRAN (R 4.4.2)\n P jsonlite           1.8.9       2024-09-20 [?] CRAN (R 4.4.2)\n P kernlab            0.9-33      2024-08-13 [?] CRAN (R 4.4.2)\n P knitr              1.49        2024-11-08 [?] CRAN (R 4.4.2)\n P labeling           0.4.3       2023-08-29 [?] CRAN (R 4.4.2)\n P lattice            0.22-6      2024-03-20 [?] CRAN (R 4.4.2)\n P lgr                0.4.4       2022-09-05 [?] CRAN (R 4.4.2)\n P lifecycle          1.0.4       2023-11-07 [?] CRAN (R 4.4.2)\n P listenv            0.9.1       2024-01-29 [?] CRAN (R 4.4.2)\n P magrittr           2.0.3       2022-03-30 [?] CRAN (R 4.4.2)\n P MASS               7.3-61      2024-06-13 [?] CRAN (R 4.4.2)\n P Matrix             1.7-1       2024-10-18 [?] CRAN (R 4.4.2)\n P mclust             6.1.1       2024-04-29 [?] CRAN (R 4.4.2)\n P mlr3             * 0.22.1      2024-11-27 [?] CRAN (R 4.4.2)\n P mlr3cluster        0.1.10      2024-10-03 [?] CRAN (R 4.4.2)\n P mlr3data           0.9.0       2024-11-08 [?] CRAN (R 4.4.2)\n P mlr3filters        0.8.1       2024-11-08 [?] CRAN (R 4.4.2)\n P mlr3fselect        1.2.1.9000  2024-12-16 [?] Github (mlr-org/mlr3fselect@ab6360a)\n P mlr3hyperband      0.6.0       2024-06-29 [?] CRAN (R 4.4.2)\n P mlr3learners       0.9.0       2024-11-23 [?] CRAN (R 4.4.2)\n P mlr3mbo            0.2.8       2024-11-21 [?] CRAN (R 4.4.2)\n P mlr3measures       1.0.0       2024-09-11 [?] CRAN (R 4.4.2)\n P mlr3misc           0.16.0      2024-11-28 [?] CRAN (R 4.4.2)\n P mlr3pipelines      0.7.1       2024-11-14 [?] CRAN (R 4.4.2)\n P mlr3tuning         1.3.0       2024-12-17 [?] CRAN (R 4.4.2)\n P mlr3tuningspaces   0.5.2       2024-11-22 [?] CRAN (R 4.4.2)\n P mlr3verse        * 0.3.0       2024-06-30 [?] CRAN (R 4.4.2)\n P mlr3viz            0.10.0.9000 2024-12-30 [?] Github (mlr-org/mlr3viz@b96b886)\n P modeltools         0.2-23      2020-03-05 [?] CRAN (R 4.4.2)\n P munsell            0.5.1       2024-04-01 [?] CRAN (R 4.4.2)\n P nnet               7.3-19      2023-05-03 [?] CRAN (R 4.4.2)\n P palmerpenguins     0.1.1       2022-08-15 [?] CRAN (R 4.4.2)\n P paradox            1.0.1       2024-07-09 [?] CRAN (R 4.4.2)\n P parallelly         1.41.0      2024-12-18 [?] CRAN (R 4.4.2)\n P pillar             1.10.0      2024-12-17 [?] CRAN (R 4.4.2)\n P pkgconfig          2.0.3       2019-09-22 [?] CRAN (R 4.4.2)\n P prabclus           2.3-4       2024-09-24 [?] CRAN (R 4.4.2)\n P progressr        * 0.15.1      2024-11-22 [?] CRAN (R 4.4.2)\n P R6                 2.5.1       2021-08-19 [?] CRAN (R 4.4.2)\n P RColorBrewer       1.1-3       2022-04-03 [?] CRAN (R 4.4.2)\n P Rcpp               1.0.13-1    2024-11-02 [?] CRAN (R 4.4.2)\n   renv               1.0.11      2024-10-12 [1] CRAN (R 4.4.2)\n P rlang              1.1.4       2024-06-04 [?] CRAN (R 4.4.2)\n P rmarkdown          2.29        2024-11-04 [?] CRAN (R 4.4.2)\n P robustbase         0.99-4-1    2024-09-27 [?] CRAN (R 4.4.2)\n P scales             1.3.0       2023-11-28 [?] CRAN (R 4.4.2)\n P sessioninfo        1.2.2       2021-12-06 [?] CRAN (R 4.4.2)\n P spacefillr         0.3.3       2024-05-22 [?] CRAN (R 4.4.2)\n P stabm              1.2.2       2023-04-04 [?] CRAN (R 4.4.2)\n P tibble             3.2.1       2023-03-20 [?] CRAN (R 4.4.2)\n P uuid               1.2-1       2024-07-29 [?] CRAN (R 4.4.2)\n P vctrs              0.6.5       2023-12-01 [?] CRAN (R 4.4.2)\n P withr              3.0.2       2024-10-28 [?] CRAN (R 4.4.2)\n P xfun               0.49        2024-10-31 [?] CRAN (R 4.4.2)\n P yaml               2.3.10      2024-07-26 [?] RSPM\n\n [1] /home/john/repos/mlr3-packages/mlr3website/mlr-org/renv/library/linux-ubuntu-focal/R-4.4/x86_64-pc-linux-gnu\n [2] /home/john/.cache/R/renv/sandbox/linux-ubuntu-focal/R-4.4/x86_64-pc-linux-gnu/db5e602d\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\nReferences\nBommert, Andrea, and Michel Lang. 2021.\n“\nstabm: Stability Measures for Feature Selection\n.”\nJournal of Open Source Software\n6 (59): 3010.\nhttps://doi.org/10.21105/JOSS.03010\n.\nDas, I. 1999.\n“\nOn characterizing the “knee” of the Pareto curve based on normal-boundary intersection\n.”\nStructural Optimization\n18 (2-3): 107–15.\nhttps://doi.org/10.1007/BF01195985/METRICS\n.\nLackner, Martin, and Piotr Skowron. 2023.\nMulti-Winner Voting with Approval Preferences\n. Springer Nature.\nhttps://doi.org/10.1007/978-3-031-09016-5\n.\nMeinshausen, Nicolai, and Peter Bühlmann. 2010.\n“\nStability Selection\n.”\nJournal of the Royal Statistical Society Series B: Statistical Methodology\n72 (4): 417–73.\nhttps://doi.org/10.1111/J.1467-9868.2010.00740.X\n.\nNogueira, Sarah, Konstantinos Sechidis, and Gavin Brown. 2018.\n“\nOn the Stability of Feature Selection Algorithms\n.”\nJournal of Machine Learning Research\n18 (174): 1–54.\nhttp://jmlr.org/papers/v18/17-514.html\n.\nSaeys, Yvan, Thomas Abeel, and Yves Van De Peer. 2008.\n“\nRobust feature selection using ensemble feature selection techniques\n.”\nMachine Learning and Knowledge Discovery in Databases\n5212 LNAI: 313–25.\nhttps://doi.org/10.1007/978-3-540-87481-2_21\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nmlr-org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Intro In this post we will show how we can use the mlr3fselect R package to perform wrapped-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resamplin...",
      "meta_keywords": null,
      "og_description": "Intro In this post we will show how we can use the mlr3fselect R package to perform wrapped-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resamplin...",
      "og_image": "https://latex.codecogs.com/png.latex?B",
      "og_title": "Wrapper-based Ensemble Feature Selection | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 23.2,
      "sitemap_lastmod": null,
      "twitter_description": "Intro In this post we will show how we can use the mlr3fselect R package to perform wrapped-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resamplin...",
      "twitter_title": "Wrapper-based Ensemble Feature Selection | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/01/wrapper-based-ensemble-feature-selection/",
      "word_count": 4631
    }
  }
}