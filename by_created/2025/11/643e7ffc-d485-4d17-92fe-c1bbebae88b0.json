{
  "uuid": "643e7ffc-d485-4d17-92fe-c1bbebae88b0",
  "created_at": "2025-11-17 20:39:00",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2023/12/r-data-processing-frameworks-how-to-speed-up-your-data-processing-pipelines-up-to-20-times/",
    "crawled_at": "2025-11-17T09:41:32.751182",
    "external_links": [
      {
        "href": "https://appsilon.com/r-data-processing-frameworks/",
        "text": "Tag: r - Appsilon | Enterprise R Shiny Dashboards"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://appsilon.com/excel-functions-in-r/",
        "text": "Here’s a couple of advanced Excel functions in R for data manipulation"
      },
      {
        "href": "https://appsilon.com/r-data-processing-frameworks/#dplyr-alternatives",
        "text": "Dplyr Alternatives – Top R Data Processing Frameworks"
      },
      {
        "href": "https://appsilon.com/r-data-processing-frameworks/#experiment",
        "text": "Dplyr vs. Arrow vs. DuckDB – R Data Processing Framework Test"
      },
      {
        "href": "https://appsilon.com/r-data-processing-frameworks/#summary",
        "text": "Summing up R Data Processing Framework Benchmarks"
      },
      {
        "href": "https://appsilon.com/r-dplyr-tutorial/",
        "text": "beginner-level data analysis techniques with dplyr"
      },
      {
        "href": "https://arrow.apache.org/docs/r/",
        "text": "here"
      },
      {
        "href": "https://duckdb.org/",
        "text": "here"
      },
      {
        "href": "https://duckdb.org/docs/api/r.html",
        "text": "here"
      },
      {
        "href": "https://appsilon.com/automated-r-data-quality-reporting/",
        "text": "Look no further than R’s data.validator package"
      },
      {
        "href": "https://appsilon.com/r-data-processing-frameworks/",
        "text": "Tag: r - Appsilon | Enterprise R Shiny Dashboards"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "R Data Processing Frameworks: How To Speed Up Your Data Processing Pipelines up to 20 Times | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/R-Processing.webp"
      },
      {
        "alt": "Image 1 - Datasets used",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 1 - Datasets used",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-1-Datasets-used.webp"
      },
      {
        "alt": "Image 2 - Benchmark #1 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 2 - Benchmark #1 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-2-Benchmark-1-results-scaled.webp"
      },
      {
        "alt": "Image 3 - Benchmark #2 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 3 - Benchmark #2 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-3-Benchmark-2-results-scaled.webp"
      },
      {
        "alt": "Image 4 - Benchmark #3 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 4 - Benchmark #3 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-4-Benchmark-3-results-scaled.webp"
      },
      {
        "alt": "Image 5 - Benchmark #4 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 5 - Benchmark #4 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-5-Benchmark-4-results-scaled.webp"
      },
      {
        "alt": "Image 6 - Benchmark #5 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 6 - Benchmark #5 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-6-Benchmark-5-results-scaled.webp"
      },
      {
        "alt": "Image 7 - Benchmark #6 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 7 - Benchmark #6 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-7-Benchmark-6-results-scaled.webp"
      },
      {
        "alt": "Image 8 - Benchmark #7 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 8 - Benchmark #7 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-8-Benchmark-7-results-scaled.webp"
      },
      {
        "alt": "Image 9 - Benchmark #8 results",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 9 - Benchmark #8 results",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-9-Benchmark-8-results-scaled.webp"
      },
      {
        "alt": "Image 10 - Total runtime comparison",
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": "Image 10 - Total runtime comparison",
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-10-Total-runtime-comparison-scaled.webp"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/dario-radecic/",
        "text": "Dario Radečić"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-380626 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">R Data Processing Frameworks: How To Speed Up Your Data Processing Pipelines up to 20 Times</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">December 5, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/dario-radecic/\">Dario Radečić</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://appsilon.com/r-data-processing-frameworks/\"> Tag: r - Appsilon | Enterprise R Shiny Dashboards</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9--><div><img alt=\"\" class=\"attachment-medium size-medium wp-post-image\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/R-Processing.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" style=\"margin-bottom: 15px;\" width=\"450\"/><noscript><img alt=\"\" class=\"attachment-medium size-medium wp-post-image\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/R-Processing.webp\" style=\"margin-bottom: 15px;\" width=\"450\"/></noscript></div><p>Picture this – the data science team you manage primarily uses R and heavily relies on <code>dplyr</code> for implementing data processing pipelines. All is good, but then out of the blue you’re working with a client that has a massive dataset, and all of a sudden <code>dplyr</code> becomes the bottleneck. You want a faster way to process data with minimum to no changes to the source code.</p>\n<p>You’re wondering, how much effort will it take to speed up your data processing pipelines 2 times, 5 times, 10 times, or even up to 20 times? The answer might surprise you – <b>far less than you think</b>.</p>\n<p>This article will show you why <code>dplyr</code> fails to perform on larger datasets, and which alternative options you have in the context of R data processing frameworks. Let’s dig in!</p>\n<blockquote><p>R is full of things you didn’t know are possible – <a href=\"https://appsilon.com/excel-functions-in-r/\" rel=\"nofollow\" target=\"_blank\">Here’s a couple of advanced Excel functions in R for data manipulation</a>.</p></blockquote>\n<h3>Table of contents:</h3>\n<ul>\n<li><strong><a href=\"https://appsilon.com/r-data-processing-frameworks/#dplyr-alternatives\" rel=\"nofollow\" target=\"_blank\">Dplyr Alternatives – Top R Data Processing Frameworks</a></strong></li>\n<li><strong><a href=\"https://appsilon.com/r-data-processing-frameworks/#experiment\" rel=\"nofollow\" target=\"_blank\">Dplyr vs. Arrow vs. DuckDB – R Data Processing Framework Test</a></strong></li>\n<li><strong><a href=\"https://appsilon.com/r-data-processing-frameworks/#summary\" rel=\"nofollow\" target=\"_blank\">Summing up R Data Processing Framework Benchmarks</a></strong></li>\n</ul>\n<hr>\n<h2 id=\"dplyr-alternatives\">Dplyr Alternatives – Top R Data Processing Frameworks</h2>\n<p>This section will introduce you to two <code>dplyr</code> alternatives that use the same interface – <code>arrow</code> and <code>duckdb</code>. We wanted to focus on these two specifically because they come up with minimal code changes, as you’ll see from the examples later. They provide the best “bang for the buck” if your data science team doesn’t have the time to learn a new data processing framework from scratch.</p>\n<h3>Which R dplyr Alternatives Can You Use?</h3>\n<p>Sure, everyone loves <code>dplyr</code>. We’ve even dedicated a full article for <a href=\"https://appsilon.com/r-dplyr-tutorial/\" rel=\"nofollow\" target=\"_blank\">beginner-level data analysis techniques with dplyr</a>. The package has a user-friendly syntax and is super easy to use for data transformation tasks. But guess what – so are the other two alternatives we’ll use today. In addition, they are usually up to 20 times faster.</p>\n<p><b>Arrow</b> is a cross-language development platform for in-memory and larger-than-memory data. The R package exposes an interface to the Arrow C++ library, allowing you to benefit from an R programming language syntax, and access to a C++ library API through a set of well-known <code>dplyr</code> backend functions. Arrow also provides zero-copy data sharing between R and Python, which might be appealing for language-agnostic data science teams.</p>\n<blockquote><p>You can learn more about Arrow for R <a href=\"https://arrow.apache.org/docs/r/\" rel=\"nofollow\" target=\"_blank\">here</a>.</p></blockquote>\n<p><b>DuckDB</b> is an open-source, embedded, in-process, relational OLAP DBMS. Its description contains pretty much every buzzword you could imagine, but it being an OLAP database means the data is organized by columns and is optimized for complex data queries (think joins, aggregations, groupings, and so on). The good thing about <code>duckdb</code> is that it comes with an R API, meaning you can use R to point to a local (or remote) instance of your database with <code>DBI</code>. Further, <code>duckdb</code> R package uses <code>dplyr</code>-like syntax, which means code changes coming from a vanilla <code>dplyr</code> will be minimal to non-existent.</p>\n<blockquote><p>You can learn more about DuckDB <a href=\"https://duckdb.org/\" rel=\"nofollow\" target=\"_blank\">here</a> and about its R API <a href=\"https://duckdb.org/docs/api/r.html\" rel=\"nofollow\" target=\"_blank\">here</a>.</p></blockquote>\n<p>So, these are the <code>dplyr</code> alternatives we’ll use to perform a series of benchmarks with the goal of comparing R data processing frameworks. They should both be faster than <code>dplyr</code> in most cases, but that’s where the actual tests come in. More on that in the following section.</p>\n<h2 id=\"experiment\">Dplyr vs. Arrow vs. DuckDB – R Data Processing Framework Test</h2>\n<p>To kick things off, let’s talk about data. We have several small to somewhat large Parquet files and a single DuckDB database (not publicly available) that has other files stored inside. The <code>dplyr</code> and <code>arrow</code> benchmarks will be based on the Parquet files, and the DuckDB benchmarks will be connected to the local database:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22244\" style=\"width: 1224px\"><img alt=\"Image 1 - Datasets used\" aria-describedby=\"caption-attachment-22244\" class=\"size-full wp-image-22244\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-1-Datasets-used.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 1 - Datasets used\" aria-describedby=\"caption-attachment-22244\" class=\"size-full wp-image-22244\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-1-Datasets-used.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22244\">Image 1 – Datasets used</p></div>\n<p>Next, we’ll discuss how the benchmarks were configured, R package versions, and the hardware used to run the tests.</p>\n<h3>Benchmark Setup and Information</h3>\n<p>As for R itself, we’ve used <b>R version 4.3.1</b>. The most important packages were installed with the following version numbers:</p>\n<ul>\n<li><code>dplyr</code> – 1.1.3</li>\n<li><code>arrow</code> – 13.0.0</li>\n<li><code>duckdb</code> – 0.8.1-3</li>\n</ul>\n<p>Each of the benchmark tests you’ll see below was run <b>3 times</b> for each R data processing framework.</p>\n<p>The hardware of choice was a <b>14″ M2 Pro MacBook Pro</b> with a 12-core CPU and 16 GB of RAM.</p>\n<p>For working on <code>dplyr</code> and <code>arrow</code> benchmarks, we’ve imported the following R packages:</p>\n<pre>library(tidyverse)\r\nlibrary(arrow)</pre>\n<p>The <code>arrow</code> benchmark results have had the following option configured:</p>\n<pre>options(arrow.pull_as_vector = TRUE)</pre>\n<p>Working with <code>duckdb</code> required a couple of extra dependencies:</p>\n<pre>library(DBI)\r\nlibrary(duckdb)</pre>\n<p>In both of these, the following code was used to load the datasets:</p>\n<pre>badges &lt;- read_parquet(\"../../data/badges.parquet\")\r\nposts &lt;- read_parquet(\"../../data/posts.parquet\")\r\ntags &lt;- read_parquet(\"../../data/tags.parquet\")\r\nusers &lt;- read_parquet(\"../../data/users.parquet\")\r\nvotes &lt;- read_parquet(\"../../data/votes.parquet\")\r\nwiki &lt;- read_parquet(\"../../data/wiki.parquet\")</pre>\n<p>In order to connect to a DuckDB database and extract the datasets, we’ve used the following code:</p>\n<pre>con &lt;- dbConnect(duckdb::duckdb(\"./data.duckdb\"))\r\n\r\nbadges &lt;- tbl(con, \"badges\")\r\nposts &lt;- tbl(con, \"posts\")\r\ntags &lt;- tbl(con, \"tags\")\r\nusers &lt;- tbl(con, \"users\")\r\nvotes &lt;- tbl(con, \"votes\")\r\nwiki &lt;- tbl(con, \"wiki\")</pre>\n<p>Finally, to actually run benchmarks, we decided to declare a <code>benchmark()</code> function which takes another function as an argument:</p>\n<pre>benchmark &lt;- function(fun) {\r\n    start &lt;- Sys.time()\r\n    res &lt;- fun()\r\n    end &lt;- Sys.time()\r\n    print(end - start)\r\n    res\r\n}</pre>\n<p>Each of the 8 benchmarks you’ll see below wraps the logic inside a separate function and then calls <code>benchmark()</code> and passes itself as an argument.</p>\n<p>So with that out of the way, let’s go over our first benchmark!</p>\n<h3>Benchmark #1: Finding the Article with the Most External Entries</h3>\n<p>The goal of the first benchmark was to use the <code>wiki</code> dataset to find the article with the most external entries on English Wikipedia in March 2022.</p>\n<p>You’ll find the code for all three R packages below. The only difference from vanilla <code>dplyr</code> is in passing some additional arguments to <code>summarise()</code> and <code>slice_max()</code> functions. Everything else is identical:</p>\n<pre>b1_dplyr &lt;- function() {\r\n  wiki %&gt;% \r\n    filter(type == \"external\") %&gt;%\r\n    group_by(curr) %&gt;%\r\n    summarise(total = sum(n)) %&gt;%\r\n    slice_max(total, n = 3) %&gt;%\r\n    pull(curr)\r\n}\r\n\r\nb1_arrow &lt;- function() {\r\n  wiki %&gt;% \r\n    filter(type == \"external\") %&gt;%\r\n    group_by(curr) %&gt;%\r\n    summarise(total = sum(n)) %&gt;%\r\n    slice_max(total, n = 3, with_ties = FALSE) %&gt;%\r\n    pull(curr)\r\n}\r\n\r\nb1_duckdb &lt;- function() {\r\n  wiki %&gt;% \r\n    filter(type == \"external\") %&gt;%\r\n    group_by(curr) %&gt;%\r\n    summarise(total = sum(n, na.rm = TRUE)) %&gt;%\r\n    slice_max(total, n = 3, with_ties = FALSE) %&gt;%\r\n    pull(curr)\r\n}</pre>\n<p>Here are the results:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22246\" style=\"width: 2570px\"><img alt=\"Image 2 - Benchmark #1 results\" aria-describedby=\"caption-attachment-22246\" class=\"size-full wp-image-22246\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-2-Benchmark-1-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 2 - Benchmark #1 results\" aria-describedby=\"caption-attachment-22246\" class=\"size-full wp-image-22246\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-2-Benchmark-1-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22246\">Image 2 – Benchmark #1 results</p></div>\n<p>As you can see, both <code>arrow</code> and <code>duckdb</code> were faster, by 10 and 7 times, respectively.</p>\n<h3>Benchmark #2: Finding Properties Across Multiple Datasets</h3>\n<p>The second test was a combination of two calculations – the first one had the task of finding the <code>DisplayName</code> property of a user that has the most badges, while the second one had to find the <code>Location</code> property for the same condition. Most of the logic is implemented in the first portion of the calculation, where the ID of the user was found, and then in the second portion, only the desired properties were extracted.</p>\n<p>As before, most of the code differences boil down to the extra arguments in a couple of functions and calling <code>collect()</code> at the end of the calculation:</p>\n<pre>b2_dplyr &lt;- function() {\r\n  tid &lt;- badges %&gt;%\r\n    left_join(users, by = join_by(UserId == Id)) %&gt;%\r\n    group_by(UserId, DisplayName) %&gt;%\r\n    summarise(NBadges = n()) %&gt;%\r\n    ungroup() %&gt;%\r\n    slice_max(NBadges, n = 1) %&gt;%\r\n    pull(UserId)\r\n  top_user &lt;- users %&gt;%\r\n    filter(Id == tid) %&gt;%\r\n    select(DisplayName, Location)\r\n\r\n  top_user\r\n}\r\n\r\n\r\nb2_arrow &lt;- function() {\r\n  tid &lt;- badges %&gt;%\r\n    left_join(users, by = join_by(UserId == Id)) %&gt;%\r\n    group_by(UserId, DisplayName) %&gt;%\r\n    summarise(NBadges = n()) %&gt;%\r\n    ungroup() %&gt;%\r\n    slice_max(NBadges, n = 1, with_ties = FALSE) %&gt;%\r\n    pull(UserId)\r\n\r\n  top_user &lt;- users %&gt;%\r\n    filter(Id == tid) %&gt;%\r\n    select(DisplayName, Location)\r\n\r\n  top_user %&gt;% collect()\r\n}\r\n\r\n\r\nb2_duckdb &lt;- function() {\r\n  tid &lt;- badges %&gt;%\r\n    left_join(users, by = join_by(UserId == Id)) %&gt;%\r\n    group_by(UserId, DisplayName) %&gt;%\r\n    summarise(NBadges = n()) %&gt;%\r\n    ungroup() %&gt;%\r\n    slice_max(NBadges, n = 1, with_ties = FALSE) %&gt;%\r\n    pull(UserId)\r\n\r\n  top_user &lt;- users %&gt;%\r\n    filter(Id == tid) %&gt;%\r\n    select(DisplayName, Location)\r\n\r\n  top_user %&gt;% collect()\r\n}</pre>\n<p>These are the results we got:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22248\" style=\"width: 2570px\"><img alt=\"Image 3 - Benchmark #2 results\" aria-describedby=\"caption-attachment-22248\" class=\"size-full wp-image-22248\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-3-Benchmark-2-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 3 - Benchmark #2 results\" aria-describedby=\"caption-attachment-22248\" class=\"size-full wp-image-22248\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-3-Benchmark-2-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22248\">Image 3 – Benchmark #2 results</p></div>\n<p>The truth is – <code>dplyr</code> wasn’t the slowest one here. <code>arrow</code> was still almost twice as fast, but <code>duckdb</code> was three times slower on average.</p>\n<h3>Benchmark #3: Finding the Number of Entries</h3>\n<p>The third test used the <code>wiki</code> dataset to find the number of entries on the article about the city from the previous benchmark on English Wikipedia in March 2022.</p>\n<p>Both <code>dplyr</code> alternatives introduce the <code>collect()</code> method at the end of the calculation and also some additional arguments to the <code>summarise()</code> function:</p>\n<pre>b3_dplyr &lt;- function() {\r\n  city &lt;- str_split(top_user %&gt;% pull(Location), \",\")[[1]][[1]]\r\n  wiki %&gt;%\r\n    filter(curr == city) %&gt;%\r\n    summarise(sum(n))\r\n}\r\n\r\nbr_arrow &lt;- function() {\r\n  city &lt;- str_split(top_user %&gt;% pull(Location), \",\")[[1]][[1]]\r\n  wiki %&gt;%\r\n    filter(curr == city) %&gt;%\r\n    summarise(sum(n)) %&gt;%\r\n    collect()\r\n}\r\n\r\nb3_duckdb &lt;- function() {\r\n  city &lt;- str_split(top_user %&gt;% pull(Location), \",\")[[1]][[1]]\r\n  wiki %&gt;%\r\n    filter(curr == city) %&gt;%\r\n    summarise(sum(n, na.rm = TRUE)) %&gt;%\r\n    collect()\r\n}</pre>\n<p>Here’s what we got out of the tests:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22250\" style=\"width: 2570px\"><img alt=\"Image 4 - Benchmark #3 results\" aria-describedby=\"caption-attachment-22250\" class=\"size-full wp-image-22250\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-4-Benchmark-3-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 4 - Benchmark #3 results\" aria-describedby=\"caption-attachment-22250\" class=\"size-full wp-image-22250\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-4-Benchmark-3-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22250\">Image 4 – Benchmark #3 results</p></div>\n<p>In absolute terms, the difference isn’t that large, but speaking relatively, <code>arrow</code> proved to be around 18 times faster, while <code>duckdb</code> was 14 times faster. Impressive!</p>\n<h3>Benchmark #4: Finding the Most Common Words with a Given Condition</h3>\n<p>The fourth test was once again a combination of two aggregations, one on the <code>posts</code> dataset, and the other on <code>wiki</code> dataset. The first one had the task of finding the most common work with at least 8 characters. The second one then found the number of occurrences of the most common word with at least 8 characters in all posts.</p>\n<p>This is where we see a couple of significant syntax differences among different R data processing frameworks. The <code>dplyr</code> package requires you to write the least amount of code, as you can see below:</p>\n<pre>b4_dplyr &lt;- function() {\r\n  theword &lt;- posts %&gt;%\r\n    select(Body) %&gt;%\r\n    mutate(Body = gsub(\"&lt;.*?&gt;\", \"\", Body)) %&gt;%\r\n    mutate(Body = gsub(\"\\n\", \" \", Body)) %&gt;%\r\n    separate_rows(Body, sep = \" \") %&gt;%\r\n    rename(Words = Body) %&gt;%\r\n    filter(nchar(Words) &gt; 7) %&gt;%\r\n    count(Words) %&gt;%\r\n    slice_max(n, n = 1) %&gt;%\r\n    pull(Words)\r\n\r\n  sum_of_n &lt;- wiki %&gt;%\r\n    filter(curr == str_to_title(theword)) %&gt;%\r\n    summarize(sum_n = sum(n)) %&gt;%\r\n    pull()\r\n  paste(theword, sum_of_n)\r\n}\r\n\r\nb4_arrow &lt;- function() {\r\n  theword &lt;- posts %&gt;%\r\n    select(Body) %&gt;%\r\n    mutate(Body = gsub(\"&lt;.*?&gt;\", \"\", Body)) %&gt;%\r\n    mutate(Body = gsub(\"\\n\", \" \", Body)) %&gt;%\r\n    collect() %&gt;%\r\n    separate_rows(Body, sep = \" \") %&gt;%\r\n    as_arrow_table() %&gt;%\r\n    rename(Words = Body) %&gt;%\r\n    filter(nchar(Words) &gt; 7) %&gt;%\r\n    count(Words) %&gt;%\r\n    slice_max(n, n = 1, with_ties = FALSE) %&gt;%\r\n    pull(Words)\r\n\r\n  sum_of_n &lt;- wiki %&gt;%\r\n    filter(curr == str_to_title(theword)) %&gt;%\r\n    summarize(sum_n = sum(n)) %&gt;%\r\n    pull()\r\n\r\n  paste(theword, sum_of_n)\r\n}\r\n\r\nb4_duckdb &lt;- function() {\r\n  theword &lt;- posts %&gt;%\r\n    select(Body) %&gt;%\r\n    mutate(Body = regexp_replace(Body, \"&lt;.*?&gt;\", \"\", \"g\")) %&gt;%\r\n    mutate(Body = regexp_replace(Body, \"\\n\", \" \")) %&gt;%\r\n    mutate(Body = string_split(Body, \" \")) %&gt;%\r\n    mutate(Body = unnest(Body)) %&gt;%\r\n    mutate(Body = lower(Body)) %&gt;%\r\n    rename(Words = Body) %&gt;%\r\n    filter(nchar(Words) &gt; 7) %&gt;%\r\n    count(Words) %&gt;%\r\n    slice_max(n, n = 1, with_ties = FALSE) %&gt;%\r\n    pull(Words)\r\n  theword &lt;- str_to_title(theword)\r\n  sum_of_n &lt;- wiki %&gt;%\r\n    filter(curr == theword) %&gt;%\r\n    summarize(sum_n = sum(n, na.rm = TRUE)) %&gt;%\r\n    pull()\r\n\r\n  paste(theword, sum_of_n)\r\n}</pre>\n<p>But at what cost? Let’s examine the benchmark results next:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22252\" style=\"width: 2570px\"><img alt=\"Image 5 - Benchmark #4 results\" aria-describedby=\"caption-attachment-22252\" class=\"size-full wp-image-22252\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-5-Benchmark-4-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 5 - Benchmark #4 results\" aria-describedby=\"caption-attachment-22252\" class=\"size-full wp-image-22252\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-5-Benchmark-4-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22252\">Image 5 – Benchmark #4 results</p></div>\n<p>This time, <code>duckdb</code> was a clear winner, outperforming <code>dplyr</code> by a factor of 9. <code>arrow</code> was only slightly faster than <code>dplyr</code>, by around 6 and a half seconds or 25%.</p>\n<h3>Benchmark #5: Finding the Largest Difference in Multiple Datasets</h3>\n<p>This test used the <code>votes</code> and <code>posts</code> datasets to first find the post with the highest difference between upvotes and downvotes, and then find the <code>DisplayName</code> property along with the actual difference between upvotes and downvotes. It’s a lot to take in at once, but most of it boils down to running multiple computations sequentially, and there aren’t many code differences between our three data processing frameworks.</p>\n<p>Feel free to take a look at the code and decide by yourself:</p>\n<pre>b5_dplyr &lt;- function() {\r\n  upvotes &lt;- votes %&gt;%\r\n    filter(VoteTypeId == 2) %&gt;%\r\n    group_by(PostId) %&gt;%\r\n    summarize(UpVotes = n()) %&gt;%\r\n    ungroup()\r\n\r\n  downvotes &lt;- votes %&gt;%\r\n    filter(VoteTypeId == 3) %&gt;%\r\n    group_by(PostId) %&gt;%\r\n    summarize(DownVotes = n()) %&gt;%\r\n    ungroup()\r\n\r\n  posts2 &lt;- posts %&gt;%\r\n    left_join(upvotes, by = c(\"Id\" = \"PostId\")) %&gt;%\r\n    left_join(downvotes, by = c(\"Id\" = \"PostId\")) %&gt;%\r\n    mutate(\r\n      UpVotes = coalesce(UpVotes, 0),\r\n      DownVotes = coalesce(DownVotes, 0)\r\n    ) %&gt;%\r\n    mutate(UpVoteRatio = UpVotes - DownVotes)\r\n\r\n  posts2 %&gt;%\r\n    inner_join(users, by = c(\"OwnerUserId\" = \"Id\")) %&gt;%\r\n    arrange(desc(UpVoteRatio)) %&gt;%\r\n    slice(1) %&gt;%\r\n    select(Score, DisplayName)\r\n}\r\n\r\nb5_arrow &lt;- function() {\r\n  upvotes &lt;- votes %&gt;%\r\n    filter(VoteTypeId == 2) %&gt;%\r\n    group_by(PostId) %&gt;%\r\n    summarize(UpVotes = n()) %&gt;%\r\n    ungroup()\r\n\r\n  downvotes &lt;- votes %&gt;%\r\n    filter(VoteTypeId == 3) %&gt;%\r\n    group_by(PostId) %&gt;%\r\n    summarize(DownVotes = n()) %&gt;%\r\n    ungroup()\r\n\r\n  posts2 &lt;- posts %&gt;%\r\n    left_join(upvotes, by = c(\"Id\" = \"PostId\")) %&gt;%\r\n    left_join(downvotes, by = c(\"Id\" = \"PostId\")) %&gt;%\r\n    mutate(\r\n      UpVotes = coalesce(UpVotes, 0),\r\n      DownVotes = coalesce(DownVotes, 0)\r\n    ) %&gt;%\r\n    mutate(UpVoteRatio = UpVotes - DownVotes)\r\n\r\n  posts2 %&gt;%\r\n    inner_join(users, by = c(\"OwnerUserId\" = \"Id\")) %&gt;%\r\n    slice_max(UpVoteRatio, n = 1, with_ties = FALSE) %&gt;%\r\n    select(Score, DisplayName) %&gt;%\r\n    collect()\r\n}\r\n\r\nb5_duckdb &lt;- function() {\r\n  upvotes &lt;- votes %&gt;%\r\n    filter(VoteTypeId == 2) %&gt;%\r\n    group_by(PostId) %&gt;%\r\n    summarize(UpVotes = n()) %&gt;%\r\n    ungroup()\r\n\r\n  downvotes &lt;- votes %&gt;%\r\n    filter(VoteTypeId == 3) %&gt;%\r\n    group_by(PostId) %&gt;%\r\n    summarize(DownVotes = n()) %&gt;%\r\n    ungroup()\r\n\r\n  posts2 &lt;- posts %&gt;%\r\n    left_join(upvotes, by = c(\"Id\" = \"PostId\")) %&gt;%\r\n    left_join(downvotes, by = c(\"Id\" = \"PostId\")) %&gt;%\r\n    mutate(\r\n      UpVotes = coalesce(UpVotes, 0),\r\n      DownVotes = coalesce(DownVotes, 0)\r\n    ) %&gt;%\r\n    mutate(UpVoteRatio = UpVotes - DownVotes)\r\n\r\n  posts2 %&gt;%\r\n    inner_join(users, by = c(\"OwnerUserId\" = \"Id\")) %&gt;%\r\n    slice_max(UpVoteRatio, n = 1, with_ties = FALSE) %&gt;%\r\n    select(Score, DisplayName) %&gt;%\r\n    collect()\r\n}</pre>\n<p>This is what we got in the end:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22254\" style=\"width: 2570px\"><img alt=\"Image 6 - Benchmark #5 results\" aria-describedby=\"caption-attachment-22254\" class=\"size-full wp-image-22254\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-6-Benchmark-5-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 6 - Benchmark #5 results\" aria-describedby=\"caption-attachment-22254\" class=\"size-full wp-image-22254\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-6-Benchmark-5-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22254\">Image 6 – Benchmark #5 results</p></div>\n<p>Once again, there is not a huge difference in absolute terms. But relatively speaking, <code>arrow</code> was 2.5 faster and <code>duckdb</code> was 7 times faster when compared to <code>dplyr</code>.</p>\n<h3>Benchmark #6: Finding the Month with the Most Posts Created</h3>\n<p>Our next test has a simple task of finding the month in which the most posts were created. That’s it!</p>\n<p>The code differences are almost negligible here – both <code>arrow</code> and <code>duckdb</code> call the <code>collect()</code> method at the end and the order of operation is somewhat different between all three. Nothing you couldn’t change for yourself in a couple of minutes:</p>\n<pre>b6_dplyr &lt;- function() {\r\n  votes %&gt;%\r\n    mutate(CreationDateDT = as.POSIXct(CreationDate)) %&gt;%\r\n    arrange(CreationDateDT) %&gt;%\r\n    group_by(Month = floor_date(CreationDateDT, \"month\")) %&gt;%\r\n    summarize(Count = n()) %&gt;%\r\n    slice_max(Count, n = 1)\r\n}\r\n\r\nb6_arrow &lt;- function() {\r\n  votes %&gt;%\r\n    arrange(CreationDate) %&gt;%\r\n    collect() %&gt;% # seems to be some bug that requires this\r\n    group_by(Month = floor_date(CreationDate, \"month\")) %&gt;%\r\n    summarize(Count = n()) %&gt;%\r\n    slice_max(Count, n = 1, with_ties = FALSE) %&gt;%\r\n    collect()\r\n}\r\n\r\nb6_duckdb &lt;- function() {\r\n  votes %&gt;%\r\n    group_by(Month = floor_date(CreationDate, \"month\")) %&gt;%\r\n    summarize(Count = n()) %&gt;%\r\n    slice_max(Count, n = 1) %&gt;%\r\n    collect()\r\n}</pre>\n<p>Here’s the outcome:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22256\" style=\"width: 2570px\"><img alt=\"Image 7 - Benchmark #6 results\" aria-describedby=\"caption-attachment-22256\" class=\"size-full wp-image-22256\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-7-Benchmark-6-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 7 - Benchmark #6 results\" aria-describedby=\"caption-attachment-22256\" class=\"size-full wp-image-22256\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-7-Benchmark-6-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22256\">Image 7 – Benchmark #6 results</p></div>\n<p><code>duckdb</code> seems to be twice as fast on average when compared to <code>dplyr</code>. On the other hand, <code>arrow</code> was slower on average by a hundredth of a second.</p>\n<h3>Benchmark #7: Finding the Month with the Biggest Decrease in the Amount of Posts</h3>\n<p>The goal of our next test was to find the month in which there was the biggest decrease in the amount of created posts. Simple and straightforward, just like with the previous one.</p>\n<p><code>arrow</code> was the most verbose framework in this test, requiring a couple of extra lines of code and calling the <code>collect()</code> function twice:</p>\n<pre>b7_dplyr &lt;- function() {\r\n  votes %&gt;%\r\n    mutate(CreationDateDT = as.POSIXct(CreationDate)) %&gt;%\r\n    group_by(Month = floor_date(CreationDateDT, \"month\")) %&gt;%\r\n    summarize(VoteCount = n(), .groups = \"drop\") %&gt;%\r\n    mutate(Diff = VoteCount - lag(VoteCount)) %&gt;%\r\n    select(Month, Diff) %&gt;%\r\n    slice_min(Diff, n = 1)\r\n}\r\n\r\nb7_arrow &lt;- function() {\r\n  votes %&gt;%\r\n    group_by(Month = floor_date(CreationDate, \"month\")) %&gt;%\r\n    summarize(VoteCount = n(), .groups = \"drop\") %&gt;%\r\n    collect() %&gt;%\r\n    mutate(Diff = VoteCount - lag(VoteCount)) %&gt;% # lag not supported in arrow\r\n    as_arrow_table() %&gt;%\r\n    arrange(Diff) %&gt;%\r\n    select(Month, Diff) %&gt;%\r\n    slice_head(n = 1) %&gt;%\r\n    collect()\r\n}\r\n\r\nb7_duckdb &lt;- function() {\r\n  votes %&gt;%\r\n    group_by(Month = floor_date(CreationDate, \"month\")) %&gt;%\r\n    summarize(VoteCount = n(), .groups = \"drop\") %&gt;%\r\n    mutate(Diff = VoteCount - lag(VoteCount)) %&gt;%\r\n    select(Month, Diff) %&gt;%\r\n    slice_min(Diff, n = 1) %&gt;%\r\n    collect()\r\n}</pre>\n<p>This is what we got:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22258\" style=\"width: 2570px\"><img alt=\"Image 8 - Benchmark #7 results\" aria-describedby=\"caption-attachment-22258\" class=\"size-full wp-image-22258\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-8-Benchmark-7-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 8 - Benchmark #7 results\" aria-describedby=\"caption-attachment-22258\" class=\"size-full wp-image-22258\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-8-Benchmark-7-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22258\">Image 8 – Benchmark #7 results</p></div>\n<p>Despite the added verbosity, <code>arrow</code> was still twice faster than <code>dplyr</code>. The <code>duckdb</code> framework was somewhere in the middle, still having a slight edge over the vanilla <code>dplyr</code>.</p>\n<h3>Benchmark #8: Finding Common Tags Along Posts by Location</h3>\n<p>For the final test, the goal was to find the most common tag along posts created by users from Poland. To get this information, the <code>Location</code> column in the <code>posts</code> dataset should contain “Poland” or “Polska”.</p>\n<p>The syntax is slightly different between our three data processing frameworks, with <code>arrow</code> being the most verbose one once again:</p>\n<pre>b8_dplyr &lt;- function() {\r\n  tags &lt;- posts %&gt;%\r\n    left_join(users, by = c(\"OwnerUserId\" = \"Id\")) %&gt;%\r\n    filter(str_detect(Location, \"Poland|Polska\")) %&gt;%\r\n    select(Tags) %&gt;%\r\n    mutate(Tags = str_replace_all(Tags, \"[&lt;&gt;]\", \" \")) %&gt;%\r\n    separate_rows(Tags, sep = \" \") %&gt;%\r\n    filter(Tags != \"\")\r\n\r\n  tags %&gt;%\r\n    count(Tags) %&gt;%\r\n    arrange(desc(n)) %&gt;%\r\n    slice(1)\r\n}\r\n\r\nb8_arrow &lt;- function() {\r\n  tags &lt;- posts %&gt;%\r\n    left_join(users, by = c(\"OwnerUserId\" = \"Id\")) %&gt;%\r\n    filter(str_detect(Location, \"Poland|Polska\")) %&gt;%\r\n    select(Tags) %&gt;%\r\n    mutate(Tags = str_replace_all(Tags, \"[&lt;&gt;]\", \" \")) %&gt;%\r\n    collect() %&gt;%\r\n    separate_rows(Tags, sep = \" \") %&gt;%\r\n    as_arrow_table() %&gt;%\r\n    filter(Tags != \"\")\r\n\r\n  tags %&gt;%\r\n    count(Tags) %&gt;%\r\n    slice_max(n, n = 1, with_ties = FALSE) %&gt;%\r\n    collect()\r\n}\r\n\r\nb8_duckdb &lt;- function() {\r\n  tags &lt;- posts %&gt;%\r\n    left_join(users, by = c(\"OwnerUserId\" = \"Id\")) %&gt;%\r\n    filter(str_detect(Location, \"Poland|Polska\")) %&gt;%\r\n    select(Tags) %&gt;%\r\n    mutate(Tags = str_replace_all(Tags, \"[&lt;&gt;]\", \" \")) %&gt;%\r\n    mutate(Tags = string_split(Tags, \" \")) %&gt;%\r\n    mutate(Tags = unnest(Tags)) %&gt;%\r\n    filter(Tags != \"\")\r\n\r\n  tags %&gt;%\r\n    count(Tags) %&gt;%\r\n    slice_max(n, n = 1, with_ties = FALSE) %&gt;%\r\n    collect()\r\n}</pre>\n<p>In the end, these are the time differences:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22260\" style=\"width: 2570px\"><img alt=\"Image 9 - Benchmark #8 results\" aria-describedby=\"caption-attachment-22260\" class=\"size-full wp-image-22260\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-9-Benchmark-8-results-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 9 - Benchmark #8 results\" aria-describedby=\"caption-attachment-22260\" class=\"size-full wp-image-22260\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-9-Benchmark-8-results-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22260\">Image 9 – Benchmark #8 results</p></div>\n<p>But this time, it was <code>duckdb</code> that was almost two times faster than <code>dplyr</code>. The <code>arrow</code> framework provided somewhat of a negligible 10% runtime decrease when compared to <code>dplyr</code>.</p>\n<h3>Conclusion: Which R Data Processing Framework Should You Use?</h3>\n<p>In the end, let’s imagine all of our 8 benchmarks make a single data processing pipeline. The question is – <b>Which R data processing framework wins in total?</b> Total here being the summation of average (of 3) runtimes for each benchmark.</p>\n<p>Here are the results:</p>\n<div class=\"wp-caption alignnone\" id=\"attachment_22262\" style=\"width: 2570px\"><img alt=\"Image 10 - Total runtime comparison\" aria-describedby=\"caption-attachment-22262\" class=\"size-full wp-image-22262\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-10-Total-runtime-comparison-scaled.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"450\"/><noscript><img alt=\"Image 10 - Total runtime comparison\" aria-describedby=\"caption-attachment-22262\" class=\"size-full wp-image-22262\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2023/12/Image-10-Total-runtime-comparison-scaled.webp\" width=\"450\"/></noscript><p class=\"wp-caption-text\" id=\"caption-attachment-22262\">Image 10 – Total runtime comparison</p></div>\n<p>It’s clear to say that <code>duckdb</code> won by a huge margin – it’s 7.4 times faster when compared to <code>dplyr</code> and 4.1 times faster when compared to <code>arrow</code>. This is mostly because of benchmark #4 results, in which <code>duckdb</code> won by 20-30 seconds in absolute terms.</p>\n<p>Still, it makes sense to compare all three in your work environment to find out which data processing framework is the fastest for your specific needs. Now you know how, so you shouldn’t have any trouble cutting down the data processing runtime by a significant factor.</p>\n<hr/>\n<h2 id=\"summary\">Summing up R Data Processing Framework Benchmarks</h2>\n<p>Long story short – it takes minimal effort (code changes) to massively speed up your data processing pipelines in R. The <code>dplyr</code> package is just fine when you’re just starting out, but you should look into the alternatives mentioned today when speed is of the essence. Spoiler alert, it always is.</p>\n<p>But switching between R data processing frameworks doesn’t have to be a long an painful experience. Packages like <code>arrow</code> and <code>duckdb</code> use the same <code>dplyr</code> interface but provide much faster results. You can change the backend in a matter of minutes, or hours in a worst-case scenario if you have a lot of data processing pipelines.</p>\n<p><i>What’s your go-to way of speeding up <code>dplyr</code>? Do you use the packages mentioned today or something else entirely?</i> Make sure to let us know in the comment section below.</p>\n<blockquote><p>Looking to automate data quality reporting in R and R Shiny? <a href=\"https://appsilon.com/automated-r-data-quality-reporting/\" rel=\"nofollow\" target=\"_blank\">Look no further than R’s data.validator package</a>.</p></blockquote>\n<p>The post appeared first on appsilon.com/blog/.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://appsilon.com/r-data-processing-frameworks/\"> Tag: r - Appsilon | Enterprise R Shiny Dashboards</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </hr></div>\n</article>",
    "main_text": "R Data Processing Frameworks: How To Speed Up Your Data Processing Pipelines up to 20 Times\nPosted on\nDecember 5, 2023\nby\nDario Radečić\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nTag: r - Appsilon | Enterprise R Shiny Dashboards\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nPicture this – the data science team you manage primarily uses R and heavily relies on\ndplyr\nfor implementing data processing pipelines. All is good, but then out of the blue you’re working with a client that has a massive dataset, and all of a sudden\ndplyr\nbecomes the bottleneck. You want a faster way to process data with minimum to no changes to the source code.\nYou’re wondering, how much effort will it take to speed up your data processing pipelines 2 times, 5 times, 10 times, or even up to 20 times? The answer might surprise you –\nfar less than you think\n.\nThis article will show you why\ndplyr\nfails to perform on larger datasets, and which alternative options you have in the context of R data processing frameworks. Let’s dig in!\nR is full of things you didn’t know are possible –\nHere’s a couple of advanced Excel functions in R for data manipulation\n.\nTable of contents:\nDplyr Alternatives – Top R Data Processing Frameworks\nDplyr vs. Arrow vs. DuckDB – R Data Processing Framework Test\nSumming up R Data Processing Framework Benchmarks\nDplyr Alternatives – Top R Data Processing Frameworks\nThis section will introduce you to two\ndplyr\nalternatives that use the same interface –\narrow\nand\nduckdb\n. We wanted to focus on these two specifically because they come up with minimal code changes, as you’ll see from the examples later. They provide the best “bang for the buck” if your data science team doesn’t have the time to learn a new data processing framework from scratch.\nWhich R dplyr Alternatives Can You Use?\nSure, everyone loves\ndplyr\n. We’ve even dedicated a full article for\nbeginner-level data analysis techniques with dplyr\n. The package has a user-friendly syntax and is super easy to use for data transformation tasks. But guess what – so are the other two alternatives we’ll use today. In addition, they are usually up to 20 times faster.\nArrow\nis a cross-language development platform for in-memory and larger-than-memory data. The R package exposes an interface to the Arrow C++ library, allowing you to benefit from an R programming language syntax, and access to a C++ library API through a set of well-known\ndplyr\nbackend functions. Arrow also provides zero-copy data sharing between R and Python, which might be appealing for language-agnostic data science teams.\nYou can learn more about Arrow for R\nhere\n.\nDuckDB\nis an open-source, embedded, in-process, relational OLAP DBMS. Its description contains pretty much every buzzword you could imagine, but it being an OLAP database means the data is organized by columns and is optimized for complex data queries (think joins, aggregations, groupings, and so on). The good thing about\nduckdb\nis that it comes with an R API, meaning you can use R to point to a local (or remote) instance of your database with\nDBI\n. Further,\nduckdb\nR package uses\ndplyr\n-like syntax, which means code changes coming from a vanilla\ndplyr\nwill be minimal to non-existent.\nYou can learn more about DuckDB\nhere\nand about its R API\nhere\n.\nSo, these are the\ndplyr\nalternatives we’ll use to perform a series of benchmarks with the goal of comparing R data processing frameworks. They should both be faster than\ndplyr\nin most cases, but that’s where the actual tests come in. More on that in the following section.\nDplyr vs. Arrow vs. DuckDB – R Data Processing Framework Test\nTo kick things off, let’s talk about data. We have several small to somewhat large Parquet files and a single DuckDB database (not publicly available) that has other files stored inside. The\ndplyr\nand\narrow\nbenchmarks will be based on the Parquet files, and the DuckDB benchmarks will be connected to the local database:\nImage 1 – Datasets used\nNext, we’ll discuss how the benchmarks were configured, R package versions, and the hardware used to run the tests.\nBenchmark Setup and Information\nAs for R itself, we’ve used\nR version 4.3.1\n. The most important packages were installed with the following version numbers:\ndplyr\n– 1.1.3\narrow\n– 13.0.0\nduckdb\n– 0.8.1-3\nEach of the benchmark tests you’ll see below was run\n3 times\nfor each R data processing framework.\nThe hardware of choice was a\n14″ M2 Pro MacBook Pro\nwith a 12-core CPU and 16 GB of RAM.\nFor working on\ndplyr\nand\narrow\nbenchmarks, we’ve imported the following R packages:\nlibrary(tidyverse)\nlibrary(arrow)\nThe\narrow\nbenchmark results have had the following option configured:\noptions(arrow.pull_as_vector = TRUE)\nWorking with\nduckdb\nrequired a couple of extra dependencies:\nlibrary(DBI)\nlibrary(duckdb)\nIn both of these, the following code was used to load the datasets:\nbadges <- read_parquet(\"../../data/badges.parquet\")\nposts <- read_parquet(\"../../data/posts.parquet\")\ntags <- read_parquet(\"../../data/tags.parquet\")\nusers <- read_parquet(\"../../data/users.parquet\")\nvotes <- read_parquet(\"../../data/votes.parquet\")\nwiki <- read_parquet(\"../../data/wiki.parquet\")\nIn order to connect to a DuckDB database and extract the datasets, we’ve used the following code:\ncon <- dbConnect(duckdb::duckdb(\"./data.duckdb\"))\n\nbadges <- tbl(con, \"badges\")\nposts <- tbl(con, \"posts\")\ntags <- tbl(con, \"tags\")\nusers <- tbl(con, \"users\")\nvotes <- tbl(con, \"votes\")\nwiki <- tbl(con, \"wiki\")\nFinally, to actually run benchmarks, we decided to declare a\nbenchmark()\nfunction which takes another function as an argument:\nbenchmark <- function(fun) {\n    start <- Sys.time()\n    res <- fun()\n    end <- Sys.time()\n    print(end - start)\n    res\n}\nEach of the 8 benchmarks you’ll see below wraps the logic inside a separate function and then calls\nbenchmark()\nand passes itself as an argument.\nSo with that out of the way, let’s go over our first benchmark!\nBenchmark #1: Finding the Article with the Most External Entries\nThe goal of the first benchmark was to use the\nwiki\ndataset to find the article with the most external entries on English Wikipedia in March 2022.\nYou’ll find the code for all three R packages below. The only difference from vanilla\ndplyr\nis in passing some additional arguments to\nsummarise()\nand\nslice_max()\nfunctions. Everything else is identical:\nb1_dplyr <- function() {\n  wiki %>% \n    filter(type == \"external\") %>%\n    group_by(curr) %>%\n    summarise(total = sum(n)) %>%\n    slice_max(total, n = 3) %>%\n    pull(curr)\n}\n\nb1_arrow <- function() {\n  wiki %>% \n    filter(type == \"external\") %>%\n    group_by(curr) %>%\n    summarise(total = sum(n)) %>%\n    slice_max(total, n = 3, with_ties = FALSE) %>%\n    pull(curr)\n}\n\nb1_duckdb <- function() {\n  wiki %>% \n    filter(type == \"external\") %>%\n    group_by(curr) %>%\n    summarise(total = sum(n, na.rm = TRUE)) %>%\n    slice_max(total, n = 3, with_ties = FALSE) %>%\n    pull(curr)\n}\nHere are the results:\nImage 2 – Benchmark #1 results\nAs you can see, both\narrow\nand\nduckdb\nwere faster, by 10 and 7 times, respectively.\nBenchmark #2: Finding Properties Across Multiple Datasets\nThe second test was a combination of two calculations – the first one had the task of finding the\nDisplayName\nproperty of a user that has the most badges, while the second one had to find the\nLocation\nproperty for the same condition. Most of the logic is implemented in the first portion of the calculation, where the ID of the user was found, and then in the second portion, only the desired properties were extracted.\nAs before, most of the code differences boil down to the extra arguments in a couple of functions and calling\ncollect()\nat the end of the calculation:\nb2_dplyr <- function() {\n  tid <- badges %>%\n    left_join(users, by = join_by(UserId == Id)) %>%\n    group_by(UserId, DisplayName) %>%\n    summarise(NBadges = n()) %>%\n    ungroup() %>%\n    slice_max(NBadges, n = 1) %>%\n    pull(UserId)\n  top_user <- users %>%\n    filter(Id == tid) %>%\n    select(DisplayName, Location)\n\n  top_user\n}\n\nb2_arrow <- function() {\n  tid <- badges %>%\n    left_join(users, by = join_by(UserId == Id)) %>%\n    group_by(UserId, DisplayName) %>%\n    summarise(NBadges = n()) %>%\n    ungroup() %>%\n    slice_max(NBadges, n = 1, with_ties = FALSE) %>%\n    pull(UserId)\n\n  top_user <- users %>%\n    filter(Id == tid) %>%\n    select(DisplayName, Location)\n\n  top_user %>% collect()\n}\n\nb2_duckdb <- function() {\n  tid <- badges %>%\n    left_join(users, by = join_by(UserId == Id)) %>%\n    group_by(UserId, DisplayName) %>%\n    summarise(NBadges = n()) %>%\n    ungroup() %>%\n    slice_max(NBadges, n = 1, with_ties = FALSE) %>%\n    pull(UserId)\n\n  top_user <- users %>%\n    filter(Id == tid) %>%\n    select(DisplayName, Location)\n\n  top_user %>% collect()\n}\nThese are the results we got:\nImage 3 – Benchmark #2 results\nThe truth is –\ndplyr\nwasn’t the slowest one here.\narrow\nwas still almost twice as fast, but\nduckdb\nwas three times slower on average.\nBenchmark #3: Finding the Number of Entries\nThe third test used the\nwiki\ndataset to find the number of entries on the article about the city from the previous benchmark on English Wikipedia in March 2022.\nBoth\ndplyr\nalternatives introduce the\ncollect()\nmethod at the end of the calculation and also some additional arguments to the\nsummarise()\nfunction:\nb3_dplyr <- function() {\n  city <- str_split(top_user %>% pull(Location), \",\")[[1]][[1]]\n  wiki %>%\n    filter(curr == city) %>%\n    summarise(sum(n))\n}\n\nbr_arrow <- function() {\n  city <- str_split(top_user %>% pull(Location), \",\")[[1]][[1]]\n  wiki %>%\n    filter(curr == city) %>%\n    summarise(sum(n)) %>%\n    collect()\n}\n\nb3_duckdb <- function() {\n  city <- str_split(top_user %>% pull(Location), \",\")[[1]][[1]]\n  wiki %>%\n    filter(curr == city) %>%\n    summarise(sum(n, na.rm = TRUE)) %>%\n    collect()\n}\nHere’s what we got out of the tests:\nImage 4 – Benchmark #3 results\nIn absolute terms, the difference isn’t that large, but speaking relatively,\narrow\nproved to be around 18 times faster, while\nduckdb\nwas 14 times faster. Impressive!\nBenchmark #4: Finding the Most Common Words with a Given Condition\nThe fourth test was once again a combination of two aggregations, one on the\nposts\ndataset, and the other on\nwiki\ndataset. The first one had the task of finding the most common work with at least 8 characters. The second one then found the number of occurrences of the most common word with at least 8 characters in all posts.\nThis is where we see a couple of significant syntax differences among different R data processing frameworks. The\ndplyr\npackage requires you to write the least amount of code, as you can see below:\nb4_dplyr <- function() {\n  theword <- posts %>%\n    select(Body) %>%\n    mutate(Body = gsub(\"<.*?>\", \"\", Body)) %>%\n    mutate(Body = gsub(\"\\n\", \" \", Body)) %>%\n    separate_rows(Body, sep = \" \") %>%\n    rename(Words = Body) %>%\n    filter(nchar(Words) > 7) %>%\n    count(Words) %>%\n    slice_max(n, n = 1) %>%\n    pull(Words)\n\n  sum_of_n <- wiki %>%\n    filter(curr == str_to_title(theword)) %>%\n    summarize(sum_n = sum(n)) %>%\n    pull()\n  paste(theword, sum_of_n)\n}\n\nb4_arrow <- function() {\n  theword <- posts %>%\n    select(Body) %>%\n    mutate(Body = gsub(\"<.*?>\", \"\", Body)) %>%\n    mutate(Body = gsub(\"\\n\", \" \", Body)) %>%\n    collect() %>%\n    separate_rows(Body, sep = \" \") %>%\n    as_arrow_table() %>%\n    rename(Words = Body) %>%\n    filter(nchar(Words) > 7) %>%\n    count(Words) %>%\n    slice_max(n, n = 1, with_ties = FALSE) %>%\n    pull(Words)\n\n  sum_of_n <- wiki %>%\n    filter(curr == str_to_title(theword)) %>%\n    summarize(sum_n = sum(n)) %>%\n    pull()\n\n  paste(theword, sum_of_n)\n}\n\nb4_duckdb <- function() {\n  theword <- posts %>%\n    select(Body) %>%\n    mutate(Body = regexp_replace(Body, \"<.*?>\", \"\", \"g\")) %>%\n    mutate(Body = regexp_replace(Body, \"\\n\", \" \")) %>%\n    mutate(Body = string_split(Body, \" \")) %>%\n    mutate(Body = unnest(Body)) %>%\n    mutate(Body = lower(Body)) %>%\n    rename(Words = Body) %>%\n    filter(nchar(Words) > 7) %>%\n    count(Words) %>%\n    slice_max(n, n = 1, with_ties = FALSE) %>%\n    pull(Words)\n  theword <- str_to_title(theword)\n  sum_of_n <- wiki %>%\n    filter(curr == theword) %>%\n    summarize(sum_n = sum(n, na.rm = TRUE)) %>%\n    pull()\n\n  paste(theword, sum_of_n)\n}\nBut at what cost? Let’s examine the benchmark results next:\nImage 5 – Benchmark #4 results\nThis time,\nduckdb\nwas a clear winner, outperforming\ndplyr\nby a factor of 9.\narrow\nwas only slightly faster than\ndplyr\n, by around 6 and a half seconds or 25%.\nBenchmark #5: Finding the Largest Difference in Multiple Datasets\nThis test used the\nvotes\nand\nposts\ndatasets to first find the post with the highest difference between upvotes and downvotes, and then find the\nDisplayName\nproperty along with the actual difference between upvotes and downvotes. It’s a lot to take in at once, but most of it boils down to running multiple computations sequentially, and there aren’t many code differences between our three data processing frameworks.\nFeel free to take a look at the code and decide by yourself:\nb5_dplyr <- function() {\n  upvotes <- votes %>%\n    filter(VoteTypeId == 2) %>%\n    group_by(PostId) %>%\n    summarize(UpVotes = n()) %>%\n    ungroup()\n\n  downvotes <- votes %>%\n    filter(VoteTypeId == 3) %>%\n    group_by(PostId) %>%\n    summarize(DownVotes = n()) %>%\n    ungroup()\n\n  posts2 <- posts %>%\n    left_join(upvotes, by = c(\"Id\" = \"PostId\")) %>%\n    left_join(downvotes, by = c(\"Id\" = \"PostId\")) %>%\n    mutate(\n      UpVotes = coalesce(UpVotes, 0),\n      DownVotes = coalesce(DownVotes, 0)\n    ) %>%\n    mutate(UpVoteRatio = UpVotes - DownVotes)\n\n  posts2 %>%\n    inner_join(users, by = c(\"OwnerUserId\" = \"Id\")) %>%\n    arrange(desc(UpVoteRatio)) %>%\n    slice(1) %>%\n    select(Score, DisplayName)\n}\n\nb5_arrow <- function() {\n  upvotes <- votes %>%\n    filter(VoteTypeId == 2) %>%\n    group_by(PostId) %>%\n    summarize(UpVotes = n()) %>%\n    ungroup()\n\n  downvotes <- votes %>%\n    filter(VoteTypeId == 3) %>%\n    group_by(PostId) %>%\n    summarize(DownVotes = n()) %>%\n    ungroup()\n\n  posts2 <- posts %>%\n    left_join(upvotes, by = c(\"Id\" = \"PostId\")) %>%\n    left_join(downvotes, by = c(\"Id\" = \"PostId\")) %>%\n    mutate(\n      UpVotes = coalesce(UpVotes, 0),\n      DownVotes = coalesce(DownVotes, 0)\n    ) %>%\n    mutate(UpVoteRatio = UpVotes - DownVotes)\n\n  posts2 %>%\n    inner_join(users, by = c(\"OwnerUserId\" = \"Id\")) %>%\n    slice_max(UpVoteRatio, n = 1, with_ties = FALSE) %>%\n    select(Score, DisplayName) %>%\n    collect()\n}\n\nb5_duckdb <- function() {\n  upvotes <- votes %>%\n    filter(VoteTypeId == 2) %>%\n    group_by(PostId) %>%\n    summarize(UpVotes = n()) %>%\n    ungroup()\n\n  downvotes <- votes %>%\n    filter(VoteTypeId == 3) %>%\n    group_by(PostId) %>%\n    summarize(DownVotes = n()) %>%\n    ungroup()\n\n  posts2 <- posts %>%\n    left_join(upvotes, by = c(\"Id\" = \"PostId\")) %>%\n    left_join(downvotes, by = c(\"Id\" = \"PostId\")) %>%\n    mutate(\n      UpVotes = coalesce(UpVotes, 0),\n      DownVotes = coalesce(DownVotes, 0)\n    ) %>%\n    mutate(UpVoteRatio = UpVotes - DownVotes)\n\n  posts2 %>%\n    inner_join(users, by = c(\"OwnerUserId\" = \"Id\")) %>%\n    slice_max(UpVoteRatio, n = 1, with_ties = FALSE) %>%\n    select(Score, DisplayName) %>%\n    collect()\n}\nThis is what we got in the end:\nImage 6 – Benchmark #5 results\nOnce again, there is not a huge difference in absolute terms. But relatively speaking,\narrow\nwas 2.5 faster and\nduckdb\nwas 7 times faster when compared to\ndplyr\n.\nBenchmark #6: Finding the Month with the Most Posts Created\nOur next test has a simple task of finding the month in which the most posts were created. That’s it!\nThe code differences are almost negligible here – both\narrow\nand\nduckdb\ncall the\ncollect()\nmethod at the end and the order of operation is somewhat different between all three. Nothing you couldn’t change for yourself in a couple of minutes:\nb6_dplyr <- function() {\n  votes %>%\n    mutate(CreationDateDT = as.POSIXct(CreationDate)) %>%\n    arrange(CreationDateDT) %>%\n    group_by(Month = floor_date(CreationDateDT, \"month\")) %>%\n    summarize(Count = n()) %>%\n    slice_max(Count, n = 1)\n}\n\nb6_arrow <- function() {\n  votes %>%\n    arrange(CreationDate) %>%\n    collect() %>% # seems to be some bug that requires this\n    group_by(Month = floor_date(CreationDate, \"month\")) %>%\n    summarize(Count = n()) %>%\n    slice_max(Count, n = 1, with_ties = FALSE) %>%\n    collect()\n}\n\nb6_duckdb <- function() {\n  votes %>%\n    group_by(Month = floor_date(CreationDate, \"month\")) %>%\n    summarize(Count = n()) %>%\n    slice_max(Count, n = 1) %>%\n    collect()\n}\nHere’s the outcome:\nImage 7 – Benchmark #6 results\nduckdb\nseems to be twice as fast on average when compared to\ndplyr\n. On the other hand,\narrow\nwas slower on average by a hundredth of a second.\nBenchmark #7: Finding the Month with the Biggest Decrease in the Amount of Posts\nThe goal of our next test was to find the month in which there was the biggest decrease in the amount of created posts. Simple and straightforward, just like with the previous one.\narrow\nwas the most verbose framework in this test, requiring a couple of extra lines of code and calling the\ncollect()\nfunction twice:\nb7_dplyr <- function() {\n  votes %>%\n    mutate(CreationDateDT = as.POSIXct(CreationDate)) %>%\n    group_by(Month = floor_date(CreationDateDT, \"month\")) %>%\n    summarize(VoteCount = n(), .groups = \"drop\") %>%\n    mutate(Diff = VoteCount - lag(VoteCount)) %>%\n    select(Month, Diff) %>%\n    slice_min(Diff, n = 1)\n}\n\nb7_arrow <- function() {\n  votes %>%\n    group_by(Month = floor_date(CreationDate, \"month\")) %>%\n    summarize(VoteCount = n(), .groups = \"drop\") %>%\n    collect() %>%\n    mutate(Diff = VoteCount - lag(VoteCount)) %>% # lag not supported in arrow\n    as_arrow_table() %>%\n    arrange(Diff) %>%\n    select(Month, Diff) %>%\n    slice_head(n = 1) %>%\n    collect()\n}\n\nb7_duckdb <- function() {\n  votes %>%\n    group_by(Month = floor_date(CreationDate, \"month\")) %>%\n    summarize(VoteCount = n(), .groups = \"drop\") %>%\n    mutate(Diff = VoteCount - lag(VoteCount)) %>%\n    select(Month, Diff) %>%\n    slice_min(Diff, n = 1) %>%\n    collect()\n}\nThis is what we got:\nImage 8 – Benchmark #7 results\nDespite the added verbosity,\narrow\nwas still twice faster than\ndplyr\n. The\nduckdb\nframework was somewhere in the middle, still having a slight edge over the vanilla\ndplyr\n.\nBenchmark #8: Finding Common Tags Along Posts by Location\nFor the final test, the goal was to find the most common tag along posts created by users from Poland. To get this information, the\nLocation\ncolumn in the\nposts\ndataset should contain “Poland” or “Polska”.\nThe syntax is slightly different between our three data processing frameworks, with\narrow\nbeing the most verbose one once again:\nb8_dplyr <- function() {\n  tags <- posts %>%\n    left_join(users, by = c(\"OwnerUserId\" = \"Id\")) %>%\n    filter(str_detect(Location, \"Poland|Polska\")) %>%\n    select(Tags) %>%\n    mutate(Tags = str_replace_all(Tags, \"[<>]\", \" \")) %>%\n    separate_rows(Tags, sep = \" \") %>%\n    filter(Tags != \"\")\n\n  tags %>%\n    count(Tags) %>%\n    arrange(desc(n)) %>%\n    slice(1)\n}\n\nb8_arrow <- function() {\n  tags <- posts %>%\n    left_join(users, by = c(\"OwnerUserId\" = \"Id\")) %>%\n    filter(str_detect(Location, \"Poland|Polska\")) %>%\n    select(Tags) %>%\n    mutate(Tags = str_replace_all(Tags, \"[<>]\", \" \")) %>%\n    collect() %>%\n    separate_rows(Tags, sep = \" \") %>%\n    as_arrow_table() %>%\n    filter(Tags != \"\")\n\n  tags %>%\n    count(Tags) %>%\n    slice_max(n, n = 1, with_ties = FALSE) %>%\n    collect()\n}\n\nb8_duckdb <- function() {\n  tags <- posts %>%\n    left_join(users, by = c(\"OwnerUserId\" = \"Id\")) %>%\n    filter(str_detect(Location, \"Poland|Polska\")) %>%\n    select(Tags) %>%\n    mutate(Tags = str_replace_all(Tags, \"[<>]\", \" \")) %>%\n    mutate(Tags = string_split(Tags, \" \")) %>%\n    mutate(Tags = unnest(Tags)) %>%\n    filter(Tags != \"\")\n\n  tags %>%\n    count(Tags) %>%\n    slice_max(n, n = 1, with_ties = FALSE) %>%\n    collect()\n}\nIn the end, these are the time differences:\nImage 9 – Benchmark #8 results\nBut this time, it was\nduckdb\nthat was almost two times faster than\ndplyr\n. The\narrow\nframework provided somewhat of a negligible 10% runtime decrease when compared to\ndplyr\n.\nConclusion: Which R Data Processing Framework Should You Use?\nIn the end, let’s imagine all of our 8 benchmarks make a single data processing pipeline. The question is –\nWhich R data processing framework wins in total?\nTotal here being the summation of average (of 3) runtimes for each benchmark.\nHere are the results:\nImage 10 – Total runtime comparison\nIt’s clear to say that\nduckdb\nwon by a huge margin – it’s 7.4 times faster when compared to\ndplyr\nand 4.1 times faster when compared to\narrow\n. This is mostly because of benchmark #4 results, in which\nduckdb\nwon by 20-30 seconds in absolute terms.\nStill, it makes sense to compare all three in your work environment to find out which data processing framework is the fastest for your specific needs. Now you know how, so you shouldn’t have any trouble cutting down the data processing runtime by a significant factor.\nSumming up R Data Processing Framework Benchmarks\nLong story short – it takes minimal effort (code changes) to massively speed up your data processing pipelines in R. The\ndplyr\npackage is just fine when you’re just starting out, but you should look into the alternatives mentioned today when speed is of the essence. Spoiler alert, it always is.\nBut switching between R data processing frameworks doesn’t have to be a long an painful experience. Packages like\narrow\nand\nduckdb\nuse the same\ndplyr\ninterface but provide much faster results. You can change the backend in a matter of minutes, or hours in a worst-case scenario if you have a lot of data processing pipelines.\nWhat’s your go-to way of speeding up\ndplyr\n? Do you use the packages mentioned today or something else entirely?\nMake sure to let us know in the comment section below.\nLooking to automate data quality reporting in R and R Shiny?\nLook no further than R’s data.validator package\n.\nThe post appeared first on appsilon.com/blog/.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nTag: r - Appsilon | Enterprise R Shiny Dashboards\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "Picture this – the data science team you manage primarily uses R and heavily relies on dplyr for implementing data processing pipelines. All is good, but then out of the blue you’re working with a client that has a massive dataset, and all of a sudden dplyr becomes the bottleneck. You want a faster way […] The post appeared first on appsilon.com/blog/.",
    "meta_keywords": null,
    "og_description": "Picture this – the data science team you manage primarily uses R and heavily relies on dplyr for implementing data processing pipelines. All is good, but then out of the blue you’re working with a client that has a massive dataset, and all of a sudden dplyr becomes the bottleneck. You want a faster way […] The post appeared first on appsilon.com/blog/.",
    "og_image": "https://wordpress.appsilon.com/wp-content/uploads/2023/12/R-Processing.webp",
    "og_title": "R Data Processing Frameworks: How To Speed Up Your Data Processing Pipelines up to 20 Times | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 16.2,
    "sitemap_lastmod": "2023-12-05T10:34:33+00:00",
    "twitter_description": "Picture this – the data science team you manage primarily uses R and heavily relies on dplyr for implementing data processing pipelines. All is good, but then out of the blue you’re working with a client that has a massive dataset, and all of a sudden dplyr becomes the bottleneck. You want a faster way […] The post appeared first on appsilon.com/blog/.",
    "twitter_title": "R Data Processing Frameworks: How To Speed Up Your Data Processing Pipelines up to 20 Times | R-bloggers",
    "url": "https://www.r-bloggers.com/2023/12/r-data-processing-frameworks-how-to-speed-up-your-data-processing-pipelines-up-to-20-times/",
    "word_count": 3243
  }
}