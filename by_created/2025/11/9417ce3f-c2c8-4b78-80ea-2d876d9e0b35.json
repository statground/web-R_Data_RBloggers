{
  "uuid": "9417ce3f-c2c8-4b78-80ea-2d876d9e0b35",
  "created_at": "2025-11-22 19:59:35",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2024/12/the-life-changing-magic-of-tidying-text-files-2/",
    "crawled_at": "2025-11-22T10:56:36.379856",
    "external_links": [
      {
        "href": "https://www.johnmackintosh.net/blog/rstats/2024-12-22-tidying-text-files/",
        "text": "Data By John"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://github.com/johnmackintosh/tidy-scotland-census",
        "text": "tidy_scotland_census"
      },
      {
        "href": "https://www.johnmackintosh.net/blog/rstats/2024-12-22-tidying-text-files/",
        "text": "Data By John"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "The life changing magic of tidying text files | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/john-mackintosh/",
        "text": "John MacKintosh"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-391816 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">The life changing magic of tidying text files</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">December 21, 2024</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/john-mackintosh/\">John MacKintosh</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.johnmackintosh.net/blog/rstats/2024-12-22-tidying-text-files/\"> Data By John</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>Our team have been doing some work with the Scotland Census 2022 data. There are several ways to download the information – you can click around on maps or use a table builder to focus on specifics, or there is a large zip download that provides all the data in CSV format. You end up with 71 files, with around 46K rows and a variable number of columns.</p>\n<ul>\n<li>The first 3 rows of each file contain generic information about the dataset and can be discarded for analysis. Because these are of varying widths, various file readers may trip up when reading them in. <code>data.table</code> suggests using <code>fill  = TRUE</code> when using <code>fread</code>, but that causes immediate failure in some cases.</li>\n<li>The last 8 rows contain text that can also be discarded. (In truth, these rows never got read in because the single column threw <code>fread</code>, which was a blessing in disguise)</li>\n<li>Once these rows have been discarded, many files have headers in multiple rows which need to be extracted, combined, and the added back as column headers.</li>\n<li>Need to account for having between 0-5 rows of column headers, with some blank rows in between, usually around line 4 or 5</li>\n<li>Some files have extra delimiters in the first 3 rows</li>\n</ul>\n<p>Obviously, for one or two files on an ad-hoc basis, you can get around this by hand, or other nefarious means. \nDoing it programatically is another issue. It’s just the right kind of problem – tricky enough so that you can’t stop thinking about it, and easy enough that you can actually achieve something.</p>\n<p>My initial approach involved 2 reads per file – I read the file in and saved as a temp file,  then used <code>scan</code> on the temp file to find the first Output Area code in the first column – this is the first row of data.\nThen I created some vectors of indices for where the data began, and where I thought the actual first line of header rows were, after skipping the first 3 rows.</p>\n<p>I tried using <code>{vroom}</code>. For this to work I needed to provide a <code>skip</code> value and set <code>col_names</code> to FALSE. There was no way to get an accurate skip value without doing a prior read or scan.</p>\n<p>Then I decided to go back to <code>fread</code> and not skip anything, set <code>header</code> to <code>FALSE</code>, and perform only one read.  data.table was smart enough to strip out the first three rows anyway, so I was left with the multiple rows containing the column headers right at the start of the table.</p>\n<p>I skimmed those off using <code>grep</code> to find the first output area, and subtracting 1 to get the correct number of header rows</p>\n<pre># find the row with the start_target value, and retrieve all the rows above it\n\nheaders &lt;- int_dt[,head(.SD,grep(start_target,V1) - 1L)]\n\n</pre>\n<p>Using tail on the data, with a negative index to account for the number of header rows, gave me the actual data. I just used <code>dim</code> of the headers data.table to get the number of rows, to save performing another <code>grep</code></p>\n<pre># remove the first n header rows - the rest of the rows are the data we need to process\n  int_dt &lt;- int_dt[,tail(.SD, -dim(headers)[1])]\n\n</pre>\n<p>After that, it was a matter of combining the headers rows and collapsing them into a character vector and setting those as the column names. Then I pivoted the data into long format, copied the value column, replaced hyphens with <code>NA</code>, and coerced to numeric. I added in options to write the file out, or to print, or to return it in case further processing was required.</p>\n<p>Here is how I used data.table’s set operation to remove instances of 2 or more underscores in the <code>variable</code> column. Note the use of <code>.I</code> to return an integer vector of rows to update</p>\n<pre>\n# replace any multiple underscores in variable column\n\n  col_name &lt;- \"variable\"\n  rows_to_change &lt;- out_dt[variable %like% \"_{2,}\",.I]\n  \n  set(out_dt, i = rows_to_change, j = col_name,\n     value =  stri_replace_all_regex(out_dt[[col_name]][rows_to_change],\n     pattern = \"_{2,}\",\n     replacement = \"\"))\n\n\n</pre>\n<p>As this is data at a very small geographic level, for all of Scotland, we don’t want to be writing these out to a CSV file (although, my function saves them as .TSV by default). \nI used the <code>arrow</code> package to write them to parquet. And, used <code>duckdb</code> to create a duckdb database.</p>\n<p>The code for all this is on my github here <a href=\"https://github.com/johnmackintosh/tidy-scotland-census\" rel=\"nofollow\" target=\"_blank\">tidy_scotland_census</a></p>\n<p>Further developments would be to filter this for specific areas - I am only really interested in Highland and Argyll and Bute- however I’ve left this for now so the code should be of use to anyone who wants to use it.</p>\n<p>There is some example code of how to use the function with purrr to write the files, or view the outputs in tidy format. \nYou could also stick them in a nested list (1.7 GB), but my immediate reaction to doing that is to try and get it straight back out again. I do recommend using purrr’s <code>safely</code> function for this sort of thing.</p>\n<p>Having sorted out the approach, I spent some time trying to make things a bit faster. Using <code>gsub</code> was slowing things down, so I replaced that with some <code>stringi</code>. Coercing to numeric also took some time, but even using a <code>set</code> approach in data.table did not speed things up. That was because I was creating a vector of indices to pass to the set syntax (<code>for j in cols</code>), and it was pretty slow operation. Switching back to subsetting and using <code>let</code> to update by reference was much faster. I’m not sure this should generally be the case, but I tried both methods with several files. I used the <code>profvis</code> package to figure out where the bottlenecks were, and it was very handy to confirm my original approach was faster.</p>\n<p>In general, this whole approach can be used elsewhere, not just for these census files.</p>\n<p>Although your CSV’s may be irregular, there is a way to deal with them and get your data into a useful shape.</p>\n<p>My top tips:</p>\n<ul>\n<li>don’t panic : look for some common ground, even if the number of rows/ columns, headers varies by file. In this case, it was seeing the first row of actual data began with the same value, and that it would occur within the first ten rows.</li>\n<li>don’t try and eat the elephant. It’s easy to chuck a function into <code>map</code> or another purrr function and apply it en-masse. But it’s easier to get things working for one step at a time on the same file, and then branch out to others.</li>\n<li>use purrr <code>safely</code>.  See the code for some functions to get data back out of the resulting list</li>\n<li>base string functions are very useful, and overlooked</li>\n</ul>\n<p>Addendum: coming back to this today, having tidied a file up in Power Query, I realised everything could be much easier. Power Query discarded the first few rows of text, and so filtering for blanks in the second column gave me the header rows. I was able to transpose those and combine them, then transpose back to give me one row of headers.</p>\n<p>I isolated the non header rows as a seperate table, read the data in again minus those rows, combined both tables and unpivoted everything but the first column.</p>\n<p>The reason I didn’t spot this earlier is that you get a much better view of the data in Power Query than in the quarter pane of RStudio. Similarly, I wasn’t getting the same sense of the data structure  when viewing it in VS Code either. (The header image for this post shows how the first few lines of one file looked in VS Code).</p>\n<p>I was able to do the same steps using R to isolate the header rows. A quick <code>fsetdiff</code> between the original data and the headers gave me the data. Then I performed the same steps as in my original approach to join the rows in the headers together and set them as the column names in the main dataset. This approach was slightly slower for the first file, but definitely quicker for the largest file in the set, which returns over 20M rows.</p>\n<p>Now, having cracked the transformation for one file in Power Query, I have to figure out how to do them all. But that’s for another time.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.johnmackintosh.net/blog/rstats/2024-12-22-tidying-text-files/\"> Data By John</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "The life changing magic of tidying text files\nPosted on\nDecember 21, 2024\nby\nJohn MacKintosh\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nData By John\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nOur team have been doing some work with the Scotland Census 2022 data. There are several ways to download the information – you can click around on maps or use a table builder to focus on specifics, or there is a large zip download that provides all the data in CSV format. You end up with 71 files, with around 46K rows and a variable number of columns.\nThe first 3 rows of each file contain generic information about the dataset and can be discarded for analysis. Because these are of varying widths, various file readers may trip up when reading them in.\ndata.table\nsuggests using\nfill  = TRUE\nwhen using\nfread\n, but that causes immediate failure in some cases.\nThe last 8 rows contain text that can also be discarded. (In truth, these rows never got read in because the single column threw\nfread\n, which was a blessing in disguise)\nOnce these rows have been discarded, many files have headers in multiple rows which need to be extracted, combined, and the added back as column headers.\nNeed to account for having between 0-5 rows of column headers, with some blank rows in between, usually around line 4 or 5\nSome files have extra delimiters in the first 3 rows\nObviously, for one or two files on an ad-hoc basis, you can get around this by hand, or other nefarious means. \nDoing it programatically is another issue. It’s just the right kind of problem – tricky enough so that you can’t stop thinking about it, and easy enough that you can actually achieve something.\nMy initial approach involved 2 reads per file – I read the file in and saved as a temp file,  then used\nscan\non the temp file to find the first Output Area code in the first column – this is the first row of data.\nThen I created some vectors of indices for where the data began, and where I thought the actual first line of header rows were, after skipping the first 3 rows.\nI tried using\n{vroom}\n. For this to work I needed to provide a\nskip\nvalue and set\ncol_names\nto FALSE. There was no way to get an accurate skip value without doing a prior read or scan.\nThen I decided to go back to\nfread\nand not skip anything, set\nheader\nto\nFALSE\n, and perform only one read.  data.table was smart enough to strip out the first three rows anyway, so I was left with the multiple rows containing the column headers right at the start of the table.\nI skimmed those off using\ngrep\nto find the first output area, and subtracting 1 to get the correct number of header rows\n# find the row with the start_target value, and retrieve all the rows above it\n\nheaders <- int_dt[,head(.SD,grep(start_target,V1) - 1L)]\nUsing tail on the data, with a negative index to account for the number of header rows, gave me the actual data. I just used\ndim\nof the headers data.table to get the number of rows, to save performing another\ngrep\n# remove the first n header rows - the rest of the rows are the data we need to process\n  int_dt <- int_dt[,tail(.SD, -dim(headers)[1])]\nAfter that, it was a matter of combining the headers rows and collapsing them into a character vector and setting those as the column names. Then I pivoted the data into long format, copied the value column, replaced hyphens with\nNA\n, and coerced to numeric. I added in options to write the file out, or to print, or to return it in case further processing was required.\nHere is how I used data.table’s set operation to remove instances of 2 or more underscores in the\nvariable\ncolumn. Note the use of\n.I\nto return an integer vector of rows to update\n# replace any multiple underscores in variable column\n\n  col_name <- \"variable\"\n  rows_to_change <- out_dt[variable %like% \"_{2,}\",.I]\n  \n  set(out_dt, i = rows_to_change, j = col_name,\n     value =  stri_replace_all_regex(out_dt[[col_name]][rows_to_change],\n     pattern = \"_{2,}\",\n     replacement = \"\"))\nAs this is data at a very small geographic level, for all of Scotland, we don’t want to be writing these out to a CSV file (although, my function saves them as .TSV by default). \nI used the\narrow\npackage to write them to parquet. And, used\nduckdb\nto create a duckdb database.\nThe code for all this is on my github here\ntidy_scotland_census\nFurther developments would be to filter this for specific areas - I am only really interested in Highland and Argyll and Bute- however I’ve left this for now so the code should be of use to anyone who wants to use it.\nThere is some example code of how to use the function with purrr to write the files, or view the outputs in tidy format. \nYou could also stick them in a nested list (1.7 GB), but my immediate reaction to doing that is to try and get it straight back out again. I do recommend using purrr’s\nsafely\nfunction for this sort of thing.\nHaving sorted out the approach, I spent some time trying to make things a bit faster. Using\ngsub\nwas slowing things down, so I replaced that with some\nstringi\n. Coercing to numeric also took some time, but even using a\nset\napproach in data.table did not speed things up. That was because I was creating a vector of indices to pass to the set syntax (\nfor j in cols\n), and it was pretty slow operation. Switching back to subsetting and using\nlet\nto update by reference was much faster. I’m not sure this should generally be the case, but I tried both methods with several files. I used the\nprofvis\npackage to figure out where the bottlenecks were, and it was very handy to confirm my original approach was faster.\nIn general, this whole approach can be used elsewhere, not just for these census files.\nAlthough your CSV’s may be irregular, there is a way to deal with them and get your data into a useful shape.\nMy top tips:\ndon’t panic : look for some common ground, even if the number of rows/ columns, headers varies by file. In this case, it was seeing the first row of actual data began with the same value, and that it would occur within the first ten rows.\ndon’t try and eat the elephant. It’s easy to chuck a function into\nmap\nor another purrr function and apply it en-masse. But it’s easier to get things working for one step at a time on the same file, and then branch out to others.\nuse purrr\nsafely\n.  See the code for some functions to get data back out of the resulting list\nbase string functions are very useful, and overlooked\nAddendum: coming back to this today, having tidied a file up in Power Query, I realised everything could be much easier. Power Query discarded the first few rows of text, and so filtering for blanks in the second column gave me the header rows. I was able to transpose those and combine them, then transpose back to give me one row of headers.\nI isolated the non header rows as a seperate table, read the data in again minus those rows, combined both tables and unpivoted everything but the first column.\nThe reason I didn’t spot this earlier is that you get a much better view of the data in Power Query than in the quarter pane of RStudio. Similarly, I wasn’t getting the same sense of the data structure  when viewing it in VS Code either. (The header image for this post shows how the first few lines of one file looked in VS Code).\nI was able to do the same steps using R to isolate the header rows. A quick\nfsetdiff\nbetween the original data and the headers gave me the data. Then I performed the same steps as in my original approach to join the rows in the headers together and set them as the column names in the main dataset. This approach was slightly slower for the first file, but definitely quicker for the largest file in the set, which returns over 20M rows.\nNow, having cracked the transformation for one file in Power Query, I have to figure out how to do them all. But that’s for another time.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nData By John\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "Bringing order to irregularly structured open data CSV files - with R, of course",
    "meta_keywords": null,
    "og_description": "Bringing order to irregularly structured open data CSV files - with R, of course",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "The life changing magic of tidying text files | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 7.8,
    "sitemap_lastmod": null,
    "twitter_description": "Bringing order to irregularly structured open data CSV files - with R, of course",
    "twitter_title": "The life changing magic of tidying text files | R-bloggers",
    "url": "https://www.r-bloggers.com/2024/12/the-life-changing-magic-of-tidying-text-files-2/",
    "word_count": 1555
  }
}