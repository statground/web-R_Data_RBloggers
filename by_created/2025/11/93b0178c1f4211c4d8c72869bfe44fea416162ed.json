{
  "id": "93b0178c1f4211c4d8c72869bfe44fea416162ed",
  "url": "https://www.r-bloggers.com/2023/11/powerquery-puzzle-solved-with-r/",
  "created_at_utc": "2025-11-17T20:39:11Z",
  "data": null,
  "raw_original": {
    "uuid": "74b6dbfa-ce9c-4b02-bd05-3846a370b74a",
    "created_at": "2025-11-17 20:39:11",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2023/11/powerquery-puzzle-solved-with-r/",
      "crawled_at": "2025-11-17T09:53:53.847667",
      "external_links": [
        {
          "href": "https://medium.com/number-around-us/powerquery-puzzle-solved-with-r-261c263d90c5",
          "text": "Numbers around us - Medium"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7128954914591956992-X7Og?utm_source=share&utm_medium=member_desktop",
          "text": "content"
        },
        {
          "href": "https://lnkd.in/dGqppTKJ",
          "text": "file"
        },
        {
          "href": "https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7129355402814574592-vLFd?utm_source=share&utm_medium=member_desktop",
          "text": "content"
        },
        {
          "href": "https://lnkd.in/drdpJ7HY",
          "text": "file"
        },
        {
          "href": "https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7131491217397219328-qa1H?utm_source=share&utm_medium=member_desktop",
          "text": "content"
        },
        {
          "href": "https://lnkd.in/gmC7jvX2",
          "text": "file"
        },
        {
          "href": "https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7131853604109385729-RFet?utm_source=share&utm_medium=member_desktop",
          "text": "content"
        },
        {
          "href": "https://lnkd.in/gpbj-w63",
          "text": "file"
        },
        {
          "href": "https://medium.com/number-around-us/powerquery-puzzle-solved-with-r-261c263d90c5",
          "text": "PowerQuery Puzzle solved with R"
        },
        {
          "href": "https://medium.com/number-around-us",
          "text": "Numbers around us"
        },
        {
          "href": "https://medium.com/number-around-us/powerquery-puzzle-solved-with-r-261c263d90c5",
          "text": "Numbers around us - Medium"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "PowerQuery Puzzle solved with R | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*xXPzoYRzHZ0ve640S3HkDQ.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*tX_FK7Tj3kaMqUIxKeZo-w.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*aq5zEHAyIyHnWTrnm0itjQ.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*SfmKSnSYrn-2AYKfvEkYIw.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*a6xd7apkSP3n7MaDtC4k6w.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:application/octet-stream;base64,R0lGODlhAQABAO+/vQAA77+977+977+9AAAAIe+/vQQBAAAAACwAAAAAAQABAAACAkQBADs=",
          "src": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=261c263d90c5"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/numbers-around-us/",
          "text": "Numbers around us"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-380196 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">PowerQuery Puzzle solved with R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 21, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/numbers-around-us/\">Numbers around us</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://medium.com/number-around-us/powerquery-puzzle-solved-with-r-261c263d90c5\"> Numbers around us - Medium</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9--><p>Excel BI’s Weekend PQ Puzzles Solutions</p><figure><img alt=\"\" data-lazy-src=\"https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*xXPzoYRzHZ0ve640S3HkDQ.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*xXPzoYRzHZ0ve640S3HkDQ.png?w=578&amp;ssl=1\"/></noscript></figure><p>Until this time I didn’t really look on PowerQuery Puzzles of ExcelBI, which he publishes every weekend. But after making some posts about Excel Puzzles, I realized that PQ ones are equally if not more interesting. Usually they need more steps to achieve a goal, and sometimes they are little bit harder. In this series I will not make another framework approaches (data.table or base), but rather explain code step by step. <br/>Today as a part of consistent publishing schedule, I start second format “PQ Puzzles Solved with R”, that will be published on Tuesday, one day after Excel Puzzles from all previous week. At the very begining I’ll start with 4 puzzles (from last two weekends). I hope you will enjoy it.</p><h3>Puzzles:</h3><p>PQ_129: <a href=\"https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7128954914591956992-X7Og?utm_source=share&amp;utm_medium=member_desktop\" rel=\"nofollow\" target=\"_blank\">content</a> <a href=\"https://lnkd.in/dGqppTKJ\" rel=\"nofollow\" target=\"_blank\">file</a><br/>PQ_130: <a href=\"https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7129355402814574592-vLFd?utm_source=share&amp;utm_medium=member_desktop\" rel=\"nofollow\" target=\"_blank\">content</a> <a href=\"https://lnkd.in/drdpJ7HY\" rel=\"nofollow\" target=\"_blank\">file</a><br/>PQ_131: <a href=\"https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7131491217397219328-qa1H?utm_source=share&amp;utm_medium=member_desktop\" rel=\"nofollow\" target=\"_blank\">content</a> <a href=\"https://lnkd.in/gmC7jvX2\" rel=\"nofollow\" target=\"_blank\">file</a><br/>PQ_132: <a href=\"https://www.linkedin.com/posts/excelbi_powerquerychallenge-daxchallenge-m-activity-7131853604109385729-RFet?utm_source=share&amp;utm_medium=member_desktop\" rel=\"nofollow\" target=\"_blank\">content</a> <a href=\"https://lnkd.in/gpbj-w63\" rel=\"nofollow\" target=\"_blank\">file</a></p><h3>PQ_129</h3><figure><img alt=\"\" data-lazy-src=\"https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*tX_FK7Tj3kaMqUIxKeZo-w.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*tX_FK7Tj3kaMqUIxKeZo-w.png?w=578&amp;ssl=1\"/></noscript></figure><p>What do we have here. Two-column table with dated measurement of temperature is provided, and we have to make something that we can call periodical report. We need to find out specific measurements, aggregates and properties from this time series to get: the highest, the lowest and average temperature per period and days when those extremes happened.</p><p>But we do not have specific intervals like Jan 1st to Feb 18th or anything similar. We have to calculate it dynamically. So… Let dive in…</p><h4>Load data and libraries</h4><pre>library(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\n\ninput = read_excel(\"PQ_Challenge_129.xlsx\", range = \"B2:C1002\")</pre><p>I do not read testing table, because it would be only working on day of publication.</p><h4>Prepare functions and mid-steps</h4><pre>extract_data &lt;- function(data, period, date_range) {\n  # Convert the first element of date_range to a Date object\n  date_range_start &lt;- as.Date(date_range[1], format = \"%Y-%m-%d\")  \n  \n  # The data manipulation begins here\n  data %&gt;%\n    # Filter rows where Date is within the specified range\n    filter(Date &lt;= today() &amp; Date &gt;= date_range_start) %&gt;%\n    # Summarise the data to find maximum, minimum, and average temperature\n    summarise(\n      MaxTemp = max(Temp),                  # Maximum temperature\n      MaxtempDate = Date[which.max(Temp)],  # Date when maximum temperature occurred\n      MinTemp = min(Temp),                  # Minimum temperature\n      MintempDate = Date[which.min(Temp)],  # Date when minimum temperature occurred\n      AvgTemp = mean(Temp)                  # Average temperature\n    ) %&gt;%\n    # Ensure only one row of summary is returned\n    slice(1) %&gt;%\n    # Add a column for the specified period\n    mutate(Period = period) %&gt;%\n    # Reorder columns to have Period first\n    select(Period, everything())\n}</pre><p>The function takes three arguments:</p><ol><li>data: a dataframe that contains temperature data.</li><li>period: a descriptive value (like a string) indicating the period of the data.</li><li>date_range: a vector of dates defining the range for data extraction.</li></ol><p>But we need to define those periods and date_ranges so, lets prepare one more auxiliary structure:</p><pre>periods &lt;- data.frame(\n  # Create a column 'Period' with descriptive names for time periods\n  Period = c(\"Last 7 Days\", \"Last 30 Days\", \"Last 365 Days\", \"Month to Date\", \"Quarter to Date\", \"Year to Date\"),\n  \n  # Create a column 'DateRangeStart' with calculated start dates for each period\n  DateRangeStart = c(\n    today() - days(6),                  # Start date for the last 7 days\n    today() - days(29),                 # Start date for the last 30 days\n    today() - days(364),                # Start date for the last 365 days\n    floor_date(today(), \"month\"),       # Start date for the current month\n    floor_date(today(), \"quarter\"),     # Start date for the current quarter\n    floor_date(today(), \"year\")),       # Start date for the current year\n  \n  stringsAsFactors = FALSE             # Ensure that strings are not converted to factors\n)</pre><p>How is it looks? Of course dates are calculated dynamically and will change every day.</p><pre>           Period DateRangeStart\n1     Last 7 Days     2023-11-14\n2    Last 30 Days     2023-10-22\n3   Last 365 Days     2022-11-21\n4   Month to Date     2023-11-01\n5 Quarter to Date     2023-10-01\n6    Year to Date     2023-01-01</pre><p>We have to process calculation for each of given periods which means intervals between start dates and today. So lets use our function which takes our original timeseries source, and two values from periods dataframe to calculate this report. We are going to map function on both input and periods data frames with following script:</p><pre>output &lt;- map2(periods$Period, periods$DateRangeStart, ~extract_data(input, .x, .y)) %&gt;%\n bind_rows() %&gt;%\n mutate(AvgTemp = round(AvgTemp, 2))</pre><p>And something weird happened. I have warnings that there is Infinity in results. Quick look and I know. We cannot calculate results for last 7 days, because I prepare this article over week after dates in source ended. But whatever… Here you have result as we get it, for Nov 20th.</p><pre> # A tibble: 5 × 6\n  Period          MaxTemp MaxtempDate         MinTemp MintempDate         AvgTemp\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dttm&gt;                &lt;dbl&gt; &lt;dttm&gt;                &lt;dbl&gt;\n1 Last 30 Days         34 2023-10-22 00:00:00     -15 2023-10-29 00:00:00   11.1 \n2 Last 365 Days        39 2023-05-29 00:00:00     -26 2022-12-19 00:00:00    7.68\n3 Month to Date        31 2023-11-02 00:00:00      -5 2023-11-04 00:00:00   16.6 \n4 Quarter to Date      34 2023-10-22 00:00:00     -17 2023-10-20 00:00:00   10.0 \n5 Year to Date         39 2023-05-29 00:00:00     -23 2023-07-05 00:00:00    7.7 </pre><p>I think it looks good and we can proceed to next puzzle.</p><h3>PQ_130</h3><figure><img alt=\"\" data-lazy-src=\"https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*aq5zEHAyIyHnWTrnm0itjQ.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*aq5zEHAyIyHnWTrnm0itjQ.png?w=578&amp;ssl=1\"/></noscript></figure><p>Here we’ve got two tables and we have to find out which keywords from first one are present in which column of the second one. Sounds weird? Maybe little bit, but what exactly we have to do?</p><h4>Load data and libraries</h4><pre>library(tidytext)\nlibrary(tidyverse)\nlibrary(readxl)\n\nT1  = read_excel(\"PQ_Challenge_130.xlsx\", range = \"A1:B12\")\nT2  = read_excel(\"PQ_Challenge_130.xlsx\", range = \"D1:G6\")\n\nTest = read_excel(\"PQ_Challenge_130.xlsx\", range = \"D11:F23\")</pre><p>In table T1 we have keywords with id, in T2 we have columns to check.</p><h4>Transform data</h4><p>Now we have to extract two columns we need to check (Text and City Country) and “slice&amp;dice” them to single words. At the end we need to put it in one table which will tell us from which column it comes.</p><pre>R1 = T2 %&gt;%\n  unnest_tokens(word, Text) %&gt;%\n  ungroup() %&gt;%\n  mutate(`Column Name` = \"Text\")\n\nR2 = T2 %&gt;%\n  unnest_tokens(word, `City / Country`) %&gt;%\n  ungroup() %&gt;%\n  mutate(`Column Name` = \"City / Country\")\n\nResult = R1 %&gt;%\n  bind_rows(R2)</pre><p>Finally we are “slicing” first column for single words as well and join it with Result table. It will give us information about description of which person has which keyword id in which column. I promise it looks more clever than that sentence.</p><pre>T1R = T1 %&gt;%\n  unnest_tokens(word, Value) %&gt;%\n  ungroup()\n\nRes1 = Result %&gt;%\n  left_join(T1R, by = \"word\") %&gt;%\n  select(`Emp ID`, No, `Column Name`) %&gt;%\n  filter(!is.na(No)) %&gt;%\n  arrange(`Emp ID`, No) %&gt;%\n  unique()</pre><p>How it looks at the end?</p><pre># A tibble: 12 × 3\n   `Emp ID`    No `Column Name` \n      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n 1        1     6 City / Country\n 2        1     9 Text          \n 3        1    11 Text          \n 4        2     4 Text          \n 5        2    10 City / Country\n 6        3     1 City / Country\n 7        3     3 City / Country\n 8        4     2 Text          \n 9        4     4 City / Country\n10        5     5 City / Country\n11        5     7 Text          \n12        5     8 City / Country</pre><p>What else left? We can check if this solution is equal to solution/target table provided by puzzle author. And it is.</p><pre>identical(Res1, Test)\n# [1] TRUE</pre><h3>PQ_131</h3><figure><img alt=\"\" data-lazy-src=\"https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*SfmKSnSYrn-2AYKfvEkYIw.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cdn-images-1.medium.com/max/1024/1*SfmKSnSYrn-2AYKfvEkYIw.png?w=578&amp;ssl=1\"/></noscript></figure><p>In this case it looks like somebody is doing accounting notes but has really narrow peace of paper and we need to expand it to shape that facts about one file are in the single row.</p><h4>Load data and libraries</h4><pre>library(tidyverse)\nlibrary(readxl) \n\ninput = read_excel(“PQ_Challenge_131.xlsx”, range = “A1:F10”, col_names = FALSE)\ntest = read_excel(“PQ_Challenge_131.xlsx”, range = “I1:R4”, col_names = TRUE)</pre><h4>Transform data</h4><p>We need to split this weird table into two elements: labels and values. It will help us to change its shape. Every odd row should be taken to labels, and every even to values. Then both of this auxiliary table should be pivoted to long version. That causes that both tables have exactly the same dimension, and that means that every label is just on the same row like its value in second table.</p><pre>labels = input %&gt;% \n filter(row_number() %% 2 == 1) %&gt;%\n pivot_longer(cols = -c(…1), names_to = “row”, values_to = “label”)\n\nvalues = input %&gt;% \n filter(row_number() %% 2 == 0) %&gt;%\n pivot_longer(cols = -c(…1), names_to = “row”, values_to = “value”)</pre><p>After this manipulation we can bind them column-wise. Then we need little cleaning and pivot it back to wide version. What does change here? Every file named Group has only one line with data about it. <br/>Lets finally check if we are having the same values as in provided solution.</p><pre>final = bind_cols(labels, values) %&gt;% \n  select(Group = ...1, label , value ) %&gt;%\n  filter(!is.na(label)) %&gt;%\n  pivot_wider(names_from = label, values_from = value) %&gt;%\n  mutate(across(everything(), ~ifelse(is.na(.), NA_integer_, as.numeric(.))))\n\nidentical(test, final)\n#&gt; [1] TRUE</pre><p>And it is identical.</p><h3>PQ_132</h3><figure><img alt=\"\" data-lazy-src=\"https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*a6xd7apkSP3n7MaDtC4k6w.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/cdn-images-1.medium.com/max/1024/1*a6xd7apkSP3n7MaDtC4k6w.png?w=578&amp;ssl=1\"/></noscript></figure><p>I have some weird thought. In the office we meet in this puzzles, they have only narrow sheets of paper. Firstly accountant need to extract data from narrow sheet to wide version, now HR need to put wide table on only five columns. But we are here to help so lets go.</p><h4>Loading data and libraries</h4><pre>library(tidyverse)\nlibrary(readxl)\n\ninput = read_excel(\"PQ_Challenge_132.xlsx\", range = \"A1:I6\")\ntest = read_excel(\"PQ_Challenge_132.xlsx\", range = \"A10:E30\")</pre><h4>Transform data</h4><p>First thing we are asked for is to conactenate first and last name in one column.</p><pre>result = input %&gt;%\n  unite(\"Full Name\", c(\"First Name\", \"Last Name\"), sep = \" \") </pre><p>Then I need to create one auxiliary structure which is empty, but will be needed because we have odd number of columns to be wrapped, so we will need it to fill the hole.</p><pre>empty = result %&gt;% \n  select(1) %&gt;%\n  mutate(Attr = NA_character_, Value = NA_character_, `Emp ID` = as.character(`Emp ID`))</pre><p>Then we do the main magic. This time first thing is to pivot it to longer version and nest rows. Then we are working on nested structure and its rows. After nesting we need to specify which row (which property of data record) is in even, and which in odd row, because it will determine if this property will finish in first or second column of unnested data.</p><pre>rest = result %&gt;% \n  mutate(across(everything(), as.character)) %&gt;%\n  pivot_longer(-1, names_to = \"Attribute\", values_to = \"Value\") %&gt;%\n  group_by(`Emp ID`) %&gt;%\n  mutate(row = row_number()) %&gt;%\n  nest(data = c(Attribute, Value)) %&gt;%\n  arrange(row) %&gt;%\n  mutate(row_even = if_else(row %% 2 == 0, \"2\",\"1\" ))</pre><p>Finally we are splitting it into two tables by even/odd differentiation. Each of those tables have to be unnested separately. And it appears that one of them is 4x5 and the other 3x5. Here we need our empty table. After those manipulations we can bind both column-wise, clean up little bit and take care of column names.</p><pre>final_one = rest %&gt;%\n  filter(row_even == \"1\") %&gt;%\n  unnest(data) %&gt;%\n  select(-row_even, -row)\n\nfinal_two = rest %&gt;%\n  filter(row_even == \"2\") %&gt;%\n  unnest(data) %&gt;%\n  select(-row_even, -row) %&gt;%\n  bind_rows(empty)\n\nfinal = final_one %&gt;%\n  bind_cols(final_two) %&gt;%\n  select(1,2,3,5,6)\n\ncolnames(final) &lt;- c(\"Emp ID\", \"Attribute1\", \"Value1\", \"Attribute2\", \"Value2\")</pre><p>Unfortunatelly unifying provided solution with our final result would need some additional adjustments, like type changing and so on. But if you would like to check it visually, you will see that it is exactly the same.</p><pre># A tibble: 20 × 5\n   `Emp ID` Attribute1    Value1            Attribute2 Value2\n   &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt; \n 1 813185   Full Name     Marisol Hunt      Gender     F     \n 2 252591   Full Name     Emmett Cotton     Gender     M     \n 3 781324   Full Name     Monica Adkins     Gender     F     \n 4 598718   Full Name     Frederick Frazier Gender     M     \n 5 435759   Full Name     Prince Hanson     Gender     M     \n 6 813185   Date of Birth 1990-02-08        Weight     50    \n 7 252591   Date of Birth 1968-10-24        Weight     71    \n 8 781324   Date of Birth 1970-05-26        Weight     44    \n 9 598718   Date of Birth 1989-03-07        Weight     65    \n10 435759   Date of Birth 1984-05-09        Weight     60    \n11 813185   Salary        154143            State      NJ    \n12 252591   Salary        78407             State      TX    \n13 781324   Salary        193067            State      NV    \n14 598718   Salary        136736            State      NJ    \n15 435759   Salary        72788             State      IN    \n16 813185   Sales         127               NA         NA    \n17 252591   Sales         3673              NA         NA    \n18 781324   Sales         783               NA         NA    \n19 598718   Sales         93783             NA         NA    \n20 435759   Sales         7893              NA         NA    </pre><p>It is, isn’t it?</p><p>Thank you for reading first part of PQ puzzles solved with R. This “episode” has 4 riddles, but next Tuesday will start normal mode which is “puzzles from last weekend only”.</p><p>I hope you like it. Look also for my general articles about R which will come on Thursdays and Excel puzzles that will always precede PQ by one day.</p><img alt=\"\" data-lazy-src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=261c263d90c5\" height=\"1\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" width=\"1\"/><noscript><img alt=\"\" height=\"1\" loading=\"lazy\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=261c263d90c5\" width=\"1\"/></noscript><hr/><p><a href=\"https://medium.com/number-around-us/powerquery-puzzle-solved-with-r-261c263d90c5\" rel=\"nofollow\" target=\"_blank\">PowerQuery Puzzle solved with R</a> was originally published in <a href=\"https://medium.com/number-around-us\" rel=\"nofollow\" target=\"_blank\">Numbers around us</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://medium.com/number-around-us/powerquery-puzzle-solved-with-r-261c263d90c5\"> Numbers around us - Medium</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
      "main_text": "PowerQuery Puzzle solved with R\nPosted on\nNovember 21, 2023\nby\nNumbers around us\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nNumbers around us - Medium\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nExcel BI’s Weekend PQ Puzzles Solutions\nUntil this time I didn’t really look on PowerQuery Puzzles of ExcelBI, which he publishes every weekend. But after making some posts about Excel Puzzles, I realized that PQ ones are equally if not more interesting. Usually they need more steps to achieve a goal, and sometimes they are little bit harder. In this series I will not make another framework approaches (data.table or base), but rather explain code step by step.\nToday as a part of consistent publishing schedule, I start second format “PQ Puzzles Solved with R”, that will be published on Tuesday, one day after Excel Puzzles from all previous week. At the very begining I’ll start with 4 puzzles (from last two weekends). I hope you will enjoy it.\nPuzzles:\nPQ_129:\ncontent\nfile\nPQ_130:\ncontent\nfile\nPQ_131:\ncontent\nfile\nPQ_132:\ncontent\nfile\nPQ_129\nWhat do we have here. Two-column table with dated measurement of temperature is provided, and we have to make something that we can call periodical report. We need to find out specific measurements, aggregates and properties from this time series to get: the highest, the lowest and average temperature per period and days when those extremes happened.\nBut we do not have specific intervals like Jan 1st to Feb 18th or anything similar. We have to calculate it dynamically. So… Let dive in…\nLoad data and libraries\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\n\ninput = read_excel(\"PQ_Challenge_129.xlsx\", range = \"B2:C1002\")\nI do not read testing table, because it would be only working on day of publication.\nPrepare functions and mid-steps\nextract_data <- function(data, period, date_range) {\n  # Convert the first element of date_range to a Date object\n  date_range_start <- as.Date(date_range[1], format = \"%Y-%m-%d\")  \n  \n  # The data manipulation begins here\n  data %>%\n    # Filter rows where Date is within the specified range\n    filter(Date <= today() & Date >= date_range_start) %>%\n    # Summarise the data to find maximum, minimum, and average temperature\n    summarise(\n      MaxTemp = max(Temp),                  # Maximum temperature\n      MaxtempDate = Date[which.max(Temp)],  # Date when maximum temperature occurred\n      MinTemp = min(Temp),                  # Minimum temperature\n      MintempDate = Date[which.min(Temp)],  # Date when minimum temperature occurred\n      AvgTemp = mean(Temp)                  # Average temperature\n    ) %>%\n    # Ensure only one row of summary is returned\n    slice(1) %>%\n    # Add a column for the specified period\n    mutate(Period = period) %>%\n    # Reorder columns to have Period first\n    select(Period, everything())\n}\nThe function takes three arguments:\ndata: a dataframe that contains temperature data.\nperiod: a descriptive value (like a string) indicating the period of the data.\ndate_range: a vector of dates defining the range for data extraction.\nBut we need to define those periods and date_ranges so, lets prepare one more auxiliary structure:\nperiods <- data.frame(\n  # Create a column 'Period' with descriptive names for time periods\n  Period = c(\"Last 7 Days\", \"Last 30 Days\", \"Last 365 Days\", \"Month to Date\", \"Quarter to Date\", \"Year to Date\"),\n  \n  # Create a column 'DateRangeStart' with calculated start dates for each period\n  DateRangeStart = c(\n    today() - days(6),                  # Start date for the last 7 days\n    today() - days(29),                 # Start date for the last 30 days\n    today() - days(364),                # Start date for the last 365 days\n    floor_date(today(), \"month\"),       # Start date for the current month\n    floor_date(today(), \"quarter\"),     # Start date for the current quarter\n    floor_date(today(), \"year\")),       # Start date for the current year\n  \n  stringsAsFactors = FALSE             # Ensure that strings are not converted to factors\n)\nHow is it looks? Of course dates are calculated dynamically and will change every day.\nPeriod DateRangeStart\n1     Last 7 Days     2023-11-14\n2    Last 30 Days     2023-10-22\n3   Last 365 Days     2022-11-21\n4   Month to Date     2023-11-01\n5 Quarter to Date     2023-10-01\n6    Year to Date     2023-01-01\nWe have to process calculation for each of given periods which means intervals between start dates and today. So lets use our function which takes our original timeseries source, and two values from periods dataframe to calculate this report. We are going to map function on both input and periods data frames with following script:\noutput <- map2(periods$Period, periods$DateRangeStart, ~extract_data(input, .x, .y)) %>%\n bind_rows() %>%\n mutate(AvgTemp = round(AvgTemp, 2))\nAnd something weird happened. I have warnings that there is Infinity in results. Quick look and I know. We cannot calculate results for last 7 days, because I prepare this article over week after dates in source ended. But whatever… Here you have result as we get it, for Nov 20th.\n# A tibble: 5 × 6\n  Period          MaxTemp MaxtempDate         MinTemp MintempDate         AvgTemp\n  <chr>             <dbl> <dttm>                <dbl> <dttm>                <dbl>\n1 Last 30 Days         34 2023-10-22 00:00:00     -15 2023-10-29 00:00:00   11.1 \n2 Last 365 Days        39 2023-05-29 00:00:00     -26 2022-12-19 00:00:00    7.68\n3 Month to Date        31 2023-11-02 00:00:00      -5 2023-11-04 00:00:00   16.6 \n4 Quarter to Date      34 2023-10-22 00:00:00     -17 2023-10-20 00:00:00   10.0 \n5 Year to Date         39 2023-05-29 00:00:00     -23 2023-07-05 00:00:00    7.7\nI think it looks good and we can proceed to next puzzle.\nPQ_130\nHere we’ve got two tables and we have to find out which keywords from first one are present in which column of the second one. Sounds weird? Maybe little bit, but what exactly we have to do?\nLoad data and libraries\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(readxl)\n\nT1  = read_excel(\"PQ_Challenge_130.xlsx\", range = \"A1:B12\")\nT2  = read_excel(\"PQ_Challenge_130.xlsx\", range = \"D1:G6\")\n\nTest = read_excel(\"PQ_Challenge_130.xlsx\", range = \"D11:F23\")\nIn table T1 we have keywords with id, in T2 we have columns to check.\nTransform data\nNow we have to extract two columns we need to check (Text and City Country) and “slice&dice” them to single words. At the end we need to put it in one table which will tell us from which column it comes.\nR1 = T2 %>%\n  unnest_tokens(word, Text) %>%\n  ungroup() %>%\n  mutate(`Column Name` = \"Text\")\n\nR2 = T2 %>%\n  unnest_tokens(word, `City / Country`) %>%\n  ungroup() %>%\n  mutate(`Column Name` = \"City / Country\")\n\nResult = R1 %>%\n  bind_rows(R2)\nFinally we are “slicing” first column for single words as well and join it with Result table. It will give us information about description of which person has which keyword id in which column. I promise it looks more clever than that sentence.\nT1R = T1 %>%\n  unnest_tokens(word, Value) %>%\n  ungroup()\n\nRes1 = Result %>%\n  left_join(T1R, by = \"word\") %>%\n  select(`Emp ID`, No, `Column Name`) %>%\n  filter(!is.na(No)) %>%\n  arrange(`Emp ID`, No) %>%\n  unique()\nHow it looks at the end?\n# A tibble: 12 × 3\n   `Emp ID`    No `Column Name` \n      <dbl> <dbl> <chr>         \n 1        1     6 City / Country\n 2        1     9 Text          \n 3        1    11 Text          \n 4        2     4 Text          \n 5        2    10 City / Country\n 6        3     1 City / Country\n 7        3     3 City / Country\n 8        4     2 Text          \n 9        4     4 City / Country\n10        5     5 City / Country\n11        5     7 Text          \n12        5     8 City / Country\nWhat else left? We can check if this solution is equal to solution/target table provided by puzzle author. And it is.\nidentical(Res1, Test)\n# [1] TRUE\nPQ_131\nIn this case it looks like somebody is doing accounting notes but has really narrow peace of paper and we need to expand it to shape that facts about one file are in the single row.\nLoad data and libraries\nlibrary(tidyverse)\nlibrary(readxl) \n\ninput = read_excel(“PQ_Challenge_131.xlsx”, range = “A1:F10”, col_names = FALSE)\ntest = read_excel(“PQ_Challenge_131.xlsx”, range = “I1:R4”, col_names = TRUE)\nTransform data\nWe need to split this weird table into two elements: labels and values. It will help us to change its shape. Every odd row should be taken to labels, and every even to values. Then both of this auxiliary table should be pivoted to long version. That causes that both tables have exactly the same dimension, and that means that every label is just on the same row like its value in second table.\nlabels = input %>% \n filter(row_number() %% 2 == 1) %>%\n pivot_longer(cols = -c(…1), names_to = “row”, values_to = “label”)\n\nvalues = input %>% \n filter(row_number() %% 2 == 0) %>%\n pivot_longer(cols = -c(…1), names_to = “row”, values_to = “value”)\nAfter this manipulation we can bind them column-wise. Then we need little cleaning and pivot it back to wide version. What does change here? Every file named Group has only one line with data about it.\nLets finally check if we are having the same values as in provided solution.\nfinal = bind_cols(labels, values) %>% \n  select(Group = ...1, label , value ) %>%\n  filter(!is.na(label)) %>%\n  pivot_wider(names_from = label, values_from = value) %>%\n  mutate(across(everything(), ~ifelse(is.na(.), NA_integer_, as.numeric(.))))\n\nidentical(test, final)\n#> [1] TRUE\nAnd it is identical.\nPQ_132\nI have some weird thought. In the office we meet in this puzzles, they have only narrow sheets of paper. Firstly accountant need to extract data from narrow sheet to wide version, now HR need to put wide table on only five columns. But we are here to help so lets go.\nLoading data and libraries\nlibrary(tidyverse)\nlibrary(readxl)\n\ninput = read_excel(\"PQ_Challenge_132.xlsx\", range = \"A1:I6\")\ntest = read_excel(\"PQ_Challenge_132.xlsx\", range = \"A10:E30\")\nTransform data\nFirst thing we are asked for is to conactenate first and last name in one column.\nresult = input %>%\n  unite(\"Full Name\", c(\"First Name\", \"Last Name\"), sep = \" \")\nThen I need to create one auxiliary structure which is empty, but will be needed because we have odd number of columns to be wrapped, so we will need it to fill the hole.\nempty = result %>% \n  select(1) %>%\n  mutate(Attr = NA_character_, Value = NA_character_, `Emp ID` = as.character(`Emp ID`))\nThen we do the main magic. This time first thing is to pivot it to longer version and nest rows. Then we are working on nested structure and its rows. After nesting we need to specify which row (which property of data record) is in even, and which in odd row, because it will determine if this property will finish in first or second column of unnested data.\nrest = result %>% \n  mutate(across(everything(), as.character)) %>%\n  pivot_longer(-1, names_to = \"Attribute\", values_to = \"Value\") %>%\n  group_by(`Emp ID`) %>%\n  mutate(row = row_number()) %>%\n  nest(data = c(Attribute, Value)) %>%\n  arrange(row) %>%\n  mutate(row_even = if_else(row %% 2 == 0, \"2\",\"1\" ))\nFinally we are splitting it into two tables by even/odd differentiation. Each of those tables have to be unnested separately. And it appears that one of them is 4x5 and the other 3x5. Here we need our empty table. After those manipulations we can bind both column-wise, clean up little bit and take care of column names.\nfinal_one = rest %>%\n  filter(row_even == \"1\") %>%\n  unnest(data) %>%\n  select(-row_even, -row)\n\nfinal_two = rest %>%\n  filter(row_even == \"2\") %>%\n  unnest(data) %>%\n  select(-row_even, -row) %>%\n  bind_rows(empty)\n\nfinal = final_one %>%\n  bind_cols(final_two) %>%\n  select(1,2,3,5,6)\n\ncolnames(final) <- c(\"Emp ID\", \"Attribute1\", \"Value1\", \"Attribute2\", \"Value2\")\nUnfortunatelly unifying provided solution with our final result would need some additional adjustments, like type changing and so on. But if you would like to check it visually, you will see that it is exactly the same.\n# A tibble: 20 × 5\n   `Emp ID` Attribute1    Value1            Attribute2 Value2\n   <chr>    <chr>         <chr>             <chr>      <chr> \n 1 813185   Full Name     Marisol Hunt      Gender     F     \n 2 252591   Full Name     Emmett Cotton     Gender     M     \n 3 781324   Full Name     Monica Adkins     Gender     F     \n 4 598718   Full Name     Frederick Frazier Gender     M     \n 5 435759   Full Name     Prince Hanson     Gender     M     \n 6 813185   Date of Birth 1990-02-08        Weight     50    \n 7 252591   Date of Birth 1968-10-24        Weight     71    \n 8 781324   Date of Birth 1970-05-26        Weight     44    \n 9 598718   Date of Birth 1989-03-07        Weight     65    \n10 435759   Date of Birth 1984-05-09        Weight     60    \n11 813185   Salary        154143            State      NJ    \n12 252591   Salary        78407             State      TX    \n13 781324   Salary        193067            State      NV    \n14 598718   Salary        136736            State      NJ    \n15 435759   Salary        72788             State      IN    \n16 813185   Sales         127               NA         NA    \n17 252591   Sales         3673              NA         NA    \n18 781324   Sales         783               NA         NA    \n19 598718   Sales         93783             NA         NA    \n20 435759   Sales         7893              NA         NA\nIt is, isn’t it?\nThank you for reading first part of PQ puzzles solved with R. This “episode” has 4 riddles, but next Tuesday will start normal mode which is “puzzles from last weekend only”.\nI hope you like it. Look also for my general articles about R which will come on Thursdays and Excel puzzles that will always precede PQ by one day.\nPowerQuery Puzzle solved with R\nwas originally published in\nNumbers around us\non Medium, where people are continuing the conversation by highlighting and responding to this story.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nNumbers around us - Medium\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Excel BI’s Weekend PQ Puzzles SolutionsUntil this time I didn’t really look on PowerQuery Puzzles of ExcelBI, which he publishes every weekend. But after making some posts about Excel Puzzles, I realized that PQ ones are equally if not more interesting...",
      "meta_keywords": null,
      "og_description": "Excel BI’s Weekend PQ Puzzles SolutionsUntil this time I didn’t really look on PowerQuery Puzzles of ExcelBI, which he publishes every weekend. But after making some posts about Excel Puzzles, I realized that PQ ones are equally if not more interesting...",
      "og_image": "https://cdn-images-1.medium.com/max/1024/1*xXPzoYRzHZ0ve640S3HkDQ.png",
      "og_title": "PowerQuery Puzzle solved with R | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 11.2,
      "sitemap_lastmod": "2023-11-21T14:01:49+00:00",
      "twitter_description": "Excel BI’s Weekend PQ Puzzles SolutionsUntil this time I didn’t really look on PowerQuery Puzzles of ExcelBI, which he publishes every weekend. But after making some posts about Excel Puzzles, I realized that PQ ones are equally if not more interesting...",
      "twitter_title": "PowerQuery Puzzle solved with R | R-bloggers",
      "url": "https://www.r-bloggers.com/2023/11/powerquery-puzzle-solved-with-r/",
      "word_count": 2234
    }
  }
}