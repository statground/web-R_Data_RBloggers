{
  "uuid": "d7b8b181-0047-40a6-ac75-30461d4fd410",
  "created_at": "2025-11-22 19:58:36",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/05/deep-dive-into-bayesian-optimization/",
    "crawled_at": "2025-11-22T10:48:55.651327",
    "external_links": [
      {
        "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-mbo/",
        "text": "mlr-org"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://github.com/mlr-org/mlr3website/",
        "text": "GitHub"
      },
      {
        "href": "https://doi.org/10.1016/0022-314X(88)90025-X",
        "text": "1988"
      },
      {
        "href": "https://gaussianprocess.org/gpml/chapters/RW.pdf",
        "text": "2006"
      },
      {
        "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-mbo/",
        "text": "mlr-org"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Deep dive into Bayesian Optimization | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/giuseppe-casalicchio/",
        "text": "Giuseppe Casalicchio"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-392892 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Deep dive into Bayesian Optimization</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 15, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/giuseppe-casalicchio/\">Giuseppe Casalicchio</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-mbo/\"> mlr-org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n<strong>JavaScript is required to unlock solutions.</strong><br/>\n    Please enable JavaScript and reload the page,<br/>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" rel=\"nofollow\" target=\"_blank\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n<section class=\"level1\" id=\"goal\">\n<h1>Goal</h1>\n<p>After this exercise, you should be able to navigate the building blocks of Bayesian optimization (BO) using <code>bbotk</code> and <code>mlr3mbo</code> for general black box optimization problems, and more specifically, hyperparameter optimization (HPO).</p>\n</section>\n<section class=\"level1\" id=\"introduction\">\n<h1>Introduction</h1>\n<p>This section is a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). BO is more complex than other tuning methods, so we will motivate theory and methodology first.</p>\n<p><strong>Black Box Optimization</strong></p>\n<p>In hyperparameter optimization, learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, this is a black box optimization problem, which considers the optimization of a function whose mathematical structure is unknown or unexploitable. The only thing we can observe is the generalization performance of the function given a hyperparameter configuration. As evaluating the performance of a learner can take a lot of time, HPO is an expensive black box optimization problem.</p>\n<p><strong>Bayesian Optimization</strong></p>\n<p>There is many ways of doing black box optimization, grid and random search being examples for simple strategies. Bayesian optimization are a class of black box optimization algorithms that rely on a ‘surrogate model’ trained on observed hyperparameter evaluations to model the black box function. This surrogate model tries to capture the unknown function between hyperparameter configuations and estimated generalization performance using (the very low number of) observed function evaluations. During each iteration, BO algorithms employ an ‘acquisition function’ to determine the next candidate point for evaluation. This function measures the expected ‘utility’ of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight.</p>\n<p>In the rest of this section, we will first provide an introduction to black box optimization with the bbotk package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black box optimizer with <code>mlr3mbo</code>.</p>\n</section>\n<section class=\"level1\" id=\"prerequisites\">\n<h1>Prerequisites</h1>\n<p>Let’s load the packages required for this exercise:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(bbotk)\nlibrary(mlr3verse)\nlibrary(mlr3mbo)\nset.seed(123)</pre>\n</div>\n<p>Before we apply BO to hyperparamter optimization (HPO), we try to optimize the following simple sinusoidal function:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>sinus_1D = function(xs) 2 * xs$x1 * sin(14 * xs$x1) * sin(xs$x2) * xs$x2</pre>\n</div>\n</section>\n<section class=\"level1\" id=\"building-blocks-of-bo\">\n<h1>1 Building Blocks of BO</h1>\n<p>Bayesian optimization (BO) usually follows this process:</p>\n<ol type=\"1\">\n<li><p>Generate and evaluate an initial design</p></li>\n<li><p>Loop:</p>\n<ul>\n<li>2.1. Fit a surrogate model on the archive of all observations made so far to model the unknown black box function.</li>\n<li>2.2. Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next.</li>\n<li>2.3. Evaluate the next candidate(s) and update the archive of all observations made so far.</li>\n<li>2.4. Check if a given termination criterion is met, if not go back to 2.1.</li>\n</ul></li>\n</ol>\n<p>The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high.</p>\n<p>BO is a highly modular algorithm: as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of <code>mlr3mbo</code> reflects this modularity, with the base class for <code>OptimizerMbo</code> holding all the key elements: the BO algorithm loop structure (<code>loop_function</code>), surrogate model (<code>Surrogate</code>), acquisition function (<code>AcqFunction</code>), and acquisition function optimizer (<code>AcqOptimizer</code>). Let’s explore the interplay and interaction of these building blocks during optimization.</p>\n<section class=\"level2\" id=\"initial-design\">\n<h2 class=\"anchored\" data-anchor-id=\"initial-design\">1.1 Initial design</h2>\n<p>The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design. <code>mlr3mbo</code> allows you to either construct this manually or let a <code>loop_function</code> do this for you. We will demonstrate the first method here.</p>\n<p>To construct an initial design, we will use one of the four design generators available in <code>paradox</code>. Let’s try grid search first, assuming an initial design of nine points on a domain of two numeric variables ranging from 0 to 1:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1))\ngenerate_design_grid(domain, resolution = 3)$data</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>      x1    x2\n   &lt;num&gt; &lt;num&gt;\n1:   0.0   0.0\n2:   0.0   0.5\n3:   0.0   1.0\n4:   0.5   0.0\n5:   0.5   0.5\n6:   0.5   1.0\n7:   1.0   0.0\n8:   1.0   0.5\n9:   1.0   1.0</pre>\n</div>\n</div>\n<p>As you can see, this is more or less a simple <code>data.table</code> that encodes the set of hyperparameter configurations we want to evaluate first, before any of the real BO magic starts.</p>\n<p><strong>Task: Construct a more refined initial design, using <code>paradox</code> to implement a Sobol design with 30 points, which has better coverage properties than grid or random search. If you are interested in why the Sobol design has favorable properties, you can take a look at the original paper by Niederreiter (<a href=\"https://doi.org/10.1016/0022-314X(88)90025-X\" rel=\"nofollow\" target=\"_blank\">1988</a>).</strong></p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-1\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-1-contents callout-collapse collapse\" id=\"callout-1\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+c29ib2xfZGVzaWduIDxzcGFuIGNsYXNzPSJvdCI+PTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImZ1Ij5nZW5lcmF0ZV9kZXNpZ25fc29ib2w8L3NwYW4+KGRvbWFpbiwgPHNwYW4gY2xhc3M9ImF0Ij5uID08L3NwYW4+IDxzcGFuIGNsYXNzPSJkdiI+MzA8L3NwYW4+KTxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5kYXRhPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTIiPjxhIGhyZWY9IiNjYjEtMiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPjxzcGFuIGNsYXNzPSJmdSI+aGVhZDwvc3Bhbj4oc29ib2xfZGVzaWduKTwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0IGNlbGwtb3V0cHV0LXN0ZG91dCI+CjxwcmU+PGNvZGU+ICAgICAgICAgIHgxICAgICAgICB4MgogICAgICAgJmx0O251bSZndDsgICAgICZsdDtudW0mZ3Q7CjE6IDAuNjIwOTQ5NCAwLjM1MzEyOTUKMjogMC4xMjA5NDk0IDAuODUzMTI5NQozOiAwLjg3MDk0OTQgMC42MDMxMjk1CjQ6IDAuMzcwOTQ5NCAwLjEwMzEyOTUKNTogMC4yNDU5NDk0IDAuNDc4MTI5NQo2OiAwLjc0NTk0OTQgMC45NzgxMjk1PC9jb2RlPjwvcHJlPgo8L2Rpdj4KPC9kaXY+\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"generate-data-from-initial-design\">\n<h2 class=\"anchored\" data-anchor-id=\"generate-data-from-initial-design\">1.2 Generate data from initial design</h2>\n<p>To generate training data for our surrogate model, we need a few more things:</p>\n<ul>\n<li>An <code>Objective</code> function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs). Objective functions can be created using different classes, all of which inherit from <code>Objective</code>. Here, we will use <code>ObjectiveRFun</code>.</li>\n</ul>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre># We have already defined our domain, but will do here again:\ndomain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1))\n# Our codomain:\ncodomain = ps(y = p_dbl(tags = \"minimize\"))\n# Our objective: \nobjective = ObjectiveRFun$new(sinus_1D, domain = domain, codomain = codomain)</pre>\n</div>\n<p>Further:</p>\n<ul>\n<li><code>OptimInstanceSingleCrit</code> to construct an optimization instance that describes the optimization problem and stores the results</li>\n<li><code>Optimizer</code> which is used to construct and configure optimization algorithms. Optimization Instance</li>\n</ul>\n<p>Let’s define our optimization instance and evaluate it on our initial Sobol design:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>instance = OptimInstanceSingleCrit$new(objective,\n  terminator = trm(\"evals\", n_evals = 20))\ninstance$eval_batch(sobol_design)</pre>\n</div>\n<p><strong>Task: Extract the training archive data from the tuning instance to find the data that we will now use to train our surrogate model with in the first iteration.</strong></p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-2\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-2-contents callout-collapse collapse\" id=\"callout-2\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"train-surrogate-model\">\n<h2 class=\"anchored\" data-anchor-id=\"train-surrogate-model\">1.3 Train surrogate model</h2>\n<p>A surrogate model wraps a regression learner that models the unknown black box function based on observed data. In <code>mlr3mbo</code>, the <code>SurrogateLearner</code> is a higher-level <code>R6</code> class inheriting from the base Surrogate class, designed to construct and manage the surrogate model, including automatic construction of the <code>TaskRegr</code> that the learner should be trained on at each iteration of the BO loop.</p>\n<p>Any regression learner in <code>mlr3</code> can be used. However, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model, the latter of which requires the <code>\"se\"</code> <code>predict_type</code> to be supported. Therefore not all learners are suitable for all scenarios. Typical choices are random forests or Gaussian processes <code>(lrn(\"regr.km\"))</code>, which we will use here. You can learn more about Gaussian processes in Williams and Rasmussen (<a href=\"https://gaussianprocess.org/gpml/chapters/RW.pdf\" rel=\"nofollow\" target=\"_blank\">2006</a>).</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>lrn_gp = lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\",\n  control = list(trace = FALSE))</pre>\n</div>\n<p>The Matérn covariance function is a kernel used in Gaussian processes to model the smoothness of the random function, offering a flexible class of smoothness parameters. The BFGS algorithm is a type of quasi-Newton method used for optimization, particularly effective in maximizing the likelihood in Gaussian process models by efficiently finding parameter estimates.</p>\n<p>A <code>SurrogateLearner</code> can be constructed by passing a <code>LearnerRegr</code> object to the sugar function <code>srlrn()</code>, alongside the archive of the instance:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>surrogate = srlrn(lrn_gp, archive = instance$archive)</pre>\n</div>\n<p>Internally, the regression learner is fit on a <code>TaskRegr</code> where features are the variables of the domain and the target is the codomain, the data is from the archive of the <code>OptimInstance</code> that is to be optimized.</p>\n<p><strong>Task: Update the surrogate model, which essentially fits the gaussian process. Then, inspect the trained random forest model that is contained within the surrogate:</strong></p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Fitting the surrogate model will require calling one of the methods of <code>surrogate</code>. See <code>?surrogate</code> for help.</p>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-3\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-3-contents callout-collapse collapse\" id=\"callout-3\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5yZXF1aXJlTmFtZXNwYWNlPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O0RpY2VLcmlnaW5nJnF1b3Q7PC9zcGFuPik8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMiI+PGEgaHJlZj0iI2NiMS0yIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5saWJyYXJ5PC9zcGFuPihEaWNlS3JpZ2luZyk8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMyI+PGEgaHJlZj0iI2NiMS0zIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PC9zcGFuPgo8c3BhbiBpZD0iY2IxLTQiPjxhIGhyZWY9IiNjYjEtNCIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPnN1cnJvZ2F0ZTxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj48c3BhbiBjbGFzcz0iZnUiPnVwZGF0ZTwvc3Bhbj4oKTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS01Ij48YSBocmVmPSIjY2IxLTUiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT5zdXJyb2dhdGU8c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+bGVhcm5lcjxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5tb2RlbDwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0IGNlbGwtb3V0cHV0LXN0ZG91dCI+CjxwcmU+PGNvZGU+CkNhbGw6CkRpY2VLcmlnaW5nOjprbShkZXNpZ24gPSBkYXRhLCByZXNwb25zZSA9IHRydXRoLCBjb3Z0eXBlID0gJnF1b3Q7bWF0ZXJuNV8yJnF1b3Q7LCAKICAgIG9wdGltLm1ldGhvZCA9ICZxdW90O0JGR1MmcXVvdDssIGNvbnRyb2wgPSBwdiRjb250cm9sKQoKVHJlbmQgIGNvZWZmLjoKICAgICAgICAgICAgICAgRXN0aW1hdGUKIChJbnRlcmNlcHQpICAgIC0wLjAxMDMKCkNvdmFyLiB0eXBlICA6IG1hdGVybjVfMiAKQ292YXIuIGNvZWZmLjoKICAgICAgICAgICAgICAgRXN0aW1hdGUKICAgdGhldGEoeDEpICAgICAwLjExNDUKICAgdGhldGEoeDIpICAgICAwLjYzNzIKClZhcmlhbmNlIGVzdGltYXRlOiAwLjE2NTM2MTQ8L2NvZGU+PC9wcmU+CjwvZGl2Pgo8L2Rpdj4=\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"define-an-acquisition-function\">\n<h2 class=\"anchored\" data-anchor-id=\"define-an-acquisition-function\">1.4 Define an acquisition function</h2>\n<p>Roughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the perceived ‘utility’ of each point of the search space if it were to be evaluated in the next iteration.</p>\n<p>A popular example is the <strong>expected improvement</strong>, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the ‘incumbent’), given the performance prediction of the surrogate model. Calculating the expected improvement requires mean and standard deviation predictions from the model.</p>\n<p>In <code>mlr3mbo</code>, acquisition functions are stored in the <code>mlr_acqfunctions</code> dictionary and can be constructed with <code>acqf()</code>, passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement to choose the next candidate for evaluation.</p>\n<p><strong>Task: Construct an aquisition function object using expected improvement. Then, update the aquisition function object.</strong></p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-4\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-4-contents callout-collapse collapse\" id=\"callout-4\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5yZXF1aXJlTmFtZXNwYWNlPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O0RpY2VLcmlnaW5nJnF1b3Q7PC9zcGFuPik8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMiI+PGEgaHJlZj0iI2NiMS0yIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5saWJyYXJ5PC9zcGFuPihEaWNlS3JpZ2luZyk8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMyI+PGEgaHJlZj0iI2NiMS0zIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PC9zcGFuPgo8c3BhbiBpZD0iY2IxLTQiPjxhIGhyZWY9IiNjYjEtNCIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPmFjcV9mdW5jdGlvbiA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+YWNxZjwvc3Bhbj4oPHNwYW4gY2xhc3M9InN0Ij4mcXVvdDtlaSZxdW90Ozwvc3Bhbj4sIDxzcGFuIGNsYXNzPSJhdCI+c3Vycm9nYXRlID08L3NwYW4+IHN1cnJvZ2F0ZSk8L3NwYW4+CjxzcGFuIGlkPSJjYjEtNSI+PGEgaHJlZj0iI2NiMS01IiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+YWNxX2Z1bmN0aW9uPHNwYW4gY2xhc3M9InNjIj4kPC9zcGFuPjxzcGFuIGNsYXNzPSJmdSI+dXBkYXRlPC9zcGFuPigpPC9zcGFuPjwvY29kZT48L3ByZT48L2Rpdj4KPC9kaXY+CjxwPlRoZSBiZXN0IHktdmFsdWUgdGhlIGFjcXVpc2l0aW9uIGZ1bmN0aW9uIGhhcyBzZWVuIHNvIGZhcjo8L3A+CjxkaXYgY2xhc3M9ImNlbGwiIGRhdGEtbGF5b3V0LWFsaWduPSJjZW50ZXIiPgo8ZGl2IGNsYXNzPSJzb3VyY2VDb2RlIiBpZD0iY2IyIj48cHJlCmNsYXNzPSJzb3VyY2VDb2RlIHIgY2VsbC1jb2RlIj48Y29kZSBjbGFzcz0ic291cmNlQ29kZSByIj48c3BhbiBpZD0iY2IyLTEiPjxhIGhyZWY9IiNjYjItMSIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPmFjcV9mdW5jdGlvbjxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj55X2Jlc3Q8L3NwYW4+PC9jb2RlPjwvcHJlPjwvZGl2Pgo8ZGl2IGNsYXNzPSJjZWxsLW91dHB1dCBjZWxsLW91dHB1dC1zdGRvdXQiPgo8cHJlPjxjb2RlPlsxXSAtMS4wNTYzNDc8L2NvZGU+PC9wcmU+CjwvZGl2Pgo8L2Rpdj4=\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"optimize-acquisition-function\">\n<h2 class=\"anchored\" data-anchor-id=\"optimize-acquisition-function\">1.5 Optimize acquisition function</h2>\n<p>Why would we need to optimize the acquisition function? Well, the acquisition function can tell us how “promising” an arbitrary hyperparameter configuration is. If we want to find the “most promising” hyperparameter configuration, we again need to optimize the acquisition function. Consequently the optimization problem of the acquisition function is handled as a black box optimization problem itself, but it is a much cheaper one than the original.</p>\n<p>An acquisition function optimizer of class <code>AcqOptimizer</code> is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods, such as the DIRECT algorithm. Consequently the optimization problem of the acquisition function can be handled as a black box optimization problem itself, but a much cheaper one than the original.</p>\n<p><code>AcqOptimizer</code> objects are constructed with <code>acqo()</code>, which takes as input an <code>Optimizer</code>, a Terminator, and the acquisition function. Optimizers are stored in the <code>mlr_optimizers</code> dictionary and can be constructed with the sugar function <code>opt()</code>. Let’s select an optimizer first:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>optimizer = opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\")</pre>\n</div>\n<p><strong>Task: Construct an acquisition function optimizer using the optimizer above, a termination criterion of your choice and the acquisition function from the previous exercise. Then, call $optimize() on the optimizer to suggest the next candidate hyperparameter configuration.</strong></p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-5\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-5-contents callout-collapse collapse\" id=\"callout-5\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"automating-bo-with-optimizermbo\">\n<h1>2 Automating BO with <code>OptimizerMbo</code></h1>\n<p>We have now shown how to run a single iteration of the BO algorithm loop manually. In practice, one would use <code>OptimizerMbo</code> to put all these pieces together to automate the process. To determine the behavior of the BO algorithm on a global level, we need a <strong>loop function</strong>. We use the Efficient Global Optimization (EGO) algorithm, aka <code>bayesopt_ego</code> provided by <code>mlr_loop_functions</code>. You do not need to pass any of these building blocks to each other manually as the <code>opt()</code> constructor will do this for you:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>loop_function = mlr_loop_functions$get(\"bayesopt_ego\")\n\nsurrogate = srlrn(lrn(\"regr.km\",\n                      covtype = \"matern5_2\",\n                      optim.method = \"BFGS\",\n                      control = list(trace = FALSE)))\n\nacq_function = acqf(\"ei\")\n\nacq_optimizer = acqo(opt(\"nloptr\",\n                         algorithm = \"NLOPT_GN_ORIG_DIRECT\"),\n                         terminator = trm(\"evals\", n_evals = 100))\n\noptimizer = opt(\"mbo\",\n  loop_function = loop_function,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)</pre>\n</div>\n<p><strong>Task: Use the MBO optimizer constructed above to solve the optimization problem. To do so, define an optimization instance (as in 1.2) and an initial design (as in 1.1). Then, evaluate the optimization instance on the initial design (as in 1.2). Then, call <code>optimizer$optimize()</code> on the instance.</strong></p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-6\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-6-contents callout-collapse collapse\" id=\"callout-6\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+aW5zdGFuY2UgPHNwYW4gY2xhc3M9Im90Ij49PC9zcGFuPiBPcHRpbUluc3RhbmNlU2luZ2xlQ3JpdDxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj48c3BhbiBjbGFzcz0iZnUiPm5ldzwvc3Bhbj4ob2JqZWN0aXZlLDwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS0yIj48YSBocmVmPSIjY2IxLTIiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT4gIDxzcGFuIGNsYXNzPSJhdCI+dGVybWluYXRvciA9PC9zcGFuPiA8c3BhbiBjbGFzcz0iZnUiPnRybTwvc3Bhbj4oPHNwYW4gY2xhc3M9InN0Ij4mcXVvdDtldmFscyZxdW90Ozwvc3Bhbj4sIDxzcGFuIGNsYXNzPSJhdCI+bl9ldmFscyA9PC9zcGFuPiA8c3BhbiBjbGFzcz0iZHYiPjEwMDwvc3Bhbj4pKTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS0zIj48YSBocmVmPSIjY2IxLTMiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT5zb2JvbF9kZXNpZ24gPHNwYW4gY2xhc3M9Im90Ij49PC9zcGFuPiA8c3BhbiBjbGFzcz0iZnUiPmdlbmVyYXRlX2Rlc2lnbl9zb2JvbDwvc3Bhbj4oZG9tYWluLCA8c3BhbiBjbGFzcz0iYXQiPm4gPTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImR2Ij4xMDA8L3NwYW4+KTxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5kYXRhPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTQiPjxhIGhyZWY9IiNjYjEtNCIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPmluc3RhbmNlPHNwYW4gY2xhc3M9InNjIj4kPC9zcGFuPjxzcGFuIGNsYXNzPSJmdSI+ZXZhbF9iYXRjaDwvc3Bhbj4oc29ib2xfZGVzaWduKTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS01Ij48YSBocmVmPSIjY2IxLTUiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT5vcHRpbWl6ZXI8c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+PHNwYW4gY2xhc3M9ImZ1Ij5vcHRpbWl6ZTwvc3Bhbj4oaW5zdGFuY2UpPC9zcGFuPjwvY29kZT48L3ByZT48L2Rpdj4KPGRpdiBjbGFzcz0iY2VsbC1vdXRwdXQgY2VsbC1vdXRwdXQtc3Rkb3V0Ij4KPHByZT48Y29kZT4gICAgICAgICAgeDEgICAgICAgIHgyICB4X2RvbWFpbiAgICAgICAgICB5CiAgICAgICAmbHQ7bnVtJmd0OyAgICAgJmx0O251bSZndDsgICAgJmx0O2xpc3QmZ3Q7ICAgICAgJmx0O251bSZndDsKMTogMC44MDYxODA1IDAuODIzMTc5NCAmbHQ7bGlzdFsyXSZndDsgLTAuOTMyMzg5NjwvY29kZT48L3ByZT4KPC9kaXY+CjwvZGl2Pg==\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level1\" id=\"bo-for-hpo-with-tunermbo\">\n<h1>3 BO for HPO with <code>TunerMbo</code></h1>\n<p><code>mlr3mbo</code> can be used for HPO by making use of <code>TunerMbo</code>, which is a wrapper around <code>OptimizerMbo</code> and works in the exact same way. As an example, we want to tune the cost and gamma parameters of <code>lrn(\"classif.svm\")</code> with a radial kernel on <code>tsk(\"sonar\")</code> with three-fold CV.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>lrn_svm = lrn(\"classif.svm\", kernel = \"radial\",\n  type = \"C-classification\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE)\n)\n\ntuner = tnr(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)</pre>\n</div>\n<p><strong>Task: Run the tuner on the lrn_svm for the sonar task and 3-fold CV. Use misclassification error as performance measure. What is the best HP configuration?</strong></p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>See <code>?mlr3tuning::tune</code> for help.</p>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-7\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-7-contents callout-collapse collapse\" id=\"callout-7\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-9f94cb8d1f20b9d8244eb9e8-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(tuner,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                <span class="fu">tsk</span>(<span class="st">&quot;sonar&quot;</span>),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                lrn_svm,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">msr</span>(<span class="st">&quot;classif.ce&quot;</span>), <span class="dv">25</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INFO  [08:36:10.023] [mlr3] Running benchmark with 24 resampling iterations
INFO  [08:36:10.155] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:10.266] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:10.369] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:10.475] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:10.578] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:10.683] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:10.794] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:10.897] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:11.008] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:11.118] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:11.152] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:11.198] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:11.243] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:11.285] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:11.327] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:11.371] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:11.413] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:11.456] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:11.498] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:11.542] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:11.554] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:11.596] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:11.638] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:11.683] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:11.699] [mlr3] Finished benchmark
INFO  [08:36:12.845] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:12.878] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:12.907] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:12.936] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:12.946] [mlr3] Finished benchmark
INFO  [08:36:13.995] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:14.027] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:14.056] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:14.084] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:14.092] [mlr3] Finished benchmark
INFO  [08:36:15.168] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:15.200] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:15.239] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:15.285] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:15.292] [mlr3] Finished benchmark
INFO  [08:36:16.416] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:16.447] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:16.476] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:16.504] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:16.512] [mlr3] Finished benchmark
INFO  [08:36:17.667] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:17.698] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:17.751] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:17.801] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:17.809] [mlr3] Finished benchmark
INFO  [08:36:19.006] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:19.037] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:19.066] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:19.098] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:19.104] [mlr3] Finished benchmark
INFO  [08:36:20.048] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:20.079] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:20.108] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:20.140] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:20.149] [mlr3] Finished benchmark
INFO  [08:36:21.146] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:21.180] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:21.225] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:21.255] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:21.262] [mlr3] Finished benchmark
INFO  [08:36:22.260] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:22.291] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:22.320] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:22.348] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:22.356] [mlr3] Finished benchmark
INFO  [08:36:23.398] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:23.430] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:23.459] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:23.488] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:23.495] [mlr3] Finished benchmark
INFO  [08:36:24.509] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:24.539] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:24.568] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:24.597] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:24.604] [mlr3] Finished benchmark
INFO  [08:36:25.666] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:25.698] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:25.727] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:25.756] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:25.763] [mlr3] Finished benchmark
INFO  [08:36:26.826] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:26.857] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:26.886] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:26.915] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:26.925] [mlr3] Finished benchmark
INFO  [08:36:28.035] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:28.066] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:28.095] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:28.124] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:28.131] [mlr3] Finished benchmark
INFO  [08:36:29.217] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:29.248] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:29.277] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:29.306] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:29.312] [mlr3] Finished benchmark
INFO  [08:36:30.453] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:30.485] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:30.514] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:30.543] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:30.550] [mlr3] Finished benchmark
INFO  [08:36:31.764] [mlr3] Running benchmark with 3 resampling iterations
INFO  [08:36:31.795] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 1/3)
INFO  [08:36:31.823] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 2/3)
INFO  [08:36:31.852] [mlr3] Applying learner &#39;classif.svm&#39; on task &#39;sonar&#39; (iter 3/3)
INFO  [08:36:31.858] [mlr3] Finished benchmark</code></pre>
</div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost     gamma learner_param_vals  x_domain classif.ce
      &lt;num&gt;     &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;
1: 3.838227 -3.839397          &lt;list[4]&gt; &lt;list[2]&gt;  0.1058661</code></pre>
</div>
</div>\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level1\" id=\"summary\">\n<h1>Summary</h1>\n<p>We have learned how Bayesian Optimization can be used to solve black box optimization problems, and HPO problems specifically. Rather than simply spending a compute budget on evaluating arbitrary configurations, we optimize an acquisition function based on a surrogate model that maps hyperparameter configurations to their estimated generalization performance, to iteratively suggest new candidates.</p>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-mbo/\"> mlr-org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Deep dive into Bayesian Optimization\nPosted on\nMay 15, 2025\nby\nGiuseppe Casalicchio\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nmlr-org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nJavaScript is required to unlock solutions.\nPlease enable JavaScript and reload the page,\nor download the source files from\nGitHub\nand run the code locally.\nGoal\nAfter this exercise, you should be able to navigate the building blocks of Bayesian optimization (BO) using\nbbotk\nand\nmlr3mbo\nfor general black box optimization problems, and more specifically, hyperparameter optimization (HPO).\nIntroduction\nThis section is a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). BO is more complex than other tuning methods, so we will motivate theory and methodology first.\nBlack Box Optimization\nIn hyperparameter optimization, learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, this is a black box optimization problem, which considers the optimization of a function whose mathematical structure is unknown or unexploitable. The only thing we can observe is the generalization performance of the function given a hyperparameter configuration. As evaluating the performance of a learner can take a lot of time, HPO is an expensive black box optimization problem.\nBayesian Optimization\nThere is many ways of doing black box optimization, grid and random search being examples for simple strategies. Bayesian optimization are a class of black box optimization algorithms that rely on a ‘surrogate model’ trained on observed hyperparameter evaluations to model the black box function. This surrogate model tries to capture the unknown function between hyperparameter configuations and estimated generalization performance using (the very low number of) observed function evaluations. During each iteration, BO algorithms employ an ‘acquisition function’ to determine the next candidate point for evaluation. This function measures the expected ‘utility’ of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight.\nIn the rest of this section, we will first provide an introduction to black box optimization with the bbotk package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black box optimizer with\nmlr3mbo\n.\nPrerequisites\nLet’s load the packages required for this exercise:\nlibrary(bbotk)\nlibrary(mlr3verse)\nlibrary(mlr3mbo)\nset.seed(123)\nBefore we apply BO to hyperparamter optimization (HPO), we try to optimize the following simple sinusoidal function:\nsinus_1D = function(xs) 2 * xs$x1 * sin(14 * xs$x1) * sin(xs$x2) * xs$x2\n1 Building Blocks of BO\nBayesian optimization (BO) usually follows this process:\nGenerate and evaluate an initial design\nLoop:\n2.1. Fit a surrogate model on the archive of all observations made so far to model the unknown black box function.\n2.2. Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next.\n2.3. Evaluate the next candidate(s) and update the archive of all observations made so far.\n2.4. Check if a given termination criterion is met, if not go back to 2.1.\nThe acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high.\nBO is a highly modular algorithm: as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of\nmlr3mbo\nreflects this modularity, with the base class for\nOptimizerMbo\nholding all the key elements: the BO algorithm loop structure (\nloop_function\n), surrogate model (\nSurrogate\n), acquisition function (\nAcqFunction\n), and acquisition function optimizer (\nAcqOptimizer\n). Let’s explore the interplay and interaction of these building blocks during optimization.\n1.1 Initial design\nThe initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design.\nmlr3mbo\nallows you to either construct this manually or let a\nloop_function\ndo this for you. We will demonstrate the first method here.\nTo construct an initial design, we will use one of the four design generators available in\nparadox\n. Let’s try grid search first, assuming an initial design of nine points on a domain of two numeric variables ranging from 0 to 1:\ndomain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1))\ngenerate_design_grid(domain, resolution = 3)$data\nx1    x2\n   <num> <num>\n1:   0.0   0.0\n2:   0.0   0.5\n3:   0.0   1.0\n4:   0.5   0.0\n5:   0.5   0.5\n6:   0.5   1.0\n7:   1.0   0.0\n8:   1.0   0.5\n9:   1.0   1.0\nAs you can see, this is more or less a simple\ndata.table\nthat encodes the set of hyperparameter configurations we want to evaluate first, before any of the real BO magic starts.\nTask: Construct a more refined initial design, using\nparadox\nto implement a Sobol design with 30 points, which has better coverage properties than grid or random search. If you are interested in why the Sobol design has favorable properties, you can take a look at the original paper by Niederreiter (\n1988\n).\nSolution\nUnlock solution\n1.2 Generate data from initial design\nTo generate training data for our surrogate model, we need a few more things:\nAn\nObjective\nfunction that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs). Objective functions can be created using different classes, all of which inherit from\nObjective\n. Here, we will use\nObjectiveRFun\n.\n# We have already defined our domain, but will do here again:\ndomain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1))\n# Our codomain:\ncodomain = ps(y = p_dbl(tags = \"minimize\"))\n# Our objective: \nobjective = ObjectiveRFun$new(sinus_1D, domain = domain, codomain = codomain)\nFurther:\nOptimInstanceSingleCrit\nto construct an optimization instance that describes the optimization problem and stores the results\nOptimizer\nwhich is used to construct and configure optimization algorithms. Optimization Instance\nLet’s define our optimization instance and evaluate it on our initial Sobol design:\ninstance = OptimInstanceSingleCrit$new(objective,\n  terminator = trm(\"evals\", n_evals = 20))\ninstance$eval_batch(sobol_design)\nTask: Extract the training archive data from the tuning instance to find the data that we will now use to train our surrogate model with in the first iteration.\nSolution\nUnlock solution\n1.3 Train surrogate model\nA surrogate model wraps a regression learner that models the unknown black box function based on observed data. In\nmlr3mbo\n, the\nSurrogateLearner\nis a higher-level\nR6\nclass inheriting from the base Surrogate class, designed to construct and manage the surrogate model, including automatic construction of the\nTaskRegr\nthat the learner should be trained on at each iteration of the BO loop.\nAny regression learner in\nmlr3\ncan be used. However, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model, the latter of which requires the\n\"se\"\npredict_type\nto be supported. Therefore not all learners are suitable for all scenarios. Typical choices are random forests or Gaussian processes\n(lrn(\"regr.km\"))\n, which we will use here. You can learn more about Gaussian processes in Williams and Rasmussen (\n2006\n).\nlrn_gp = lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\",\n  control = list(trace = FALSE))\nThe Matérn covariance function is a kernel used in Gaussian processes to model the smoothness of the random function, offering a flexible class of smoothness parameters. The BFGS algorithm is a type of quasi-Newton method used for optimization, particularly effective in maximizing the likelihood in Gaussian process models by efficiently finding parameter estimates.\nA\nSurrogateLearner\ncan be constructed by passing a\nLearnerRegr\nobject to the sugar function\nsrlrn()\n, alongside the archive of the instance:\nsurrogate = srlrn(lrn_gp, archive = instance$archive)\nInternally, the regression learner is fit on a\nTaskRegr\nwhere features are the variables of the domain and the target is the codomain, the data is from the archive of the\nOptimInstance\nthat is to be optimized.\nTask: Update the surrogate model, which essentially fits the gaussian process. Then, inspect the trained random forest model that is contained within the surrogate:\nHint 1:\nFitting the surrogate model will require calling one of the methods of\nsurrogate\n. See\n?surrogate\nfor help.\nSolution\nUnlock solution\n1.4 Define an acquisition function\nRoughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the perceived ‘utility’ of each point of the search space if it were to be evaluated in the next iteration.\nA popular example is the\nexpected improvement\n, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the ‘incumbent’), given the performance prediction of the surrogate model. Calculating the expected improvement requires mean and standard deviation predictions from the model.\nIn\nmlr3mbo\n, acquisition functions are stored in the\nmlr_acqfunctions\ndictionary and can be constructed with\nacqf()\n, passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement to choose the next candidate for evaluation.\nTask: Construct an aquisition function object using expected improvement. Then, update the aquisition function object.\nSolution\nUnlock solution\n1.5 Optimize acquisition function\nWhy would we need to optimize the acquisition function? Well, the acquisition function can tell us how “promising” an arbitrary hyperparameter configuration is. If we want to find the “most promising” hyperparameter configuration, we again need to optimize the acquisition function. Consequently the optimization problem of the acquisition function is handled as a black box optimization problem itself, but it is a much cheaper one than the original.\nAn acquisition function optimizer of class\nAcqOptimizer\nis used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods, such as the DIRECT algorithm. Consequently the optimization problem of the acquisition function can be handled as a black box optimization problem itself, but a much cheaper one than the original.\nAcqOptimizer\nobjects are constructed with\nacqo()\n, which takes as input an\nOptimizer\n, a Terminator, and the acquisition function. Optimizers are stored in the\nmlr_optimizers\ndictionary and can be constructed with the sugar function\nopt()\n. Let’s select an optimizer first:\noptimizer = opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\")\nTask: Construct an acquisition function optimizer using the optimizer above, a termination criterion of your choice and the acquisition function from the previous exercise. Then, call $optimize() on the optimizer to suggest the next candidate hyperparameter configuration.\nSolution\nUnlock solution\n2 Automating BO with\nOptimizerMbo\nWe have now shown how to run a single iteration of the BO algorithm loop manually. In practice, one would use\nOptimizerMbo\nto put all these pieces together to automate the process. To determine the behavior of the BO algorithm on a global level, we need a\nloop function\n. We use the Efficient Global Optimization (EGO) algorithm, aka\nbayesopt_ego\nprovided by\nmlr_loop_functions\n. You do not need to pass any of these building blocks to each other manually as the\nopt()\nconstructor will do this for you:\nloop_function = mlr_loop_functions$get(\"bayesopt_ego\")\n\nsurrogate = srlrn(lrn(\"regr.km\",\n                      covtype = \"matern5_2\",\n                      optim.method = \"BFGS\",\n                      control = list(trace = FALSE)))\n\nacq_function = acqf(\"ei\")\n\nacq_optimizer = acqo(opt(\"nloptr\",\n                         algorithm = \"NLOPT_GN_ORIG_DIRECT\"),\n                         terminator = trm(\"evals\", n_evals = 100))\n\noptimizer = opt(\"mbo\",\n  loop_function = loop_function,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)\nTask: Use the MBO optimizer constructed above to solve the optimization problem. To do so, define an optimization instance (as in 1.2) and an initial design (as in 1.1). Then, evaluate the optimization instance on the initial design (as in 1.2). Then, call\noptimizer$optimize()\non the instance.\nSolution\nUnlock solution\n3 BO for HPO with\nTunerMbo\nmlr3mbo\ncan be used for HPO by making use of\nTunerMbo\n, which is a wrapper around\nOptimizerMbo\nand works in the exact same way. As an example, we want to tune the cost and gamma parameters of\nlrn(\"classif.svm\")\nwith a radial kernel on\ntsk(\"sonar\")\nwith three-fold CV.\nlrn_svm = lrn(\"classif.svm\", kernel = \"radial\",\n  type = \"C-classification\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE)\n)\n\ntuner = tnr(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)\nTask: Run the tuner on the lrn_svm for the sonar task and 3-fold CV. Use misclassification error as performance measure. What is the best HP configuration?\nHint 1:\nSee\n?mlr3tuning::tune\nfor help.\nSolution\nUnlock solution\nSummary\nWe have learned how Bayesian Optimization can be used to solve black box optimization problems, and HPO problems specifically. Rather than simply spending a compute budget on evaluating arbitrary configurations, we optimize an acquisition function based on a surrogate model that maps hyperparameter configurations to their estimated generalization performance, to iteratively suggest new candidates.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nmlr-org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal After this exercise, you should be able to navigate the building blocks of Bayesian optimization (BO) using bbotk and mlr3mbo for general black box optimization problems, and more specifically, hyperparameter optimization (HPO). Introduction This section is a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). BO is more complex than other tuning methods, so we will motivate theory and methodology first. Black Box Optimization In hyperparameter optimization, learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, this is a black box optimization problem, which considers the optimization of a function whose mathematical structure is unknown or unexploitable. The only thing we can observe is the generalization performance of the function given a hyperparameter configuration. As evaluating the performance of a learner can take a lot of time, HPO is an expensive black box optimization problem. Bayesian Optimization There is many ways of doing black box optimization, grid and random search being examples for simple strategies. Bayesian optimization are a class of black box optimization algorithms that rely on a ‘surrogate model’ trained on observed hyperparameter evaluations to model the black box function. This surrogate model tries to capture the unknown function between hyperparameter configuations and estimated generalization performance using (the very low number of) observed function evaluations. During each iteration, BO algorithms employ an ‘acquisition function’ to determine the next candidate point for evaluation. This function measures the expected ‘utility’ of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight. In the rest of this section, we will first provide an introduction to black box optimization with the bbotk package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black box optimizer with mlr3mbo. Prerequisites Let’s load the packages required for this exercise: library(bbotk) library(mlr3verse) library(mlr3mbo) set.seed(123) Before we apply BO to hyperparamter optimization (HPO), we try to optimize the following simple sinusoidal function: sinus_1D = function(xs) 2 * xs$x1 * sin(14 * xs$x1) * sin(xs$x2) * xs$x2 1 Building Blocks of BO Bayesian optimization (BO) usually follows this process: Generate and evaluate an initial design Loop: 2.1. Fit a surrogate model on the archive of all observations made so far to model the unknown black box function. 2.2. Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next. 2.3. Evaluate the next candidate(s) and update the archive of all observations made so far. 2.4. Check if a given termination criterion is met, if not go back to 2.1. The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high. BO is a highly modular algorithm: as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of mlr3mbo reflects this modularity, with the base class for OptimizerMbo holding all the key elements: the BO algorithm loop structure (loop_function), surrogate model (Surrogate), acquisition function (AcqFunction), and acquisition function optimizer (AcqOptimizer). Let’s explore the interplay and interaction of these building blocks during optimization. 1.1 Initial design The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design. mlr3mbo allows you to either construct this manually or let a loop_function do this for you. We will demonstrate the first method here. To construct an initial design, we will use one of the four design generators available in paradox. Let’s try grid search first, assuming an initial design of nine points on a domain of two numeric variables ranging from 0 to 1: domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1)) generate_design_grid(domain, resolution = 3)$data x1 x2 1: 0.0 0.0 2: 0.0 0.5 3: 0.0 1.0 4: 0.5 0.0 5: 0.5 0.5 6: 0.5 1.0 7: 1.0 0.0 8: 1.0 0.5 9: 1.0 1.0 As you can see, this is more or less a simple data.table that encodes the set of hyperparameter configurations we want to evaluate first, before any of the real BO magic starts. Task: Construct a more refined initial design, using paradox to implement a Sobol design with 30 points, which has better coverage properties than grid or random search. If you are interested in why the Sobol design has favorable properties, you can take a look at the original paper by Niederreiter (1988). Solution Unlock solution 1.2 Generate data from initial design To generate training data for our surrogate model, we need a few more things: An Objective function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs). Objective functions can be created using different classes, all of which inherit from Objective. Here, we will use ObjectiveRFun. # We have already defined our domain, but will do here again: domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1)) # Our codomain: codomain = ps(y = p_dbl(tags = \"minimize\")) # Our objective: objective = ObjectiveRFun$new(sinus_1D, domain = domain, codomain = codomain) Further: OptimInstanceSingleCrit to construct an optimization instance that describes the optimization problem and stores the results Optimizer which is used to construct and configure optimization algorithms. Optimization Instance Let’s define our optimization instance and evaluate it on our initial Sobol design: instance = OptimInstanceSingleCrit$new(objective, terminator = trm(\"evals\", n_evals = 20)) instance$eval_batch(sobol_design) Task: Extract the training archive data from the tuning instance to find the data that we will now use to train our surrogate model with in the first iteration. Solution Unlock solution 1.3 Train surrogate model A surrogate model wraps a regression learner that models the unknown black box function based on observed data. In mlr3mbo, the SurrogateLearner is a higher-level R6 class inheriting from the base Surrogate class, designed to construct and manage the surrogate model, including automatic construction of the TaskRegr that the learner should be trained on at each iteration of the BO loop. Any regression learner in mlr3 can be used. However, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model, the latter of which requires the \"se\" predict_type to be supported. Therefore not all learners are suitable for all scenarios. Typical choices are random forests or Gaussian processes (lrn(\"regr.km\")), which we will use here. You can learn more about Gaussian processes in Williams and Rasmussen (2006). lrn_gp = lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\", control = list(trace = FALSE)) The Matérn covariance function is a kernel used in Gaussian processes to model the smoothness of the random function, offering a flexible class of smoothness parameters. The BFGS algorithm is a type of quasi-Newton method used for optimization, particularly effective in maximizing the likelihood in Gaussian process models by efficiently finding parameter estimates. A SurrogateLearner can be constructed by passing a LearnerRegr object to the sugar function srlrn(), alongside the archive of the instance: surrogate = srlrn(lrn_gp, archive = instance$archive) Internally, the regression learner is fit on a TaskRegr where features are the variables of the domain and the target is the codomain, the data is from the archive of the OptimInstance that is to be optimized. Task: Update the surrogate model, which essentially fits the gaussian process. Then, inspect the trained random forest model that is contained within the surrogate: Hint 1: Fitting the surrogate model will require calling one of the methods of surrogate. See ?surrogate for help. Solution Unlock solution 1.4 Define an acquisition function Roughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the perceived ‘utility’ of each point of the search space if it were to be evaluated in the next iteration. A popular example is the expected improvement, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the ‘incumbent’), given the performance prediction of the surrogate model. Calculating the expected improvement requires mean and standard deviation predictions from the model. In mlr3mbo, acquisition functions are stored in the mlr_acqfunctions dictionary and can be constructed with acqf(), passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement to choose the next candidate for evaluation. Task: Construct an aquisition function object using expected improvement. Then, update the aquisition function object. Solution Unlock solution 1.5 Optimize acquisition function Why would we need to optimize the acquisition function? Well, the acquisition function can tell us how “promising” an arbitrary hyperparameter configuration is. If we want to find the “most promising” hyperparameter configuration, we again need to optimize the acquisition function. Consequently the optimization problem of the acquisition function is handled as a black box optimization problem itself, but it is a much cheaper one than the original. An acquisition function optimizer of class AcqOptimizer is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods, such as the DIRECT algorithm. Consequently the optimization problem of the acquisition function can be handled as a black box optimization problem itself, but a much cheaper one than the original. AcqOptimizer objects are constructed with acqo(), which takes as input an Optimizer, a Terminator, and the acquisition function. Optimizers are stored in the mlr_optimizers dictionary and can be constructed with the sugar function opt(). Let’s select an optimizer first: optimizer = opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\") Task: Construct an acquisition function optimizer using the optimizer above, a termination criterion of your choice and the acquisition function from the previous exercise. Then, call $optimize() on the optimizer to suggest the next candidate hyperparameter configuration. Solution Unlock solution 2 Automating BO with OptimizerMbo We have now shown how to run a single iteration of the BO algorithm loop manually. In practice, one would use OptimizerMbo to put all these pieces together to automate the process. To determine the behavior of the BO algorithm on a global level, we need a loop function. We use the Efficient Global Optimization (EGO) algorithm, aka bayesopt_ego provided by mlr_loop_functions. You do not need to pass any of these building blocks to each other manually as the opt() constructor will do this for you: loop_function = mlr_loop_functions$get(\"bayesopt_ego\") surrogate = srlrn(lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\", control = list(trace = FALSE))) acq_function = acqf(\"ei\") acq_optimizer = acqo(opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"), terminator = trm(\"evals\", n_evals = 100)) optimizer = opt(\"mbo\", loop_function = loop_function, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer) Task: Use the MBO optimizer constructed above to solve the optimization problem. To do so, define an optimization instance (as in 1.2) and an initial design (as in 1.1). Then, evaluate the optimization instance on the initial design (as in 1.2). Then, call optimizer$optimize() on the instance. Solution Unlock solution 3 BO for HPO with TunerMbo mlr3mbo can be used for HPO by making use of TunerMbo, which is a wrapper around OptimizerMbo and works in the exact same way. As an example, we want to tune the cost and gamma parameters of lrn(\"classif.svm\") with a radial kernel on tsk(\"sonar\") with three-fold CV. lrn_svm = lrn(\"classif.svm\", kernel = \"radial\", type = \"C-classification\", cost = to_tune(1e-5, 1e5, logscale = TRUE), gamma = to_tune(1e-5, 1e5, logscale = TRUE) ) tuner = tnr(\"mbo\", loop_function = bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer) Task: Run the tuner on the lrn_svm for the sonar task and 3-fold CV. Use misclassification error as performance measure. What is the best HP configuration? Hint 1: See ?mlr3tuning::tune for help. Solution Unlock solution Summary We have learned how Bayesian Optimization can be used to solve black box optimization problems, and HPO problems specifically. Rather than simply spending a compute budget on evaluating arbitrary configurations, we optimize an acquisition function based on a surrogate model that maps hyperparameter configurations to their estimated generalization performance, to iteratively suggest new candidates.",
    "meta_keywords": null,
    "og_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal After this exercise, you should be able to navigate the building blocks of Bayesian optimization (BO) using bbotk and mlr3mbo for general black box optimization problems, and more specifically, hyperparameter optimization (HPO). Introduction This section is a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). BO is more complex than other tuning methods, so we will motivate theory and methodology first. Black Box Optimization In hyperparameter optimization, learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, this is a black box optimization problem, which considers the optimization of a function whose mathematical structure is unknown or unexploitable. The only thing we can observe is the generalization performance of the function given a hyperparameter configuration. As evaluating the performance of a learner can take a lot of time, HPO is an expensive black box optimization problem. Bayesian Optimization There is many ways of doing black box optimization, grid and random search being examples for simple strategies. Bayesian optimization are a class of black box optimization algorithms that rely on a ‘surrogate model’ trained on observed hyperparameter evaluations to model the black box function. This surrogate model tries to capture the unknown function between hyperparameter configuations and estimated generalization performance using (the very low number of) observed function evaluations. During each iteration, BO algorithms employ an ‘acquisition function’ to determine the next candidate point for evaluation. This function measures the expected ‘utility’ of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight. In the rest of this section, we will first provide an introduction to black box optimization with the bbotk package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black box optimizer with mlr3mbo. Prerequisites Let’s load the packages required for this exercise: library(bbotk) library(mlr3verse) library(mlr3mbo) set.seed(123) Before we apply BO to hyperparamter optimization (HPO), we try to optimize the following simple sinusoidal function: sinus_1D = function(xs) 2 * xs$x1 * sin(14 * xs$x1) * sin(xs$x2) * xs$x2 1 Building Blocks of BO Bayesian optimization (BO) usually follows this process: Generate and evaluate an initial design Loop: 2.1. Fit a surrogate model on the archive of all observations made so far to model the unknown black box function. 2.2. Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next. 2.3. Evaluate the next candidate(s) and update the archive of all observations made so far. 2.4. Check if a given termination criterion is met, if not go back to 2.1. The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high. BO is a highly modular algorithm: as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of mlr3mbo reflects this modularity, with the base class for OptimizerMbo holding all the key elements: the BO algorithm loop structure (loop_function), surrogate model (Surrogate), acquisition function (AcqFunction), and acquisition function optimizer (AcqOptimizer). Let’s explore the interplay and interaction of these building blocks during optimization. 1.1 Initial design The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design. mlr3mbo allows you to either construct this manually or let a loop_function do this for you. We will demonstrate the first method here. To construct an initial design, we will use one of the four design generators available in paradox. Let’s try grid search first, assuming an initial design of nine points on a domain of two numeric variables ranging from 0 to 1: domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1)) generate_design_grid(domain, resolution = 3)$data x1 x2 1: 0.0 0.0 2: 0.0 0.5 3: 0.0 1.0 4: 0.5 0.0 5: 0.5 0.5 6: 0.5 1.0 7: 1.0 0.0 8: 1.0 0.5 9: 1.0 1.0 As you can see, this is more or less a simple data.table that encodes the set of hyperparameter configurations we want to evaluate first, before any of the real BO magic starts. Task: Construct a more refined initial design, using paradox to implement a Sobol design with 30 points, which has better coverage properties than grid or random search. If you are interested in why the Sobol design has favorable properties, you can take a look at the original paper by Niederreiter (1988). Solution Unlock solution 1.2 Generate data from initial design To generate training data for our surrogate model, we need a few more things: An Objective function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs). Objective functions can be created using different classes, all of which inherit from Objective. Here, we will use ObjectiveRFun. # We have already defined our domain, but will do here again: domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1)) # Our codomain: codomain = ps(y = p_dbl(tags = \"minimize\")) # Our objective: objective = ObjectiveRFun$new(sinus_1D, domain = domain, codomain = codomain) Further: OptimInstanceSingleCrit to construct an optimization instance that describes the optimization problem and stores the results Optimizer which is used to construct and configure optimization algorithms. Optimization Instance Let’s define our optimization instance and evaluate it on our initial Sobol design: instance = OptimInstanceSingleCrit$new(objective, terminator = trm(\"evals\", n_evals = 20)) instance$eval_batch(sobol_design) Task: Extract the training archive data from the tuning instance to find the data that we will now use to train our surrogate model with in the first iteration. Solution Unlock solution 1.3 Train surrogate model A surrogate model wraps a regression learner that models the unknown black box function based on observed data. In mlr3mbo, the SurrogateLearner is a higher-level R6 class inheriting from the base Surrogate class, designed to construct and manage the surrogate model, including automatic construction of the TaskRegr that the learner should be trained on at each iteration of the BO loop. Any regression learner in mlr3 can be used. However, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model, the latter of which requires the \"se\" predict_type to be supported. Therefore not all learners are suitable for all scenarios. Typical choices are random forests or Gaussian processes (lrn(\"regr.km\")), which we will use here. You can learn more about Gaussian processes in Williams and Rasmussen (2006). lrn_gp = lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\", control = list(trace = FALSE)) The Matérn covariance function is a kernel used in Gaussian processes to model the smoothness of the random function, offering a flexible class of smoothness parameters. The BFGS algorithm is a type of quasi-Newton method used for optimization, particularly effective in maximizing the likelihood in Gaussian process models by efficiently finding parameter estimates. A SurrogateLearner can be constructed by passing a LearnerRegr object to the sugar function srlrn(), alongside the archive of the instance: surrogate = srlrn(lrn_gp, archive = instance$archive) Internally, the regression learner is fit on a TaskRegr where features are the variables of the domain and the target is the codomain, the data is from the archive of the OptimInstance that is to be optimized. Task: Update the surrogate model, which essentially fits the gaussian process. Then, inspect the trained random forest model that is contained within the surrogate: Hint 1: Fitting the surrogate model will require calling one of the methods of surrogate. See ?surrogate for help. Solution Unlock solution 1.4 Define an acquisition function Roughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the perceived ‘utility’ of each point of the search space if it were to be evaluated in the next iteration. A popular example is the expected improvement, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the ‘incumbent’), given the performance prediction of the surrogate model. Calculating the expected improvement requires mean and standard deviation predictions from the model. In mlr3mbo, acquisition functions are stored in the mlr_acqfunctions dictionary and can be constructed with acqf(), passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement to choose the next candidate for evaluation. Task: Construct an aquisition function object using expected improvement. Then, update the aquisition function object. Solution Unlock solution 1.5 Optimize acquisition function Why would we need to optimize the acquisition function? Well, the acquisition function can tell us how “promising” an arbitrary hyperparameter configuration is. If we want to find the “most promising” hyperparameter configuration, we again need to optimize the acquisition function. Consequently the optimization problem of the acquisition function is handled as a black box optimization problem itself, but it is a much cheaper one than the original. An acquisition function optimizer of class AcqOptimizer is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods, such as the DIRECT algorithm. Consequently the optimization problem of the acquisition function can be handled as a black box optimization problem itself, but a much cheaper one than the original. AcqOptimizer objects are constructed with acqo(), which takes as input an Optimizer, a Terminator, and the acquisition function. Optimizers are stored in the mlr_optimizers dictionary and can be constructed with the sugar function opt(). Let’s select an optimizer first: optimizer = opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\") Task: Construct an acquisition function optimizer using the optimizer above, a termination criterion of your choice and the acquisition function from the previous exercise. Then, call $optimize() on the optimizer to suggest the next candidate hyperparameter configuration. Solution Unlock solution 2 Automating BO with OptimizerMbo We have now shown how to run a single iteration of the BO algorithm loop manually. In practice, one would use OptimizerMbo to put all these pieces together to automate the process. To determine the behavior of the BO algorithm on a global level, we need a loop function. We use the Efficient Global Optimization (EGO) algorithm, aka bayesopt_ego provided by mlr_loop_functions. You do not need to pass any of these building blocks to each other manually as the opt() constructor will do this for you: loop_function = mlr_loop_functions$get(\"bayesopt_ego\") surrogate = srlrn(lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\", control = list(trace = FALSE))) acq_function = acqf(\"ei\") acq_optimizer = acqo(opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"), terminator = trm(\"evals\", n_evals = 100)) optimizer = opt(\"mbo\", loop_function = loop_function, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer) Task: Use the MBO optimizer constructed above to solve the optimization problem. To do so, define an optimization instance (as in 1.2) and an initial design (as in 1.1). Then, evaluate the optimization instance on the initial design (as in 1.2). Then, call optimizer$optimize() on the instance. Solution Unlock solution 3 BO for HPO with TunerMbo mlr3mbo can be used for HPO by making use of TunerMbo, which is a wrapper around OptimizerMbo and works in the exact same way. As an example, we want to tune the cost and gamma parameters of lrn(\"classif.svm\") with a radial kernel on tsk(\"sonar\") with three-fold CV. lrn_svm = lrn(\"classif.svm\", kernel = \"radial\", type = \"C-classification\", cost = to_tune(1e-5, 1e5, logscale = TRUE), gamma = to_tune(1e-5, 1e5, logscale = TRUE) ) tuner = tnr(\"mbo\", loop_function = bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer) Task: Run the tuner on the lrn_svm for the sonar task and 3-fold CV. Use misclassification error as performance measure. What is the best HP configuration? Hint 1: See ?mlr3tuning::tune for help. Solution Unlock solution Summary We have learned how Bayesian Optimization can be used to solve black box optimization problems, and HPO problems specifically. Rather than simply spending a compute budget on evaluating arbitrary configurations, we optimize an acquisition function based on a surrogate model that maps hyperparameter configurations to their estimated generalization performance, to iteratively suggest new candidates.",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "Deep dive into Bayesian Optimization | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 11.9,
    "sitemap_lastmod": null,
    "twitter_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal After this exercise, you should be able to navigate the building blocks of Bayesian optimization (BO) using bbotk and mlr3mbo for general black box optimization problems, and more specifically, hyperparameter optimization (HPO). Introduction This section is a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). BO is more complex than other tuning methods, so we will motivate theory and methodology first. Black Box Optimization In hyperparameter optimization, learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, this is a black box optimization problem, which considers the optimization of a function whose mathematical structure is unknown or unexploitable. The only thing we can observe is the generalization performance of the function given a hyperparameter configuration. As evaluating the performance of a learner can take a lot of time, HPO is an expensive black box optimization problem. Bayesian Optimization There is many ways of doing black box optimization, grid and random search being examples for simple strategies. Bayesian optimization are a class of black box optimization algorithms that rely on a ‘surrogate model’ trained on observed hyperparameter evaluations to model the black box function. This surrogate model tries to capture the unknown function between hyperparameter configuations and estimated generalization performance using (the very low number of) observed function evaluations. During each iteration, BO algorithms employ an ‘acquisition function’ to determine the next candidate point for evaluation. This function measures the expected ‘utility’ of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight. In the rest of this section, we will first provide an introduction to black box optimization with the bbotk package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black box optimizer with mlr3mbo. Prerequisites Let’s load the packages required for this exercise: library(bbotk) library(mlr3verse) library(mlr3mbo) set.seed(123) Before we apply BO to hyperparamter optimization (HPO), we try to optimize the following simple sinusoidal function: sinus_1D = function(xs) 2 * xs$x1 * sin(14 * xs$x1) * sin(xs$x2) * xs$x2 1 Building Blocks of BO Bayesian optimization (BO) usually follows this process: Generate and evaluate an initial design Loop: 2.1. Fit a surrogate model on the archive of all observations made so far to model the unknown black box function. 2.2. Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next. 2.3. Evaluate the next candidate(s) and update the archive of all observations made so far. 2.4. Check if a given termination criterion is met, if not go back to 2.1. The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high. BO is a highly modular algorithm: as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of mlr3mbo reflects this modularity, with the base class for OptimizerMbo holding all the key elements: the BO algorithm loop structure (loop_function), surrogate model (Surrogate), acquisition function (AcqFunction), and acquisition function optimizer (AcqOptimizer). Let’s explore the interplay and interaction of these building blocks during optimization. 1.1 Initial design The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design. mlr3mbo allows you to either construct this manually or let a loop_function do this for you. We will demonstrate the first method here. To construct an initial design, we will use one of the four design generators available in paradox. Let’s try grid search first, assuming an initial design of nine points on a domain of two numeric variables ranging from 0 to 1: domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1)) generate_design_grid(domain, resolution = 3)$data x1 x2 1: 0.0 0.0 2: 0.0 0.5 3: 0.0 1.0 4: 0.5 0.0 5: 0.5 0.5 6: 0.5 1.0 7: 1.0 0.0 8: 1.0 0.5 9: 1.0 1.0 As you can see, this is more or less a simple data.table that encodes the set of hyperparameter configurations we want to evaluate first, before any of the real BO magic starts. Task: Construct a more refined initial design, using paradox to implement a Sobol design with 30 points, which has better coverage properties than grid or random search. If you are interested in why the Sobol design has favorable properties, you can take a look at the original paper by Niederreiter (1988). Solution Unlock solution 1.2 Generate data from initial design To generate training data for our surrogate model, we need a few more things: An Objective function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs). Objective functions can be created using different classes, all of which inherit from Objective. Here, we will use ObjectiveRFun. # We have already defined our domain, but will do here again: domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1)) # Our codomain: codomain = ps(y = p_dbl(tags = \"minimize\")) # Our objective: objective = ObjectiveRFun$new(sinus_1D, domain = domain, codomain = codomain) Further: OptimInstanceSingleCrit to construct an optimization instance that describes the optimization problem and stores the results Optimizer which is used to construct and configure optimization algorithms. Optimization Instance Let’s define our optimization instance and evaluate it on our initial Sobol design: instance = OptimInstanceSingleCrit$new(objective, terminator = trm(\"evals\", n_evals = 20)) instance$eval_batch(sobol_design) Task: Extract the training archive data from the tuning instance to find the data that we will now use to train our surrogate model with in the first iteration. Solution Unlock solution 1.3 Train surrogate model A surrogate model wraps a regression learner that models the unknown black box function based on observed data. In mlr3mbo, the SurrogateLearner is a higher-level R6 class inheriting from the base Surrogate class, designed to construct and manage the surrogate model, including automatic construction of the TaskRegr that the learner should be trained on at each iteration of the BO loop. Any regression learner in mlr3 can be used. However, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model, the latter of which requires the \"se\" predict_type to be supported. Therefore not all learners are suitable for all scenarios. Typical choices are random forests or Gaussian processes (lrn(\"regr.km\")), which we will use here. You can learn more about Gaussian processes in Williams and Rasmussen (2006). lrn_gp = lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\", control = list(trace = FALSE)) The Matérn covariance function is a kernel used in Gaussian processes to model the smoothness of the random function, offering a flexible class of smoothness parameters. The BFGS algorithm is a type of quasi-Newton method used for optimization, particularly effective in maximizing the likelihood in Gaussian process models by efficiently finding parameter estimates. A SurrogateLearner can be constructed by passing a LearnerRegr object to the sugar function srlrn(), alongside the archive of the instance: surrogate = srlrn(lrn_gp, archive = instance$archive) Internally, the regression learner is fit on a TaskRegr where features are the variables of the domain and the target is the codomain, the data is from the archive of the OptimInstance that is to be optimized. Task: Update the surrogate model, which essentially fits the gaussian process. Then, inspect the trained random forest model that is contained within the surrogate: Hint 1: Fitting the surrogate model will require calling one of the methods of surrogate. See ?surrogate for help. Solution Unlock solution 1.4 Define an acquisition function Roughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the perceived ‘utility’ of each point of the search space if it were to be evaluated in the next iteration. A popular example is the expected improvement, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the ‘incumbent’), given the performance prediction of the surrogate model. Calculating the expected improvement requires mean and standard deviation predictions from the model. In mlr3mbo, acquisition functions are stored in the mlr_acqfunctions dictionary and can be constructed with acqf(), passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement to choose the next candidate for evaluation. Task: Construct an aquisition function object using expected improvement. Then, update the aquisition function object. Solution Unlock solution 1.5 Optimize acquisition function Why would we need to optimize the acquisition function? Well, the acquisition function can tell us how “promising” an arbitrary hyperparameter configuration is. If we want to find the “most promising” hyperparameter configuration, we again need to optimize the acquisition function. Consequently the optimization problem of the acquisition function is handled as a black box optimization problem itself, but it is a much cheaper one than the original. An acquisition function optimizer of class AcqOptimizer is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods, such as the DIRECT algorithm. Consequently the optimization problem of the acquisition function can be handled as a black box optimization problem itself, but a much cheaper one than the original. AcqOptimizer objects are constructed with acqo(), which takes as input an Optimizer, a Terminator, and the acquisition function. Optimizers are stored in the mlr_optimizers dictionary and can be constructed with the sugar function opt(). Let’s select an optimizer first: optimizer = opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\") Task: Construct an acquisition function optimizer using the optimizer above, a termination criterion of your choice and the acquisition function from the previous exercise. Then, call $optimize() on the optimizer to suggest the next candidate hyperparameter configuration. Solution Unlock solution 2 Automating BO with OptimizerMbo We have now shown how to run a single iteration of the BO algorithm loop manually. In practice, one would use OptimizerMbo to put all these pieces together to automate the process. To determine the behavior of the BO algorithm on a global level, we need a loop function. We use the Efficient Global Optimization (EGO) algorithm, aka bayesopt_ego provided by mlr_loop_functions. You do not need to pass any of these building blocks to each other manually as the opt() constructor will do this for you: loop_function = mlr_loop_functions$get(\"bayesopt_ego\") surrogate = srlrn(lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\", control = list(trace = FALSE))) acq_function = acqf(\"ei\") acq_optimizer = acqo(opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"), terminator = trm(\"evals\", n_evals = 100)) optimizer = opt(\"mbo\", loop_function = loop_function, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer) Task: Use the MBO optimizer constructed above to solve the optimization problem. To do so, define an optimization instance (as in 1.2) and an initial design (as in 1.1). Then, evaluate the optimization instance on the initial design (as in 1.2). Then, call optimizer$optimize() on the instance. Solution Unlock solution 3 BO for HPO with TunerMbo mlr3mbo can be used for HPO by making use of TunerMbo, which is a wrapper around OptimizerMbo and works in the exact same way. As an example, we want to tune the cost and gamma parameters of lrn(\"classif.svm\") with a radial kernel on tsk(\"sonar\") with three-fold CV. lrn_svm = lrn(\"classif.svm\", kernel = \"radial\", type = \"C-classification\", cost = to_tune(1e-5, 1e5, logscale = TRUE), gamma = to_tune(1e-5, 1e5, logscale = TRUE) ) tuner = tnr(\"mbo\", loop_function = bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer) Task: Run the tuner on the lrn_svm for the sonar task and 3-fold CV. Use misclassification error as performance measure. What is the best HP configuration? Hint 1: See ?mlr3tuning::tune for help. Solution Unlock solution Summary We have learned how Bayesian Optimization can be used to solve black box optimization problems, and HPO problems specifically. Rather than simply spending a compute budget on evaluating arbitrary configurations, we optimize an acquisition function based on a surrogate model that maps hyperparameter configurations to their estimated generalization performance, to iteratively suggest new candidates.",
    "twitter_title": "Deep dive into Bayesian Optimization | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/05/deep-dive-into-bayesian-optimization/",
    "word_count": 2385
  }
}