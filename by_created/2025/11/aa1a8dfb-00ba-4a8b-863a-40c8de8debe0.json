{
  "uuid": "aa1a8dfb-00ba-4a8b-863a-40c8de8debe0",
  "created_at": "2025-11-22 19:59:11",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/02/building-llm-powered-applications-with-shiny-for-python-practical-insights/",
    "crawled_at": "2025-11-22T10:53:40.999674",
    "external_links": [
      {
        "href": "https://appsilon.com/llm-apps-shiny-python/",
        "text": "Appsilon | Enterprise R Shiny Dashboards"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://www.appsilon.com/guides/shiny-python-guide#1-introduction-to-shiny-for-python",
        "text": "Shiny for Python applications"
      },
      {
        "href": "https://www.appsilon.com/guides/shiny-python-guide#1-introduction-to-shiny-for-python",
        "text": "Find out in our ultimate guide, packed with insights and hands-on tips."
      },
      {
        "href": "https://www.langchain.com/",
        "text": "LangChain"
      },
      {
        "href": "https://platform.openai.com/",
        "text": "OpenAI"
      },
      {
        "href": "https://docs.anthropic.com/en/release-notes/api",
        "text": "Claude APIs"
      },
      {
        "href": "https://github.com/Appsilon/tapyr-template",
        "text": "Tapyr template"
      },
      {
        "href": "https://www.appsilon.com/post/introducing-tapyr",
        "text": "Learn how Tapyr makes building and deploying them easier than ever."
      },
      {
        "href": "https://www.appsilon.com/post/introducing-tapyr",
        "text": "‍"
      },
      {
        "href": "https://docs.pydantic.dev/latest/",
        "text": "Pydantic"
      },
      {
        "href": "https://www.appsilon.com/post/challenges-of-working-with-cloud-llms",
        "text": "See what challenges we faced and how we addressed them in our latest case study."
      },
      {
        "href": "https://www.appsilon.com/post/challenges-of-working-with-cloud-llms",
        "text": "‍"
      },
      {
        "href": "https://shiny.posit.co/py/components/display-messages/chat/",
        "text": "ui.Chat component"
      },
      {
        "href": "https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html",
        "text": "LangChain with Pydantic"
      },
      {
        "href": "https://platform.openai.com/docs/guides/structured-outputs",
        "text": "OpenAI’s native structured output"
      },
      {
        "href": "https://github.com/Appsilon/tapyr-template",
        "text": "Tapyr template"
      },
      {
        "href": "https://www.appsilon.com/contact-us",
        "text": "Reach out to Appsilon"
      },
      {
        "href": "https://appsilon.com/llm-apps-shiny-python/",
        "text": "Appsilon | Enterprise R Shiny Dashboards"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Building LLM-Powered Applications with Shiny for Python: Practical Insights | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://wordpress.appsilon.com/wp-content/uploads/2025/02/67a64ab6cea3d6a0776f02ae_LLM-p-1080.webp"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/cdn.prod.website-files.com/654fd3ad88635290d9845b9e/67a64ac5251764c28c66f88f_AD_4nXdbg2bTTzJ6ZSfksBK1EJmJl_-9ovfNG-vycNs8QgzUibKbTjFncoms04xVp_xQW7ZA5fAkevQb_Rkj_j345me-ZzefTpwvn9SVwNxmGoGjFrkAK5J_5s6Bg-og4ekE8KIksVytqg.png?w=578&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/cdn.prod.website-files.com/654fd3ad88635290d9845b9e/67a64ac5f7cf2680d48eaa32_AD_4nXcHxUB62o54p5RDGF9_HWXYnMKRHpBPmV05-5i9GDMHLwDIwJ-OzACon3u1k5SHrz5G2CEqoyAihz1sg_8ZlwHuRBfG09PemXDreLQrb3aI3f1tYvCW2HXuFMzvdoGesZgp9b5l.png?w=578&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/piotr-storozenko/",
        "text": "Piotr Storożenko"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-390827 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Building LLM-Powered Applications with Shiny for Python: Practical Insights</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">February 26, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/piotr-storozenko/\">Piotr Storożenko</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://appsilon.com/llm-apps-shiny-python/\"> Appsilon | Enterprise R Shiny Dashboards</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><div><img alt=\"\" class=\"attachment-medium size-medium wp-post-image\" data-lazy-src=\"https://wordpress.appsilon.com/wp-content/uploads/2025/02/67a64ab6cea3d6a0776f02ae_LLM-p-1080.webp\" decoding=\"async\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" style=\"margin-bottom: 15px;\" width=\"450\"/><noscript><img alt=\"\" class=\"attachment-medium size-medium wp-post-image\" decoding=\"async\" loading=\"lazy\" src=\"https://wordpress.appsilon.com/wp-content/uploads/2025/02/67a64ab6cea3d6a0776f02ae_LLM-p-1080.webp\" style=\"margin-bottom: 15px;\" width=\"450\"/></noscript></div><p>At Appsilon, we’ve been integrating Large Language Models into <a href=\"https://www.appsilon.com/guides/shiny-python-guide#1-introduction-to-shiny-for-python\" rel=\"nofollow\" target=\"_blank\">Shiny for Python applications</a> for a while now. One thing became clear: the challenge isn’t in the initial integration. Shiny for Python’s ui.Chat component makes that straightforward. The real complexity lies in building applications that can evolve with your needs.</p>\n<blockquote><p>Python meets Shiny: what can you build? <a href=\"https://www.appsilon.com/guides/shiny-python-guide#1-introduction-to-shiny-for-python\" rel=\"nofollow\" target=\"_blank\">Find out in our ultimate guide, packed with insights and hands-on tips.</a></p></blockquote>\n<p>We’ve seen projects start with simple text completion, grow to include structured outputs, and eventually handle image processing. Each iteration brought new technical decisions: Use a unified LLM interface like <a href=\"https://www.langchain.com/\" rel=\"nofollow\" target=\"_blank\">LangChain</a>, or work directly with <a href=\"https://platform.openai.com/\" rel=\"nofollow\" target=\"_blank\">OpenAI</a> and <a href=\"https://docs.anthropic.com/en/release-notes/api\" rel=\"nofollow\" target=\"_blank\">Claude APIs</a>? Streaming or non-streaming responses?</p>\n<p>This post shares key architectural insights from our journey. You’ll learn how to structure your project to make future iterations possible and efficient. We’ll focus on practical decisions that impact maintainability and flexibility, drawing from our experience building production applications.</p>\n<p>Let’s dive in!</p>\n<figure class=\"w-richtext-align-fullwidth w-richtext-figure-type-image\">\n<div><img alt=\"\" data-lazy-src=\"https://i0.wp.com/cdn.prod.website-files.com/654fd3ad88635290d9845b9e/67a64ac5251764c28c66f88f_AD_4nXdbg2bTTzJ6ZSfksBK1EJmJl_-9ovfNG-vycNs8QgzUibKbTjFncoms04xVp_xQW7ZA5fAkevQb_Rkj_j345me-ZzefTpwvn9SVwNxmGoGjFrkAK5J_5s6Bg-og4ekE8KIksVytqg.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" decoding=\"async\" src=\"https://i0.wp.com/cdn.prod.website-files.com/654fd3ad88635290d9845b9e/67a64ac5251764c28c66f88f_AD_4nXdbg2bTTzJ6ZSfksBK1EJmJl_-9ovfNG-vycNs8QgzUibKbTjFncoms04xVp_xQW7ZA5fAkevQb_Rkj_j345me-ZzefTpwvn9SVwNxmGoGjFrkAK5J_5s6Bg-og4ekE8KIksVytqg.png?w=578&amp;ssl=1\"/></noscript></div>\n</figure>\n<h2>Smart Architectural Choices</h2>\n<p>The key to maintaining velocity in LLM projects is the separation of concerns. Your chat UI shouldn’t need to know whether you’re using GPT-4o, Claude, or a local model.</p>\n<p>Here’s what works for us:</p>\n<ol role=\"list\">\n<li><strong>Separate LLM provider logic</strong> – Create a dedicated LLM handler class that encapsulates all provider-specific code. We learned this the hard way – when we needed to switch from LangChain to raw OpenAI for image processing, having tightly coupled provider code meant touching multiple parts of the application. A clean separation makes these transitions manageable.</li>\n<li><strong>Enable testing without APIs</strong> – Create a mock LLM handler class that implements the same interface as your real handler. This lets you test your application’s UI behavior without making expensive API calls. If your app uses streaming responses, mock those too – it’s crucial to test your application under the same conditions it will run in production.</li>\n<li><strong>Structured project layout </strong>– We use our open-source <a href=\"https://github.com/Appsilon/tapyr-template\" rel=\"nofollow\" target=\"_blank\">Tapyr template</a> for Shiny Python applications. It provides a battle-tested structure that naturally supports these separations, handles environment setup, and sets up logging. This becomes crucial when your chat application grows to include features like conversation history or document processing.</li>\n</ol>\n<blockquote><p>Enterprise-ready Shiny for Python dashboards? <a href=\"https://www.appsilon.com/post/introducing-tapyr\" rel=\"nofollow\" target=\"_blank\">Learn how Tapyr makes building and deploying them easier than ever.</a></p></blockquote>\n<p><a href=\"https://www.appsilon.com/post/introducing-tapyr\" rel=\"nofollow\" target=\"_blank\">‍</a>This might seem like overengineering for a simple chat application. However, we’ve found that LLM projects rarely stay simple. What starts as basic text completion often evolves into multi-modal interactions with structured outputs. Good architecture makes these transitions smooth without much overhead.</p>\n<h2>‍<strong>Table Comparing the Functionality of Different Ways of Interacting With LLMs in Python</strong>‍</h2>\n<div class=\"w-embed\">\n<table>\n<thead>\n<tr>\n<th>Functionality</th>\n<th>LangChain</th>\n<th>OpenAI API (Completion)</th>\n<th>OpenAI API (Assistants)</th>\n<th>Anthropic Claude</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Using Structured Output</td>\n<td>Excellent structured output, works well with streaming</td>\n<td>Basic Pydantic support, less robust than LangChain</td>\n<td>No Pydantic support/response format</td>\n<td>Basic JSON-specified response structure</td>\n</tr>\n<tr>\n<td>Response format with Pydantic</td>\n<td>Works well with Pydantic</td>\n<td>Basic Pydantic support</td>\n<td>No Pydantic support</td>\n<td>No Pydantic support</td>\n</tr>\n<tr>\n<td>Image Attachments</td>\n<td>No support for sending images to API</td>\n<td>Supports image analysis well</td>\n<td>Supports image analysis well</td>\n<td>Supports image analysis well</td>\n</tr>\n<tr>\n<td>Adding Files</td>\n<td>No capability</td>\n<td>No capability</td>\n<td>Supports uploading files, model can refer to them</td>\n<td>No capability</td>\n</tr>\n<tr>\n<td>Streaming Response</td>\n<td>Excellent UX for structured streaming responses</td>\n<td>Supports streaming responses</td>\n<td>Supports streaming responses</td>\n<td>Supports streaming responses</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>When we mention “excellent UX for structured streaming responses,” we’re talking about the ability to show partial results to users in real-time, while maintaining a structured format. With LangChain, you can see responses building token by token – imagine watching a table populate gradually or JSON fields growing incrementally.</p>\n<p>While OpenAI’s structured output API also streams responses, it works at the field level – you’ll see complete <a href=\"https://docs.pydantic.dev/latest/\" rel=\"nofollow\" target=\"_blank\">Pydantic</a> fields appear one at a time. Both approaches avoid making users wait for the complete response, but LangChain’s token-by-token streaming often feels more fluid, especially for elements like code blocks where seeing the gradual construction can be more engaging.</p>\n<blockquote><p>Cloud LLMs come with trade-offs. <a href=\"https://www.appsilon.com/post/challenges-of-working-with-cloud-llms\" rel=\"nofollow\" target=\"_blank\">See what challenges we faced and how we addressed them in our latest case study.</a></p></blockquote>\n<p><a href=\"https://www.appsilon.com/post/challenges-of-working-with-cloud-llms\" rel=\"nofollow\" target=\"_blank\">‍</a>Real-world Integration NotesGetting started with LLMs in Shiny for Python is straightforward. The <a href=\"https://shiny.posit.co/py/components/display-messages/chat/\" rel=\"nofollow\" target=\"_blank\">ui.Chat component</a> provides everything you need for basic chat functionality. But as with most LLM projects, you quickly want to extend it in unexpected ways.A typical journey looks like this: You begin with basic text interactions.</p>\n<p>Then, you want structured responses to better integrate with your application logic.<a href=\"https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html\" rel=\"nofollow\" target=\"_blank\"> LangChain with Pydantic</a> makes this easy. But as your application grows, you might need to handle images, only to discover LangChain doesn’t support them yet. So you might need to switch to<a href=\"https://platform.openai.com/docs/guides/structured-outputs\" rel=\"nofollow\" target=\"_blank\"> OpenAI’s native structured output</a> for better control, performance, and set of custom features. Each step brings new integration challenges.</p>\n<p>Our experience taught us when to prototype. For image handling, we knew upfront we’d need to switch from LangChain to raw OpenAI calls, so creating a PoC was an obvious choice. What caught us off guard was transitioning between structured output implementations.</p>\n<p>Since both LangChain and OpenAI use Pydantic for validation, we assumed the switch would be straightforward. Instead, we discovered subtle differences – like OpenAI’s sensitivity to field descriptions in Pydantic models.</p>\n<p>This taught us a valuable lesson: create isolated proofs of concept even when tools seem perfectly compatible.</p>\n<figure class=\"w-richtext-align-fullwidth w-richtext-figure-type-image\">\n<div><img alt=\"\" data-lazy-src=\"https://i0.wp.com/cdn.prod.website-files.com/654fd3ad88635290d9845b9e/67a64ac5f7cf2680d48eaa32_AD_4nXcHxUB62o54p5RDGF9_HWXYnMKRHpBPmV05-5i9GDMHLwDIwJ-OzACon3u1k5SHrz5G2CEqoyAihz1sg_8ZlwHuRBfG09PemXDreLQrb3aI3f1tYvCW2HXuFMzvdoGesZgp9b5l.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" decoding=\"async\" src=\"https://i0.wp.com/cdn.prod.website-files.com/654fd3ad88635290d9845b9e/67a64ac5f7cf2680d48eaa32_AD_4nXcHxUB62o54p5RDGF9_HWXYnMKRHpBPmV05-5i9GDMHLwDIwJ-OzACon3u1k5SHrz5G2CEqoyAihz1sg_8ZlwHuRBfG09PemXDreLQrb3aI3f1tYvCW2HXuFMzvdoGesZgp9b5l.png?w=578&amp;ssl=1\"/></noscript></div>\n</figure>\n<p>‍<strong>The most valuable lesson?</strong> It’s not about choosing the perfect tools upfront. It’s about building your application so it can adapt as requirements evolve. Whether you’re using LangChain’s abstractions or raw provider APIs, good architecture makes these transitions manageable.</p>\n<h2>Lessons Learned</h2>\n<p>Building LLM-powered applications is an iterative process.</p>\n<p>Here are our key takeaways:</p>\n<ol role=\"list\">\n<li><strong>Create small proofs of concept</strong> – not just for new features, but also when switching between seemingly compatible tools. This habit saves significant refactoring time later.</li>\n<li><strong>Design your code for change</strong>. A clear separation between LLM logic and application code isn’t overengineering – it’s preparation for inevitable evolution.</li>\n<li>When starting a new Shiny for Python project with LLMs, consider using our open-source<a href=\"https://github.com/Appsilon/tapyr-template\" rel=\"nofollow\" target=\"_blank\"> Tapyr template</a>. It provides the structure needed to maintain <strong>clean separation of concerns</strong> as your application grows.</li>\n</ol>\n<blockquote><p>Want to discuss your Shiny for Python project?<a href=\"https://www.appsilon.com/contact-us\" rel=\"nofollow\" target=\"_blank\"> Reach out to Appsilon</a>. We’re here to help you build robust, scalable LLM applications.</p></blockquote>\n<p>The post appeared first on appsilon.com/blog/.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://appsilon.com/llm-apps-shiny-python/\"> Appsilon | Enterprise R Shiny Dashboards</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Building LLM-Powered Applications with Shiny for Python: Practical Insights\nPosted on\nFebruary 26, 2025\nby\nPiotr Storożenko\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nAppsilon | Enterprise R Shiny Dashboards\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nAt Appsilon, we’ve been integrating Large Language Models into\nShiny for Python applications\nfor a while now. One thing became clear: the challenge isn’t in the initial integration. Shiny for Python’s ui.Chat component makes that straightforward. The real complexity lies in building applications that can evolve with your needs.\nPython meets Shiny: what can you build?\nFind out in our ultimate guide, packed with insights and hands-on tips.\nWe’ve seen projects start with simple text completion, grow to include structured outputs, and eventually handle image processing. Each iteration brought new technical decisions: Use a unified LLM interface like\nLangChain\n, or work directly with\nOpenAI\nand\nClaude APIs\n? Streaming or non-streaming responses?\nThis post shares key architectural insights from our journey. You’ll learn how to structure your project to make future iterations possible and efficient. We’ll focus on practical decisions that impact maintainability and flexibility, drawing from our experience building production applications.\nLet’s dive in!\nSmart Architectural Choices\nThe key to maintaining velocity in LLM projects is the separation of concerns. Your chat UI shouldn’t need to know whether you’re using GPT-4o, Claude, or a local model.\nHere’s what works for us:\nSeparate LLM provider logic\n– Create a dedicated LLM handler class that encapsulates all provider-specific code. We learned this the hard way – when we needed to switch from LangChain to raw OpenAI for image processing, having tightly coupled provider code meant touching multiple parts of the application. A clean separation makes these transitions manageable.\nEnable testing without APIs\n– Create a mock LLM handler class that implements the same interface as your real handler. This lets you test your application’s UI behavior without making expensive API calls. If your app uses streaming responses, mock those too – it’s crucial to test your application under the same conditions it will run in production.\nStructured project layout\n– We use our open-source\nTapyr template\nfor Shiny Python applications. It provides a battle-tested structure that naturally supports these separations, handles environment setup, and sets up logging. This becomes crucial when your chat application grows to include features like conversation history or document processing.\nEnterprise-ready Shiny for Python dashboards?\nLearn how Tapyr makes building and deploying them easier than ever.\n‍\nThis might seem like overengineering for a simple chat application. However, we’ve found that LLM projects rarely stay simple. What starts as basic text completion often evolves into multi-modal interactions with structured outputs. Good architecture makes these transitions smooth without much overhead.\n‍\nTable Comparing the Functionality of Different Ways of Interacting With LLMs in Python\n‍\nFunctionality\nLangChain\nOpenAI API (Completion)\nOpenAI API (Assistants)\nAnthropic Claude\nUsing Structured Output\nExcellent structured output, works well with streaming\nBasic Pydantic support, less robust than LangChain\nNo Pydantic support/response format\nBasic JSON-specified response structure\nResponse format with Pydantic\nWorks well with Pydantic\nBasic Pydantic support\nNo Pydantic support\nNo Pydantic support\nImage Attachments\nNo support for sending images to API\nSupports image analysis well\nSupports image analysis well\nSupports image analysis well\nAdding Files\nNo capability\nNo capability\nSupports uploading files, model can refer to them\nNo capability\nStreaming Response\nExcellent UX for structured streaming responses\nSupports streaming responses\nSupports streaming responses\nSupports streaming responses\nWhen we mention “excellent UX for structured streaming responses,” we’re talking about the ability to show partial results to users in real-time, while maintaining a structured format. With LangChain, you can see responses building token by token – imagine watching a table populate gradually or JSON fields growing incrementally.\nWhile OpenAI’s structured output API also streams responses, it works at the field level – you’ll see complete\nPydantic\nfields appear one at a time. Both approaches avoid making users wait for the complete response, but LangChain’s token-by-token streaming often feels more fluid, especially for elements like code blocks where seeing the gradual construction can be more engaging.\nCloud LLMs come with trade-offs.\nSee what challenges we faced and how we addressed them in our latest case study.\n‍\nReal-world Integration NotesGetting started with LLMs in Shiny for Python is straightforward. The\nui.Chat component\nprovides everything you need for basic chat functionality. But as with most LLM projects, you quickly want to extend it in unexpected ways.A typical journey looks like this: You begin with basic text interactions.\nThen, you want structured responses to better integrate with your application logic.\nLangChain with Pydantic\nmakes this easy. But as your application grows, you might need to handle images, only to discover LangChain doesn’t support them yet. So you might need to switch to\nOpenAI’s native structured output\nfor better control, performance, and set of custom features. Each step brings new integration challenges.\nOur experience taught us when to prototype. For image handling, we knew upfront we’d need to switch from LangChain to raw OpenAI calls, so creating a PoC was an obvious choice. What caught us off guard was transitioning between structured output implementations.\nSince both LangChain and OpenAI use Pydantic for validation, we assumed the switch would be straightforward. Instead, we discovered subtle differences – like OpenAI’s sensitivity to field descriptions in Pydantic models.\nThis taught us a valuable lesson: create isolated proofs of concept even when tools seem perfectly compatible.\n‍\nThe most valuable lesson?\nIt’s not about choosing the perfect tools upfront. It’s about building your application so it can adapt as requirements evolve. Whether you’re using LangChain’s abstractions or raw provider APIs, good architecture makes these transitions manageable.\nLessons Learned\nBuilding LLM-powered applications is an iterative process.\nHere are our key takeaways:\nCreate small proofs of concept\n– not just for new features, but also when switching between seemingly compatible tools. This habit saves significant refactoring time later.\nDesign your code for change\n. A clear separation between LLM logic and application code isn’t overengineering – it’s preparation for inevitable evolution.\nWhen starting a new Shiny for Python project with LLMs, consider using our open-source\nTapyr template\n. It provides the structure needed to maintain\nclean separation of concerns\nas your application grows.\nWant to discuss your Shiny for Python project?\nReach out to Appsilon\n. We’re here to help you build robust, scalable LLM applications.\nThe post appeared first on appsilon.com/blog/.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nAppsilon | Enterprise R Shiny Dashboards\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "At Appsilon, we’ve been integrating Large Language Models into Shiny for Python applications for a while now. One thing became clear: the challenge isn’t in the initial integration. Shiny for Python’s ui.Chat component makes that straightforward. The real complexity lies in building applications that can evolve with your needs. Python meets Shiny: what can you […] The post appeared first on appsilon.com/blog/.",
    "meta_keywords": null,
    "og_description": "At Appsilon, we’ve been integrating Large Language Models into Shiny for Python applications for a while now. One thing became clear: the challenge isn’t in the initial integration. Shiny for Python’s ui.Chat component makes that straightforward. The real complexity lies in building applications that can evolve with your needs. Python meets Shiny: what can you […] The post appeared first on appsilon.com/blog/.",
    "og_image": "https://wordpress.appsilon.com/wp-content/uploads/2025/02/67a64ab6cea3d6a0776f02ae_LLM-p-1080.webp",
    "og_title": "Building LLM-Powered Applications with Shiny for Python: Practical Insights | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 6,
    "sitemap_lastmod": null,
    "twitter_description": "At Appsilon, we’ve been integrating Large Language Models into Shiny for Python applications for a while now. One thing became clear: the challenge isn’t in the initial integration. Shiny for Python’s ui.Chat component makes that straightforward. The real complexity lies in building applications that can evolve with your needs. Python meets Shiny: what can you […] The post appeared first on appsilon.com/blog/.",
    "twitter_title": "Building LLM-Powered Applications with Shiny for Python: Practical Insights | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/02/building-llm-powered-applications-with-shiny-for-python-practical-insights/",
    "word_count": 1208
  }
}