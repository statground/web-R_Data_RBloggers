{
  "id": "5621398c2179146321370a4032315c42f6a51f17",
  "url": "https://www.r-bloggers.com/2023/11/interpreting-the-likelihood-ratio-cost/",
  "created_at_utc": "2025-11-17T20:39:17Z",
  "data": null,
  "raw_original": {
    "uuid": "ee2e4a02-1b8f-4338-9ab1-dff9130016fd",
    "created_at": "2025-11-17 20:39:17",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2023/11/interpreting-the-likelihood-ratio-cost/",
      "crawled_at": "2025-11-17T09:57:03.660535",
      "external_links": [
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost",
          "text": "Valerio Gherardi"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Forensic_science",
          "text": "Forensic\nScience"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn1",
          "text": "1"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn2",
          "text": "2"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn3",
          "text": "3"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn4",
          "text": "4"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn5",
          "text": "5"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn6",
          "text": "6"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn7",
          "text": "7"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn8",
          "text": "8"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn9",
          "text": "9"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn10",
          "text": "10"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn11",
          "text": "11"
        },
        {
          "href": "https://doi.org/10.1016/j.csl.2005.08.001",
          "text": "https://doi.org/10.1016/j.csl.2005.08.001"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref1",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref2",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref3",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref4",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref5",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref6",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref7",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref8",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref9",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref10",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref11",
          "text": "↩︎"
        },
        {
          "href": "https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost",
          "text": "Valerio Gherardi"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Interpreting the Likelihood Ratio cost | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-3-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-4-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/vgherard/",
          "text": "vgherard"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-380218 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Interpreting the Likelihood Ratio cost</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 14, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/vgherard/\">vgherard</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost\"> Valerio Gherardi</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<h2 id=\"intro\">Intro</h2>\n<p>During the last few months, I’ve been working on a machine learning\nalgorithm with applications in <a href=\"https://en.wikipedia.org/wiki/Forensic_science\" rel=\"nofollow\" target=\"_blank\">Forensic\nScience</a>, a.k.a. Criminalistics. In this field, one common task for\nthe data analyst is to present the <em>trier-of-fact</em> (the person or\npeople who determine the facts in a legal proceeding) with a numerical\nassessment of the strength of the evidence provided by available data\ntowards different hypotheses. In more familiar terms, the forensic\nexpert is responsible of computing the likelihoods (or likelihood\nratios) of data under competing hypotheses, which are then used by the\ntrier-of-fact to produce Bayesian posterior probabilities for the\nhypotheses in question<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn1\" id=\"fnref1\" rel=\"nofollow\" target=\"_blank\"><sup>1</sup></a>.</p>\n<p>In relation to this, forensic scientists have developed a bunch of\ntechniques to evaluate the performance of a likelihood ratio model in\ndiscriminating between two alternative hypothesis. In particular, I have\ncome across the so called <em>Likelihood Ratio Cost</em>, usually\ndefined as:</p>\n<p><span class=\"math display\">\\[\nC_{\\text{LLR}} = \\frac{1}{2N_1} \\sum _{Y_i=1} \\log(1+r(X_i)\n^{-1})+\\frac{1}{2N_0} \\sum _{Y_i=0} \\log(1+r(X_i)), (\\#eq:CLLR)\n\\]</span> where we assume we have data consisting of <span class=\"math inline\">\\(N_1+N_0\\)</span> independent identically\ndistributed observations <span class=\"math inline\">\\((X_i,\\,Y_i)\\)</span>, with binary <span class=\"math inline\">\\(Y\\)</span>; <span class=\"math inline\">\\(N_1\\)</span> and <span class=\"math inline\">\\(N_0\\)</span> stand for the number of positive\n(<span class=\"math inline\">\\(Y=1\\)</span>) and negative (<span class=\"math inline\">\\(Y=0\\)</span>) cases; and <span class=\"math inline\">\\(r(X)\\)</span> is a model for the likelihood ratio\n<span class=\"math inline\">\\(\\Lambda(X) \\equiv \\frac{\\text{Pr}(X\\vert Y =\n1)}{\\text{Pr}(X\\vert Y = 0)}\\)</span>.</p>\n<p>The main reason for writing this note was to understand a bit better\nwhat it means to optimize Eq. @ref(eq:CLLR), which does not look\nimmediately obvious to me from its definition<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn2\" id=\"fnref2\" rel=\"nofollow\" target=\"_blank\"><sup>2</sup></a>. In particular: is the\npopulation minimizer of Eq. @ref(eq:CLLR) the actual likelihood ratio?\nAnd in what sense is a model with lower <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> better than one with a\ncorrespondingly higher value?</p>\n<p>The short answers to these questions are: yes; and: <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> optimization seeks for the\nmodel with the best predictive performance in a Bayesian inference\nsetting with uninformative prior on <span class=\"math inline\">\\(Y\\)</span>, assuming that this prior actually\nreflects reality (<em>i.e.</em> <span class=\"math inline\">\\(\\text{Pr}(Y=1) = \\text{Pr}(Y=0) =\n\\frac{1}{2}\\)</span>). The mathematical details are given in the rest of\nthe post.</p>\n<!-- Strictly speaking, there are several aspects to what I have simply referred to as the \"performance of a likelihood ratio model\". First of all, there is the -->\n<!-- left-over uncertainty on $Y$ after measuring $X$, which is an intrinsic property of the data and is independent of modeling. Second, $X$ may not correspond to raw data, but rather be the result of some data-processing/summary, which will in general reduce the amount of available information on $Y$. Finally, in the general case, the likelihood ratio $r(X)$ will not be an exact model, but only an approximation estimated from data. All this aspects get captured and mixed by Eq. \\@ref(eq:CLLR), luckily in a way that can be actually decomposed (see below). -->\n<h2 id=\"cross-entropy-with-random-weights\">Cross-entropy with random\nweights</h2>\n<p>We start with a mathematical digression, which will turn out useful\nfor further developments. Let <span class=\"math inline\">\\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\)</span> be\nindependent draws from a joint distribution, with binary <span class=\"math inline\">\\(Y_i \\in \\{0,\\,1\\}\\)</span>. Given a function <span class=\"math inline\">\\(w=w(\\boldsymbol Y)\\)</span> that is symmetric in\nits arguments<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn3\" id=\"fnref3\" rel=\"nofollow\" target=\"_blank\"><sup>3</sup></a>, we define the random functional:</p>\n<p><span class=\"math display\">\\[\n\\mathcal L_N^w[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i\n\\log(f(X_i))+ w({\\boldsymbol Y}^c)( Y_i^c)\n\\log(f(X_i)^c)\\right],(\\#eq:WeightedLoss)\n\\]</span> where <span class=\"math inline\">\\(f=f(X)\\)</span> is any\nfunction satisfying <span class=\"math inline\">\\(f(X)\\in [0,\\,1]\\)</span>\nfor all <span class=\"math inline\">\\(X\\)</span>, and we let <span class=\"math inline\">\\(q^c = 1-q\\)</span> for any number <span class=\"math inline\">\\(q \\in [0,\\,1]\\)</span>. Notice that for <span class=\"math inline\">\\(w(\\boldsymbol{Y}) \\equiv 1\\)</span>, this is just\nthe usual cross-entropy loss.</p>\n<p>We now look for the population minimizer of @ref(eq:WeightedLoss),\n<em>i.e.</em> the function <span class=\"math inline\">\\(f_*\\)</span> that\nminimizes the functional <span class=\"math inline\">\\(f \\mapsto \\mathbb\nE(\\mathcal L _N ^w [f])\\)</span><a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn4\" id=\"fnref4\" rel=\"nofollow\" target=\"_blank\"><sup>4</sup></a>. Writing the expectation as:</p>\n<p><span class=\"math display\">\\[\n\\mathbb E(\\mathcal L _N ^w [f]) = -\\frac{1}{N}\\sum _{i=1} ^N \\mathbb\nE\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)\\cdot\n\\log(f(X_i))+E(Y_i^c\\cdot w(\\boldsymbol Y ^c)\\vert X_i)\\cdot\n\\log(f^c(X_i))\\right],\n\\]</span> we can easily see that <span class=\"math inline\">\\(\\mathbb\nE(\\mathcal L _N ^w [f])\\)</span> is a convex functional with a unique\nminimum given by:</p>\n<p><span class=\"math display\">\\[\nf_*(X_i) = \\frac{1}{1+r(X_i)^{-1}},\\quad r_*(X_i) = \\dfrac{E(Y_i\\cdot\nw(\\boldsymbol Y)\\vert X_i)}{E(Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert\nX_i)}.(\\#eq:PopMinimizer)\n\\]</span> The corresponding expected loss is:</p>\n<p><span class=\"math display\">\\[\n\\mathbb E(\\mathcal L _N ^w [f_*]) = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot\nw(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal\nH(f_*(X_i))\\right],\n\\]</span> where <span class=\"math inline\">\\(\\mathcal H(p) = -p \\log (p)\n-(1-p) \\log(1-p)\\)</span> is the entropy of a binary random variable\n<span class=\"math inline\">\\(Z\\)</span> with probability <span class=\"math inline\">\\(p = \\text{Pr}(Z=1)\\)</span> (the index <span class=\"math inline\">\\(i\\)</span> in the previous expression can be any\nindex, since data points are assumed to be identically distributed).</p>\n<p>Before looking at values of <span class=\"math inline\">\\(f\\)</span>\nother than <span class=\"math inline\">\\(f_*\\)</span>, we observe that the\nprevious expectation can be succintly expressed as:</p>\n<p><span class=\"math display\">\\[\n\\mathbb E(\\mathcal L _N ^w [f_*]) = k \\cdot H^\\prime(Y\\vert X),\n\\]</span> where <span class=\"math display\">\\[\nk = \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol\nY^c))(\\#eq:DefKappa)\n\\]</span> and <span class=\"math inline\">\\(H'(Y\\vert X)\\)</span> is\nthe conditional entropy of <span class=\"math inline\">\\(Y\\vert X\\)</span>\nwith respect to a <em>different</em> probability measure <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span>, defined by:</p>\n<p><span class=\"math display\">\\[\n\\text{Pr}^\\prime(E) = t \\cdot \\text {Pr}(E \\vert Y = 1) + (1-t)\\cdot\n\\text {Pr}(E \\vert Y = 0), (\\#eq:DefPrPrime)\n\\]</span> where <span class=\"math inline\">\\(t=\\text{Pr}^\\prime(Y=1)\\in\n[0,\\,1]\\)</span> is fixed by the requirement<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn5\" id=\"fnref5\" rel=\"nofollow\" target=\"_blank\"><sup>5</sup></a>:</p>\n<p><span class=\"math display\">\\[\n\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{\\text\n{Pr} (Y=1)}{\\text{Pr} (Y=0)}\\cdot\\dfrac{\\mathbb E(w(\\boldsymbol Y)\\vert\n\\sum _i Y_i &gt;0)}{\\mathbb E(w(\\boldsymbol Y^c)\\vert \\sum _i Y_i^c\n&gt;0)}. (\\#eq:DefPrPrime2)\n\\]</span> In terms of <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span>, the population\nminimizers <span class=\"math inline\">\\(f_*\\)</span> and <span class=\"math inline\">\\(r_*\\)</span> in Eq. @ref(eq:PopMinimizer) can be\nsimply expressed as:</p>\n<p><span class=\"math display\">\\[\nr_*(X)=\\dfrac{\\text {Pr}^\\prime(Y=1\\vert X)}{\\text {Pr}^\\prime(Y=0\\vert\nX)},\\qquad f_*(X)=\\text {Pr}^\\prime(Y=1\\vert X). (\\#eq:PopMinimizer2)\n\\]</span> If now <span class=\"math inline\">\\(f\\)</span> is an arbitrary\nfunction, we have:</p>\n<p><span class=\"math display\">\\[\n\\begin{split}\n\\mathbb E(\\mathcal L _N ^w [f]) – \\mathbb E(\\mathcal L _N ^w [f_*])\n&amp;= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot\nw(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert\nf(X_i))\\right]\n&amp;= k\\cdot D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\n\\end{split}\n\\]</span> where <span class=\"math inline\">\\(\\mathcal D(p\\vert \\vert q) =\np \\log (\\frac{p}{q}) + (1-p) \\log (\\frac{1-p}{1-q})\\)</span>, and <span class=\"math inline\">\\(D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime\n_f)\\)</span> is the Kullback-Liebler divergence between the measure\n<span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span> and the measure\n<span class=\"math inline\">\\(\\text{Pr}^\\prime _f\\)</span> defined by:</p>\n<p><span class=\"math display\">\\[\n\\text{Pr}^\\prime _f(Y = 1\\vert X)=f(X),\\qquad \\text{Pr}^\\prime\n_f(X)=\\text{Pr}^\\prime(X)\n\\]</span> (notice that <span class=\"math inline\">\\(\\text {Pr} ^{\\prime}\n_{f_*} \\equiv \\text{Pr} ^{\\prime}\\)</span> by definition). Finally,\nsuppose that <span class=\"math inline\">\\(X = g(\\widetilde X)\\)</span>\nfor some random variable <span class=\"math inline\">\\(\\widetilde\nX\\)</span>, and define the corresponding functional:</p>\n<p><span class=\"math display\">\\[\n\\widetilde{\\mathcal L} _N^w[\\widetilde f]  = -\\frac{1}{N}\\sum_{i=1} ^N\n\\left[w(\\boldsymbol Y)Y_i \\log(\\widetilde f(\\widetilde X))+\nw({\\boldsymbol Y}^c)( Y_i^c) \\log(\\widetilde f(\\widetilde X)^c)\\right].\n\\]</span> Then <span class=\"math inline\">\\(\\mathcal L _N ^w [f] =\n\\widetilde{\\mathcal L} _N^w[f \\circ g]\\)</span>. If <span class=\"math inline\">\\(\\widetilde f _* =\\)</span> is the population\nminimizer of <span class=\"math inline\">\\(\\widetilde{\\mathcal L}\n_N^w\\)</span>, it follows that <span class=\"math inline\">\\(\\mathbb E\n(\\widetilde{\\mathcal L} _N^w[\\widetilde f _*]) \\leq \\mathbb E(\\mathcal L\n_N ^w [f_*])\\)</span>.</p>\n<p>Putting everything together, we can decompose the expected loss for a\nfunction <span class=\"math inline\">\\(f=f(X)\\)</span>, where <span class=\"math inline\">\\(X= g(\\widetilde X)\\)</span>, in the following\nsuggestive way:</p>\n<p><span class=\"math display\">\\[\n\\begin{split}\n\\mathbb E(\\mathcal L _N ^w [f]) &amp;= (L_N ^w)_\\text{min}+(L_N\n^w)_\\text{proc} +(L_N ^w)_\\text{missp},\\\\\n(L_N ^w)_\\text{min}&amp;\\equiv\\mathbb E(\\widetilde{\\mathcal L}\n_N^w[{\\widetilde f} _*])  \\\\ &amp;=\n\\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot\nw(\\boldsymbol Y^c)\\vert \\widetilde X _i)\\cdot \\mathcal H({\\widetilde f}\n_*(\\widetilde X _i))\\right]\\\\\n&amp;=k\\cdot H^\\prime(Y\\vert \\widetilde X),\\\\\n(L_N ^w)_\\text{proc}&amp;\\equiv\\mathbb E(\\mathcal L _N ^w\n[f_*]-\\widetilde{\\mathcal L} _N^w[\\phi_*])  \\\\&amp; =\n\\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot\nw(\\boldsymbol Y^c)\\vert X_i)\\cdot  \\mathcal H(f_*(X_i))\n\\right]- (L_N ^w)_\\text{min}\\\\\n&amp; = k\\cdot I^\\prime(Y; \\widetilde X\\vert X),\\\\\n(L_N ^w)_\\text{missp} &amp; \\equiv \\mathbb E(\\mathcal L _N ^w [f]) –\n\\mathbb E(\\mathcal L _N ^w [f_*]) \\\\&amp;= \\mathbb E\\left[ \\mathbb\nE(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert\nX_i)\\cdot  \\mathcal  D(f_*(X_i)\\vert \\vert f(X_i))\\right]\\\\ &amp;=k\\cdot\nD(\\text {Pr}^\\prime\\vert \\vert \\text {Pr}^\\prime _f),\n\\end{split} (\\#eq:DecompositionWeightedLoss)\n\\]</span> where <span class=\"math inline\">\\(k\\)</span> is defined in Eq.\n@ref(eq:DefKappa). In the equation for <span class=\"math inline\">\\((L^w\n_N)_\\text{proc}\\)</span> we introduced the conditional mutual\ninformation (with respect to the measure <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span>), that satisfies <span class=\"citation\">(Cover and Thomas 2006)</span>:</p>\n<p><span class=\"math display\">\\[\nI(\\widetilde X;Y\\vert X) = I(\\widetilde X,Y)-I(X,Y) = H(Y\\vert\nX)-H(Y\\vert \\widetilde X).\n\\]</span></p>\n<p>The three components in Eq. @ref(eq:DecompositionWeightedLoss) can be\ninterpreted as follows: <span class=\"math inline\">\\((L_N\n^w)_\\text{min}\\)</span> represents the minimum expected loss achievable,\ngiven the data available <span class=\"math inline\">\\(\\widetilde\nX\\)</span>; <span class=\"math inline\">\\((L_N ^w)_\\text{proc}\\)</span>\naccounts for the information lost in the processing transformation <span class=\"math inline\">\\(X=g(\\widetilde X)\\)</span>; finally <span class=\"math inline\">\\((L_N ^w)_\\text{missp}\\)</span> is due to\nmisspecification, <em>i.e.</em> the fact that the model <span class=\"math inline\">\\(f(X)\\)</span> for the true posterior probability\n<span class=\"math inline\">\\(f_*(X)\\)</span> is an approximation.</p>\n<p>All the information-theoretic quantities (and their corresponding\noperative interpretations hinted in the previous paragraph) make\nreference to the measure <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span> defined by Eqs.\n@ref(eq:DefPrPrime) and @ref(eq:DefPrPrime2). This is merely the result\nof altering the proportion of positive (<span class=\"math inline\">\\(Y=1\\)</span>) and negative (<span class=\"math inline\">\\(Y=0\\)</span>) examples in the <span class=\"math inline\">\\(X\\)</span>–<span class=\"math inline\">\\(Y\\)</span>\njoint distribution by a factor dictated by the weight function <span class=\"math inline\">\\(w\\)</span> – while keeping conditional\ndistributions such as <span class=\"math inline\">\\(X\\vert Y\\)</span>\nunchanged.</p>\n<h2 id=\"a-familiar-case-cross-entropy-loss\">A familiar case:\ncross-entropy loss</h2>\n<p>For <span class=\"math inline\">\\(w(\\boldsymbol {Y}) = 1\\)</span>, the\nfunctional <span class=\"math inline\">\\(\\mathcal {L} _{N}\n^{w}[f]\\)</span> coincides with the usual cross-entropy loss<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn6\" id=\"fnref6\" rel=\"nofollow\" target=\"_blank\"><sup>6</sup></a>:</p>\n<p><span class=\"math display\">\\[\n\\text{CE}[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[Y_i \\log(f(X_i))+ (1-Y_i)\n\\log(1-f(X_i))\\right].(\\#eq:CrossEntropyLoss)\n\\]</span></p>\n<p>From Eq. @ref(eq:DefPrPrime2) we see that the measure <span class=\"math inline\">\\(\\text{Pr}^{\\prime}\\)</span> coincides with the\noriginal <span class=\"math inline\">\\(\\text{Pr}\\)</span>, so that by Eq.\n@ref(eq:PopMinimizer) the population minimizer of\n@ref(eq:CrossEntropyLoss) is <span class=\"math inline\">\\(f_{*}(X) =\n\\text{Pr}(Y=1\\vert X)\\)</span> (independently of sample size). Since\n<span class=\"math inline\">\\(k = 1\\)</span> (<em>cf.</em> Eq.\n@ref(eq:DefKappa)), the decomposition @ref(eq:DecompositionWeightedLoss)\nreads:</p>\n<p><span class=\"math display\">\\[\n\\begin{split}\n\\mathbb E(\\text{CE} [f]) &amp;=\n(\\text{CE})_\\text{min}+(\\text{CE})_\\text{proc}\n+(\\text{CE})_\\text{missp},\\\\\n(\\text{CE})_\\text{min}&amp;=H(Y\\vert \\widetilde X),\\\\\n(\\text{CE})_\\text{proc}&amp;= I(Y; \\widetilde X\\vert X),\\\\\n(\\text{CE})_\\text{missp} &amp;=D(\\text {Pr}\\vert \\vert \\text {Pr} _{f}),\n\\end{split} (\\#eq:DecompositionCE)\n\\]</span></p>\n<p>where conditional entropy <span class=\"math inline\">\\(H\\)</span>,\nmutual information <span class=\"math inline\">\\(I\\)</span> and relative\nentropy <span class=\"math inline\">\\(D\\)</span> now simply refer to the\noriginal measure <span class=\"math inline\">\\(\\text{Pr}\\)</span>.</p>\n<h2 id=\"the-likelihood-ratio-cost\">The Likelihood Ratio Cost</h2>\n<p>The quantity <span class=\"math inline\">\\(C_{\\text{LLR}}\\)</span>\ndefined in Eq. @ref(eq:CLLR) can be put in the general form\n@ref(eq:WeightedLoss), if we let <span class=\"math inline\">\\(f(X) =\n(1+r(X)^{-1})^{-1}\\)</span> and<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn7\" id=\"fnref7\" rel=\"nofollow\" target=\"_blank\"><sup>7</sup></a>:</p>\n<p><span class=\"math display\">\\[\nw(\\boldsymbol Y) = \\left(\\dfrac{2}{N}\\sum _{i = 1}^{N}Y_j \\right)^{-1}\n\\]</span> In what follows, I will consider a slight modification of the\nusual <span class=\"math inline\">\\(C_\\text{LLR}\\)</span>, defined by the\nweight function:</p>\n<p><span class=\"math display\">\\[\nw(\\boldsymbol Y) = \\dfrac{1}{2(N-1)}\\sum _{i = 1}^{N}(1-Y_j).\n\\]</span> This yields Eq. @ref(eq:CLLR) multiplied by <span class=\"math inline\">\\(\\dfrac{N_1N_0}{N(N-1)}\\)</span>, which I will keep\ndenoting as <span class=\"math inline\">\\(C_\\text{LLR}\\)</span>, with a\nslight abuse of notation.</p>\n<p>We can easily compute<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn8\" id=\"fnref8\" rel=\"nofollow\" target=\"_blank\"><sup>8</sup></a>:</p>\n<p><span class=\"math display\">\\[\n\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=1,\n(\\#eq:PriorCLLR)\n\\]</span> so that, by Eq. @ref(eq:PopMinimizer), the population\nminimizer of <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> is:</p>\n<p><span class=\"math display\">\\[\nr_*(X) = \\Lambda (X),\\quad f_*(X)=\\dfrac{1}{1+\\Lambda(X)^{-1}},\n\\]</span></p>\n<p>where <span class=\"math inline\">\\(\\Lambda(X)\\)</span> denotes the\n<em>likelihood-ratio</em> of <span class=\"math inline\">\\(X\\)</span>,\nschematically:</p>\n<p><span class=\"math display\">\\[\n\\Lambda(X)\\equiv \\dfrac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y =\n0)}.\n\\]</span></p>\n<p>The constant <span class=\"math inline\">\\(k\\)</span> in Eq.\n@ref(eq:DefKappa) is:</p>\n<p><span class=\"math display\">\\[\nk = \\text{Pr}(Y = 1)\\text{Pr}(Y = 0)=\\text{Var}(Y)\n\\]</span></p>\n<p>The general decomposition @ref(eq:DecompositionWeightedLoss) becomes:\n<span class=\"math display\">\\[\n\\begin{split}\n\\mathbb E(C_\\text{LLR} [f]) &amp;=\n(C_\\text{LLR})_\\text{min}+(C_\\text{LLR})_\\text{proc}\n+(C_\\text{LLR})_\\text{missp},\\\\\n(C_\\text{LLR})_\\text{min}&amp;=\\text{Var}(Y)\\cdot H^{\\prime}(Y\\vert\n\\widetilde X),\\\\\n(C_\\text{LLR})_\\text{proc}&amp;= \\text{Var}(Y)\\cdot I^{\\prime}(Y;\n\\widetilde X\\vert X),\\\\\n(C_\\text{LLR})_\\text{missp} &amp;=\\text{Var}(Y)\\cdot D^{\\prime}(\\text\n{Pr}\\vert \\vert \\text {Pr} _{f}),\n\\end{split} (\\#eq:DecompositionCE)\n\\]</span></p>\n<p>where <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span> is now\ngiven by @ref(eq:PriorCLLR).</p>\n<h2 id=\"discussion\">Discussion</h2>\n<p>The table below provides a comparison between cross-entropy and\nlikelihood-ratio cost, summarizing the results from previous\nsections.</p>\n<table>\n<colgroup>\n<col width=\"15%\"/>\n<col width=\"38%\"/>\n<col width=\"46%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Cross-entropy</th>\n<th>Likelihood Ratio Cost</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><span class=\"math inline\">\\(f_*(X)\\)</span></td>\n<td><span class=\"math inline\">\\(\\text{Pr}(Y = 1\\vert X)\\)</span></td>\n<td><span class=\"math inline\">\\((1+\\Lambda(X)^{-1})^{-1}\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td><span class=\"math inline\">\\(r_*(X)\\)</span>`</td>\n<td>Posterior odds ratio</td>\n<td>Likelihood ratio</td>\n</tr>\n<tr class=\"odd\">\n<td>Minimum Loss</td>\n<td><span class=\"math inline\">\\(H(Y\\vert \\widetilde X)\\)</span></td>\n<td><span class=\"math inline\">\\(\\text{Var}(Y) \\cdot H^\\prime(Y\\vert\n\\widetilde X)\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td>Processing Loss</td>\n<td><span class=\"math inline\">\\(I(Y; \\widetilde X\\vert X)\\)</span></td>\n<td><span class=\"math inline\">\\(\\text{Var}(Y) \\cdot I^\\prime(Y;\n\\widetilde X\\vert X)\\)</span></td>\n</tr>\n<tr class=\"odd\">\n<td>Misspecification Loss</td>\n<td><span class=\"math inline\">\\(D(f_*\\vert\\vert f)\\)</span></td>\n<td><span class=\"math inline\">\\(\\text{Var}(Y) \\cdot\nD^\\prime(f_*\\vert\\vert f)\\)</span></td>\n</tr>\n<tr class=\"even\">\n<td>Reference measure</td>\n<td><span class=\"math inline\">\\(\\text{Pr}\\)</span></td>\n<td><span class=\"math inline\">\\(\\text{Pr}^{\\prime} =\n\\frac{\\text{Pr}(\\cdot \\vert Y = 1)+\\text{Pr}(\\cdot \\vert Y =\n0)}{2}\\)</span></td>\n</tr>\n</tbody>\n</table>\n<p>The objective of <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> is\nfound to be the likelihood ratio, as terminology suggests. The\ninterpretation of model selection according to <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> minimization turns out to be\nslightly more involved, compared to cross-entropy, which we first\nreview.</p>\n<p>Suppose we are given a set of predictive models <span class=\"math inline\">\\(\\{\\mathcal M_i\\}_{i\\in I}\\)</span>, each of which\nconsists of a processing transformation, <span class=\"math inline\">\\(\\widetilde X \\mapsto X\\)</span>, and an estimate\nof the posterior probability <span class=\"math inline\">\\(\\text{Pr}(Y =\n1\\vert X)\\)</span>. When the sample size <span class=\"math inline\">\\(N\n\\to \\infty\\)</span>, cross-entropy minimization will almost certainly\nselect the model that minimizes <span class=\"math inline\">\\(I(Y;\n\\widetilde X\\vert X) + D(f_*\\vert \\vert f)\\)</span>. Following standard\nInformation Theory arguments, we can interpret this model as the\nstatistically optimal compression algorithm for <span class=\"math inline\">\\(Y\\)</span>, assuming <span class=\"math inline\">\\(X\\)</span> to be available at both the encoding\nand decoding ends.</p>\n<p>The previous argument carries over <em>mutatis mutandi</em> to <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> minimization, with an\nimportant qualification: optimal average compression is now achieved for\ndata distributed according to a different probability measure <span class=\"math inline\">\\(\\text{Pr}'(\\cdot) = \\frac{1}{2}\\text\n{Pr}(\\cdot\\vert Y = 1) + \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y =\n0)\\)</span>. In particular, according to <span class=\"math inline\">\\(\\text{Pr}'\\)</span>, the likelihood ratio\ncoincides with the posterior odds ratio, and <span class=\"math inline\">\\((1+\\Lambda(X)^{-1})^{-1}\\)</span> coincides with\nposterior probability, which clarifies why we can measure differences\nfrom the true likelihood-ratio through the Kullback-Liebler\ndivergence.</p>\n<p>The measure <span class=\"math inline\">\\(\\text{Pr}'\\)</span> is\nnot just an abstruse mathematical construct: it is the result of\nbalanced sampling from the original distribution, <em>i.e.</em> taking\nan equal number of positive and negative cases<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn9\" id=\"fnref9\" rel=\"nofollow\" target=\"_blank\"><sup>9</sup></a>. If the <span class=\"math inline\">\\((X,\\,Y)\\)</span> distribution is already balanced,\neither by design or because of some underlying symmetry in the data\ngenerating process, our analysis implies that likelihood-ratio cost and\ncross-entropy minimization are essentially equivalent for <span class=\"math inline\">\\(N\\to \\infty\\)</span>. In general, with <span class=\"math inline\">\\(\\text{Pr} (Y=1) \\neq \\text{Pr} (Y=0)\\)</span>,\nthis is not the case<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn10\" id=\"fnref10\" rel=\"nofollow\" target=\"_blank\"><sup>10</sup></a>.</p>\n<p>The fact that <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> seeks\nfor optimal predictors according to the balanced measure <span class=\"math inline\">\\(\\text{Pr}'\\)</span> is, one could argue, not\ncompletely crazy from the point of view of forensic science, where\n“<span class=\"math inline\">\\(Y\\in\\{0,1\\}\\)</span>” often stands for a\nsort verdict (guilty <em>vs.</em> not guilty, say). Indeed, optimizing\nwith respect to <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span>\nmeans that our predictions are designed to be optimal in a world in\nwhich the verdict could be <em>a priori</em> <span class=\"math inline\">\\(Y=0\\)</span> or <span class=\"math inline\">\\(Y=1\\)</span> with equal probability – which is\nwhat an unbiased trier-of-fact should ideally assume. Minimizing <span class=\"math inline\">\\(C_\\text{LLR}\\)</span>, we guard ourselves against\nany bias that may be implicit in the training dataset, extraneous to the\n<span class=\"math inline\">\\(X\\)</span>–<span class=\"math inline\">\\(Y\\)</span> relation and not explicitly modeled, a\nfeature that may be regarded as desirable from a legal standpoint.</p>\n<h2 id=\"simulated-example\">Simulated example</h2>\n<p>In general, the posterior odd ratio and likelihood ratio differ only\nby a constant, so it is reasonable to try to fit the same functional\nform to both of them. Let us illustrate with a simulated example of this\ntype the differences between cross-entropy and <span class=\"math inline\">\\(C_{\\text{LLR}}\\)</span> optimization mentioned in\nthe previous Section.</p>\n<p>Suppose that <span class=\"math inline\">\\(X \\in \\mathbb R\\)</span> has\nconditional density: <span class=\"math display\">\\[\n\\phi(X\\vert Y) = (2\\pi\\sigma _Y^2)^{-\\frac{1}{2}}\n\\exp(-\\frac{(X-\\mu_Y)^2}{2\\sigma _Y^2})\n\\]</span> and <span class=\"math inline\">\\(Y\\)</span> has marginal\nprobability <span class=\"math inline\">\\(\\text{Pr}(Y = 1) = \\pi\\)</span>.\nThe true likelihood-ratio and posterior odds ratio are respectively\ngiven by:</p>\n<p><span class=\"math display\">\\[\n\\begin{split}\n\\Lambda (X) &amp;\n    \\equiv \\frac{\\phi(X\\vert Y=1)}{\\phi(X\\vert Y=0)}\n    = e ^ {a X^2 + bX +c},\\\\\n\\rho (X) &amp;\n    \\equiv \\frac{\\text{Pr}(Y = 1\\vert X)}{\\text{Pr}(Y = 0\\vert X)}\n    = e ^ {a X ^ 2 + bX +c+d},\n\\end{split}\n\\]</span> where we have defined:</p>\n<p><span class=\"math display\">\\[\na  \\equiv \\dfrac{\\sigma _1 ^2 -\\sigma_0 ^2}{2\\sigma _0 ^2\\sigma_1\n^2},\\quad\nb  \\equiv \\mu _1 – \\mu _0, \\quad\nc  \\equiv \\dfrac{\\mu_0^2}{2\\sigma_0^2} -\\dfrac{\\mu_1 ^2}{2\\sigma\n_1^2}+\\ln(\\frac{\\sigma _0 }{\\sigma _1 }),\\quad\nd  \\equiv \\ln (\\frac {\\pi}{1-\\pi}) .\n\\]</span></p>\n<p>Suppose that we fit an exponential function <span class=\"math inline\">\\(r(X)=e^{mX +q}\\)</span> to <span class=\"math inline\">\\(\\Lambda(X)\\)</span> by likelihood-ratio cost\nminimization, and similarly <span class=\"math inline\">\\(r'(X)=e^{m'X+q'}\\)</span> to <span class=\"math inline\">\\(\\rho(X)\\)</span> by cross-entropy minimization<a class=\"footnote-ref\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fn11\" id=\"fnref11\" rel=\"nofollow\" target=\"_blank\"><sup>11</sup></a>. Due to\nthe previous discussion, one could reasonably expect the results of the\ntwo procedure to differ in some way, which is demonstrated below by\nsimulation.</p>\n<p>The chunk of R code below defines the function and data used for the\nsimulation. In particular, I’m considering a heavily unbalanced case\n(<span class=\"math inline\">\\(\\text{Pr}(Y = 1) = 0.1\\%\\)</span>) in which\nnegative cases give rise to a sharply localized <span class=\"math inline\">\\(X\\)</span> peak around <span class=\"math inline\">\\(X=0\\)</span> (<span class=\"math inline\">\\(\\mu _0 =\n0\\)</span>, <span class=\"math inline\">\\(\\sigma_0 = .25\\)</span>), while\nthe few positive cases give rise to a broader signal centered at <span class=\"math inline\">\\(X=1\\)</span> (<span class=\"math inline\">\\(\\mu _1 =\n1\\)</span>, <span class=\"math inline\">\\(\\sigma _1 = 1\\)</span>).</p>\n<pre># Tidyverse facilities for plotting\nlibrary(dplyr)\nlibrary(ggplot2) \n\n# Loss functions\nweighted_loss &lt;- function(par, data, w) {\n    m &lt;- par[[1]]\n    q &lt;- par[[2]]\n    x &lt;- data$x\n    y &lt;- data$y\n    \n    z &lt;- m * x + q\n    p &lt;- 1 / (1 + exp(-z))\n    \n    -mean(y * w(y) * log(p) + (1-y) * w(1-y) * log(1-p))\n}\n\ncross_entropy &lt;- function(par, data) \n    weighted_loss(par, data, w = \\(y) 1)\n\ncllr &lt;- function(par, data) \n    weighted_loss(par, data, w = \\(y) mean(1-y))\n\n\n# Data generating process\nrxy &lt;- function(n, pi = .001, mu1 = 1, mu0 = 0, sd1 = 1, sd0 = 0.25) { \n    y &lt;- runif(n) &lt; pi\n    x &lt;- rnorm(n, mean = y * mu1 + (1-y) * mu0, sd = y * sd1 + (1-y) * sd0)\n    data.frame(x = x, y = y)\n}\npi &lt;- formals(rxy)$pi\n\n\n# Simulation\nset.seed(840)\ndata &lt;- rxy(n = 1e6)\npar_cllr &lt;- optim(c(1,0), cllr, data = data)$par\npar_cross_entropy &lt;- optim(c(1,0), cross_entropy, data = data)$par\npar_cross_entropy[2] &lt;- par_cross_entropy[2] - log(pi / (1-pi))\n\n\n# Helpers to extract LLRs from models\nllr &lt;- function(x, par)\n    par[1] * x + par[2] \n\nllr_true &lt;- function(x) {\n    mu1 &lt;- formals(rxy)$mu1 \n    mu0 &lt;- formals(rxy)$mu0 \n    sd1 &lt;- formals(rxy)$sd1\n    sd0 &lt;- formals(rxy)$sd0\n        \n    a &lt;- 0.5 * (sd1 ^2 - sd0 ^2) / (sd1 ^2 * sd0 ^2)\n    b &lt;- mu1 / (sd1^2) - mu0 / (sd0^2)\n    c &lt;- 0.5 * (mu0^2 / (sd0^2) - mu1^2 / (sd1^2)) + log(sd0 / sd1)\n    a * x * x + b * x + c\n}</pre>\n<p>So, what do our best estimates look like? The plot below shows the\nbest fit lines for the log-likelihood ratio from <span class=\"math inline\">\\(C_{\\text{LLR}}\\)</span> minimization (in solid\nred) and cross-entropy minimization (in solid blue). The true\nlog-likelihood ratio parabola is the black line. Also shown are the\n<span class=\"math inline\">\\(\\text{LLR}=0\\)</span> line (in dashed red)\nand the <span class=\"math inline\">\\(\\text{LLR}=\\ln(\\frac{1-\\pi}{\\pi})\\)</span> (in\ndashed blue), which are the appropriate Bayes thresholds for classifying\na data point as positive (<span class=\"math inline\">\\(Y=1\\)</span>),\nassuming data comes from a balanced and unbalanced distribution,\nrespectively.</p>\n<pre>ggplot() + \n    geom_function(fun = \\(x) llr(x, par_cllr), color = \"red\") + \n    geom_function(fun = \\(x) llr(x, par_cross_entropy), color = \"blue\") +\n    geom_function(fun = \\(x) llr_true(x), color = \"black\") +\n    geom_hline(aes(yintercept = 0), linetype = \"dashed\", color = \"red\") +\n        geom_hline(aes(yintercept = -log(pi / (1-pi))), \n                             linetype = \"dashed\", color = \"blue\") +\n        ylim(c(-10,10)) + xlim(c(-1, 2)) +\n    xlab(\"X\") + ylab(\"Log-Likelihood Ratio\")</pre>\n<p><img data-lazy-src=\"https://i0.wp.com/vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-3-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-3-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>The reason why the lines differ is that they are designed to solve a\ndifferent predictive problem: as we’ve argued above, minimizing <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> looks for the best <span class=\"math inline\">\\(Y\\vert X\\)</span> conditional probability estimate\naccording to the balanced measure <span class=\"math inline\">\\(\\text{Pr}'\\)</span>, whereas cross-entropy\nminimization does the same for the original measure <span class=\"math inline\">\\(\\text{Pr}\\)</span>. This is how data looks like\nunder the two measures (the histograms are stacked - in the unbalanced\ncase, positive examples are invisible on the linear scale of the\nplot):</p>\n<pre>test_data &lt;- bind_rows(\n    rxy(n = 1e6, pi = 0.5) |&gt; mutate(type = \"Balanced\", llr_thresh = 0),\n    rxy(n = 1e6) |&gt; mutate(type = \"Unbalanced\", llr_thresh = -log(pi / (1-pi)))\n    )\n\ntest_data |&gt; \n    ggplot(aes(x = x, fill = y)) + \n    geom_histogram(bins = 100) +\n    facet_grid(type ~ ., scales = \"free_y\") +\n    xlim(c(-2, 4))</pre>\n<p><img data-lazy-src=\"https://i1.wp.com/vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-4-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-4-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>These differences are reflected in the misclassification rates of the\nresulting classifiers defined by <span class=\"math inline\">\\(\\hat\nY(X)=I(\\text{LLR}(X)&gt;\\text{threshold})\\)</span>, where the\nappropriate threshold is zero in the balanced case, and <span class=\"math inline\">\\(\\ln(\\frac{1-\\pi}{\\pi})\\)</span> in the unbalanced\ncase. According to intuition, we see that the <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> optimizer beats the\ncross-entropy optimizer on the balanced sample, while performing\nsignificantly worse on the unbalanced one.</p>\n<pre>test_data |&gt;\n    mutate(\n        llr_cllr = llr(x, par_cllr),\n        llr_cross_entropy = llr(x, par_cross_entropy),\n        llr_true = llr_true(x)\n        ) |&gt;\n    group_by(type) |&gt;\n    summarise(\n        cllr = 1 - mean((llr_cllr &gt; llr_thresh) == y),\n        cross_entropy = 1 - mean((llr_cross_entropy &gt; llr_thresh) == y),\n        true_llr = 1 - mean((llr_true &gt; llr_thresh) == y)\n        )\r\n# A tibble: 2 × 4\n  type           cllr cross_entropy true_llr\n  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Balanced   0.166         0.185    0.140   \n2 Unbalanced 0.000994      0.000637 0.000518</pre>\n<h2 id=\"final-remarks\">Final remarks</h2>\n<p>Our main conclusion in a nutshell is that <span class=\"math inline\">\\(C_\\text{LLR}\\)</span> minimization is equivalent,\n<em>in the infinite sample limit</em>, to cross-entropy minimization on\na balanced version of the original distribution. We haven’t discussed\nwhat happens for finite samples where variance starts to play a role,\naffecting the <em>efficiency</em> of loss functions as model\noptimization and selection criteria. For instance, for a well specified\nmodel of likelihood ratio, how do the convergence properties of <span class=\"math inline\">\\(C_{\\text{LLR}}\\)</span> and cross-entropy\nestimators compare to each other? I expect that answering questions like\nthis would require a much more in-depth study than the one performed\nhere (likely, with simulation playing a central role).</p>\n<div class=\"references csl-bib-body hanging-indent\" id=\"refs\">\n<div class=\"csl-entry\" id=\"ref-BRUMMER2006230\">\nBrümmer, Niko, and Johan du Preez. 2006. <span>“Application-Independent\nEvaluation of Speaker Detection.”</span> <em>Computer Speech &amp;\nLanguage</em> 20 (2): 230–75. https://doi.org/<a href=\"https://doi.org/10.1016/j.csl.2005.08.001\" rel=\"nofollow\" target=\"_blank\">https://doi.org/10.1016/j.csl.2005.08.001</a>.\n</div>\n<div class=\"csl-entry\" id=\"ref-Cover2006\">\nCover, Thomas M., and Joy A. Thomas. 2006. <em>Elements of Information\nTheory 2nd Edition (Wiley Series in Telecommunications and Signal\nProcessing)</em>. Hardcover; Wiley-Interscience.\n</div>\n</div>\n<div class=\"footnotes footnotes-end-of-document\">\n<hr>\n<ol>\n<li id=\"fn1\"><p>This is how I understood things should\n<em>theoretically</em> work, from discussions with friends who are\nactually working on this field. I have no idea on how much day-to-day\npractice comes close to this mathematical ideal, and whether there exist\nalternative frameworks to the one I have just described.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref1\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn2\"><p>The Likelihood Ratio Cost was introduced in <span class=\"citation\">(Brümmer and du Preez 2006)</span>. The reference looks\nvery complete, but I find its notation and terminology so unfamiliar\nthat I decided to do my own investigation and leave this reading for a\nsecond moment.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref2\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn3\"><p>That is to say, <span class=\"math inline\">\\(w(Y_{\\sigma(1)},\\,Y_{\\sigma(2)},\\dots,\\,Y_{\\sigma(N)})=w(Y_1,\\,Y_2,\\dots,\\,Y_N)\\)</span>\nfor any permutation <span class=\"math inline\">\\(\\sigma\\)</span> of the\nset <span class=\"math inline\">\\(\\{1,\\,2,\\,\\dots,\\,N\\}\\)</span>.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref3\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn4\"><p><em>Nota bene:</em> the function <span class=\"math inline\">\\(f\\)</span> is here assumed to be fixed, whereas\nthe randomness in the quantity <span class=\"math inline\">\\(L _N ^w\n[f]\\)</span> only comes from the paired observations <span class=\"math inline\">\\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\)</span>.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref4\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn5\"><p>Notice that, due to symmetry <span class=\"math inline\">\\(\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i &gt;0)\n= \\mathbb E(w(\\boldsymbol Y)\\vert Y_1 = 1)\\)</span>, which might be\neasier to compute.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref5\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn6\"><p>Here and below I relax a bit the notation, as most\ndetails should be clear from context.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref6\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn7\"><p>The quantity <span class=\"math inline\">\\(w(\\boldsymbol\nY)\\)</span> is not defined when all <span class=\"math inline\">\\(Y_i\\)</span>’s are zero, as the right-hand side of\nEq. @ref(eq:CLLR) itself. In this case, we make the convention <span class=\"math inline\">\\(w(\\boldsymbol Y) = 0\\)</span>.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref7\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn8\"><p>For the original loss in Eq. @ref(eq:CLLR), without the\nmodification discussed above, the result would have been <span class=\"math inline\">\\(\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime\n(Y=0)}=\\dfrac{1-\\text {Pr}(Y=0)^N}{1-\\text {Pr}(Y=1)^N}.\\)</span><a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref8\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn9\"><p>Formally, given an i.i.d. stochastic process <span class=\"math inline\">\\(Z_i = (X_i,\\,Y_i)\\)</span>, we can define a new\nstochastic process <span class=\"math inline\">\\(Z_i ^\\prime =\n(X_i^\\prime,\\,Y_i^\\prime)\\)</span> such that <span class=\"math inline\">\\(Z_i ^\\prime = Z_{2i - 1}\\)</span> if <span class=\"math inline\">\\(Y_{2i-1}\\neq Y_{2i}\\)</span>, and <span class=\"math inline\">\\(Z_i ^\\prime = \\perp\\)</span> (not defined)\notherwise. Discarding <span class=\"math inline\">\\(\\perp\\)</span> values,\nwe obtain an i.i.d. stochastic process whose individual observations are\ndistributed according to <span class=\"math inline\">\\(\\text{Pr}^\\prime\\)</span>.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref9\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn10\"><p>There is another case in which <span class=\"math inline\">\\(C_{\\text{LLR}}\\)</span> and cross-entropy\nminimization converge to the same answer as <span class=\"math inline\">\\(N\\to \\infty\\)</span>: when used for model\nselection among a class of models for the likelihood or posterior odds\nratio that contains their correct functional form.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref10\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn11\"><p>This is just logistic regression. It could be a\nreasonable approximation if <span class=\"math inline\">\\(\\sigma_0\n^2\\approx \\sigma_1 ^2\\)</span>, which however I will assume below to be\nbadly violated.<a class=\"footnote-back\" href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost#fnref11\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n</ol>\n</hr></div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost\"> Valerio Gherardi</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
      "main_text": "Interpreting the Likelihood Ratio cost\nPosted on\nNovember 14, 2023\nby\nvgherard\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nValerio Gherardi\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIntro\nDuring the last few months, I’ve been working on a machine learning\nalgorithm with applications in\nForensic\nScience\n, a.k.a. Criminalistics. In this field, one common task for\nthe data analyst is to present the\ntrier-of-fact\n(the person or\npeople who determine the facts in a legal proceeding) with a numerical\nassessment of the strength of the evidence provided by available data\ntowards different hypotheses. In more familiar terms, the forensic\nexpert is responsible of computing the likelihoods (or likelihood\nratios) of data under competing hypotheses, which are then used by the\ntrier-of-fact to produce Bayesian posterior probabilities for the\nhypotheses in question\n1\n.\nIn relation to this, forensic scientists have developed a bunch of\ntechniques to evaluate the performance of a likelihood ratio model in\ndiscriminating between two alternative hypothesis. In particular, I have\ncome across the so called\nLikelihood Ratio Cost\n, usually\ndefined as:\n\\[\nC_{\\text{LLR}} = \\frac{1}{2N_1} \\sum _{Y_i=1} \\log(1+r(X_i)\n^{-1})+\\frac{1}{2N_0} \\sum _{Y_i=0} \\log(1+r(X_i)), (\\#eq:CLLR)\n\\]\nwhere we assume we have data consisting of\n\\(N_1+N_0\\)\nindependent identically\ndistributed observations\n\\((X_i,\\,Y_i)\\)\n, with binary\n\\(Y\\)\n;\n\\(N_1\\)\nand\n\\(N_0\\)\nstand for the number of positive\n(\n\\(Y=1\\)\n) and negative (\n\\(Y=0\\)\n) cases; and\n\\(r(X)\\)\nis a model for the likelihood ratio\n\\(\\Lambda(X) \\equiv \\frac{\\text{Pr}(X\\vert Y =\n1)}{\\text{Pr}(X\\vert Y = 0)}\\)\n.\nThe main reason for writing this note was to understand a bit better\nwhat it means to optimize Eq. @ref(eq:CLLR), which does not look\nimmediately obvious to me from its definition\n2\n. In particular: is the\npopulation minimizer of Eq. @ref(eq:CLLR) the actual likelihood ratio?\nAnd in what sense is a model with lower\n\\(C_\\text{LLR}\\)\nbetter than one with a\ncorrespondingly higher value?\nThe short answers to these questions are: yes; and:\n\\(C_\\text{LLR}\\)\noptimization seeks for the\nmodel with the best predictive performance in a Bayesian inference\nsetting with uninformative prior on\n\\(Y\\)\n, assuming that this prior actually\nreflects reality (\ni.e.\n\\(\\text{Pr}(Y=1) = \\text{Pr}(Y=0) =\n\\frac{1}{2}\\)\n). The mathematical details are given in the rest of\nthe post.\nCross-entropy with random\nweights\nWe start with a mathematical digression, which will turn out useful\nfor further developments. Let\n\\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\)\nbe\nindependent draws from a joint distribution, with binary\n\\(Y_i \\in \\{0,\\,1\\}\\)\n. Given a function\n\\(w=w(\\boldsymbol Y)\\)\nthat is symmetric in\nits arguments\n3\n, we define the random functional:\n\\[\n\\mathcal L_N^w[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i\n\\log(f(X_i))+ w({\\boldsymbol Y}^c)( Y_i^c)\n\\log(f(X_i)^c)\\right],(\\#eq:WeightedLoss)\n\\]\nwhere\n\\(f=f(X)\\)\nis any\nfunction satisfying\n\\(f(X)\\in [0,\\,1]\\)\nfor all\n\\(X\\)\n, and we let\n\\(q^c = 1-q\\)\nfor any number\n\\(q \\in [0,\\,1]\\)\n. Notice that for\n\\(w(\\boldsymbol{Y}) \\equiv 1\\)\n, this is just\nthe usual cross-entropy loss.\nWe now look for the population minimizer of @ref(eq:WeightedLoss),\ni.e.\nthe function\n\\(f_*\\)\nthat\nminimizes the functional\n\\(f \\mapsto \\mathbb\nE(\\mathcal L _N ^w [f])\\)\n4\n. Writing the expectation as:\n\\[\n\\mathbb E(\\mathcal L _N ^w [f]) = -\\frac{1}{N}\\sum _{i=1} ^N \\mathbb\nE\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)\\cdot\n\\log(f(X_i))+E(Y_i^c\\cdot w(\\boldsymbol Y ^c)\\vert X_i)\\cdot\n\\log(f^c(X_i))\\right],\n\\]\nwe can easily see that\n\\(\\mathbb\nE(\\mathcal L _N ^w [f])\\)\nis a convex functional with a unique\nminimum given by:\n\\[\nf_*(X_i) = \\frac{1}{1+r(X_i)^{-1}},\\quad r_*(X_i) = \\dfrac{E(Y_i\\cdot\nw(\\boldsymbol Y)\\vert X_i)}{E(Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert\nX_i)}.(\\#eq:PopMinimizer)\n\\]\nThe corresponding expected loss is:\n\\[\n\\mathbb E(\\mathcal L _N ^w [f_*]) = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot\nw(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal\nH(f_*(X_i))\\right],\n\\]\nwhere\n\\(\\mathcal H(p) = -p \\log (p)\n-(1-p) \\log(1-p)\\)\nis the entropy of a binary random variable\n\\(Z\\)\nwith probability\n\\(p = \\text{Pr}(Z=1)\\)\n(the index\n\\(i\\)\nin the previous expression can be any\nindex, since data points are assumed to be identically distributed).\nBefore looking at values of\n\\(f\\)\nother than\n\\(f_*\\)\n, we observe that the\nprevious expectation can be succintly expressed as:\n\\[\n\\mathbb E(\\mathcal L _N ^w [f_*]) = k \\cdot H^\\prime(Y\\vert X),\n\\]\nwhere\n\\[\nk = \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol\nY^c))(\\#eq:DefKappa)\n\\]\nand\n\\(H'(Y\\vert X)\\)\nis\nthe conditional entropy of\n\\(Y\\vert X\\)\nwith respect to a\ndifferent\nprobability measure\n\\(\\text{Pr}^\\prime\\)\n, defined by:\n\\[\n\\text{Pr}^\\prime(E) = t \\cdot \\text {Pr}(E \\vert Y = 1) + (1-t)\\cdot\n\\text {Pr}(E \\vert Y = 0), (\\#eq:DefPrPrime)\n\\]\nwhere\n\\(t=\\text{Pr}^\\prime(Y=1)\\in\n[0,\\,1]\\)\nis fixed by the requirement\n5\n:\n\\[\n\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{\\text\n{Pr} (Y=1)}{\\text{Pr} (Y=0)}\\cdot\\dfrac{\\mathbb E(w(\\boldsymbol Y)\\vert\n\\sum _i Y_i >0)}{\\mathbb E(w(\\boldsymbol Y^c)\\vert \\sum _i Y_i^c\n>0)}. (\\#eq:DefPrPrime2)\n\\]\nIn terms of\n\\(\\text{Pr}^\\prime\\)\n, the population\nminimizers\n\\(f_*\\)\nand\n\\(r_*\\)\nin Eq. @ref(eq:PopMinimizer) can be\nsimply expressed as:\n\\[\nr_*(X)=\\dfrac{\\text {Pr}^\\prime(Y=1\\vert X)}{\\text {Pr}^\\prime(Y=0\\vert\nX)},\\qquad f_*(X)=\\text {Pr}^\\prime(Y=1\\vert X). (\\#eq:PopMinimizer2)\n\\]\nIf now\n\\(f\\)\nis an arbitrary\nfunction, we have:\n\\[\n\\begin{split}\n\\mathbb E(\\mathcal L _N ^w [f]) – \\mathbb E(\\mathcal L _N ^w [f_*])\n&= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot\nw(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert\nf(X_i))\\right]\n&= k\\cdot D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\n\\end{split}\n\\]\nwhere\n\\(\\mathcal D(p\\vert \\vert q) =\np \\log (\\frac{p}{q}) + (1-p) \\log (\\frac{1-p}{1-q})\\)\n, and\n\\(D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime\n_f)\\)\nis the Kullback-Liebler divergence between the measure\n\\(\\text{Pr}^\\prime\\)\nand the measure\n\\(\\text{Pr}^\\prime _f\\)\ndefined by:\n\\[\n\\text{Pr}^\\prime _f(Y = 1\\vert X)=f(X),\\qquad \\text{Pr}^\\prime\n_f(X)=\\text{Pr}^\\prime(X)\n\\]\n(notice that\n\\(\\text {Pr} ^{\\prime}\n_{f_*} \\equiv \\text{Pr} ^{\\prime}\\)\nby definition). Finally,\nsuppose that\n\\(X = g(\\widetilde X)\\)\nfor some random variable\n\\(\\widetilde\nX\\)\n, and define the corresponding functional:\n\\[\n\\widetilde{\\mathcal L} _N^w[\\widetilde f]  = -\\frac{1}{N}\\sum_{i=1} ^N\n\\left[w(\\boldsymbol Y)Y_i \\log(\\widetilde f(\\widetilde X))+\nw({\\boldsymbol Y}^c)( Y_i^c) \\log(\\widetilde f(\\widetilde X)^c)\\right].\n\\]\nThen\n\\(\\mathcal L _N ^w [f] =\n\\widetilde{\\mathcal L} _N^w[f \\circ g]\\)\n. If\n\\(\\widetilde f _* =\\)\nis the population\nminimizer of\n\\(\\widetilde{\\mathcal L}\n_N^w\\)\n, it follows that\n\\(\\mathbb E\n(\\widetilde{\\mathcal L} _N^w[\\widetilde f _*]) \\leq \\mathbb E(\\mathcal L\n_N ^w [f_*])\\)\n.\nPutting everything together, we can decompose the expected loss for a\nfunction\n\\(f=f(X)\\)\n, where\n\\(X= g(\\widetilde X)\\)\n, in the following\nsuggestive way:\n\\[\n\\begin{split}\n\\mathbb E(\\mathcal L _N ^w [f]) &= (L_N ^w)_\\text{min}+(L_N\n^w)_\\text{proc} +(L_N ^w)_\\text{missp},\\\\\n(L_N ^w)_\\text{min}&\\equiv\\mathbb E(\\widetilde{\\mathcal L}\n_N^w[{\\widetilde f} _*])  \\\\ &=\n\\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot\nw(\\boldsymbol Y^c)\\vert \\widetilde X _i)\\cdot \\mathcal H({\\widetilde f}\n_*(\\widetilde X _i))\\right]\\\\\n&=k\\cdot H^\\prime(Y\\vert \\widetilde X),\\\\\n(L_N ^w)_\\text{proc}&\\equiv\\mathbb E(\\mathcal L _N ^w\n[f_*]-\\widetilde{\\mathcal L} _N^w[\\phi_*])  \\\\& =\n\\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot\nw(\\boldsymbol Y^c)\\vert X_i)\\cdot  \\mathcal H(f_*(X_i))\n\\right]- (L_N ^w)_\\text{min}\\\\\n& = k\\cdot I^\\prime(Y; \\widetilde X\\vert X),\\\\\n(L_N ^w)_\\text{missp} & \\equiv \\mathbb E(\\mathcal L _N ^w [f]) –\n\\mathbb E(\\mathcal L _N ^w [f_*]) \\\\&= \\mathbb E\\left[ \\mathbb\nE(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert\nX_i)\\cdot  \\mathcal  D(f_*(X_i)\\vert \\vert f(X_i))\\right]\\\\ &=k\\cdot\nD(\\text {Pr}^\\prime\\vert \\vert \\text {Pr}^\\prime _f),\n\\end{split} (\\#eq:DecompositionWeightedLoss)\n\\]\nwhere\n\\(k\\)\nis defined in Eq.\n@ref(eq:DefKappa). In the equation for\n\\((L^w\n_N)_\\text{proc}\\)\nwe introduced the conditional mutual\ninformation (with respect to the measure\n\\(\\text{Pr}^\\prime\\)\n), that satisfies\n(Cover and Thomas 2006)\n:\n\\[\nI(\\widetilde X;Y\\vert X) = I(\\widetilde X,Y)-I(X,Y) = H(Y\\vert\nX)-H(Y\\vert \\widetilde X).\n\\]\nThe three components in Eq. @ref(eq:DecompositionWeightedLoss) can be\ninterpreted as follows:\n\\((L_N\n^w)_\\text{min}\\)\nrepresents the minimum expected loss achievable,\ngiven the data available\n\\(\\widetilde\nX\\)\n;\n\\((L_N ^w)_\\text{proc}\\)\naccounts for the information lost in the processing transformation\n\\(X=g(\\widetilde X)\\)\n; finally\n\\((L_N ^w)_\\text{missp}\\)\nis due to\nmisspecification,\ni.e.\nthe fact that the model\n\\(f(X)\\)\nfor the true posterior probability\n\\(f_*(X)\\)\nis an approximation.\nAll the information-theoretic quantities (and their corresponding\noperative interpretations hinted in the previous paragraph) make\nreference to the measure\n\\(\\text{Pr}^\\prime\\)\ndefined by Eqs.\n@ref(eq:DefPrPrime) and @ref(eq:DefPrPrime2). This is merely the result\nof altering the proportion of positive (\n\\(Y=1\\)\n) and negative (\n\\(Y=0\\)\n) examples in the\n\\(X\\)\n–\n\\(Y\\)\njoint distribution by a factor dictated by the weight function\n\\(w\\)\n– while keeping conditional\ndistributions such as\n\\(X\\vert Y\\)\nunchanged.\nA familiar case:\ncross-entropy loss\nFor\n\\(w(\\boldsymbol {Y}) = 1\\)\n, the\nfunctional\n\\(\\mathcal {L} _{N}\n^{w}[f]\\)\ncoincides with the usual cross-entropy loss\n6\n:\n\\[\n\\text{CE}[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[Y_i \\log(f(X_i))+ (1-Y_i)\n\\log(1-f(X_i))\\right].(\\#eq:CrossEntropyLoss)\n\\]\nFrom Eq. @ref(eq:DefPrPrime2) we see that the measure\n\\(\\text{Pr}^{\\prime}\\)\ncoincides with the\noriginal\n\\(\\text{Pr}\\)\n, so that by Eq.\n@ref(eq:PopMinimizer) the population minimizer of\n@ref(eq:CrossEntropyLoss) is\n\\(f_{*}(X) =\n\\text{Pr}(Y=1\\vert X)\\)\n(independently of sample size). Since\n\\(k = 1\\)\n(\ncf.\nEq.\n@ref(eq:DefKappa)), the decomposition @ref(eq:DecompositionWeightedLoss)\nreads:\n\\[\n\\begin{split}\n\\mathbb E(\\text{CE} [f]) &=\n(\\text{CE})_\\text{min}+(\\text{CE})_\\text{proc}\n+(\\text{CE})_\\text{missp},\\\\\n(\\text{CE})_\\text{min}&=H(Y\\vert \\widetilde X),\\\\\n(\\text{CE})_\\text{proc}&= I(Y; \\widetilde X\\vert X),\\\\\n(\\text{CE})_\\text{missp} &=D(\\text {Pr}\\vert \\vert \\text {Pr} _{f}),\n\\end{split} (\\#eq:DecompositionCE)\n\\]\nwhere conditional entropy\n\\(H\\)\n,\nmutual information\n\\(I\\)\nand relative\nentropy\n\\(D\\)\nnow simply refer to the\noriginal measure\n\\(\\text{Pr}\\)\n.\nThe Likelihood Ratio Cost\nThe quantity\n\\(C_{\\text{LLR}}\\)\ndefined in Eq. @ref(eq:CLLR) can be put in the general form\n@ref(eq:WeightedLoss), if we let\n\\(f(X) =\n(1+r(X)^{-1})^{-1}\\)\nand\n7\n:\n\\[\nw(\\boldsymbol Y) = \\left(\\dfrac{2}{N}\\sum _{i = 1}^{N}Y_j \\right)^{-1}\n\\]\nIn what follows, I will consider a slight modification of the\nusual\n\\(C_\\text{LLR}\\)\n, defined by the\nweight function:\n\\[\nw(\\boldsymbol Y) = \\dfrac{1}{2(N-1)}\\sum _{i = 1}^{N}(1-Y_j).\n\\]\nThis yields Eq. @ref(eq:CLLR) multiplied by\n\\(\\dfrac{N_1N_0}{N(N-1)}\\)\n, which I will keep\ndenoting as\n\\(C_\\text{LLR}\\)\n, with a\nslight abuse of notation.\nWe can easily compute\n8\n:\n\\[\n\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=1,\n(\\#eq:PriorCLLR)\n\\]\nso that, by Eq. @ref(eq:PopMinimizer), the population\nminimizer of\n\\(C_\\text{LLR}\\)\nis:\n\\[\nr_*(X) = \\Lambda (X),\\quad f_*(X)=\\dfrac{1}{1+\\Lambda(X)^{-1}},\n\\]\nwhere\n\\(\\Lambda(X)\\)\ndenotes the\nlikelihood-ratio\nof\n\\(X\\)\n,\nschematically:\n\\[\n\\Lambda(X)\\equiv \\dfrac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y =\n0)}.\n\\]\nThe constant\n\\(k\\)\nin Eq.\n@ref(eq:DefKappa) is:\n\\[\nk = \\text{Pr}(Y = 1)\\text{Pr}(Y = 0)=\\text{Var}(Y)\n\\]\nThe general decomposition @ref(eq:DecompositionWeightedLoss) becomes:\n\\[\n\\begin{split}\n\\mathbb E(C_\\text{LLR} [f]) &=\n(C_\\text{LLR})_\\text{min}+(C_\\text{LLR})_\\text{proc}\n+(C_\\text{LLR})_\\text{missp},\\\\\n(C_\\text{LLR})_\\text{min}&=\\text{Var}(Y)\\cdot H^{\\prime}(Y\\vert\n\\widetilde X),\\\\\n(C_\\text{LLR})_\\text{proc}&= \\text{Var}(Y)\\cdot I^{\\prime}(Y;\n\\widetilde X\\vert X),\\\\\n(C_\\text{LLR})_\\text{missp} &=\\text{Var}(Y)\\cdot D^{\\prime}(\\text\n{Pr}\\vert \\vert \\text {Pr} _{f}),\n\\end{split} (\\#eq:DecompositionCE)\n\\]\nwhere\n\\(\\text{Pr}^\\prime\\)\nis now\ngiven by @ref(eq:PriorCLLR).\nDiscussion\nThe table below provides a comparison between cross-entropy and\nlikelihood-ratio cost, summarizing the results from previous\nsections.\nCross-entropy\nLikelihood Ratio Cost\n\\(f_*(X)\\)\n\\(\\text{Pr}(Y = 1\\vert X)\\)\n\\((1+\\Lambda(X)^{-1})^{-1}\\)\n\\(r_*(X)\\)\n`\nPosterior odds ratio\nLikelihood ratio\nMinimum Loss\n\\(H(Y\\vert \\widetilde X)\\)\n\\(\\text{Var}(Y) \\cdot H^\\prime(Y\\vert\n\\widetilde X)\\)\nProcessing Loss\n\\(I(Y; \\widetilde X\\vert X)\\)\n\\(\\text{Var}(Y) \\cdot I^\\prime(Y;\n\\widetilde X\\vert X)\\)\nMisspecification Loss\n\\(D(f_*\\vert\\vert f)\\)\n\\(\\text{Var}(Y) \\cdot\nD^\\prime(f_*\\vert\\vert f)\\)\nReference measure\n\\(\\text{Pr}\\)\n\\(\\text{Pr}^{\\prime} =\n\\frac{\\text{Pr}(\\cdot \\vert Y = 1)+\\text{Pr}(\\cdot \\vert Y =\n0)}{2}\\)\nThe objective of\n\\(C_\\text{LLR}\\)\nis\nfound to be the likelihood ratio, as terminology suggests. The\ninterpretation of model selection according to\n\\(C_\\text{LLR}\\)\nminimization turns out to be\nslightly more involved, compared to cross-entropy, which we first\nreview.\nSuppose we are given a set of predictive models\n\\(\\{\\mathcal M_i\\}_{i\\in I}\\)\n, each of which\nconsists of a processing transformation,\n\\(\\widetilde X \\mapsto X\\)\n, and an estimate\nof the posterior probability\n\\(\\text{Pr}(Y =\n1\\vert X)\\)\n. When the sample size\n\\(N\n\\to \\infty\\)\n, cross-entropy minimization will almost certainly\nselect the model that minimizes\n\\(I(Y;\n\\widetilde X\\vert X) + D(f_*\\vert \\vert f)\\)\n. Following standard\nInformation Theory arguments, we can interpret this model as the\nstatistically optimal compression algorithm for\n\\(Y\\)\n, assuming\n\\(X\\)\nto be available at both the encoding\nand decoding ends.\nThe previous argument carries over\nmutatis mutandi\nto\n\\(C_\\text{LLR}\\)\nminimization, with an\nimportant qualification: optimal average compression is now achieved for\ndata distributed according to a different probability measure\n\\(\\text{Pr}'(\\cdot) = \\frac{1}{2}\\text\n{Pr}(\\cdot\\vert Y = 1) + \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y =\n0)\\)\n. In particular, according to\n\\(\\text{Pr}'\\)\n, the likelihood ratio\ncoincides with the posterior odds ratio, and\n\\((1+\\Lambda(X)^{-1})^{-1}\\)\ncoincides with\nposterior probability, which clarifies why we can measure differences\nfrom the true likelihood-ratio through the Kullback-Liebler\ndivergence.\nThe measure\n\\(\\text{Pr}'\\)\nis\nnot just an abstruse mathematical construct: it is the result of\nbalanced sampling from the original distribution,\ni.e.\ntaking\nan equal number of positive and negative cases\n9\n. If the\n\\((X,\\,Y)\\)\ndistribution is already balanced,\neither by design or because of some underlying symmetry in the data\ngenerating process, our analysis implies that likelihood-ratio cost and\ncross-entropy minimization are essentially equivalent for\n\\(N\\to \\infty\\)\n. In general, with\n\\(\\text{Pr} (Y=1) \\neq \\text{Pr} (Y=0)\\)\n,\nthis is not the case\n10\n.\nThe fact that\n\\(C_\\text{LLR}\\)\nseeks\nfor optimal predictors according to the balanced measure\n\\(\\text{Pr}'\\)\nis, one could argue, not\ncompletely crazy from the point of view of forensic science, where\n“\n\\(Y\\in\\{0,1\\}\\)\n” often stands for a\nsort verdict (guilty\nvs.\nnot guilty, say). Indeed, optimizing\nwith respect to\n\\(\\text{Pr}^\\prime\\)\nmeans that our predictions are designed to be optimal in a world in\nwhich the verdict could be\na priori\n\\(Y=0\\)\nor\n\\(Y=1\\)\nwith equal probability – which is\nwhat an unbiased trier-of-fact should ideally assume. Minimizing\n\\(C_\\text{LLR}\\)\n, we guard ourselves against\nany bias that may be implicit in the training dataset, extraneous to the\n\\(X\\)\n–\n\\(Y\\)\nrelation and not explicitly modeled, a\nfeature that may be regarded as desirable from a legal standpoint.\nSimulated example\nIn general, the posterior odd ratio and likelihood ratio differ only\nby a constant, so it is reasonable to try to fit the same functional\nform to both of them. Let us illustrate with a simulated example of this\ntype the differences between cross-entropy and\n\\(C_{\\text{LLR}}\\)\noptimization mentioned in\nthe previous Section.\nSuppose that\n\\(X \\in \\mathbb R\\)\nhas\nconditional density:\n\\[\n\\phi(X\\vert Y) = (2\\pi\\sigma _Y^2)^{-\\frac{1}{2}}\n\\exp(-\\frac{(X-\\mu_Y)^2}{2\\sigma _Y^2})\n\\]\nand\n\\(Y\\)\nhas marginal\nprobability\n\\(\\text{Pr}(Y = 1) = \\pi\\)\n.\nThe true likelihood-ratio and posterior odds ratio are respectively\ngiven by:\n\\[\n\\begin{split}\n\\Lambda (X) &\n    \\equiv \\frac{\\phi(X\\vert Y=1)}{\\phi(X\\vert Y=0)}\n    = e ^ {a X^2 + bX +c},\\\\\n\\rho (X) &\n    \\equiv \\frac{\\text{Pr}(Y = 1\\vert X)}{\\text{Pr}(Y = 0\\vert X)}\n    = e ^ {a X ^ 2 + bX +c+d},\n\\end{split}\n\\]\nwhere we have defined:\n\\[\na  \\equiv \\dfrac{\\sigma _1 ^2 -\\sigma_0 ^2}{2\\sigma _0 ^2\\sigma_1\n^2},\\quad\nb  \\equiv \\mu _1 – \\mu _0, \\quad\nc  \\equiv \\dfrac{\\mu_0^2}{2\\sigma_0^2} -\\dfrac{\\mu_1 ^2}{2\\sigma\n_1^2}+\\ln(\\frac{\\sigma _0 }{\\sigma _1 }),\\quad\nd  \\equiv \\ln (\\frac {\\pi}{1-\\pi}) .\n\\]\nSuppose that we fit an exponential function\n\\(r(X)=e^{mX +q}\\)\nto\n\\(\\Lambda(X)\\)\nby likelihood-ratio cost\nminimization, and similarly\n\\(r'(X)=e^{m'X+q'}\\)\nto\n\\(\\rho(X)\\)\nby cross-entropy minimization\n11\n. Due to\nthe previous discussion, one could reasonably expect the results of the\ntwo procedure to differ in some way, which is demonstrated below by\nsimulation.\nThe chunk of R code below defines the function and data used for the\nsimulation. In particular, I’m considering a heavily unbalanced case\n(\n\\(\\text{Pr}(Y = 1) = 0.1\\%\\)\n) in which\nnegative cases give rise to a sharply localized\n\\(X\\)\npeak around\n\\(X=0\\)\n(\n\\(\\mu _0 =\n0\\)\n,\n\\(\\sigma_0 = .25\\)\n), while\nthe few positive cases give rise to a broader signal centered at\n\\(X=1\\)\n(\n\\(\\mu _1 =\n1\\)\n,\n\\(\\sigma _1 = 1\\)\n).\n# Tidyverse facilities for plotting\nlibrary(dplyr)\nlibrary(ggplot2) \n\n# Loss functions\nweighted_loss <- function(par, data, w) {\n    m <- par[[1]]\n    q <- par[[2]]\n    x <- data$x\n    y <- data$y\n    \n    z <- m * x + q\n    p <- 1 / (1 + exp(-z))\n    \n    -mean(y * w(y) * log(p) + (1-y) * w(1-y) * log(1-p))\n}\n\ncross_entropy <- function(par, data) \n    weighted_loss(par, data, w = \\(y) 1)\n\ncllr <- function(par, data) \n    weighted_loss(par, data, w = \\(y) mean(1-y))\n\n# Data generating process\nrxy <- function(n, pi = .001, mu1 = 1, mu0 = 0, sd1 = 1, sd0 = 0.25) { \n    y <- runif(n) < pi\n    x <- rnorm(n, mean = y * mu1 + (1-y) * mu0, sd = y * sd1 + (1-y) * sd0)\n    data.frame(x = x, y = y)\n}\npi <- formals(rxy)$pi\n\n# Simulation\nset.seed(840)\ndata <- rxy(n = 1e6)\npar_cllr <- optim(c(1,0), cllr, data = data)$par\npar_cross_entropy <- optim(c(1,0), cross_entropy, data = data)$par\npar_cross_entropy[2] <- par_cross_entropy[2] - log(pi / (1-pi))\n\n# Helpers to extract LLRs from models\nllr <- function(x, par)\n    par[1] * x + par[2] \n\nllr_true <- function(x) {\n    mu1 <- formals(rxy)$mu1 \n    mu0 <- formals(rxy)$mu0 \n    sd1 <- formals(rxy)$sd1\n    sd0 <- formals(rxy)$sd0\n        \n    a <- 0.5 * (sd1 ^2 - sd0 ^2) / (sd1 ^2 * sd0 ^2)\n    b <- mu1 / (sd1^2) - mu0 / (sd0^2)\n    c <- 0.5 * (mu0^2 / (sd0^2) - mu1^2 / (sd1^2)) + log(sd0 / sd1)\n    a * x * x + b * x + c\n}\nSo, what do our best estimates look like? The plot below shows the\nbest fit lines for the log-likelihood ratio from\n\\(C_{\\text{LLR}}\\)\nminimization (in solid\nred) and cross-entropy minimization (in solid blue). The true\nlog-likelihood ratio parabola is the black line. Also shown are the\n\\(\\text{LLR}=0\\)\nline (in dashed red)\nand the\n\\(\\text{LLR}=\\ln(\\frac{1-\\pi}{\\pi})\\)\n(in\ndashed blue), which are the appropriate Bayes thresholds for classifying\na data point as positive (\n\\(Y=1\\)\n),\nassuming data comes from a balanced and unbalanced distribution,\nrespectively.\nggplot() + \n    geom_function(fun = \\(x) llr(x, par_cllr), color = \"red\") + \n    geom_function(fun = \\(x) llr(x, par_cross_entropy), color = \"blue\") +\n    geom_function(fun = \\(x) llr_true(x), color = \"black\") +\n    geom_hline(aes(yintercept = 0), linetype = \"dashed\", color = \"red\") +\n        geom_hline(aes(yintercept = -log(pi / (1-pi))), \n                             linetype = \"dashed\", color = \"blue\") +\n        ylim(c(-10,10)) + xlim(c(-1, 2)) +\n    xlab(\"X\") + ylab(\"Log-Likelihood Ratio\")\nThe reason why the lines differ is that they are designed to solve a\ndifferent predictive problem: as we’ve argued above, minimizing\n\\(C_\\text{LLR}\\)\nlooks for the best\n\\(Y\\vert X\\)\nconditional probability estimate\naccording to the balanced measure\n\\(\\text{Pr}'\\)\n, whereas cross-entropy\nminimization does the same for the original measure\n\\(\\text{Pr}\\)\n. This is how data looks like\nunder the two measures (the histograms are stacked - in the unbalanced\ncase, positive examples are invisible on the linear scale of the\nplot):\ntest_data <- bind_rows(\n    rxy(n = 1e6, pi = 0.5) |> mutate(type = \"Balanced\", llr_thresh = 0),\n    rxy(n = 1e6) |> mutate(type = \"Unbalanced\", llr_thresh = -log(pi / (1-pi)))\n    )\n\ntest_data |> \n    ggplot(aes(x = x, fill = y)) + \n    geom_histogram(bins = 100) +\n    facet_grid(type ~ ., scales = \"free_y\") +\n    xlim(c(-2, 4))\nThese differences are reflected in the misclassification rates of the\nresulting classifiers defined by\n\\(\\hat\nY(X)=I(\\text{LLR}(X)>\\text{threshold})\\)\n, where the\nappropriate threshold is zero in the balanced case, and\n\\(\\ln(\\frac{1-\\pi}{\\pi})\\)\nin the unbalanced\ncase. According to intuition, we see that the\n\\(C_\\text{LLR}\\)\noptimizer beats the\ncross-entropy optimizer on the balanced sample, while performing\nsignificantly worse on the unbalanced one.\ntest_data |>\n    mutate(\n        llr_cllr = llr(x, par_cllr),\n        llr_cross_entropy = llr(x, par_cross_entropy),\n        llr_true = llr_true(x)\n        ) |>\n    group_by(type) |>\n    summarise(\n        cllr = 1 - mean((llr_cllr > llr_thresh) == y),\n        cross_entropy = 1 - mean((llr_cross_entropy > llr_thresh) == y),\n        true_llr = 1 - mean((llr_true > llr_thresh) == y)\n        )\n# A tibble: 2 × 4\n  type           cllr cross_entropy true_llr\n  <chr>         <dbl>         <dbl>    <dbl>\n1 Balanced   0.166         0.185    0.140   \n2 Unbalanced 0.000994      0.000637 0.000518\nFinal remarks\nOur main conclusion in a nutshell is that\n\\(C_\\text{LLR}\\)\nminimization is equivalent,\nin the infinite sample limit\n, to cross-entropy minimization on\na balanced version of the original distribution. We haven’t discussed\nwhat happens for finite samples where variance starts to play a role,\naffecting the\nefficiency\nof loss functions as model\noptimization and selection criteria. For instance, for a well specified\nmodel of likelihood ratio, how do the convergence properties of\n\\(C_{\\text{LLR}}\\)\nand cross-entropy\nestimators compare to each other? I expect that answering questions like\nthis would require a much more in-depth study than the one performed\nhere (likely, with simulation playing a central role).\nBrümmer, Niko, and Johan du Preez. 2006.\n“Application-Independent\nEvaluation of Speaker Detection.”\nComputer Speech &\nLanguage\n20 (2): 230–75. https://doi.org/\nhttps://doi.org/10.1016/j.csl.2005.08.001\n.\nCover, Thomas M., and Joy A. Thomas. 2006.\nElements of Information\nTheory 2nd Edition (Wiley Series in Telecommunications and Signal\nProcessing)\n. Hardcover; Wiley-Interscience.\nThis is how I understood things should\ntheoretically\nwork, from discussions with friends who are\nactually working on this field. I have no idea on how much day-to-day\npractice comes close to this mathematical ideal, and whether there exist\nalternative frameworks to the one I have just described.\n↩︎\nThe Likelihood Ratio Cost was introduced in\n(Brümmer and du Preez 2006)\n. The reference looks\nvery complete, but I find its notation and terminology so unfamiliar\nthat I decided to do my own investigation and leave this reading for a\nsecond moment.\n↩︎\nThat is to say,\n\\(w(Y_{\\sigma(1)},\\,Y_{\\sigma(2)},\\dots,\\,Y_{\\sigma(N)})=w(Y_1,\\,Y_2,\\dots,\\,Y_N)\\)\nfor any permutation\n\\(\\sigma\\)\nof the\nset\n\\(\\{1,\\,2,\\,\\dots,\\,N\\}\\)\n.\n↩︎\nNota bene:\nthe function\n\\(f\\)\nis here assumed to be fixed, whereas\nthe randomness in the quantity\n\\(L _N ^w\n[f]\\)\nonly comes from the paired observations\n\\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\)\n.\n↩︎\nNotice that, due to symmetry\n\\(\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i >0)\n= \\mathbb E(w(\\boldsymbol Y)\\vert Y_1 = 1)\\)\n, which might be\neasier to compute.\n↩︎\nHere and below I relax a bit the notation, as most\ndetails should be clear from context.\n↩︎\nThe quantity\n\\(w(\\boldsymbol\nY)\\)\nis not defined when all\n\\(Y_i\\)\n’s are zero, as the right-hand side of\nEq. @ref(eq:CLLR) itself. In this case, we make the convention\n\\(w(\\boldsymbol Y) = 0\\)\n.\n↩︎\nFor the original loss in Eq. @ref(eq:CLLR), without the\nmodification discussed above, the result would have been\n\\(\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime\n(Y=0)}=\\dfrac{1-\\text {Pr}(Y=0)^N}{1-\\text {Pr}(Y=1)^N}.\\)\n↩︎\nFormally, given an i.i.d. stochastic process\n\\(Z_i = (X_i,\\,Y_i)\\)\n, we can define a new\nstochastic process\n\\(Z_i ^\\prime =\n(X_i^\\prime,\\,Y_i^\\prime)\\)\nsuch that\n\\(Z_i ^\\prime = Z_{2i - 1}\\)\nif\n\\(Y_{2i-1}\\neq Y_{2i}\\)\n, and\n\\(Z_i ^\\prime = \\perp\\)\n(not defined)\notherwise. Discarding\n\\(\\perp\\)\nvalues,\nwe obtain an i.i.d. stochastic process whose individual observations are\ndistributed according to\n\\(\\text{Pr}^\\prime\\)\n.\n↩︎\nThere is another case in which\n\\(C_{\\text{LLR}}\\)\nand cross-entropy\nminimization converge to the same answer as\n\\(N\\to \\infty\\)\n: when used for model\nselection among a class of models for the likelihood or posterior odds\nratio that contains their correct functional form.\n↩︎\nThis is just logistic regression. It could be a\nreasonable approximation if\n\\(\\sigma_0\n^2\\approx \\sigma_1 ^2\\)\n, which however I will assume below to be\nbadly violated.\n↩︎\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nValerio Gherardi\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Intro During the last few months, I’ve been working on a machine learning algorithm with applications in Forensic Science, a.k.a. Criminalistics. In this field, one common task for the data analyst is to present the trier-of-fact (the person or people who determine the facts in a legal proceeding) with a numerical assessment of the strength of the evidence provided by available data towards different hypotheses. In more familiar terms, the forensic expert is responsible of computing the likelihoods (or likelihood ratios) of data under competing hypotheses, which are then used by the trier-of-fact to produce Bayesian posterior probabilities for the hypotheses in question1. In relation to this, forensic scientists have developed a bunch of techniques to evaluate the performance of a likelihood ratio model in discriminating between two alternative hypothesis. In particular, I have come across the so called Likelihood Ratio Cost, usually defined as: \\[ C_{\\text{LLR}} = \\frac{1}{2N_1} \\sum _{Y_i=1} \\log(1+r(X_i) ^{-1})+\\frac{1}{2N_0} \\sum _{Y_i=0} \\log(1+r(X_i)), (\\#eq:CLLR) \\] where we assume we have data consisting of \\(N_1+N_0\\) independent identically distributed observations \\((X_i,\\,Y_i)\\), with binary \\(Y\\); \\(N_1\\) and \\(N_0\\) stand for the number of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) cases; and \\(r(X)\\) is a model for the likelihood ratio \\(\\Lambda(X) \\equiv \\frac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}\\). The main reason for writing this note was to understand a bit better what it means to optimize Eq. @ref(eq:CLLR), which does not look immediately obvious to me from its definition2. In particular: is the population minimizer of Eq. @ref(eq:CLLR) the actual likelihood ratio? And in what sense is a model with lower \\(C_\\text{LLR}\\) better than one with a correspondingly higher value? The short answers to these questions are: yes; and: \\(C_\\text{LLR}\\) optimization seeks for the model with the best predictive performance in a Bayesian inference setting with uninformative prior on \\(Y\\), assuming that this prior actually reflects reality (i.e. \\(\\text{Pr}(Y=1) = \\text{Pr}(Y=0) = \\frac{1}{2}\\)). The mathematical details are given in the rest of the post. Cross-entropy with random weights We start with a mathematical digression, which will turn out useful for further developments. Let \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\) be independent draws from a joint distribution, with binary \\(Y_i \\in \\{0,\\,1\\}\\). Given a function \\(w=w(\\boldsymbol Y)\\) that is symmetric in its arguments3, we define the random functional: \\[ \\mathcal L_N^w[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(f(X_i))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(f(X_i)^c)\\right],(\\#eq:WeightedLoss) \\] where \\(f=f(X)\\) is any function satisfying \\(f(X)\\in [0,\\,1]\\) for all \\(X\\), and we let \\(q^c = 1-q\\) for any number \\(q \\in [0,\\,1]\\). Notice that for \\(w(\\boldsymbol{Y}) \\equiv 1\\), this is just the usual cross-entropy loss. We now look for the population minimizer of @ref(eq:WeightedLoss), i.e. the function \\(f_*\\) that minimizes the functional \\(f \\mapsto \\mathbb E(\\mathcal L _N ^w [f])\\)4. Writing the expectation as: \\[ \\mathbb E(\\mathcal L _N ^w [f]) = -\\frac{1}{N}\\sum _{i=1} ^N \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)\\cdot \\log(f(X_i))+E(Y_i^c\\cdot w(\\boldsymbol Y ^c)\\vert X_i)\\cdot \\log(f^c(X_i))\\right], \\] we can easily see that \\(\\mathbb E(\\mathcal L _N ^w [f])\\) is a convex functional with a unique minimum given by: \\[ f_*(X_i) = \\frac{1}{1+r(X_i)^{-1}},\\quad r_*(X_i) = \\dfrac{E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)}{E(Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)}.(\\#eq:PopMinimizer) \\] The corresponding expected loss is: \\[ \\mathbb E(\\mathcal L _N ^w [f_*]) = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i))\\right], \\] where \\(\\mathcal H(p) = -p \\log (p) -(1-p) \\log(1-p)\\) is the entropy of a binary random variable \\(Z\\) with probability \\(p = \\text{Pr}(Z=1)\\) (the index \\(i\\) in the previous expression can be any index, since data points are assumed to be identically distributed). Before looking at values of \\(f\\) other than \\(f_*\\), we observe that the previous expectation can be succintly expressed as: \\[ \\mathbb E(\\mathcal L _N ^w [f_*]) = k \\cdot H^\\prime(Y\\vert X), \\] where \\[ k = \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c))(\\#eq:DefKappa) \\] and \\(H'(Y\\vert X)\\) is the conditional entropy of \\(Y\\vert X\\) with respect to a different probability measure \\(\\text{Pr}^\\prime\\), defined by: \\[ \\text{Pr}^\\prime(E) = t \\cdot \\text {Pr}(E \\vert Y = 1) + (1-t)\\cdot \\text {Pr}(E \\vert Y = 0), (\\#eq:DefPrPrime) \\] where \\(t=\\text{Pr}^\\prime(Y=1)\\in [0,\\,1]\\) is fixed by the requirement5: \\[ \\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{\\text {Pr} (Y=1)}{\\text{Pr} (Y=0)}\\cdot\\dfrac{\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i >0)}{\\mathbb E(w(\\boldsymbol Y^c)\\vert \\sum _i Y_i^c >0)}. (\\#eq:DefPrPrime2) \\] In terms of \\(\\text{Pr}^\\prime\\), the population minimizers \\(f_*\\) and \\(r_*\\) in Eq. @ref(eq:PopMinimizer) can be simply expressed as: \\[ r_*(X)=\\dfrac{\\text {Pr}^\\prime(Y=1\\vert X)}{\\text {Pr}^\\prime(Y=0\\vert X)},\\qquad f_*(X)=\\text {Pr}^\\prime(Y=1\\vert X). (\\#eq:PopMinimizer2) \\] If now \\(f\\) is an arbitrary function, we have: \\[ \\begin{split} \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right] &= k\\cdot D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f) \\end{split} \\] where \\(\\mathcal D(p\\vert \\vert q) = p \\log (\\frac{p}{q}) + (1-p) \\log (\\frac{1-p}{1-q})\\), and \\(D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\\) is the Kullback-Liebler divergence between the measure \\(\\text{Pr}^\\prime\\) and the measure \\(\\text{Pr}^\\prime _f\\) defined by: \\[ \\text{Pr}^\\prime _f(Y = 1\\vert X)=f(X),\\qquad \\text{Pr}^\\prime _f(X)=\\text{Pr}^\\prime(X) \\] (notice that \\(\\text {Pr} ^{\\prime} _{f_*} \\equiv \\text{Pr} ^{\\prime}\\) by definition). Finally, suppose that \\(X = g(\\widetilde X)\\) for some random variable \\(\\widetilde X\\), and define the corresponding functional: \\[ \\widetilde{\\mathcal L} _N^w[\\widetilde f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(\\widetilde f(\\widetilde X))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(\\widetilde f(\\widetilde X)^c)\\right]. \\] Then \\(\\mathcal L _N ^w [f] = \\widetilde{\\mathcal L} _N^w[f \\circ g]\\). If \\(\\widetilde f _* =\\) is the population minimizer of \\(\\widetilde{\\mathcal L} _N^w\\), it follows that \\(\\mathbb E (\\widetilde{\\mathcal L} _N^w[\\widetilde f _*]) \\leq \\mathbb E(\\mathcal L _N ^w [f_*])\\). Putting everything together, we can decompose the expected loss for a function \\(f=f(X)\\), where \\(X= g(\\widetilde X)\\), in the following suggestive way: \\[ \\begin{split} \\mathbb E(\\mathcal L _N ^w [f]) &= (L_N ^w)_\\text{min}+(L_N ^w)_\\text{proc} +(L_N ^w)_\\text{missp},\\ (L_N ^w)_\\text{min}&\\equiv\\mathbb E(\\widetilde{\\mathcal L} _N^w[{\\widetilde f} _*]) \\ &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert \\widetilde X _i)\\cdot \\mathcal H({\\widetilde f} _*(\\widetilde X _i))\\right]\\ &=k\\cdot H^\\prime(Y\\vert \\widetilde X),\\ (L_N ^w)_\\text{proc}&\\equiv\\mathbb E(\\mathcal L _N ^w [f_*]-\\widetilde{\\mathcal L} _N^w[\\phi_*]) \\& = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i)) \\right]- (L_N ^w)_\\text{min}\\ & = k\\cdot I^\\prime(Y; \\widetilde X\\vert X),\\ (L_N ^w)_\\text{missp} & \\equiv \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) \\&= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right]\\ &=k\\cdot D(\\text {Pr}^\\prime\\vert \\vert \\text {Pr}^\\prime _f), \\end{split} (\\#eq:DecompositionWeightedLoss) \\] where \\(k\\) is defined in Eq. @ref(eq:DefKappa). In the equation for \\((L^w _N)_\\text{proc}\\) we introduced the conditional mutual information (with respect to the measure \\(\\text{Pr}^\\prime\\)), that satisfies (Cover and Thomas 2006): \\[ I(\\widetilde X;Y\\vert X) = I(\\widetilde X,Y)-I(X,Y) = H(Y\\vert X)-H(Y\\vert \\widetilde X). \\] The three components in Eq. @ref(eq:DecompositionWeightedLoss) can be interpreted as follows: \\((L_N ^w)_\\text{min}\\) represents the minimum expected loss achievable, given the data available \\(\\widetilde X\\); \\((L_N ^w)_\\text{proc}\\) accounts for the information lost in the processing transformation \\(X=g(\\widetilde X)\\); finally \\((L_N ^w)_\\text{missp}\\) is due to misspecification, i.e. the fact that the model \\(f(X)\\) for the true posterior probability \\(f_*(X)\\) is an approximation. All the information-theoretic quantities (and their corresponding operative interpretations hinted in the previous paragraph) make reference to the measure \\(\\text{Pr}^\\prime\\) defined by Eqs. @ref(eq:DefPrPrime) and @ref(eq:DefPrPrime2). This is merely the result of altering the proportion of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) examples in the \\(X\\)-\\(Y\\) joint distribution by a factor dictated by the weight function \\(w\\) - while keeping conditional distributions such as \\(X\\vert Y\\) unchanged. A familiar case: cross-entropy loss For \\(w(\\boldsymbol {Y}) = 1\\), the functional \\(\\mathcal {L} _{N} ^{w}[f]\\) coincides with the usual cross-entropy loss6: \\[ \\text{CE}[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[Y_i \\log(f(X_i))+ (1-Y_i) \\log(1-f(X_i))\\right].(\\#eq:CrossEntropyLoss) \\] From Eq. @ref(eq:DefPrPrime2) we see that the measure \\(\\text{Pr}^{\\prime}\\) coincides with the original \\(\\text{Pr}\\), so that by Eq. @ref(eq:PopMinimizer) the population minimizer of @ref(eq:CrossEntropyLoss) is \\(f_{*}(X) = \\text{Pr}(Y=1\\vert X)\\) (independently of sample size). Since \\(k = 1\\) (cf. Eq. @ref(eq:DefKappa)), the decomposition @ref(eq:DecompositionWeightedLoss) reads: \\[ \\begin{split} \\mathbb E(\\text{CE} [f]) &= (\\text{CE})_\\text{min}+(\\text{CE})_\\text{proc} +(\\text{CE})_\\text{missp},\\ (\\text{CE})_\\text{min}&=H(Y\\vert \\widetilde X),\\ (\\text{CE})_\\text{proc}&= I(Y; \\widetilde X\\vert X),\\ (\\text{CE})_\\text{missp} &=D(\\text {Pr}\\vert \\vert \\text {Pr} _{f}), \\end{split} (\\#eq:DecompositionCE) \\] where conditional entropy \\(H\\), mutual information \\(I\\) and relative entropy \\(D\\) now simply refer to the original measure \\(\\text{Pr}\\). The Likelihood Ratio Cost The quantity \\(C_{\\text{LLR}}\\) defined in Eq. @ref(eq:CLLR) can be put in the general form @ref(eq:WeightedLoss), if we let \\(f(X) = (1+r(X)^{-1})^{-1}\\) and7: \\[ w(\\boldsymbol Y) = \\left(\\dfrac{2}{N}\\sum _{i = 1}^{N}Y_j \\right)^{-1} \\] In what follows, I will consider a slight modification of the usual \\(C_\\text{LLR}\\), defined by the weight function: \\[ w(\\boldsymbol Y) = \\dfrac{1}{2(N-1)}\\sum _{i = 1}^{N}(1-Y_j). \\] This yields Eq. @ref(eq:CLLR) multiplied by \\(\\dfrac{N_1N_0}{N(N-1)}\\), which I will keep denoting as \\(C_\\text{LLR}\\), with a slight abuse of notation. We can easily compute8: \\[ \\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=1, (\\#eq:PriorCLLR) \\] so that, by Eq. @ref(eq:PopMinimizer), the population minimizer of \\(C_\\text{LLR}\\) is: \\[ r_*(X) = \\Lambda (X),\\quad f_*(X)=\\dfrac{1}{1+\\Lambda(X)^{-1}}, \\] where \\(\\Lambda(X)\\) denotes the likelihood-ratio of \\(X\\), schematically: \\[ \\Lambda(X)\\equiv \\dfrac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}. \\] The constant \\(k\\) in Eq. @ref(eq:DefKappa) is: \\[ k = \\text{Pr}(Y = 1)\\text{Pr}(Y = 0)=\\text{Var}(Y) \\] The general decomposition @ref(eq:DecompositionWeightedLoss) becomes: \\[ \\begin{split} \\mathbb E(C_\\text{LLR} [f]) &= (C_\\text{LLR})_\\text{min}+(C_\\text{LLR})_\\text{proc} +(C_\\text{LLR})_\\text{missp},\\ (C_\\text{LLR})_\\text{min}&=\\text{Var}(Y)\\cdot H^{\\prime}(Y\\vert \\widetilde X),\\ (C_\\text{LLR})_\\text{proc}&= \\text{Var}(Y)\\cdot I^{\\prime}(Y; \\widetilde X\\vert X),\\ (C_\\text{LLR})_\\text{missp} &=\\text{Var}(Y)\\cdot D^{\\prime}(\\text {Pr}\\vert \\vert \\text {Pr} _{f}), \\end{split} (\\#eq:DecompositionCE) \\] where \\(\\text{Pr}^\\prime\\) is now given by @ref(eq:PriorCLLR). Discussion The table below provides a comparison between cross-entropy and likelihood-ratio cost, summarizing the results from previous sections. Cross-entropy Likelihood Ratio Cost \\(f_*(X)\\) \\(\\text{Pr}(Y = 1\\vert X)\\) \\((1+\\Lambda(X)^{-1})^{-1}\\) \\(r_*(X)\\)` Posterior odds ratio Likelihood ratio Minimum Loss \\(H(Y\\vert \\widetilde X)\\) \\(\\text{Var}(Y) \\cdot H^\\prime(Y\\vert \\widetilde X)\\) Processing Loss \\(I(Y; \\widetilde X\\vert X)\\) \\(\\text{Var}(Y) \\cdot I^\\prime(Y; \\widetilde X\\vert X)\\) Misspecification Loss \\(D(f_*\\vert\\vert f)\\) \\(\\text{Var}(Y) \\cdot D^\\prime(f_*\\vert\\vert f)\\) Reference measure \\(\\text{Pr}\\) \\(\\text{Pr}^{\\prime} = \\frac{\\text{Pr}(\\cdot \\vert Y = 1)+\\text{Pr}(\\cdot \\vert Y = 0)}{2}\\) The objective of \\(C_\\text{LLR}\\) is found to be the likelihood ratio, as terminology suggests. The interpretation of model selection according to \\(C_\\text{LLR}\\) minimization turns out to be slightly more involved, compared to cross-entropy, which we first review. Suppose we are given a set of predictive models \\(\\{\\mathcal M_i\\}_{i\\in I}\\), each of which consists of a processing transformation, \\(\\widetilde X \\mapsto X\\), and an estimate of the posterior probability \\(\\text{Pr}(Y = 1\\vert X)\\). When the sample size \\(N \\to \\infty\\), cross-entropy minimization will almost certainly select the model that minimizes \\(I(Y; \\widetilde X\\vert X) + D(f_*\\vert \\vert f)\\). Following standard Information Theory arguments, we can interpret this model as the statistically optimal compression algorithm for \\(Y\\), assuming \\(X\\) to be available at both the encoding and decoding ends. The previous argument carries over mutatis mutandi to \\(C_\\text{LLR}\\) minimization, with an important qualification: optimal average compression is now achieved for data distributed according to a different probability measure \\(\\text{Pr}'(\\cdot) = \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 1) + \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 0)\\). In particular, according to \\(\\text{Pr}'\\), the likelihood ratio coincides with the posterior odds ratio, and \\((1+\\Lambda(X)^{-1})^{-1}\\) coincides with posterior probability, which clarifies why we can measure differences from the true likelihood-ratio through the Kullback-Liebler divergence. The measure \\(\\text{Pr}'\\) is not just an abstruse mathematical construct: it is the result of balanced sampling from the original distribution, i.e. taking an equal number of positive and negative cases9. If the \\((X,\\,Y)\\) distribution is already balanced, either by design or because of some underlying symmetry in the data generating process, our analysis implies that likelihood-ratio cost and cross-entropy minimization are essentially equivalent for \\(N\\to \\infty\\). In general, with \\(\\text{Pr} (Y=1) \\neq \\text{Pr} (Y=0)\\), this is not the case10. The fact that \\(C_\\text{LLR}\\) seeks for optimal predictors according to the balanced measure \\(\\text{Pr}'\\) is, one could argue, not completely crazy from the point of view of forensic science, where “\\(Y\\in\\{0,1\\}\\)” often stands for a sort verdict (guilty vs. not guilty, say). Indeed, optimizing with respect to \\(\\text{Pr}^\\prime\\) means that our predictions are designed to be optimal in a world in which the verdict could be a priori \\(Y=0\\) or \\(Y=1\\) with equal probability - which is what an unbiased trier-of-fact should ideally assume. Minimizing \\(C_\\text{LLR}\\), we guard ourselves against any bias that may be implicit in the training dataset, extraneous to the \\(X\\)-\\(Y\\) relation and not explicitly modeled, a feature that may be regarded as desirable from a legal standpoint. Simulated example In general, the posterior odd ratio and likelihood ratio differ only by a constant, so it is reasonable to try to fit the same functional form to both of them. Let us illustrate with a simulated example of this type the differences between cross-entropy and \\(C_{\\text{LLR}}\\) optimization mentioned in the previous Section. Suppose that \\(X \\in \\mathbb R\\) has conditional density: \\[ \\phi(X\\vert Y) = (2\\pi\\sigma _Y^2)^{-\\frac{1}{2}} \\exp(-\\frac{(X-\\mu_Y)^2}{2\\sigma _Y^2}) \\] and \\(Y\\) has marginal probability \\(\\text{Pr}(Y = 1) = \\pi\\). The true likelihood-ratio and posterior odds ratio are respectively given by: \\[ \\begin{split} \\Lambda (X) & \\equiv \\frac{\\phi(X\\vert Y=1)}{\\phi(X\\vert Y=0)} = e ^ {a X^2 + bX +c},\\ \\rho (X) & \\equiv \\frac{\\text{Pr}(Y = 1\\vert X)}{\\text{Pr}(Y = 0\\vert X)} = e ^ {a X ^ 2 + bX +c+d}, \\end{split} \\] where we have defined: \\[ a \\equiv \\dfrac{\\sigma _1 ^2 -\\sigma_0 ^2}{2\\sigma _0 ^2\\sigma_1 ^2},\\quad b \\equiv \\mu _1 - \\mu _0, \\quad c \\equiv \\dfrac{\\mu_0^2}{2\\sigma_0^2} -\\dfrac{\\mu_1 ^2}{2\\sigma _1^2}+\\ln(\\frac{\\sigma _0 }{\\sigma _1 }),\\quad d \\equiv \\ln (\\frac {\\pi}{1-\\pi}) . \\] Suppose that we fit an exponential function \\(r(X)=e^{mX +q}\\) to \\(\\Lambda(X)\\) by likelihood-ratio cost minimization, and similarly \\(r'(X)=e^{m'X+q'}\\) to \\(\\rho(X)\\) by cross-entropy minimization11. Due to the previous discussion, one could reasonably expect the results of the two procedure to differ in some way, which is demonstrated below by simulation. The chunk of R code below defines the function and data used for the simulation. In particular, I’m considering a heavily unbalanced case (\\(\\text{Pr}(Y = 1) = 0.1\\%\\)) in which negative cases give rise to a sharply localized \\(X\\) peak around \\(X=0\\) (\\(\\mu _0 = 0\\), \\(\\sigma_0 = .25\\)), while the few positive cases give rise to a broader signal centered at \\(X=1\\) (\\(\\mu _1 = 1\\), \\(\\sigma _1 = 1\\)). # Tidyverse facilities for plotting library(dplyr) library(ggplot2) # Loss functions weighted_loss",
      "meta_keywords": null,
      "og_description": "Intro During the last few months, I’ve been working on a machine learning algorithm with applications in Forensic Science, a.k.a. Criminalistics. In this field, one common task for the data analyst is to present the trier-of-fact (the person or people who determine the facts in a legal proceeding) with a numerical assessment of the strength of the evidence provided by available data towards different hypotheses. In more familiar terms, the forensic expert is responsible of computing the likelihoods (or likelihood ratios) of data under competing hypotheses, which are then used by the trier-of-fact to produce Bayesian posterior probabilities for the hypotheses in question1. In relation to this, forensic scientists have developed a bunch of techniques to evaluate the performance of a likelihood ratio model in discriminating between two alternative hypothesis. In particular, I have come across the so called Likelihood Ratio Cost, usually defined as: \\[ C_{\\text{LLR}} = \\frac{1}{2N_1} \\sum _{Y_i=1} \\log(1+r(X_i) ^{-1})+\\frac{1}{2N_0} \\sum _{Y_i=0} \\log(1+r(X_i)), (\\#eq:CLLR) \\] where we assume we have data consisting of \\(N_1+N_0\\) independent identically distributed observations \\((X_i,\\,Y_i)\\), with binary \\(Y\\); \\(N_1\\) and \\(N_0\\) stand for the number of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) cases; and \\(r(X)\\) is a model for the likelihood ratio \\(\\Lambda(X) \\equiv \\frac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}\\). The main reason for writing this note was to understand a bit better what it means to optimize Eq. @ref(eq:CLLR), which does not look immediately obvious to me from its definition2. In particular: is the population minimizer of Eq. @ref(eq:CLLR) the actual likelihood ratio? And in what sense is a model with lower \\(C_\\text{LLR}\\) better than one with a correspondingly higher value? The short answers to these questions are: yes; and: \\(C_\\text{LLR}\\) optimization seeks for the model with the best predictive performance in a Bayesian inference setting with uninformative prior on \\(Y\\), assuming that this prior actually reflects reality (i.e. \\(\\text{Pr}(Y=1) = \\text{Pr}(Y=0) = \\frac{1}{2}\\)). The mathematical details are given in the rest of the post. Cross-entropy with random weights We start with a mathematical digression, which will turn out useful for further developments. Let \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\) be independent draws from a joint distribution, with binary \\(Y_i \\in \\{0,\\,1\\}\\). Given a function \\(w=w(\\boldsymbol Y)\\) that is symmetric in its arguments3, we define the random functional: \\[ \\mathcal L_N^w[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(f(X_i))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(f(X_i)^c)\\right],(\\#eq:WeightedLoss) \\] where \\(f=f(X)\\) is any function satisfying \\(f(X)\\in [0,\\,1]\\) for all \\(X\\), and we let \\(q^c = 1-q\\) for any number \\(q \\in [0,\\,1]\\). Notice that for \\(w(\\boldsymbol{Y}) \\equiv 1\\), this is just the usual cross-entropy loss. We now look for the population minimizer of @ref(eq:WeightedLoss), i.e. the function \\(f_*\\) that minimizes the functional \\(f \\mapsto \\mathbb E(\\mathcal L _N ^w [f])\\)4. Writing the expectation as: \\[ \\mathbb E(\\mathcal L _N ^w [f]) = -\\frac{1}{N}\\sum _{i=1} ^N \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)\\cdot \\log(f(X_i))+E(Y_i^c\\cdot w(\\boldsymbol Y ^c)\\vert X_i)\\cdot \\log(f^c(X_i))\\right], \\] we can easily see that \\(\\mathbb E(\\mathcal L _N ^w [f])\\) is a convex functional with a unique minimum given by: \\[ f_*(X_i) = \\frac{1}{1+r(X_i)^{-1}},\\quad r_*(X_i) = \\dfrac{E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)}{E(Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)}.(\\#eq:PopMinimizer) \\] The corresponding expected loss is: \\[ \\mathbb E(\\mathcal L _N ^w [f_*]) = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i))\\right], \\] where \\(\\mathcal H(p) = -p \\log (p) -(1-p) \\log(1-p)\\) is the entropy of a binary random variable \\(Z\\) with probability \\(p = \\text{Pr}(Z=1)\\) (the index \\(i\\) in the previous expression can be any index, since data points are assumed to be identically distributed). Before looking at values of \\(f\\) other than \\(f_*\\), we observe that the previous expectation can be succintly expressed as: \\[ \\mathbb E(\\mathcal L _N ^w [f_*]) = k \\cdot H^\\prime(Y\\vert X), \\] where \\[ k = \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c))(\\#eq:DefKappa) \\] and \\(H'(Y\\vert X)\\) is the conditional entropy of \\(Y\\vert X\\) with respect to a different probability measure \\(\\text{Pr}^\\prime\\), defined by: \\[ \\text{Pr}^\\prime(E) = t \\cdot \\text {Pr}(E \\vert Y = 1) + (1-t)\\cdot \\text {Pr}(E \\vert Y = 0), (\\#eq:DefPrPrime) \\] where \\(t=\\text{Pr}^\\prime(Y=1)\\in [0,\\,1]\\) is fixed by the requirement5: \\[ \\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{\\text {Pr} (Y=1)}{\\text{Pr} (Y=0)}\\cdot\\dfrac{\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i >0)}{\\mathbb E(w(\\boldsymbol Y^c)\\vert \\sum _i Y_i^c >0)}. (\\#eq:DefPrPrime2) \\] In terms of \\(\\text{Pr}^\\prime\\), the population minimizers \\(f_*\\) and \\(r_*\\) in Eq. @ref(eq:PopMinimizer) can be simply expressed as: \\[ r_*(X)=\\dfrac{\\text {Pr}^\\prime(Y=1\\vert X)}{\\text {Pr}^\\prime(Y=0\\vert X)},\\qquad f_*(X)=\\text {Pr}^\\prime(Y=1\\vert X). (\\#eq:PopMinimizer2) \\] If now \\(f\\) is an arbitrary function, we have: \\[ \\begin{split} \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right] &= k\\cdot D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f) \\end{split} \\] where \\(\\mathcal D(p\\vert \\vert q) = p \\log (\\frac{p}{q}) + (1-p) \\log (\\frac{1-p}{1-q})\\), and \\(D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\\) is the Kullback-Liebler divergence between the measure \\(\\text{Pr}^\\prime\\) and the measure \\(\\text{Pr}^\\prime _f\\) defined by: \\[ \\text{Pr}^\\prime _f(Y = 1\\vert X)=f(X),\\qquad \\text{Pr}^\\prime _f(X)=\\text{Pr}^\\prime(X) \\] (notice that \\(\\text {Pr} ^{\\prime} _{f_*} \\equiv \\text{Pr} ^{\\prime}\\) by definition). Finally, suppose that \\(X = g(\\widetilde X)\\) for some random variable \\(\\widetilde X\\), and define the corresponding functional: \\[ \\widetilde{\\mathcal L} _N^w[\\widetilde f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(\\widetilde f(\\widetilde X))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(\\widetilde f(\\widetilde X)^c)\\right]. \\] Then \\(\\mathcal L _N ^w [f] = \\widetilde{\\mathcal L} _N^w[f \\circ g]\\). If \\(\\widetilde f _* =\\) is the population minimizer of \\(\\widetilde{\\mathcal L} _N^w\\), it follows that \\(\\mathbb E (\\widetilde{\\mathcal L} _N^w[\\widetilde f _*]) \\leq \\mathbb E(\\mathcal L _N ^w [f_*])\\). Putting everything together, we can decompose the expected loss for a function \\(f=f(X)\\), where \\(X= g(\\widetilde X)\\), in the following suggestive way: \\[ \\begin{split} \\mathbb E(\\mathcal L _N ^w [f]) &= (L_N ^w)_\\text{min}+(L_N ^w)_\\text{proc} +(L_N ^w)_\\text{missp},\\ (L_N ^w)_\\text{min}&\\equiv\\mathbb E(\\widetilde{\\mathcal L} _N^w[{\\widetilde f} _*]) \\ &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert \\widetilde X _i)\\cdot \\mathcal H({\\widetilde f} _*(\\widetilde X _i))\\right]\\ &=k\\cdot H^\\prime(Y\\vert \\widetilde X),\\ (L_N ^w)_\\text{proc}&\\equiv\\mathbb E(\\mathcal L _N ^w [f_*]-\\widetilde{\\mathcal L} _N^w[\\phi_*]) \\& = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i)) \\right]- (L_N ^w)_\\text{min}\\ & = k\\cdot I^\\prime(Y; \\widetilde X\\vert X),\\ (L_N ^w)_\\text{missp} & \\equiv \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) \\&= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right]\\ &=k\\cdot D(\\text {Pr}^\\prime\\vert \\vert \\text {Pr}^\\prime _f), \\end{split} (\\#eq:DecompositionWeightedLoss) \\] where \\(k\\) is defined in Eq. @ref(eq:DefKappa). In the equation for \\((L^w _N)_\\text{proc}\\) we introduced the conditional mutual information (with respect to the measure \\(\\text{Pr}^\\prime\\)), that satisfies (Cover and Thomas 2006): \\[ I(\\widetilde X;Y\\vert X) = I(\\widetilde X,Y)-I(X,Y) = H(Y\\vert X)-H(Y\\vert \\widetilde X). \\] The three components in Eq. @ref(eq:DecompositionWeightedLoss) can be interpreted as follows: \\((L_N ^w)_\\text{min}\\) represents the minimum expected loss achievable, given the data available \\(\\widetilde X\\); \\((L_N ^w)_\\text{proc}\\) accounts for the information lost in the processing transformation \\(X=g(\\widetilde X)\\); finally \\((L_N ^w)_\\text{missp}\\) is due to misspecification, i.e. the fact that the model \\(f(X)\\) for the true posterior probability \\(f_*(X)\\) is an approximation. All the information-theoretic quantities (and their corresponding operative interpretations hinted in the previous paragraph) make reference to the measure \\(\\text{Pr}^\\prime\\) defined by Eqs. @ref(eq:DefPrPrime) and @ref(eq:DefPrPrime2). This is merely the result of altering the proportion of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) examples in the \\(X\\)-\\(Y\\) joint distribution by a factor dictated by the weight function \\(w\\) - while keeping conditional distributions such as \\(X\\vert Y\\) unchanged. A familiar case: cross-entropy loss For \\(w(\\boldsymbol {Y}) = 1\\), the functional \\(\\mathcal {L} _{N} ^{w}[f]\\) coincides with the usual cross-entropy loss6: \\[ \\text{CE}[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[Y_i \\log(f(X_i))+ (1-Y_i) \\log(1-f(X_i))\\right].(\\#eq:CrossEntropyLoss) \\] From Eq. @ref(eq:DefPrPrime2) we see that the measure \\(\\text{Pr}^{\\prime}\\) coincides with the original \\(\\text{Pr}\\), so that by Eq. @ref(eq:PopMinimizer) the population minimizer of @ref(eq:CrossEntropyLoss) is \\(f_{*}(X) = \\text{Pr}(Y=1\\vert X)\\) (independently of sample size). Since \\(k = 1\\) (cf. Eq. @ref(eq:DefKappa)), the decomposition @ref(eq:DecompositionWeightedLoss) reads: \\[ \\begin{split} \\mathbb E(\\text{CE} [f]) &= (\\text{CE})_\\text{min}+(\\text{CE})_\\text{proc} +(\\text{CE})_\\text{missp},\\ (\\text{CE})_\\text{min}&=H(Y\\vert \\widetilde X),\\ (\\text{CE})_\\text{proc}&= I(Y; \\widetilde X\\vert X),\\ (\\text{CE})_\\text{missp} &=D(\\text {Pr}\\vert \\vert \\text {Pr} _{f}), \\end{split} (\\#eq:DecompositionCE) \\] where conditional entropy \\(H\\), mutual information \\(I\\) and relative entropy \\(D\\) now simply refer to the original measure \\(\\text{Pr}\\). The Likelihood Ratio Cost The quantity \\(C_{\\text{LLR}}\\) defined in Eq. @ref(eq:CLLR) can be put in the general form @ref(eq:WeightedLoss), if we let \\(f(X) = (1+r(X)^{-1})^{-1}\\) and7: \\[ w(\\boldsymbol Y) = \\left(\\dfrac{2}{N}\\sum _{i = 1}^{N}Y_j \\right)^{-1} \\] In what follows, I will consider a slight modification of the usual \\(C_\\text{LLR}\\), defined by the weight function: \\[ w(\\boldsymbol Y) = \\dfrac{1}{2(N-1)}\\sum _{i = 1}^{N}(1-Y_j). \\] This yields Eq. @ref(eq:CLLR) multiplied by \\(\\dfrac{N_1N_0}{N(N-1)}\\), which I will keep denoting as \\(C_\\text{LLR}\\), with a slight abuse of notation. We can easily compute8: \\[ \\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=1, (\\#eq:PriorCLLR) \\] so that, by Eq. @ref(eq:PopMinimizer), the population minimizer of \\(C_\\text{LLR}\\) is: \\[ r_*(X) = \\Lambda (X),\\quad f_*(X)=\\dfrac{1}{1+\\Lambda(X)^{-1}}, \\] where \\(\\Lambda(X)\\) denotes the likelihood-ratio of \\(X\\), schematically: \\[ \\Lambda(X)\\equiv \\dfrac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}. \\] The constant \\(k\\) in Eq. @ref(eq:DefKappa) is: \\[ k = \\text{Pr}(Y = 1)\\text{Pr}(Y = 0)=\\text{Var}(Y) \\] The general decomposition @ref(eq:DecompositionWeightedLoss) becomes: \\[ \\begin{split} \\mathbb E(C_\\text{LLR} [f]) &= (C_\\text{LLR})_\\text{min}+(C_\\text{LLR})_\\text{proc} +(C_\\text{LLR})_\\text{missp},\\ (C_\\text{LLR})_\\text{min}&=\\text{Var}(Y)\\cdot H^{\\prime}(Y\\vert \\widetilde X),\\ (C_\\text{LLR})_\\text{proc}&= \\text{Var}(Y)\\cdot I^{\\prime}(Y; \\widetilde X\\vert X),\\ (C_\\text{LLR})_\\text{missp} &=\\text{Var}(Y)\\cdot D^{\\prime}(\\text {Pr}\\vert \\vert \\text {Pr} _{f}), \\end{split} (\\#eq:DecompositionCE) \\] where \\(\\text{Pr}^\\prime\\) is now given by @ref(eq:PriorCLLR). Discussion The table below provides a comparison between cross-entropy and likelihood-ratio cost, summarizing the results from previous sections. Cross-entropy Likelihood Ratio Cost \\(f_*(X)\\) \\(\\text{Pr}(Y = 1\\vert X)\\) \\((1+\\Lambda(X)^{-1})^{-1}\\) \\(r_*(X)\\)` Posterior odds ratio Likelihood ratio Minimum Loss \\(H(Y\\vert \\widetilde X)\\) \\(\\text{Var}(Y) \\cdot H^\\prime(Y\\vert \\widetilde X)\\) Processing Loss \\(I(Y; \\widetilde X\\vert X)\\) \\(\\text{Var}(Y) \\cdot I^\\prime(Y; \\widetilde X\\vert X)\\) Misspecification Loss \\(D(f_*\\vert\\vert f)\\) \\(\\text{Var}(Y) \\cdot D^\\prime(f_*\\vert\\vert f)\\) Reference measure \\(\\text{Pr}\\) \\(\\text{Pr}^{\\prime} = \\frac{\\text{Pr}(\\cdot \\vert Y = 1)+\\text{Pr}(\\cdot \\vert Y = 0)}{2}\\) The objective of \\(C_\\text{LLR}\\) is found to be the likelihood ratio, as terminology suggests. The interpretation of model selection according to \\(C_\\text{LLR}\\) minimization turns out to be slightly more involved, compared to cross-entropy, which we first review. Suppose we are given a set of predictive models \\(\\{\\mathcal M_i\\}_{i\\in I}\\), each of which consists of a processing transformation, \\(\\widetilde X \\mapsto X\\), and an estimate of the posterior probability \\(\\text{Pr}(Y = 1\\vert X)\\). When the sample size \\(N \\to \\infty\\), cross-entropy minimization will almost certainly select the model that minimizes \\(I(Y; \\widetilde X\\vert X) + D(f_*\\vert \\vert f)\\). Following standard Information Theory arguments, we can interpret this model as the statistically optimal compression algorithm for \\(Y\\), assuming \\(X\\) to be available at both the encoding and decoding ends. The previous argument carries over mutatis mutandi to \\(C_\\text{LLR}\\) minimization, with an important qualification: optimal average compression is now achieved for data distributed according to a different probability measure \\(\\text{Pr}'(\\cdot) = \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 1) + \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 0)\\). In particular, according to \\(\\text{Pr}'\\), the likelihood ratio coincides with the posterior odds ratio, and \\((1+\\Lambda(X)^{-1})^{-1}\\) coincides with posterior probability, which clarifies why we can measure differences from the true likelihood-ratio through the Kullback-Liebler divergence. The measure \\(\\text{Pr}'\\) is not just an abstruse mathematical construct: it is the result of balanced sampling from the original distribution, i.e. taking an equal number of positive and negative cases9. If the \\((X,\\,Y)\\) distribution is already balanced, either by design or because of some underlying symmetry in the data generating process, our analysis implies that likelihood-ratio cost and cross-entropy minimization are essentially equivalent for \\(N\\to \\infty\\). In general, with \\(\\text{Pr} (Y=1) \\neq \\text{Pr} (Y=0)\\), this is not the case10. The fact that \\(C_\\text{LLR}\\) seeks for optimal predictors according to the balanced measure \\(\\text{Pr}'\\) is, one could argue, not completely crazy from the point of view of forensic science, where “\\(Y\\in\\{0,1\\}\\)” often stands for a sort verdict (guilty vs. not guilty, say). Indeed, optimizing with respect to \\(\\text{Pr}^\\prime\\) means that our predictions are designed to be optimal in a world in which the verdict could be a priori \\(Y=0\\) or \\(Y=1\\) with equal probability - which is what an unbiased trier-of-fact should ideally assume. Minimizing \\(C_\\text{LLR}\\), we guard ourselves against any bias that may be implicit in the training dataset, extraneous to the \\(X\\)-\\(Y\\) relation and not explicitly modeled, a feature that may be regarded as desirable from a legal standpoint. Simulated example In general, the posterior odd ratio and likelihood ratio differ only by a constant, so it is reasonable to try to fit the same functional form to both of them. Let us illustrate with a simulated example of this type the differences between cross-entropy and \\(C_{\\text{LLR}}\\) optimization mentioned in the previous Section. Suppose that \\(X \\in \\mathbb R\\) has conditional density: \\[ \\phi(X\\vert Y) = (2\\pi\\sigma _Y^2)^{-\\frac{1}{2}} \\exp(-\\frac{(X-\\mu_Y)^2}{2\\sigma _Y^2}) \\] and \\(Y\\) has marginal probability \\(\\text{Pr}(Y = 1) = \\pi\\). The true likelihood-ratio and posterior odds ratio are respectively given by: \\[ \\begin{split} \\Lambda (X) & \\equiv \\frac{\\phi(X\\vert Y=1)}{\\phi(X\\vert Y=0)} = e ^ {a X^2 + bX +c},\\ \\rho (X) & \\equiv \\frac{\\text{Pr}(Y = 1\\vert X)}{\\text{Pr}(Y = 0\\vert X)} = e ^ {a X ^ 2 + bX +c+d}, \\end{split} \\] where we have defined: \\[ a \\equiv \\dfrac{\\sigma _1 ^2 -\\sigma_0 ^2}{2\\sigma _0 ^2\\sigma_1 ^2},\\quad b \\equiv \\mu _1 - \\mu _0, \\quad c \\equiv \\dfrac{\\mu_0^2}{2\\sigma_0^2} -\\dfrac{\\mu_1 ^2}{2\\sigma _1^2}+\\ln(\\frac{\\sigma _0 }{\\sigma _1 }),\\quad d \\equiv \\ln (\\frac {\\pi}{1-\\pi}) . \\] Suppose that we fit an exponential function \\(r(X)=e^{mX +q}\\) to \\(\\Lambda(X)\\) by likelihood-ratio cost minimization, and similarly \\(r'(X)=e^{m'X+q'}\\) to \\(\\rho(X)\\) by cross-entropy minimization11. Due to the previous discussion, one could reasonably expect the results of the two procedure to differ in some way, which is demonstrated below by simulation. The chunk of R code below defines the function and data used for the simulation. In particular, I’m considering a heavily unbalanced case (\\(\\text{Pr}(Y = 1) = 0.1\\%\\)) in which negative cases give rise to a sharply localized \\(X\\) peak around \\(X=0\\) (\\(\\mu _0 = 0\\), \\(\\sigma_0 = .25\\)), while the few positive cases give rise to a broader signal centered at \\(X=1\\) (\\(\\mu _1 = 1\\), \\(\\sigma _1 = 1\\)). # Tidyverse facilities for plotting library(dplyr) library(ggplot2) # Loss functions weighted_loss",
      "og_image": "https://vgherard.github.io/posts/file1d5854377d46_files/figure-html/unnamed-chunk-3-1.png",
      "og_title": "Interpreting the Likelihood Ratio cost | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 22.2,
      "sitemap_lastmod": "2023-11-14T10:00:00+00:00",
      "twitter_description": "Intro During the last few months, I’ve been working on a machine learning algorithm with applications in Forensic Science, a.k.a. Criminalistics. In this field, one common task for the data analyst is to present the trier-of-fact (the person or people who determine the facts in a legal proceeding) with a numerical assessment of the strength of the evidence provided by available data towards different hypotheses. In more familiar terms, the forensic expert is responsible of computing the likelihoods (or likelihood ratios) of data under competing hypotheses, which are then used by the trier-of-fact to produce Bayesian posterior probabilities for the hypotheses in question1. In relation to this, forensic scientists have developed a bunch of techniques to evaluate the performance of a likelihood ratio model in discriminating between two alternative hypothesis. In particular, I have come across the so called Likelihood Ratio Cost, usually defined as: \\[ C_{\\text{LLR}} = \\frac{1}{2N_1} \\sum _{Y_i=1} \\log(1+r(X_i) ^{-1})+\\frac{1}{2N_0} \\sum _{Y_i=0} \\log(1+r(X_i)), (\\#eq:CLLR) \\] where we assume we have data consisting of \\(N_1+N_0\\) independent identically distributed observations \\((X_i,\\,Y_i)\\), with binary \\(Y\\); \\(N_1\\) and \\(N_0\\) stand for the number of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) cases; and \\(r(X)\\) is a model for the likelihood ratio \\(\\Lambda(X) \\equiv \\frac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}\\). The main reason for writing this note was to understand a bit better what it means to optimize Eq. @ref(eq:CLLR), which does not look immediately obvious to me from its definition2. In particular: is the population minimizer of Eq. @ref(eq:CLLR) the actual likelihood ratio? And in what sense is a model with lower \\(C_\\text{LLR}\\) better than one with a correspondingly higher value? The short answers to these questions are: yes; and: \\(C_\\text{LLR}\\) optimization seeks for the model with the best predictive performance in a Bayesian inference setting with uninformative prior on \\(Y\\), assuming that this prior actually reflects reality (i.e. \\(\\text{Pr}(Y=1) = \\text{Pr}(Y=0) = \\frac{1}{2}\\)). The mathematical details are given in the rest of the post. Cross-entropy with random weights We start with a mathematical digression, which will turn out useful for further developments. Let \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\) be independent draws from a joint distribution, with binary \\(Y_i \\in \\{0,\\,1\\}\\). Given a function \\(w=w(\\boldsymbol Y)\\) that is symmetric in its arguments3, we define the random functional: \\[ \\mathcal L_N^w[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(f(X_i))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(f(X_i)^c)\\right],(\\#eq:WeightedLoss) \\] where \\(f=f(X)\\) is any function satisfying \\(f(X)\\in [0,\\,1]\\) for all \\(X\\), and we let \\(q^c = 1-q\\) for any number \\(q \\in [0,\\,1]\\). Notice that for \\(w(\\boldsymbol{Y}) \\equiv 1\\), this is just the usual cross-entropy loss. We now look for the population minimizer of @ref(eq:WeightedLoss), i.e. the function \\(f_*\\) that minimizes the functional \\(f \\mapsto \\mathbb E(\\mathcal L _N ^w [f])\\)4. Writing the expectation as: \\[ \\mathbb E(\\mathcal L _N ^w [f]) = -\\frac{1}{N}\\sum _{i=1} ^N \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)\\cdot \\log(f(X_i))+E(Y_i^c\\cdot w(\\boldsymbol Y ^c)\\vert X_i)\\cdot \\log(f^c(X_i))\\right], \\] we can easily see that \\(\\mathbb E(\\mathcal L _N ^w [f])\\) is a convex functional with a unique minimum given by: \\[ f_*(X_i) = \\frac{1}{1+r(X_i)^{-1}},\\quad r_*(X_i) = \\dfrac{E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)}{E(Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)}.(\\#eq:PopMinimizer) \\] The corresponding expected loss is: \\[ \\mathbb E(\\mathcal L _N ^w [f_*]) = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i))\\right], \\] where \\(\\mathcal H(p) = -p \\log (p) -(1-p) \\log(1-p)\\) is the entropy of a binary random variable \\(Z\\) with probability \\(p = \\text{Pr}(Z=1)\\) (the index \\(i\\) in the previous expression can be any index, since data points are assumed to be identically distributed). Before looking at values of \\(f\\) other than \\(f_*\\), we observe that the previous expectation can be succintly expressed as: \\[ \\mathbb E(\\mathcal L _N ^w [f_*]) = k \\cdot H^\\prime(Y\\vert X), \\] where \\[ k = \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c))(\\#eq:DefKappa) \\] and \\(H'(Y\\vert X)\\) is the conditional entropy of \\(Y\\vert X\\) with respect to a different probability measure \\(\\text{Pr}^\\prime\\), defined by: \\[ \\text{Pr}^\\prime(E) = t \\cdot \\text {Pr}(E \\vert Y = 1) + (1-t)\\cdot \\text {Pr}(E \\vert Y = 0), (\\#eq:DefPrPrime) \\] where \\(t=\\text{Pr}^\\prime(Y=1)\\in [0,\\,1]\\) is fixed by the requirement5: \\[ \\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{\\text {Pr} (Y=1)}{\\text{Pr} (Y=0)}\\cdot\\dfrac{\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i >0)}{\\mathbb E(w(\\boldsymbol Y^c)\\vert \\sum _i Y_i^c >0)}. (\\#eq:DefPrPrime2) \\] In terms of \\(\\text{Pr}^\\prime\\), the population minimizers \\(f_*\\) and \\(r_*\\) in Eq. @ref(eq:PopMinimizer) can be simply expressed as: \\[ r_*(X)=\\dfrac{\\text {Pr}^\\prime(Y=1\\vert X)}{\\text {Pr}^\\prime(Y=0\\vert X)},\\qquad f_*(X)=\\text {Pr}^\\prime(Y=1\\vert X). (\\#eq:PopMinimizer2) \\] If now \\(f\\) is an arbitrary function, we have: \\[ \\begin{split} \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right] &= k\\cdot D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f) \\end{split} \\] where \\(\\mathcal D(p\\vert \\vert q) = p \\log (\\frac{p}{q}) + (1-p) \\log (\\frac{1-p}{1-q})\\), and \\(D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\\) is the Kullback-Liebler divergence between the measure \\(\\text{Pr}^\\prime\\) and the measure \\(\\text{Pr}^\\prime _f\\) defined by: \\[ \\text{Pr}^\\prime _f(Y = 1\\vert X)=f(X),\\qquad \\text{Pr}^\\prime _f(X)=\\text{Pr}^\\prime(X) \\] (notice that \\(\\text {Pr} ^{\\prime} _{f_*} \\equiv \\text{Pr} ^{\\prime}\\) by definition). Finally, suppose that \\(X = g(\\widetilde X)\\) for some random variable \\(\\widetilde X\\), and define the corresponding functional: \\[ \\widetilde{\\mathcal L} _N^w[\\widetilde f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(\\widetilde f(\\widetilde X))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(\\widetilde f(\\widetilde X)^c)\\right]. \\] Then \\(\\mathcal L _N ^w [f] = \\widetilde{\\mathcal L} _N^w[f \\circ g]\\). If \\(\\widetilde f _* =\\) is the population minimizer of \\(\\widetilde{\\mathcal L} _N^w\\), it follows that \\(\\mathbb E (\\widetilde{\\mathcal L} _N^w[\\widetilde f _*]) \\leq \\mathbb E(\\mathcal L _N ^w [f_*])\\). Putting everything together, we can decompose the expected loss for a function \\(f=f(X)\\), where \\(X= g(\\widetilde X)\\), in the following suggestive way: \\[ \\begin{split} \\mathbb E(\\mathcal L _N ^w [f]) &= (L_N ^w)_\\text{min}+(L_N ^w)_\\text{proc} +(L_N ^w)_\\text{missp},\\ (L_N ^w)_\\text{min}&\\equiv\\mathbb E(\\widetilde{\\mathcal L} _N^w[{\\widetilde f} _*]) \\ &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert \\widetilde X _i)\\cdot \\mathcal H({\\widetilde f} _*(\\widetilde X _i))\\right]\\ &=k\\cdot H^\\prime(Y\\vert \\widetilde X),\\ (L_N ^w)_\\text{proc}&\\equiv\\mathbb E(\\mathcal L _N ^w [f_*]-\\widetilde{\\mathcal L} _N^w[\\phi_*]) \\& = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i)) \\right]- (L_N ^w)_\\text{min}\\ & = k\\cdot I^\\prime(Y; \\widetilde X\\vert X),\\ (L_N ^w)_\\text{missp} & \\equiv \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) \\&= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right]\\ &=k\\cdot D(\\text {Pr}^\\prime\\vert \\vert \\text {Pr}^\\prime _f), \\end{split} (\\#eq:DecompositionWeightedLoss) \\] where \\(k\\) is defined in Eq. @ref(eq:DefKappa). In the equation for \\((L^w _N)_\\text{proc}\\) we introduced the conditional mutual information (with respect to the measure \\(\\text{Pr}^\\prime\\)), that satisfies (Cover and Thomas 2006): \\[ I(\\widetilde X;Y\\vert X) = I(\\widetilde X,Y)-I(X,Y) = H(Y\\vert X)-H(Y\\vert \\widetilde X). \\] The three components in Eq. @ref(eq:DecompositionWeightedLoss) can be interpreted as follows: \\((L_N ^w)_\\text{min}\\) represents the minimum expected loss achievable, given the data available \\(\\widetilde X\\); \\((L_N ^w)_\\text{proc}\\) accounts for the information lost in the processing transformation \\(X=g(\\widetilde X)\\); finally \\((L_N ^w)_\\text{missp}\\) is due to misspecification, i.e. the fact that the model \\(f(X)\\) for the true posterior probability \\(f_*(X)\\) is an approximation. All the information-theoretic quantities (and their corresponding operative interpretations hinted in the previous paragraph) make reference to the measure \\(\\text{Pr}^\\prime\\) defined by Eqs. @ref(eq:DefPrPrime) and @ref(eq:DefPrPrime2). This is merely the result of altering the proportion of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) examples in the \\(X\\)-\\(Y\\) joint distribution by a factor dictated by the weight function \\(w\\) - while keeping conditional distributions such as \\(X\\vert Y\\) unchanged. A familiar case: cross-entropy loss For \\(w(\\boldsymbol {Y}) = 1\\), the functional \\(\\mathcal {L} _{N} ^{w}[f]\\) coincides with the usual cross-entropy loss6: \\[ \\text{CE}[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[Y_i \\log(f(X_i))+ (1-Y_i) \\log(1-f(X_i))\\right].(\\#eq:CrossEntropyLoss) \\] From Eq. @ref(eq:DefPrPrime2) we see that the measure \\(\\text{Pr}^{\\prime}\\) coincides with the original \\(\\text{Pr}\\), so that by Eq. @ref(eq:PopMinimizer) the population minimizer of @ref(eq:CrossEntropyLoss) is \\(f_{*}(X) = \\text{Pr}(Y=1\\vert X)\\) (independently of sample size). Since \\(k = 1\\) (cf. Eq. @ref(eq:DefKappa)), the decomposition @ref(eq:DecompositionWeightedLoss) reads: \\[ \\begin{split} \\mathbb E(\\text{CE} [f]) &= (\\text{CE})_\\text{min}+(\\text{CE})_\\text{proc} +(\\text{CE})_\\text{missp},\\ (\\text{CE})_\\text{min}&=H(Y\\vert \\widetilde X),\\ (\\text{CE})_\\text{proc}&= I(Y; \\widetilde X\\vert X),\\ (\\text{CE})_\\text{missp} &=D(\\text {Pr}\\vert \\vert \\text {Pr} _{f}), \\end{split} (\\#eq:DecompositionCE) \\] where conditional entropy \\(H\\), mutual information \\(I\\) and relative entropy \\(D\\) now simply refer to the original measure \\(\\text{Pr}\\). The Likelihood Ratio Cost The quantity \\(C_{\\text{LLR}}\\) defined in Eq. @ref(eq:CLLR) can be put in the general form @ref(eq:WeightedLoss), if we let \\(f(X) = (1+r(X)^{-1})^{-1}\\) and7: \\[ w(\\boldsymbol Y) = \\left(\\dfrac{2}{N}\\sum _{i = 1}^{N}Y_j \\right)^{-1} \\] In what follows, I will consider a slight modification of the usual \\(C_\\text{LLR}\\), defined by the weight function: \\[ w(\\boldsymbol Y) = \\dfrac{1}{2(N-1)}\\sum _{i = 1}^{N}(1-Y_j). \\] This yields Eq. @ref(eq:CLLR) multiplied by \\(\\dfrac{N_1N_0}{N(N-1)}\\), which I will keep denoting as \\(C_\\text{LLR}\\), with a slight abuse of notation. We can easily compute8: \\[ \\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=1, (\\#eq:PriorCLLR) \\] so that, by Eq. @ref(eq:PopMinimizer), the population minimizer of \\(C_\\text{LLR}\\) is: \\[ r_*(X) = \\Lambda (X),\\quad f_*(X)=\\dfrac{1}{1+\\Lambda(X)^{-1}}, \\] where \\(\\Lambda(X)\\) denotes the likelihood-ratio of \\(X\\), schematically: \\[ \\Lambda(X)\\equiv \\dfrac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}. \\] The constant \\(k\\) in Eq. @ref(eq:DefKappa) is: \\[ k = \\text{Pr}(Y = 1)\\text{Pr}(Y = 0)=\\text{Var}(Y) \\] The general decomposition @ref(eq:DecompositionWeightedLoss) becomes: \\[ \\begin{split} \\mathbb E(C_\\text{LLR} [f]) &= (C_\\text{LLR})_\\text{min}+(C_\\text{LLR})_\\text{proc} +(C_\\text{LLR})_\\text{missp},\\ (C_\\text{LLR})_\\text{min}&=\\text{Var}(Y)\\cdot H^{\\prime}(Y\\vert \\widetilde X),\\ (C_\\text{LLR})_\\text{proc}&= \\text{Var}(Y)\\cdot I^{\\prime}(Y; \\widetilde X\\vert X),\\ (C_\\text{LLR})_\\text{missp} &=\\text{Var}(Y)\\cdot D^{\\prime}(\\text {Pr}\\vert \\vert \\text {Pr} _{f}), \\end{split} (\\#eq:DecompositionCE) \\] where \\(\\text{Pr}^\\prime\\) is now given by @ref(eq:PriorCLLR). Discussion The table below provides a comparison between cross-entropy and likelihood-ratio cost, summarizing the results from previous sections. Cross-entropy Likelihood Ratio Cost \\(f_*(X)\\) \\(\\text{Pr}(Y = 1\\vert X)\\) \\((1+\\Lambda(X)^{-1})^{-1}\\) \\(r_*(X)\\)` Posterior odds ratio Likelihood ratio Minimum Loss \\(H(Y\\vert \\widetilde X)\\) \\(\\text{Var}(Y) \\cdot H^\\prime(Y\\vert \\widetilde X)\\) Processing Loss \\(I(Y; \\widetilde X\\vert X)\\) \\(\\text{Var}(Y) \\cdot I^\\prime(Y; \\widetilde X\\vert X)\\) Misspecification Loss \\(D(f_*\\vert\\vert f)\\) \\(\\text{Var}(Y) \\cdot D^\\prime(f_*\\vert\\vert f)\\) Reference measure \\(\\text{Pr}\\) \\(\\text{Pr}^{\\prime} = \\frac{\\text{Pr}(\\cdot \\vert Y = 1)+\\text{Pr}(\\cdot \\vert Y = 0)}{2}\\) The objective of \\(C_\\text{LLR}\\) is found to be the likelihood ratio, as terminology suggests. The interpretation of model selection according to \\(C_\\text{LLR}\\) minimization turns out to be slightly more involved, compared to cross-entropy, which we first review. Suppose we are given a set of predictive models \\(\\{\\mathcal M_i\\}_{i\\in I}\\), each of which consists of a processing transformation, \\(\\widetilde X \\mapsto X\\), and an estimate of the posterior probability \\(\\text{Pr}(Y = 1\\vert X)\\). When the sample size \\(N \\to \\infty\\), cross-entropy minimization will almost certainly select the model that minimizes \\(I(Y; \\widetilde X\\vert X) + D(f_*\\vert \\vert f)\\). Following standard Information Theory arguments, we can interpret this model as the statistically optimal compression algorithm for \\(Y\\), assuming \\(X\\) to be available at both the encoding and decoding ends. The previous argument carries over mutatis mutandi to \\(C_\\text{LLR}\\) minimization, with an important qualification: optimal average compression is now achieved for data distributed according to a different probability measure \\(\\text{Pr}'(\\cdot) = \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 1) + \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 0)\\). In particular, according to \\(\\text{Pr}'\\), the likelihood ratio coincides with the posterior odds ratio, and \\((1+\\Lambda(X)^{-1})^{-1}\\) coincides with posterior probability, which clarifies why we can measure differences from the true likelihood-ratio through the Kullback-Liebler divergence. The measure \\(\\text{Pr}'\\) is not just an abstruse mathematical construct: it is the result of balanced sampling from the original distribution, i.e. taking an equal number of positive and negative cases9. If the \\((X,\\,Y)\\) distribution is already balanced, either by design or because of some underlying symmetry in the data generating process, our analysis implies that likelihood-ratio cost and cross-entropy minimization are essentially equivalent for \\(N\\to \\infty\\). In general, with \\(\\text{Pr} (Y=1) \\neq \\text{Pr} (Y=0)\\), this is not the case10. The fact that \\(C_\\text{LLR}\\) seeks for optimal predictors according to the balanced measure \\(\\text{Pr}'\\) is, one could argue, not completely crazy from the point of view of forensic science, where “\\(Y\\in\\{0,1\\}\\)” often stands for a sort verdict (guilty vs. not guilty, say). Indeed, optimizing with respect to \\(\\text{Pr}^\\prime\\) means that our predictions are designed to be optimal in a world in which the verdict could be a priori \\(Y=0\\) or \\(Y=1\\) with equal probability - which is what an unbiased trier-of-fact should ideally assume. Minimizing \\(C_\\text{LLR}\\), we guard ourselves against any bias that may be implicit in the training dataset, extraneous to the \\(X\\)-\\(Y\\) relation and not explicitly modeled, a feature that may be regarded as desirable from a legal standpoint. Simulated example In general, the posterior odd ratio and likelihood ratio differ only by a constant, so it is reasonable to try to fit the same functional form to both of them. Let us illustrate with a simulated example of this type the differences between cross-entropy and \\(C_{\\text{LLR}}\\) optimization mentioned in the previous Section. Suppose that \\(X \\in \\mathbb R\\) has conditional density: \\[ \\phi(X\\vert Y) = (2\\pi\\sigma _Y^2)^{-\\frac{1}{2}} \\exp(-\\frac{(X-\\mu_Y)^2}{2\\sigma _Y^2}) \\] and \\(Y\\) has marginal probability \\(\\text{Pr}(Y = 1) = \\pi\\). The true likelihood-ratio and posterior odds ratio are respectively given by: \\[ \\begin{split} \\Lambda (X) & \\equiv \\frac{\\phi(X\\vert Y=1)}{\\phi(X\\vert Y=0)} = e ^ {a X^2 + bX +c},\\ \\rho (X) & \\equiv \\frac{\\text{Pr}(Y = 1\\vert X)}{\\text{Pr}(Y = 0\\vert X)} = e ^ {a X ^ 2 + bX +c+d}, \\end{split} \\] where we have defined: \\[ a \\equiv \\dfrac{\\sigma _1 ^2 -\\sigma_0 ^2}{2\\sigma _0 ^2\\sigma_1 ^2},\\quad b \\equiv \\mu _1 - \\mu _0, \\quad c \\equiv \\dfrac{\\mu_0^2}{2\\sigma_0^2} -\\dfrac{\\mu_1 ^2}{2\\sigma _1^2}+\\ln(\\frac{\\sigma _0 }{\\sigma _1 }),\\quad d \\equiv \\ln (\\frac {\\pi}{1-\\pi}) . \\] Suppose that we fit an exponential function \\(r(X)=e^{mX +q}\\) to \\(\\Lambda(X)\\) by likelihood-ratio cost minimization, and similarly \\(r'(X)=e^{m'X+q'}\\) to \\(\\rho(X)\\) by cross-entropy minimization11. Due to the previous discussion, one could reasonably expect the results of the two procedure to differ in some way, which is demonstrated below by simulation. The chunk of R code below defines the function and data used for the simulation. In particular, I’m considering a heavily unbalanced case (\\(\\text{Pr}(Y = 1) = 0.1\\%\\)) in which negative cases give rise to a sharply localized \\(X\\) peak around \\(X=0\\) (\\(\\mu _0 = 0\\), \\(\\sigma_0 = .25\\)), while the few positive cases give rise to a broader signal centered at \\(X=1\\) (\\(\\mu _1 = 1\\), \\(\\sigma _1 = 1\\)). # Tidyverse facilities for plotting library(dplyr) library(ggplot2) # Loss functions weighted_loss",
      "twitter_title": "Interpreting the Likelihood Ratio cost | R-bloggers",
      "url": "https://www.r-bloggers.com/2023/11/interpreting-the-likelihood-ratio-cost/",
      "word_count": 4435
    }
  }
}