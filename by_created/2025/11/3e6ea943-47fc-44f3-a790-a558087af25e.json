{
  "uuid": "3e6ea943-47fc-44f3-a790-a558087af25e",
  "created_at": "2025-11-17 20:38:39",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2024/01/one-billion-row-challenge-using-base-r/",
    "crawled_at": "2025-11-17T09:22:20.635681",
    "external_links": [
      {
        "href": "http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/",
        "text": "schochastics - all things R"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://www.morling.dev/blog/one-billion-row-challenge/",
        "text": "One Billion Row challenge"
      },
      {
        "href": "https://news.ycombinator.com/item?id=38851337",
        "text": "Hacker News"
      },
      {
        "href": "https://github.com/gunnarmorling/1brc/discussions/categories/show-and-tell",
        "text": "show & tell"
      },
      {
        "href": "https://github.com/alejandrohagan/1br",
        "text": "GitHub repository"
      },
      {
        "href": "https://github.com/alejandrohagan/1br/issues/5",
        "text": "critical discussions"
      },
      {
        "href": "https://github.com/tidyverse/dplyr",
        "text": "dplyr"
      },
      {
        "href": "https://github.com/Rdatatable/data.table",
        "text": "data.table"
      },
      {
        "href": "https://github.com/SebKrantz/collapse",
        "text": "collapse"
      },
      {
        "href": "https://github.com/pola-rs/r-polars",
        "text": "polars"
      },
      {
        "href": "https://github.com/alejandrohagan/1br/issues/5#issuecomment-1879737918",
        "text": "fastest"
      },
      {
        "href": "https://github.com/alejandrohagan/1br/blob/main/generate_data.R",
        "text": "script"
      },
      {
        "href": "https://github.com/RfastOfficial/Rfast/",
        "text": "Rfast"
      },
      {
        "href": "https://github.com/eddelbuettel/dang/blob/master/R/reorderMicrobenchmarkResults.R",
        "text": "Dirk Eddelbuettel"
      },
      {
        "href": "https://creativecommons.org/licenses/by/4.0/",
        "text": "CC BY 4.0"
      },
      {
        "href": "http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge",
        "text": "http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge"
      },
      {
        "href": "http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/",
        "text": "schochastics - all things R"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "One billion row challenge using base R | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e6-1.png?w=450"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e8-1.png?w=450"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/david-schoch/",
        "text": "David Schoch"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-381366 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">One billion row challenge using base R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 7, 2024</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/david-schoch/\">David Schoch</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/\"> schochastics - all things R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<p><em>One of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.</em><sup>1</sup></p>\n<p>The <a href=\"https://www.morling.dev/blog/one-billion-row-challenge/\" rel=\"nofollow\" target=\"_blank\">One Billion Row challenge</a> by Gunnar Morling is as follows:</p>\n<blockquote class=\"blockquote\">\n<p>write a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. There’s just one caveat: the file has 1,000,000,000 rows!</p>\n</blockquote>\n<p>I didn’t take long, also thanks to <a href=\"https://news.ycombinator.com/item?id=38851337\" rel=\"nofollow\" target=\"_blank\">Hacker News</a>, that the challenge spread to other programming languages. The original repository contains a <a href=\"https://github.com/gunnarmorling/1brc/discussions/categories/show-and-tell\" rel=\"nofollow\" target=\"_blank\">show &amp; tell</a> where other results can be discussed.</p>\n<p>Obviously it also spread to R and there is a <a href=\"https://github.com/alejandrohagan/1br\" rel=\"nofollow\" target=\"_blank\">GitHub repository</a> from Alejandro Hagan dedicated to the challenge. There were some <a href=\"https://github.com/alejandrohagan/1br/issues/5\" rel=\"nofollow\" target=\"_blank\">critical discussions</a> on the seemingly bad performance of <code>data.table</code> but that issue thread also evolved to a discussion on other solutions.</p>\n<p>The obvious candidates for fast solutions with R are <a href=\"https://github.com/tidyverse/dplyr\" rel=\"nofollow\" target=\"_blank\"><code>dplyr</code></a>, <a href=\"https://github.com/Rdatatable/data.table\" rel=\"nofollow\" target=\"_blank\"><code>data.table</code></a>, <a href=\"https://github.com/SebKrantz/collapse\" rel=\"nofollow\" target=\"_blank\"><code>collapse</code></a>, and <a href=\"https://github.com/pola-rs/r-polars\" rel=\"nofollow\" target=\"_blank\"><code>polars</code></a>. From those, it appears that polars might solve the tasks the <a href=\"https://github.com/alejandrohagan/1br/issues/5#issuecomment-1879737918\" rel=\"nofollow\" target=\"_blank\">fastest</a>.</p>\n<p>I was curious, how far one can get with base R.</p>\n<section class=\"level2\" id=\"creating-the-data\">\n<h2 class=\"anchored\" data-anchor-id=\"creating-the-data\">Creating the data</h2>\n<p>The R repository contains a <a href=\"https://github.com/alejandrohagan/1br/blob/main/generate_data.R\" rel=\"nofollow\" target=\"_blank\">script</a> to generate benchmark data. For the purpose of this post, I created files with 1e6 and 1e8 rows. Unfortunately, my personal laptop cannot handle 1 billion rows without dying.</p>\n</section>\n<section class=\"level2\" id=\"reading-the-data\">\n<h2 class=\"anchored\" data-anchor-id=\"reading-the-data\">Reading the data</h2>\n<p>All base R functions will profit from reading the state column as a factor instead of a usual string.</p>\n<div class=\"cell\">\n<pre>D &lt;- data.table::fread(\"measurements1e6.csv\", stringsAsFactors = TRUE)\nD</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>         measurement state\n      1:    0.981969    NC\n      2:    0.468715    MA\n      3:   -0.107971    TX\n      4:   -0.212878    VT\n      5:    1.158098    OR\n     ---                  \n 999996:    0.743249    FL\n 999997:   -1.685561    KS\n 999998:   -0.118455    TX\n 999999:    1.277437    MS\n1000000:   -0.280085    MD</pre>\n</div>\n</div>\n<p>Who would have thought that <code>stringAsFactors = TRUE</code> can be useful.</p>\n</section>\n<section class=\"level2\" id=\"the-obvious-aggregate-and-splitlapply\">\n<h2 class=\"anchored\" data-anchor-id=\"the-obvious-aggregate-and-splitlapply\">The obvious: aggregate and split/lapply</h2>\n<p>The most obvious choice for me was to use <code>aggregate()</code>.</p>\n<div class=\"cell\">\n<pre>sum_stats_vec &lt;- function(x) c(min = min(x), max = max(x), mean = mean(x))\naggregate(measurement ~ state, data = D, FUN = sum_stats_vec) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>  state measurement.min measurement.max measurement.mean\n1    AK    -4.104494064     4.271009490      0.003082000\n2    AL    -3.635024969     4.538671919     -0.007811050\n3    AR    -3.813800485     4.101114941      0.008453876\n4    AZ    -4.415050930     3.965124829      0.000228734\n5    CA    -4.159725627     4.102446367      0.013649303\n6    CO    -3.860489118     4.231415117     -0.001334096</pre>\n</div>\n</div>\n<p>I was pretty sure that this might be the best solution.</p>\n<p>The other obvious solution is to split the data frame according to stats and then <code>lapply</code> the stats calculation on each list element.</p>\n<div class=\"cell\">\n<pre>split_lapply &lt;- function(D) {\n    result &lt;- lapply(split(D, D$state), function(x) {\n        stats &lt;- sum_stats_vec(x$measurement)\n        data.frame(\n            state = unique(x$state),\n            min = stats[1],\n            max = stats[2],\n            mean = stats[3]\n        )\n    })\n    do.call(\"rbind\", result)\n}\nsplit_lapply(D) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   state      min     max         mean\nAK    AK -4.10449 4.27101  0.003082000\nAL    AL -3.63502 4.53867 -0.007811050\nAR    AR -3.81380 4.10111  0.008453876\nAZ    AZ -4.41505 3.96512  0.000228734\nCA    CA -4.15973 4.10245  0.013649303\nCO    CO -3.86049 4.23142 -0.001334096</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"the-elegant-by\">\n<h2 class=\"anchored\" data-anchor-id=\"the-elegant-by\">The elegant: by</h2>\n<p>I stumbled upon <code>by</code> when searching for alternatives. I think it is a quite elegant way of solving a group/summarize task with base R. Unfortunately it returns a list and not a data frame or matrix (I made that an implicit requirement).</p>\n<p>In the help for <code>by</code> I stumbled upon a function I wasn’t aware of yet: <code>array2DF</code>!</p>\n<div class=\"cell\">\n<pre>array2DF(by(D$measurement, D$state, sum_stats_vec)) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>  D$state      min     max         mean\n1      AK -4.10449 4.27101  0.003082000\n2      AL -3.63502 4.53867 -0.007811050\n3      AR -3.81380 4.10111  0.008453876\n4      AZ -4.41505 3.96512  0.000228734\n5      CA -4.15973 4.10245  0.013649303\n6      CO -3.86049 4.23142 -0.001334096</pre>\n</div>\n</div>\n<p>Does exactly what is needed here. For the benchmarks, I will also include a version without the <code>array2DF</code> call, to check its overhead.</p>\n</section>\n<section class=\"level2\" id=\"another-apply-tapply\">\n<h2 class=\"anchored\" data-anchor-id=\"another-apply-tapply\">Another apply: tapply</h2>\n<p>In the help for <code>by</code>, I also stumbled upon this sentence</p>\n<blockquote class=\"blockquote\">\n<p>Function <code>by</code> is an object-oriented wrapper for <code>tapply</code> applied to data frames.</p>\n</blockquote>\n<p>So maybe we can construct a solution that uses tapply, but without any inbuilt overhead in <code>by</code>.</p>\n<div class=\"cell\">\n<pre>do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>        min     max         mean\nAK -4.10449 4.27101  0.003082000\nAL -3.63502 4.53867 -0.007811050\nAR -3.81380 4.10111  0.008453876\nAZ -4.41505 3.96512  0.000228734\nCA -4.15973 4.10245  0.013649303\nCO -3.86049 4.23142 -0.001334096</pre>\n</div>\n</div>\n<p>At this point, I was also curious if the <code>do.call(\"rbind\",list)</code> can be sped up, so I constructed a second tapply solution.</p>\n<div class=\"cell\">\n<pre>sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>            AK          AL          AR           AZ         CA         CO\n[1,] -4.104494 -3.63502497 -3.81380048 -4.415050930 -4.1597256 -3.8604891\n[2,]  4.271009  4.53867192  4.10111494  3.965124829  4.1024464  4.2314151\n[3,]  0.003082 -0.00781105  0.00845388  0.000228734  0.0136493 -0.0013341\n              CT          DE          FL          GA          HI         IA\n[1,] -3.91246424 -4.13994780 -3.74126137 -3.94660998 -4.01231812 -3.7908228\n[2,]  4.42439303  4.10320343  3.61461913  3.92890675  3.54210746  4.1666241\n[3,]  0.00365691  0.00300726  0.00364475  0.00998416  0.00831473  0.0121738\n              ID          IL         IN         KS          KY          LA\n[1,] -3.96453912 -3.89433528 -4.0396596 -4.0290851 -4.04857519 -3.83912209\n[2,]  3.57787653  3.79539851  3.8997433  3.6210916  3.86743373  3.56924325\n[3,]  0.00364403  0.00282924 -0.0116455  0.0147175 -0.00141356  0.00150667\n              MA         MD         ME          MI          MN           MO\n[1,] -4.03713867 -3.7447056 -3.9230949 -3.95286252 -4.41058377 -3.939103495\n[2,]  3.79263060  3.7162898  3.6639929  4.15578354  4.31107098  4.428823170\n[3,] -0.00695907 -0.0104974  0.0017803  0.00578387  0.00222535 -0.000530262\n              MS         MT          NC         ND         NE          NH\n[1,] -4.11033773 -4.0871006 -4.37736785 -4.0689381 -4.0600632 -3.91574348\n[2,]  4.04043436  4.1620591  3.98113456  3.7439708  4.1786719  4.12714205\n[3,]  0.00401617 -0.0051991 -0.00791303 -0.0065667 -0.0019952  0.00173286\n              NJ         NM         NV          NY          OH          OK\n[1,] -3.89749099 -3.8077381 -4.4999793 -4.10688778 -3.97073238 -4.01749904\n[2,]  4.09297430  3.8533329  3.9841584  3.77539030  4.05541378  3.92196743\n[3,] -0.00821633 -0.0045123  0.0059095 -0.00401249  0.00391791  0.00272036\n               OR          PA          RI         SC          SD          TN\n[1,] -3.755405173 -3.87864920 -3.65614672 -3.6360017 -4.25212184 -3.63011318\n[2,]  4.299120255  4.18986336  4.25751403  4.1131445  3.74296173  3.92052537\n[3,]  0.000427857 -0.00303136 -0.00419174 -0.0110226  0.00774345  0.00671216\n              TX          UT         VA           VT          WA          WI\n[1,] -4.20588875 -4.03481832 -4.2980081 -3.749369728 -3.76023936 -3.76302646\n[2,]  3.92935839  3.99530205  3.8614655  3.990961895  3.87463693  4.32264119\n[3,] -0.00784648 -0.00561814 -0.0124286 -0.000579563  0.00684453  0.00297232\n             WV          WY\n[1,] -3.6034317 -3.86099776\n[2,]  3.9029500  4.11653276\n[3,] -0.0090395 -0.00238027</pre>\n</div>\n</div>\n<p>and we should obviously also include our new found <code>array2DF</code></p>\n<div class=\"cell\">\n<pre>array2DF(tapply(D$measurement, D$state, sum_stats_vec)) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>  Var1      min     max         mean\n1   AK -4.10449 4.27101  0.003082000\n2   AL -3.63502 4.53867 -0.007811050\n3   AR -3.81380 4.10111  0.008453876\n4   AZ -4.41505 3.96512  0.000228734\n5   CA -4.15973 4.10245  0.013649303\n6   CO -3.86049 4.23142 -0.001334096</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"the-obscure-reduce\">\n<h2 class=\"anchored\" data-anchor-id=\"the-obscure-reduce\">The obscure: reduce</h2>\n<p>I thought that this should be it, but then I remembered <code>reduce</code> exists. The solution is somewhat similar to split/lapply.</p>\n<div class=\"cell\">\n<pre>reduce &lt;- function(D) {\n    state_list &lt;- split(D$measurement, D$state)\n    Reduce(function(x, y) {\n        res &lt;- sum_stats_vec(state_list[[y]])\n        rbind(x, data.frame(state = y, mean = res[1], min = res[2], max = res[3]))\n    }, names(state_list), init = NULL)\n}\n\nreduce(D) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>     state     mean     min          max\nmin     AK -4.10449 4.27101  0.003082000\nmin1    AL -3.63502 4.53867 -0.007811050\nmin2    AR -3.81380 4.10111  0.008453876\nmin3    AZ -4.41505 3.96512  0.000228734\nmin4    CA -4.15973 4.10245  0.013649303\nmin5    CO -3.86049 4.23142 -0.001334096</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"the-unfair-contender-rfast\">\n<h2 class=\"anchored\" data-anchor-id=\"the-unfair-contender-rfast\">The unfair contender: Rfast</h2>\n<p>Pondering about how this functions could be sped up in general, I remembered the package <a href=\"https://github.com/RfastOfficial/Rfast/\" rel=\"nofollow\" target=\"_blank\"><code>Rfast</code></a> and managed to construct a solution using this package.</p>\n<div class=\"cell\">\n<pre>Rfast &lt;- function(D) {\n    lev_int &lt;- as.numeric(D$state)\n    minmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\n    data.frame(\n        state = levels(D$state),\n        mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n        min = minmax[1, ],\n        max = minmax[2, ]\n    )\n}\n\nRfast(D) |&gt; head()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>  state         mean      min     max\n1    AK  0.003082000 -4.10449 4.27101\n2    AL -0.007811050 -3.63502 4.53867\n3    AR  0.008453876 -3.81380 4.10111\n4    AZ  0.000228734 -4.41505 3.96512\n5    CA  0.013649303 -4.15973 4.10245\n6    CO -0.001334096 -3.86049 4.23142</pre>\n</div>\n</div>\n<p>Pretty sure that this will be the fastest, maybe even competitive with the other big packages!</p>\n</section>\n<section class=\"level2\" id=\"benchmark\">\n<h2 class=\"anchored\" data-anchor-id=\"benchmark\">Benchmark</h2>\n<p>For better readability I reorder the benchmark results from <code>microbenchmark</code> according to median runtime, with a function provided by <a href=\"https://github.com/eddelbuettel/dang/blob/master/R/reorderMicrobenchmarkResults.R\" rel=\"nofollow\" target=\"_blank\">Dirk Eddelbuettel</a>.</p>\n<div class=\"cell\">\n<pre>reorderMicrobenchmarkResults &lt;- function(res, order = \"median\") {\n    stopifnot(\"Argument 'res' must be a 'microbenchmark' result\" = inherits(res, \"microbenchmark\"))\n\n    smry &lt;- summary(res)\n    res$expr &lt;- factor(res$expr,\n        levels = levels(res$expr)[order(smry[[\"median\"]])],\n        ordered = TRUE\n    )\n    res\n}</pre>\n</div>\n<p>First up the “small” dataset with 1e6 rows. I added the <code>dplyr</code> and <code>data.table</code> results as references.</p>\n<div class=\"cell\">\n<pre>sum_stats_list &lt;- function(x) list(min = min(x), max = max(x), mean = mean(x))\nsum_stats_tibble &lt;- function(x) tibble::tibble(min = min(x), max = max(x), mean = mean(x))\n\nbench1e6 &lt;- microbenchmark::microbenchmark(\n    aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |&gt; dplyr::group_by(state) |&gt; dplyr::summarise(sum_stats_tibble(measurement)) |&gt; dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 25\n)</pre>\n</div>\n<div class=\"cell\">\n<pre>ggplot2::autoplot(reorderMicrobenchmarkResults(bench1e6))</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e6-1.png?w=450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e6-1.png?w=450\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<div class=\"cell\">\n<div class=\"cell-output-display\">\n<table class=\"table table-sm table-striped small\">\n<colgroup>\n<col style=\"width: 21%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 7%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">expr</th>\n<th style=\"text-align: right;\">min</th>\n<th style=\"text-align: right;\">lq</th>\n<th style=\"text-align: right;\">mean</th>\n<th style=\"text-align: right;\">median</th>\n<th style=\"text-align: right;\">uq</th>\n<th style=\"text-align: right;\">max</th>\n<th style=\"text-align: right;\">neval</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">datatable</td>\n<td style=\"text-align: right;\">13.4325</td>\n<td style=\"text-align: right;\">15.2802</td>\n<td style=\"text-align: right;\">16.8016</td>\n<td style=\"text-align: right;\">15.5527</td>\n<td style=\"text-align: right;\">17.6287</td>\n<td style=\"text-align: right;\">22.9133</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Rfast</td>\n<td style=\"text-align: right;\">13.5639</td>\n<td style=\"text-align: right;\">16.5295</td>\n<td style=\"text-align: right;\">19.1905</td>\n<td style=\"text-align: right;\">18.0986</td>\n<td style=\"text-align: right;\">21.3507</td>\n<td style=\"text-align: right;\">33.5924</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">array2DF_tapply</td>\n<td style=\"text-align: right;\">27.5491</td>\n<td style=\"text-align: right;\">30.9502</td>\n<td style=\"text-align: right;\">37.5948</td>\n<td style=\"text-align: right;\">32.3145</td>\n<td style=\"text-align: right;\">36.7538</td>\n<td style=\"text-align: right;\">132.9784</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">docall_tapply</td>\n<td style=\"text-align: right;\">28.4628</td>\n<td style=\"text-align: right;\">30.7052</td>\n<td style=\"text-align: right;\">39.9436</td>\n<td style=\"text-align: right;\">32.3174</td>\n<td style=\"text-align: right;\">33.7132</td>\n<td style=\"text-align: right;\">99.6504</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">sapply_tapply</td>\n<td style=\"text-align: right;\">28.8092</td>\n<td style=\"text-align: right;\">30.1596</td>\n<td style=\"text-align: right;\">46.0100</td>\n<td style=\"text-align: right;\">33.3971</td>\n<td style=\"text-align: right;\">37.7851</td>\n<td style=\"text-align: right;\">101.5850</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">raw_by</td>\n<td style=\"text-align: right;\">40.9116</td>\n<td style=\"text-align: right;\">44.1075</td>\n<td style=\"text-align: right;\">52.2293</td>\n<td style=\"text-align: right;\">45.9647</td>\n<td style=\"text-align: right;\">48.4539</td>\n<td style=\"text-align: right;\">119.5348</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">array2DF_by</td>\n<td style=\"text-align: right;\">43.2958</td>\n<td style=\"text-align: right;\">45.8177</td>\n<td style=\"text-align: right;\">58.4240</td>\n<td style=\"text-align: right;\">48.8741</td>\n<td style=\"text-align: right;\">52.2750</td>\n<td style=\"text-align: right;\">132.8981</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">reduce</td>\n<td style=\"text-align: right;\">50.1459</td>\n<td style=\"text-align: right;\">54.1688</td>\n<td style=\"text-align: right;\">62.8492</td>\n<td style=\"text-align: right;\">59.3776</td>\n<td style=\"text-align: right;\">62.6560</td>\n<td style=\"text-align: right;\">143.3252</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">dplyr</td>\n<td style=\"text-align: right;\">62.6194</td>\n<td style=\"text-align: right;\">66.1931</td>\n<td style=\"text-align: right;\">82.9065</td>\n<td style=\"text-align: right;\">68.9263</td>\n<td style=\"text-align: right;\">71.7941</td>\n<td style=\"text-align: right;\">364.2103</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">split_lapply</td>\n<td style=\"text-align: right;\">84.7424</td>\n<td style=\"text-align: right;\">90.0375</td>\n<td style=\"text-align: right;\">110.7041</td>\n<td style=\"text-align: right;\">96.4528</td>\n<td style=\"text-align: right;\">113.7201</td>\n<td style=\"text-align: right;\">168.3835</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">aggregate</td>\n<td style=\"text-align: right;\">319.9465</td>\n<td style=\"text-align: right;\">335.0173</td>\n<td style=\"text-align: right;\">386.0137</td>\n<td style=\"text-align: right;\">369.7291</td>\n<td style=\"text-align: right;\">429.6735</td>\n<td style=\"text-align: right;\">538.2916</td>\n<td style=\"text-align: right;\">25</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<p>First of, I was very surprised by the bad performance of <code>aggregate</code>. I looked at the source code and it appears to be a more fancy lapply/split type of functions with a lot of <code>if/else</code> and <code>for</code> which do slow down the function heavily. For the benchmark with the bigger dataset, I actually discarded the function because it was way too slow.</p>\n<p>Apart from that, there are three groups. <code>Rfast</code> and <code>data.table</code> are the fastest clearly the fastest. The second group are the <code>tapply</code> versions. I am quite pleased with the fact that the data frame building via <code>do.call</code>, <code>sapply</code> and <code>array2DF</code> are very much comparable, because I really like my <code>array2DF</code> discovery. The remaining solutions are pretty much comparable. I am surprised though, that <code>dplyr</code> falls behind many of the base solutions.<sup>2</sup></p>\n<p>Moving on to the 100 million file to see if size makes a difference.</p>\n<div class=\"cell\">\n<pre>D &lt;- data.table::fread(\"measurements1e8.csv\", stringsAsFactors = TRUE)\n\nbench1e8 &lt;- microbenchmark::microbenchmark(\n    # aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |&gt; dplyr::group_by(state) |&gt; dplyr::summarise(sum_stats_tibble(measurement)) |&gt; dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 10\n)</pre>\n</div>\n<div class=\"cell\">\n<pre>ggplot2::autoplot(reorderMicrobenchmarkResults(bench1e8))</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e8-1.png?w=450\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e8-1.png?w=450\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<div class=\"cell\">\n<div class=\"cell-output-display\">\n<table class=\"table table-sm table-striped small\">\n<colgroup>\n<col style=\"width: 22%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 11%\"/>\n<col style=\"width: 8%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">expr</th>\n<th style=\"text-align: right;\">min</th>\n<th style=\"text-align: right;\">lq</th>\n<th style=\"text-align: right;\">mean</th>\n<th style=\"text-align: right;\">median</th>\n<th style=\"text-align: right;\">uq</th>\n<th style=\"text-align: right;\">max</th>\n<th style=\"text-align: right;\">neval</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Rfast</td>\n<td style=\"text-align: right;\">1.61404</td>\n<td style=\"text-align: right;\">2.00889</td>\n<td style=\"text-align: right;\">2.06342</td>\n<td style=\"text-align: right;\">2.09445</td>\n<td style=\"text-align: right;\">2.14546</td>\n<td style=\"text-align: right;\">2.41314</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">datatable</td>\n<td style=\"text-align: right;\">2.17933</td>\n<td style=\"text-align: right;\">2.20079</td>\n<td style=\"text-align: right;\">2.29543</td>\n<td style=\"text-align: right;\">2.23828</td>\n<td style=\"text-align: right;\">2.33378</td>\n<td style=\"text-align: right;\">2.70186</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">dplyr</td>\n<td style=\"text-align: right;\">2.80742</td>\n<td style=\"text-align: right;\">2.87777</td>\n<td style=\"text-align: right;\">3.04344</td>\n<td style=\"text-align: right;\">3.00719</td>\n<td style=\"text-align: right;\">3.17387</td>\n<td style=\"text-align: right;\">3.45470</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">reduce</td>\n<td style=\"text-align: right;\">2.72298</td>\n<td style=\"text-align: right;\">2.98724</td>\n<td style=\"text-align: right;\">3.19725</td>\n<td style=\"text-align: right;\">3.12715</td>\n<td style=\"text-align: right;\">3.46963</td>\n<td style=\"text-align: right;\">3.77594</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">docall_tapply</td>\n<td style=\"text-align: right;\">2.82332</td>\n<td style=\"text-align: right;\">2.91701</td>\n<td style=\"text-align: right;\">3.22054</td>\n<td style=\"text-align: right;\">3.25852</td>\n<td style=\"text-align: right;\">3.32141</td>\n<td style=\"text-align: right;\">3.73731</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">sapply_tapply</td>\n<td style=\"text-align: right;\">2.78456</td>\n<td style=\"text-align: right;\">2.81968</td>\n<td style=\"text-align: right;\">3.19675</td>\n<td style=\"text-align: right;\">3.29218</td>\n<td style=\"text-align: right;\">3.43617</td>\n<td style=\"text-align: right;\">3.66894</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">array2DF_tapply</td>\n<td style=\"text-align: right;\">3.11413</td>\n<td style=\"text-align: right;\">3.17007</td>\n<td style=\"text-align: right;\">3.37320</td>\n<td style=\"text-align: right;\">3.30678</td>\n<td style=\"text-align: right;\">3.56680</td>\n<td style=\"text-align: right;\">3.70316</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">array2DF_by</td>\n<td style=\"text-align: right;\">5.01008</td>\n<td style=\"text-align: right;\">5.06045</td>\n<td style=\"text-align: right;\">5.48980</td>\n<td style=\"text-align: right;\">5.42499</td>\n<td style=\"text-align: right;\">5.82906</td>\n<td style=\"text-align: right;\">6.01545</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">raw_by</td>\n<td style=\"text-align: right;\">4.78366</td>\n<td style=\"text-align: right;\">5.33826</td>\n<td style=\"text-align: right;\">5.46399</td>\n<td style=\"text-align: right;\">5.56182</td>\n<td style=\"text-align: right;\">5.67001</td>\n<td style=\"text-align: right;\">6.04721</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">split_lapply</td>\n<td style=\"text-align: right;\">5.95249</td>\n<td style=\"text-align: right;\">6.44981</td>\n<td style=\"text-align: right;\">6.55665</td>\n<td style=\"text-align: right;\">6.56029</td>\n<td style=\"text-align: right;\">6.84631</td>\n<td style=\"text-align: right;\">6.87408</td>\n<td style=\"text-align: right;\">10</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<p>Again we see three groups, but this time with clearer cut-offs. <code>Rfast</code> and <code>data.table</code> dominate and Rfast actually has a slight edge! The second group are <code>tapply</code>, <code>reduce</code> and <code>dplyr</code>. Surprisingly, <code>by</code> falls of here, together with <code>split/lapply</code>.</p>\n</section>\n<section class=\"level2\" id=\"summary\">\n<h2 class=\"anchored\" data-anchor-id=\"summary\">Summary</h2>\n<p>This was a fun little exercise, and I think I learned a lot of new things about base R, especially the existence of <code>arry2DF</code>!</p>\n<p>What was surprising is how competitive base R actually is with the “big guns”. I was expecting a var bigger margin between data.table and the base solutions, but that was not the case.</p>\n</section>\n<div class=\"default\" id=\"quarto-appendix\"><section class=\"quarto-appendix-contents\"><h2 class=\"anchored quarto-appendix-heading\">Reuse</h2><div class=\"quarto-appendix-contents\" id=\"quarto-reuse\"><div><a href=\"https://creativecommons.org/licenses/by/4.0/\" rel=\"nofollow\" target=\"_blank\">CC BY 4.0</a></div></div></section><section class=\"quarto-appendix-contents\"><h2 class=\"anchored quarto-appendix-heading\">Citation</h2><div><div class=\"quarto-appendix-secondary-label\">BibTeX citation:</div><pre>@online{schoch2024,\n  author = {Schoch, David},\n  title = {One Billion Row Challenge Using Base {R}},\n  date = {2024-01-08},\n  url = {http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge},\n  langid = {en}\n}\n</pre><div class=\"quarto-appendix-secondary-label\">For attribution, please cite this work as:</div><div class=\"csl-entry quarto-appendix-citeas\" id=\"ref-schoch2024\">\nSchoch, David. 2024. <span>“One Billion Row Challenge Using Base\nR.”</span> January 8, 2024. <a href=\"http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge\" rel=\"nofollow\" target=\"_blank\">http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge</a>.\n</div></div></section></div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/\"> schochastics - all things R</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
    "main_text": "One billion row challenge using base R\nPosted on\nJanuary 7, 2024\nby\nDavid Schoch\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nschochastics - all things R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nOne of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.\n1\nThe\nOne Billion Row challenge\nby Gunnar Morling is as follows:\nwrite a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. There’s just one caveat: the file has 1,000,000,000 rows!\nI didn’t take long, also thanks to\nHacker News\n, that the challenge spread to other programming languages. The original repository contains a\nshow & tell\nwhere other results can be discussed.\nObviously it also spread to R and there is a\nGitHub repository\nfrom Alejandro Hagan dedicated to the challenge. There were some\ncritical discussions\non the seemingly bad performance of\ndata.table\nbut that issue thread also evolved to a discussion on other solutions.\nThe obvious candidates for fast solutions with R are\ndplyr\n,\ndata.table\n,\ncollapse\n, and\npolars\n. From those, it appears that polars might solve the tasks the\nfastest\n.\nI was curious, how far one can get with base R.\nCreating the data\nThe R repository contains a\nscript\nto generate benchmark data. For the purpose of this post, I created files with 1e6 and 1e8 rows. Unfortunately, my personal laptop cannot handle 1 billion rows without dying.\nReading the data\nAll base R functions will profit from reading the state column as a factor instead of a usual string.\nD <- data.table::fread(\"measurements1e6.csv\", stringsAsFactors = TRUE)\nD\nmeasurement state\n      1:    0.981969    NC\n      2:    0.468715    MA\n      3:   -0.107971    TX\n      4:   -0.212878    VT\n      5:    1.158098    OR\n     ---                  \n 999996:    0.743249    FL\n 999997:   -1.685561    KS\n 999998:   -0.118455    TX\n 999999:    1.277437    MS\n1000000:   -0.280085    MD\nWho would have thought that\nstringAsFactors = TRUE\ncan be useful.\nThe obvious: aggregate and split/lapply\nThe most obvious choice for me was to use\naggregate()\n.\nsum_stats_vec <- function(x) c(min = min(x), max = max(x), mean = mean(x))\naggregate(measurement ~ state, data = D, FUN = sum_stats_vec) |> head()\nstate measurement.min measurement.max measurement.mean\n1    AK    -4.104494064     4.271009490      0.003082000\n2    AL    -3.635024969     4.538671919     -0.007811050\n3    AR    -3.813800485     4.101114941      0.008453876\n4    AZ    -4.415050930     3.965124829      0.000228734\n5    CA    -4.159725627     4.102446367      0.013649303\n6    CO    -3.860489118     4.231415117     -0.001334096\nI was pretty sure that this might be the best solution.\nThe other obvious solution is to split the data frame according to stats and then\nlapply\nthe stats calculation on each list element.\nsplit_lapply <- function(D) {\n    result <- lapply(split(D, D$state), function(x) {\n        stats <- sum_stats_vec(x$measurement)\n        data.frame(\n            state = unique(x$state),\n            min = stats[1],\n            max = stats[2],\n            mean = stats[3]\n        )\n    })\n    do.call(\"rbind\", result)\n}\nsplit_lapply(D) |> head()\nstate      min     max         mean\nAK    AK -4.10449 4.27101  0.003082000\nAL    AL -3.63502 4.53867 -0.007811050\nAR    AR -3.81380 4.10111  0.008453876\nAZ    AZ -4.41505 3.96512  0.000228734\nCA    CA -4.15973 4.10245  0.013649303\nCO    CO -3.86049 4.23142 -0.001334096\nThe elegant: by\nI stumbled upon\nby\nwhen searching for alternatives. I think it is a quite elegant way of solving a group/summarize task with base R. Unfortunately it returns a list and not a data frame or matrix (I made that an implicit requirement).\nIn the help for\nby\nI stumbled upon a function I wasn’t aware of yet:\narray2DF\n!\narray2DF(by(D$measurement, D$state, sum_stats_vec)) |> head()\nD$state      min     max         mean\n1      AK -4.10449 4.27101  0.003082000\n2      AL -3.63502 4.53867 -0.007811050\n3      AR -3.81380 4.10111  0.008453876\n4      AZ -4.41505 3.96512  0.000228734\n5      CA -4.15973 4.10245  0.013649303\n6      CO -3.86049 4.23142 -0.001334096\nDoes exactly what is needed here. For the benchmarks, I will also include a version without the\narray2DF\ncall, to check its overhead.\nAnother apply: tapply\nIn the help for\nby\n, I also stumbled upon this sentence\nFunction\nby\nis an object-oriented wrapper for\ntapply\napplied to data frames.\nSo maybe we can construct a solution that uses tapply, but without any inbuilt overhead in\nby\n.\ndo.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)) |> head()\nmin     max         mean\nAK -4.10449 4.27101  0.003082000\nAL -3.63502 4.53867 -0.007811050\nAR -3.81380 4.10111  0.008453876\nAZ -4.41505 3.96512  0.000228734\nCA -4.15973 4.10245  0.013649303\nCO -3.86049 4.23142 -0.001334096\nAt this point, I was also curious if the\ndo.call(\"rbind\",list)\ncan be sped up, so I constructed a second tapply solution.\nsapply(tapply(D$measurement, D$state, sum_stats_vec), rbind) |> head()\nAK          AL          AR           AZ         CA         CO\n[1,] -4.104494 -3.63502497 -3.81380048 -4.415050930 -4.1597256 -3.8604891\n[2,]  4.271009  4.53867192  4.10111494  3.965124829  4.1024464  4.2314151\n[3,]  0.003082 -0.00781105  0.00845388  0.000228734  0.0136493 -0.0013341\n              CT          DE          FL          GA          HI         IA\n[1,] -3.91246424 -4.13994780 -3.74126137 -3.94660998 -4.01231812 -3.7908228\n[2,]  4.42439303  4.10320343  3.61461913  3.92890675  3.54210746  4.1666241\n[3,]  0.00365691  0.00300726  0.00364475  0.00998416  0.00831473  0.0121738\n              ID          IL         IN         KS          KY          LA\n[1,] -3.96453912 -3.89433528 -4.0396596 -4.0290851 -4.04857519 -3.83912209\n[2,]  3.57787653  3.79539851  3.8997433  3.6210916  3.86743373  3.56924325\n[3,]  0.00364403  0.00282924 -0.0116455  0.0147175 -0.00141356  0.00150667\n              MA         MD         ME          MI          MN           MO\n[1,] -4.03713867 -3.7447056 -3.9230949 -3.95286252 -4.41058377 -3.939103495\n[2,]  3.79263060  3.7162898  3.6639929  4.15578354  4.31107098  4.428823170\n[3,] -0.00695907 -0.0104974  0.0017803  0.00578387  0.00222535 -0.000530262\n              MS         MT          NC         ND         NE          NH\n[1,] -4.11033773 -4.0871006 -4.37736785 -4.0689381 -4.0600632 -3.91574348\n[2,]  4.04043436  4.1620591  3.98113456  3.7439708  4.1786719  4.12714205\n[3,]  0.00401617 -0.0051991 -0.00791303 -0.0065667 -0.0019952  0.00173286\n              NJ         NM         NV          NY          OH          OK\n[1,] -3.89749099 -3.8077381 -4.4999793 -4.10688778 -3.97073238 -4.01749904\n[2,]  4.09297430  3.8533329  3.9841584  3.77539030  4.05541378  3.92196743\n[3,] -0.00821633 -0.0045123  0.0059095 -0.00401249  0.00391791  0.00272036\n               OR          PA          RI         SC          SD          TN\n[1,] -3.755405173 -3.87864920 -3.65614672 -3.6360017 -4.25212184 -3.63011318\n[2,]  4.299120255  4.18986336  4.25751403  4.1131445  3.74296173  3.92052537\n[3,]  0.000427857 -0.00303136 -0.00419174 -0.0110226  0.00774345  0.00671216\n              TX          UT         VA           VT          WA          WI\n[1,] -4.20588875 -4.03481832 -4.2980081 -3.749369728 -3.76023936 -3.76302646\n[2,]  3.92935839  3.99530205  3.8614655  3.990961895  3.87463693  4.32264119\n[3,] -0.00784648 -0.00561814 -0.0124286 -0.000579563  0.00684453  0.00297232\n             WV          WY\n[1,] -3.6034317 -3.86099776\n[2,]  3.9029500  4.11653276\n[3,] -0.0090395 -0.00238027\nand we should obviously also include our new found\narray2DF\narray2DF(tapply(D$measurement, D$state, sum_stats_vec)) |> head()\nVar1      min     max         mean\n1   AK -4.10449 4.27101  0.003082000\n2   AL -3.63502 4.53867 -0.007811050\n3   AR -3.81380 4.10111  0.008453876\n4   AZ -4.41505 3.96512  0.000228734\n5   CA -4.15973 4.10245  0.013649303\n6   CO -3.86049 4.23142 -0.001334096\nThe obscure: reduce\nI thought that this should be it, but then I remembered\nreduce\nexists. The solution is somewhat similar to split/lapply.\nreduce <- function(D) {\n    state_list <- split(D$measurement, D$state)\n    Reduce(function(x, y) {\n        res <- sum_stats_vec(state_list[[y]])\n        rbind(x, data.frame(state = y, mean = res[1], min = res[2], max = res[3]))\n    }, names(state_list), init = NULL)\n}\n\nreduce(D) |> head()\nstate     mean     min          max\nmin     AK -4.10449 4.27101  0.003082000\nmin1    AL -3.63502 4.53867 -0.007811050\nmin2    AR -3.81380 4.10111  0.008453876\nmin3    AZ -4.41505 3.96512  0.000228734\nmin4    CA -4.15973 4.10245  0.013649303\nmin5    CO -3.86049 4.23142 -0.001334096\nThe unfair contender: Rfast\nPondering about how this functions could be sped up in general, I remembered the package\nRfast\nand managed to construct a solution using this package.\nRfast <- function(D) {\n    lev_int <- as.numeric(D$state)\n    minmax <- Rfast::group(D$measurement, lev_int, method = \"min.max\")\n    data.frame(\n        state = levels(D$state),\n        mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n        min = minmax[1, ],\n        max = minmax[2, ]\n    )\n}\n\nRfast(D) |> head()\nstate         mean      min     max\n1    AK  0.003082000 -4.10449 4.27101\n2    AL -0.007811050 -3.63502 4.53867\n3    AR  0.008453876 -3.81380 4.10111\n4    AZ  0.000228734 -4.41505 3.96512\n5    CA  0.013649303 -4.15973 4.10245\n6    CO -0.001334096 -3.86049 4.23142\nPretty sure that this will be the fastest, maybe even competitive with the other big packages!\nBenchmark\nFor better readability I reorder the benchmark results from\nmicrobenchmark\naccording to median runtime, with a function provided by\nDirk Eddelbuettel\n.\nreorderMicrobenchmarkResults <- function(res, order = \"median\") {\n    stopifnot(\"Argument 'res' must be a 'microbenchmark' result\" = inherits(res, \"microbenchmark\"))\n\n    smry <- summary(res)\n    res$expr <- factor(res$expr,\n        levels = levels(res$expr)[order(smry[[\"median\"]])],\n        ordered = TRUE\n    )\n    res\n}\nFirst up the “small” dataset with 1e6 rows. I added the\ndplyr\nand\ndata.table\nresults as references.\nsum_stats_list <- function(x) list(min = min(x), max = max(x), mean = mean(x))\nsum_stats_tibble <- function(x) tibble::tibble(min = min(x), max = max(x), mean = mean(x))\n\nbench1e6 <- microbenchmark::microbenchmark(\n    aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 25\n)\nggplot2::autoplot(reorderMicrobenchmarkResults(bench1e6))\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\ndatatable\n13.4325\n15.2802\n16.8016\n15.5527\n17.6287\n22.9133\n25\nRfast\n13.5639\n16.5295\n19.1905\n18.0986\n21.3507\n33.5924\n25\narray2DF_tapply\n27.5491\n30.9502\n37.5948\n32.3145\n36.7538\n132.9784\n25\ndocall_tapply\n28.4628\n30.7052\n39.9436\n32.3174\n33.7132\n99.6504\n25\nsapply_tapply\n28.8092\n30.1596\n46.0100\n33.3971\n37.7851\n101.5850\n25\nraw_by\n40.9116\n44.1075\n52.2293\n45.9647\n48.4539\n119.5348\n25\narray2DF_by\n43.2958\n45.8177\n58.4240\n48.8741\n52.2750\n132.8981\n25\nreduce\n50.1459\n54.1688\n62.8492\n59.3776\n62.6560\n143.3252\n25\ndplyr\n62.6194\n66.1931\n82.9065\n68.9263\n71.7941\n364.2103\n25\nsplit_lapply\n84.7424\n90.0375\n110.7041\n96.4528\n113.7201\n168.3835\n25\naggregate\n319.9465\n335.0173\n386.0137\n369.7291\n429.6735\n538.2916\n25\nFirst of, I was very surprised by the bad performance of\naggregate\n. I looked at the source code and it appears to be a more fancy lapply/split type of functions with a lot of\nif/else\nand\nfor\nwhich do slow down the function heavily. For the benchmark with the bigger dataset, I actually discarded the function because it was way too slow.\nApart from that, there are three groups.\nRfast\nand\ndata.table\nare the fastest clearly the fastest. The second group are the\ntapply\nversions. I am quite pleased with the fact that the data frame building via\ndo.call\n,\nsapply\nand\narray2DF\nare very much comparable, because I really like my\narray2DF\ndiscovery. The remaining solutions are pretty much comparable. I am surprised though, that\ndplyr\nfalls behind many of the base solutions.\n2\nMoving on to the 100 million file to see if size makes a difference.\nD <- data.table::fread(\"measurements1e8.csv\", stringsAsFactors = TRUE)\n\nbench1e8 <- microbenchmark::microbenchmark(\n    # aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 10\n)\nggplot2::autoplot(reorderMicrobenchmarkResults(bench1e8))\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\nRfast\n1.61404\n2.00889\n2.06342\n2.09445\n2.14546\n2.41314\n10\ndatatable\n2.17933\n2.20079\n2.29543\n2.23828\n2.33378\n2.70186\n10\ndplyr\n2.80742\n2.87777\n3.04344\n3.00719\n3.17387\n3.45470\n10\nreduce\n2.72298\n2.98724\n3.19725\n3.12715\n3.46963\n3.77594\n10\ndocall_tapply\n2.82332\n2.91701\n3.22054\n3.25852\n3.32141\n3.73731\n10\nsapply_tapply\n2.78456\n2.81968\n3.19675\n3.29218\n3.43617\n3.66894\n10\narray2DF_tapply\n3.11413\n3.17007\n3.37320\n3.30678\n3.56680\n3.70316\n10\narray2DF_by\n5.01008\n5.06045\n5.48980\n5.42499\n5.82906\n6.01545\n10\nraw_by\n4.78366\n5.33826\n5.46399\n5.56182\n5.67001\n6.04721\n10\nsplit_lapply\n5.95249\n6.44981\n6.55665\n6.56029\n6.84631\n6.87408\n10\nAgain we see three groups, but this time with clearer cut-offs.\nRfast\nand\ndata.table\ndominate and Rfast actually has a slight edge! The second group are\ntapply\n,\nreduce\nand\ndplyr\n. Surprisingly,\nby\nfalls of here, together with\nsplit/lapply\n.\nSummary\nThis was a fun little exercise, and I think I learned a lot of new things about base R, especially the existence of\narry2DF\n!\nWhat was surprising is how competitive base R actually is with the “big guns”. I was expecting a var bigger margin between data.table and the base solutions, but that was not the case.\nReuse\nCC BY 4.0\nCitation\nBibTeX citation:\n@online{schoch2024,\n  author = {Schoch, David},\n  title = {One Billion Row Challenge Using Base {R}},\n  date = {2024-01-08},\n  url = {http://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchoch, David. 2024.\n“One Billion Row Challenge Using Base\nR.”\nJanuary 8, 2024.\nhttp://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nschochastics - all things R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "One of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.1 The One Billion Row challenge by Gunnar Morling is as follows: write a Java program for retrieving temperature measurement value...",
    "meta_keywords": null,
    "og_description": "One of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.1 The One Billion Row challenge by Gunnar Morling is as follows: write a Java program for retrieving temperature measurement value...",
    "og_image": "https://blog.schochastics.net/posts/2024-01-08_one-billion-rows-challenge/index_files/figure-html/plot-bench1e6-1.png",
    "og_title": "One billion row challenge using base R | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 12.8,
    "sitemap_lastmod": "2024-01-07T23:00:00+00:00",
    "twitter_description": "One of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.1 The One Billion Row challenge by Gunnar Morling is as follows: write a Java program for retrieving temperature measurement value...",
    "twitter_title": "One billion row challenge using base R | R-bloggers",
    "url": "https://www.r-bloggers.com/2024/01/one-billion-row-challenge-using-base-r/",
    "word_count": 2557
  }
}