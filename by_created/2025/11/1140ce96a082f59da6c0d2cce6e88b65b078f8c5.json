{
  "id": "1140ce96a082f59da6c0d2cce6e88b65b078f8c5",
  "url": "https://www.r-bloggers.com/2023/10/multistep-horizon-loss-optimized-forecasting-with-adam/",
  "created_at_utc": "2025-11-17T20:39:29Z",
  "data": null,
  "raw_original": {
    "uuid": "e43547dc-f526-4c96-8d70-e106d368b421",
    "created_at": "2025-11-17 20:39:29",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2023/10/multistep-horizon-loss-optimized-forecasting-with-adam/",
      "crawled_at": "2025-11-17T10:04:28.899758",
      "external_links": [
        {
          "href": "https://petolau.github.io/Multistep-loss-optimized-forecasting-with-ADAM/",
          "text": "Peter Laurinec"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://cran.r-project.org/package=smooth",
          "text": "smoothpackage"
        },
        {
          "href": "https://otexts.com/fpp2/ets-forecasting.html",
          "text": "ETS model"
        },
        {
          "href": "https://petolau.github.io/Bootstrapping-time-series-for-improving-forecasting-in-R/",
          "text": "bootstrapping"
        },
        {
          "href": "https://github.com/PetoLau/petolau.github.io/blob/master/_Rscripts/14post_ADAMsmoothForecast.R",
          "text": "R script"
        },
        {
          "href": "https://www.sciencedirect.com/science/article/pii/S0169207016000121",
          "text": "MAAPE"
        },
        {
          "href": "https://openforecast.org/adam/diagnostics.html",
          "text": "adam forecasting book"
        },
        {
          "href": "https://petolau.github.io/Multistep-loss-optimized-forecasting-with-ADAM/",
          "text": "Peter Laurinec"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Multistep horizon loss optimized forecasting with ADAM | R-bloggers",
      "images": [
        {
          "alt": "plot of chunk unnamed-chunk-3",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-3",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-3-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-4",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-4",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-4-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-5",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-5",
          "base64": null,
          "src": "https://i0.wp.com/petolau.github.io/images/post_14/unnamed-chunk-5-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-7",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-7",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-7-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-8",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-8",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-8-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-12",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-12",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-12-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-14",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-14",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-14-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-15",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-15",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-15-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-16",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-16",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-16-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-17",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-17",
          "base64": null,
          "src": "https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-17-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-18",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-18",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-18-1.png?w=578&ssl=1"
        },
        {
          "alt": "plot of chunk unnamed-chunk-19",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "plot of chunk unnamed-chunk-19",
          "base64": null,
          "src": "https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-19-1.png?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/peter-laurinec/",
          "text": "Peter Laurinec"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-379454 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Multistep horizon loss optimized forecasting with ADAM</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">October 25, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/peter-laurinec/\">Peter Laurinec</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://petolau.github.io/Multistep-loss-optimized-forecasting-with-ADAM/\"> Peter Laurinec</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9--><p>Recently, during work on some forecasting task at PowereX, I stumbled upon an interesting improvement in time series forecasting modeling.\nIn general, in regression modeling, we use evaluation/loss metrics based on one step ahead (one value) horizon.\nBut, in many forecasting applications, we are interested in multistep horizon forecasting (e.g. whole 1 day ahead), so some multistep losses would be great to optimize during training of forecasting models.\nExcept obviously in some RNN architectures, this wasn‚Äôt easily possible in used libraries/frameworks in the past time.</p>\n<p>So, I was very happy to see multistep losses in <a href=\"https://cran.r-project.org/package=smooth\" rel=\"nofollow\" target=\"_blank\"><code>smooth</code> package</a> and its <code>adam()</code> wrapper and to try it. I already used it in one particular task in my work, and it went successfully to production!</p>\n<p>My motivation and what I will show you in this blog post is:</p>\n<ul>\n<li>Tribute to the <code>smooth</code> package and its <code>adam()</code> wrapper ‚Äì especially for its multistep horizon forecasting losses.</li>\n<li>Try to use the mentioned losses for forecasting household consumption times series -&gt; very noisy with daily/weekly seasons, no trend.</li>\n<li>Show model diagnostic methods with <code>ggplot2</code> visualizations.</li>\n</ul>\n<h3 id=\"household-consumption-data\">Household consumption data</h3>\n<p>Firstly, let‚Äôs load all the needed packages for forecasting.</p>\n<figure class=\"highlight\"><pre>library(data.table)\nlibrary(lubridate)\nlibrary(TSrepr)\nlibrary(smooth)\nlibrary(forecast)\nlibrary(ggplot2)</pre></figure>\n<p>Next, I will read one household electricity consumption load data. There are data from <code>2023-01-01</code> to <code>2023-08-31</code>. This household heats space and water with electricity ‚Äì so there are multiple seasonalities that vary during the year.</p>\n<figure class=\"highlight\"><pre>data_household_consumption &lt;- fread(\"_Rscripts/household_consumption.csv\") # Date_Time in CET, Load in kW\nstr(data_household_consumption)</pre></figure>\n<figure class=\"highlight\"><pre>## Classes 'data.table' and 'data.frame':\t23328 obs. of  2 variables:\n##  $ Date_Time: POSIXct, format: \"2023-01-01 00:00:00\" \"2023-01-01 00:15:00\" \"2023-01-01 00:30:00\" \"2023-01-01 00:45:00\" ...\n##  $ Load     : num  0.362 0.622 0.435 0.236 0.322 0.565 0.338 0.467 0.639 0.46 ...\n##  - attr(*, \".internal.selfref\")=&lt;externalptr&gt;</pre></figure>\n<figure class=\"highlight\"><pre>summary(data_household_consumption)</pre></figure>\n<figure class=\"highlight\"><pre>##    Date_Time                        Load       \n##  Min.   :2023-01-01 00:00:00   Min.   :0.0000  \n##  1st Qu.:2023-03-02 17:56:15   1st Qu.:0.1140  \n##  Median :2023-05-02 11:52:30   Median :0.4440  \n##  Mean   :2023-05-02 11:52:30   Mean   :0.7468  \n##  3rd Qu.:2023-07-02 05:48:45   3rd Qu.:1.1150  \n##  Max.   :2023-08-31 23:45:00   Max.   :7.1510</pre></figure>\n<p>Let‚Äôs visualize consumption for one month.</p>\n<figure class=\"highlight\"><pre>ggplot(data_household_consumption[Date_Time &gt;= as.Date(\"2023-04-01\") &amp; Date_Time &lt;= as.Date(\"2023-05-01\")]) +\n  geom_line(aes(Date_Time, Load)) +\n  labs(y = \"Load (kW)\") +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-3\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-3-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-3\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-3-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see that the behavior of the time series is very noisy but with clear daily pattern.</p>\n<p>Let‚Äôs check more in detail, what we will face during forecasting, so I will show you the weekly average pattern (+/- standard deviation) by month.</p>\n<figure class=\"highlight\"><pre>data_household_consumption[, week_period := 1:.N, by = .(Week = lubridate::week(Date_Time))]\ndata_household_consumption[, month := lubridate::month(Date_Time, label = T)]\ndata_weekly_profiles &lt;- copy(data_household_consumption[, .(Mean = mean(Load),\n                                                            SD = sd(Load)),\n                                                        by = .(month, week_period)])\n \nggplot(data_weekly_profiles, aes(x = week_period, Mean)) + \n  facet_wrap(~month) +\n  geom_ribbon(data = data_weekly_profiles, aes(ymin = Mean - SD, ymax = Mean + SD),\n              fill = \"firebrick2\", alpha = 0.4) +\n  geom_line(linewidth = 0.9) + \n  geom_vline(xintercept = c(96, 96*4, 96*5, 96*6), linetype = 2, linewidth = 0.8) +\n  theme(title = element_text(size = 14),\n        axis.text = element_text(size = 10),\n        axis.title = element_text(size = 12, face = \"bold\")) +\n  labs(title = \"Mean weekly profile +- deviation (SD)\") +\n  labs(x = \"Time\", y = \"Load (kW)\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-4\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-4-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-4\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-4-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see that consumption patterns change over months‚Ä¶</p>\n<p>I will show you also an average daily pattern by month.</p>\n<figure class=\"highlight\"><pre>data_household_consumption[, day_period := 1:.N, by = .(Date = date(Date_Time))]\ndata_daily_profiles &lt;- copy(data_household_consumption[, .(Mean = mean(Load),\n                                                            SD = sd(Load)),\n                                                        by = .(month, day_period)])\n \nggplot(data_daily_profiles, aes(x = day_period, Mean)) + \n  facet_wrap(~month) +\n  geom_ribbon(data = data_daily_profiles, aes(ymin = Mean - SD, ymax = Mean + SD),\n              fill = \"firebrick2\", alpha = 0.4) +\n  geom_line(linewidth = 0.9) + \n  theme(title = element_text(size = 14),\n        axis.text = element_text(size = 10),\n        axis.title = element_text(size = 12, face = \"bold\")) +\n  labs(title = \"Mean daily profile +- deviation (SD)\") +\n  labs(x = \"Time\", y = \"Load (kW)\")</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-5\" data-lazy-src=\"https://i0.wp.com/petolau.github.io/images/post_14/unnamed-chunk-5-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-5\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/petolau.github.io/images/post_14/unnamed-chunk-5-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>It‚Äôs obvious that during winter months there is different behavior of electricity consumption, and much more consumed electricity because of heating needs.</p>\n<h3 id=\"smooth-and-adam\">smooth and ADAM</h3>\n<p>The main goal of this blog post is to try experimentally multistep losses in <a href=\"https://otexts.com/fpp2/ets-forecasting.html\" rel=\"nofollow\" target=\"_blank\">ETS model</a> and compare them against classical losses (MSE and MAE).</p>\n<p>In <code>adam</code> function we have several possibilities for multistep losses, check in here: https://openforecast.org/adam/multistepLosses.html.</p>\n<p>I will use these in experiments:</p>\n<ul>\n<li>MSEh, MAEh</li>\n<li>TMSE ‚Äì Trace MSE</li>\n<li>GTMSE ‚Äì Geometric Trace MSE</li>\n<li>MSCE ‚Äì Mean Squared Cumulative Error</li>\n<li>GPL ‚Äì General Predictive Likelihood</li>\n</ul>\n<p>Let‚Äôs try <code>adam</code> on 4 weeks sample of data.</p>\n<figure class=\"highlight\"><pre>win_days &lt;- 28 # 4 weeks\nall_dates &lt;- data_household_consumption[, unique(date(Date_Time))]\ndata_household_consumption_4weeks &lt;- copy(data_household_consumption[Date_Time &gt;= all_dates[1] &amp; Date_Time &lt; all_dates[win_days + 1]])</pre></figure>\n<p>Firstly, I will try only the additive ETS model with classic MSE loss.</p>\n<figure class=\"highlight\"><pre>adam_model_classic &lt;- adam(data = data_household_consumption_4weeks[, Load],\n                           model = \"XXX\",\n                           lags = c(1, 96, 96*7),\n                           loss = \"MSE\",\n                           h = 96,\n                           holdout = T,\n                           ic = \"AICc\",\n                           initial = \"back\")\n \nadam_model_classic</pre></figure>\n<figure class=\"highlight\"><pre>## Time elapsed: 0.99 seconds\n## Model estimated using adam() function: ETS(ANA)[96, 672]\n## Distribution assumed in the model: Normal\n## Loss function type: MSE; Loss function value: 0.2732\n## Persistence vector g:\n##  alpha gamma1 gamma2 \n## 0.5246 0.0000 0.0791 \n## \n## Sample size: 2592\n## Number of estimated parameters: 3\n## Number of degrees of freedom: 2589\n## Information criteria:\n##      AIC     AICc      BIC     BICc \n## 3998.436 3998.445 4016.017 4016.053 \n## \n## Forecast errors:\n## ME: 0.041; MAE: 0.534; RMSE: 0.726\n## sCE: 425.014%; Asymmetry: -15.5%; sMAE: 57.586%; sMSE: 61.353%\n## MASE: 1.304; RMSSE: 1.2; rMAE: 0.845; rRMSE: 0.897</pre></figure>\n<figure class=\"highlight\"><pre>plot(adam_model_classic, which = 7)</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-7\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-7-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-7\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-7-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see that <strong>ANA</strong> model is selected, so additive on level and season components, trend component was not used.</p>\n<p>Let‚Äôs try multistep loss - GPL.</p>\n<figure class=\"highlight\"><pre>adam_model_multistep &lt;- adam(data = data_household_consumption_4weeks[, Load],\n                             model = \"XXX\",\n                             lags = c(1, 96, 96*7),\n                             loss = \"GPL\",\n                             h = 96,\n                             holdout = T,\n                             ic = \"AICc\",\n                             initial = \"back\")\n \nadam_model_multistep</pre></figure>\n<figure class=\"highlight\"><pre>## Time elapsed: 25.37 seconds\n## Model estimated using adam() function: ETS(ANA)[96, 672]\n## Distribution assumed in the model: Normal\n## Loss function type: GPL; Loss function value: -135.4807\n## Persistence vector g:\n##  alpha gamma1 gamma2 \n## 0.0004 0.0000 0.0884 \n## \n## Sample size: 2592\n## Number of estimated parameters: 3\n## Number of degrees of freedom: 2589\n## Information criteria:\n##      AIC     AICc      BIC     BICc \n## 3703.800 3703.809 3721.380 3721.417 \n## \n## Forecast errors:\n## ME: 0.075; MAE: 0.526; RMSE: 0.732\n## sCE: 778.763%; Asymmetry: -7.8%; sMAE: 56.684%; sMSE: 62.332%\n## MASE: 1.284; RMSSE: 1.21; rMAE: 0.832; rRMSE: 0.904</pre></figure>\n<figure class=\"highlight\"><pre>plot(adam_model_multistep, which = 7)</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-8\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-8-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-8\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-8-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Also, ANA was selected.\nBetter AICc and MAE with multistep loss, but RMSE was worse than in the MSE case.\nLet‚Äôs see on whole dataset‚Ä¶</p>\n<h3 id=\"prediction-simulations\">Prediction simulations</h3>\n<p>I will roll predictions day by day with a whole 1-day prediction horizon with a train-set of length 28 days (<code>win_days</code>) on our household consumption data -&gt; so 214 predictions will be made.\nI will use all multistep losses mentioned above, MAE, MSE, and STL+ARIMA, STL+ETS methods from the <code>forecast</code> package as benchmarks (as was used also in my previous blog post about <a href=\"https://petolau.github.io/Bootstrapping-time-series-for-improving-forecasting-in-R/\" rel=\"nofollow\" target=\"_blank\">bootstrapping</a>).\nYou can check <a href=\"https://github.com/PetoLau/petolau.github.io/blob/master/_Rscripts/14post_ADAMsmoothForecast.R\" rel=\"nofollow\" target=\"_blank\">R script</a> how to simulate and compute this in parallel on my GitHub repository.</p>\n<p>Now, let‚Äôs see generated predictions.</p>\n<figure class=\"highlight\"><pre>load(\"_rmd/data_household_predictions.Rdata\")\ndata_predictions_all</pre></figure>\n<figure class=\"highlight\"><pre>##                   Date_Time Load_real Load_predicted    model\n##      1: 2023-01-30 00:00:00     0.274     -0.7679345  STL+ETS\n##      2: 2023-01-30 00:15:00     0.363     -0.7959601  STL+ETS\n##      3: 2023-01-30 00:30:00     0.342     -0.7443988  STL+ETS\n##      4: 2023-01-30 00:45:00     0.391     -0.8461708  STL+ETS\n##      5: 2023-01-30 01:00:00     0.230     -0.8708582  STL+ETS\n##     ---                                                      \n## 205436: 2023-08-31 22:45:00     0.080      0.4030999 ADAM-GPL\n## 205437: 2023-08-31 23:00:00     0.039      0.2060178 ADAM-GPL\n## 205438: 2023-08-31 23:15:00     0.087      0.2489654 ADAM-GPL\n## 205439: 2023-08-31 23:30:00     0.080      0.1719251 ADAM-GPL\n## 205440: 2023-08-31 23:45:00     0.040      0.4710469 ADAM-GPL</pre></figure>\n<p>I will compute forecasting evaluation metrics as RMSE, MAE, and <a href=\"https://www.sciencedirect.com/science/article/pii/S0169207016000121\" rel=\"nofollow\" target=\"_blank\">MAAPE</a>.</p>\n<figure class=\"highlight\"><pre>data_predictions_all[, .(RMSE = rmse(Load_real, Load_predicted),\n                         MAE = mae(Load_real, Load_predicted),\n                         MAAPE = maape(Load_real, Load_predicted)),\n                     by = .(model)][order(MAE)]</pre></figure>\n<figure class=\"highlight\"><pre>##          model      RMSE       MAE    MAAPE\n##  1:  ADAM-MSCE 0.7645952 0.5588521 77.19498\n##  2:  ADAM-MAEh 0.7768230 0.5596729 75.60581\n##  3:  ADAM-TMSE 0.7703792 0.5622345 77.03544\n##  4:  ADAM-MSEh 0.7713433 0.5623389 76.99981\n##  5: ADAM-GTMSE 0.7704186 0.5626845 77.15252\n##  6:   ADAM-GPL 0.7704062 0.5629255 77.17820\n##  7:   ADAM-MAE 0.8690460 0.6133876 78.18846\n##  8:   ADAM-MSE 0.8701501 0.6152678 78.22642\n##  9:  STL+ARIMA 0.8751174 0.6159549 78.60016\n## 10:    STL+ETS 0.9403524 0.6500436 76.13823</pre></figure>\n<p>We can see that all multistep losses are better than benchmark methods! That‚Äôs very nice results!</p>\n<p>For further analysis, we will check only the two best multistep losses and two benchmarks.</p>\n<figure class=\"highlight\"><pre>models_sub &lt;- c(\"ADAM-MSCE\", \"ADAM-MAEh\", \"ADAM-MAE\", \"STL+ARIMA\")</pre></figure>\n<p>Let‚Äôs see all generated prediction graphs with selected models:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all[model %in% models_sub], aes(Date_Time, Load_predicted)) +\n  facet_wrap(~model, ncol = 1) +\n  geom_line(data = data_predictions_all[model %in% models_sub],\n            aes(Date_Time, Load_real),\n            color = \"black\") +\n  geom_line(color = \"firebrick1\") +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-12\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-12-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-12\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-12-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>There is a much lower variance of predictions with multistep losses ETS than in benchmarks, which‚Äôs interesting, but in nature of these losses logical also -&gt; again check https://openforecast.org/adam/multistepLosses.html#multistepLosses.</p>\n<h3 id=\"models-diagnostics\">Models diagnostics</h3>\n<p>Next, I will try to demonstrate how to compare results from multiple models and analyze them with model diagnostic methods, which are also described in <a href=\"https://openforecast.org/adam/diagnostics.html\" rel=\"nofollow\" target=\"_blank\">adam forecasting book</a>.</p>\n<p>Let‚Äôs create residuals and abs(residuals) columns for investigation purposes.</p>\n<figure class=\"highlight\"><pre>data_predictions_all[, Error := Load_real - Load_predicted]\ndata_predictions_all[, AE := abs(Error)]</pre></figure>\n<p>Let‚Äôs see the boxplot of absolute errors:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all[model %in% models_sub],\n       aes(model, AE, fill = model)) +\n  geom_boxplot() +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-14\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-14-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-14\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-14-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We see narrower IQR with multistep losses, that‚Äôs a good feature.</p>\n<p>Predictions vs real values scatter plot:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all[model %in% models_sub],\n       aes(Load_predicted, Load_real)) +\n  facet_wrap(~model, scales = \"fixed\", ncol = 2) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", \n              linetype = \"dashed\", linewidth = 1) +\n  geom_smooth(alpha = 0.4) +\n  labs(title = \"Predicted vs Real values\") +\n  theme_bw()</pre></figure>\n<figure class=\"highlight\"><pre>## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-15\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-15-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-15\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-15-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>That is not very nice behavior in general, actually :), there are a lot of below-zero predictions (but fewer with multistep losses!) and no high-load predictions with multistep losses.</p>\n<p>Residuals vs real values scatter plot:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all[model %in% models_sub],\n       aes(Error, Load_real)) +\n  facet_wrap(~model, scales = \"fixed\") +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  xlab(\"resid (predicted_value - Load_real)\") +\n  labs(title = \"Residual vs Real values\") +\n  theme_bw()</pre></figure>\n<figure class=\"highlight\"><pre>## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-16\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-16-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-16\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-16-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Again, it is not a very nice plot üôÇ the conclusions are very similar to with previous one.</p>\n<p>Heteroscedasticity check - so real values vs. absolute errors.</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all[model %in% models_sub],\n       aes(Load_real, AE)) +\n  facet_wrap(~model, scales = \"fixed\") +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  ylab(\"abs(resid)\") +\n  labs(title = \"heteroscedasticity check\") + \n  theme_bw()</pre></figure>\n<figure class=\"highlight\"><pre>## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-17\" data-lazy-src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-17-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-17\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/petolau.github.io/images/post_14/unnamed-chunk-17-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>Our models have no heteroscedasticity at all, so absolute errors are not similarly distributed around real values distribution.</p>\n<p>Let‚Äôs see assumptions of normally distributed errors - Q-Q plot check:</p>\n<figure class=\"highlight\"><pre>ggplot(data_predictions_all[model %in% models_sub],\n       aes(sample = Error, group = model)) +\n  facet_wrap(~model, scales = \"fixed\") +\n  stat_qq() + stat_qq_line() +\n  ylab(\"resid (predicted_value - Load_real)\") +\n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-18\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-18-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-18\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-18-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>We can see a nicer Q-Q plot with multistep losses - values are sliding on the optimal line much more than with benchmarks.</p>\n<p>The last check is about the assumption that multistep forecasting errors have zero mean (will check it hourly - not by 15 minutes):</p>\n<figure class=\"highlight\"><pre>data_predictions_all[, Hour := lubridate::hour(Date_Time)]\n \nggplot(data_predictions_all[model %in% models_sub],\n       aes(Hour, Error, group = Hour)) +\n  facet_wrap(~model, scales = \"free_y\", ncol = 1) +\n  geom_boxplot(alpha = 0.7, color = \"black\", fill = \"gray\") +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_point(data = data_predictions_all[model %in% models_sub,\n                                         .(Error = mean(Error)),\n                                         by = .(model, Hour)],\n             aes(Hour, Error, group = Hour), color = \"firebrick2\") +\n  xlab(\"Hour (horizon)\") +\n  ylab(\"resid (predicted_value - Load_real)\") +\n  labs(title = \"Multistep forecast errors have zero mean check\") + \n  theme_bw()</pre></figure>\n<p><img alt=\"plot of chunk unnamed-chunk-19\" data-lazy-src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-19-1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"plot of chunk unnamed-chunk-19\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/petolau.github.io/images/post_14/unnamed-chunk-19-1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>All models are quite around zero, so good.</p>\n<h3 id=\"summary\">Summary</h3>\n<p>In this blog post, I showed that multistep losses in the ETS model have a great impact on forecast behavior:</p>\n<ul>\n<li>lower variance,</li>\n<li>and more accurate forecasts,</li>\n</ul>\n<p>in our case of 1 day ahead household electricity consumption forecasting.</p>\n<p>In general, household electricity consumption forecasting with ETS is not satisfactory as seen with model diagnostic methods that were showed -&gt; higher values of consumption load are not well covered, etc.</p>\n<p>In the future, I would like to show the possibility of improving forecasting accuracy with some ensemble methods, even with an ensemble of ‚Äúweak‚Äù models such as ETS.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://petolau.github.io/Multistep-loss-optimized-forecasting-with-ADAM/\"> Peter Laurinec</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
      "main_text": "Multistep horizon loss optimized forecasting with ADAM\nPosted on\nOctober 25, 2023\nby\nPeter Laurinec\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nPeter Laurinec\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nRecently, during work on some forecasting task at PowereX, I stumbled upon an interesting improvement in time series forecasting modeling.\nIn general, in regression modeling, we use evaluation/loss metrics based on one step ahead (one value) horizon.\nBut, in many forecasting applications, we are interested in multistep horizon forecasting (e.g. whole 1 day ahead), so some multistep losses would be great to optimize during training of forecasting models.\nExcept obviously in some RNN architectures, this wasn‚Äôt easily possible in used libraries/frameworks in the past time.\nSo, I was very happy to see multistep losses in\nsmooth\npackage\nand its\nadam()\nwrapper and to try it. I already used it in one particular task in my work, and it went successfully to production!\nMy motivation and what I will show you in this blog post is:\nTribute to the\nsmooth\npackage and its\nadam()\nwrapper ‚Äì especially for its multistep horizon forecasting losses.\nTry to use the mentioned losses for forecasting household consumption times series -> very noisy with daily/weekly seasons, no trend.\nShow model diagnostic methods with\nggplot2\nvisualizations.\nHousehold consumption data\nFirstly, let‚Äôs load all the needed packages for forecasting.\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(TSrepr)\nlibrary(smooth)\nlibrary(forecast)\nlibrary(ggplot2)\nNext, I will read one household electricity consumption load data. There are data from\n2023-01-01\nto\n2023-08-31\n. This household heats space and water with electricity ‚Äì so there are multiple seasonalities that vary during the year.\ndata_household_consumption <- fread(\"_Rscripts/household_consumption.csv\") # Date_Time in CET, Load in kW\nstr(data_household_consumption)\n## Classes 'data.table' and 'data.frame':\t23328 obs. of  2 variables:\n##  $ Date_Time: POSIXct, format: \"2023-01-01 00:00:00\" \"2023-01-01 00:15:00\" \"2023-01-01 00:30:00\" \"2023-01-01 00:45:00\" ...\n##  $ Load     : num  0.362 0.622 0.435 0.236 0.322 0.565 0.338 0.467 0.639 0.46 ...\n##  - attr(*, \".internal.selfref\")=<externalptr>\nsummary(data_household_consumption)\n##    Date_Time                        Load       \n##  Min.   :2023-01-01 00:00:00   Min.   :0.0000  \n##  1st Qu.:2023-03-02 17:56:15   1st Qu.:0.1140  \n##  Median :2023-05-02 11:52:30   Median :0.4440  \n##  Mean   :2023-05-02 11:52:30   Mean   :0.7468  \n##  3rd Qu.:2023-07-02 05:48:45   3rd Qu.:1.1150  \n##  Max.   :2023-08-31 23:45:00   Max.   :7.1510\nLet‚Äôs visualize consumption for one month.\nggplot(data_household_consumption[Date_Time >= as.Date(\"2023-04-01\") & Date_Time <= as.Date(\"2023-05-01\")]) +\n  geom_line(aes(Date_Time, Load)) +\n  labs(y = \"Load (kW)\") +\n  theme_bw()\nWe can see that the behavior of the time series is very noisy but with clear daily pattern.\nLet‚Äôs check more in detail, what we will face during forecasting, so I will show you the weekly average pattern (+/- standard deviation) by month.\ndata_household_consumption[, week_period := 1:.N, by = .(Week = lubridate::week(Date_Time))]\ndata_household_consumption[, month := lubridate::month(Date_Time, label = T)]\ndata_weekly_profiles <- copy(data_household_consumption[, .(Mean = mean(Load),\n                                                            SD = sd(Load)),\n                                                        by = .(month, week_period)])\n \nggplot(data_weekly_profiles, aes(x = week_period, Mean)) + \n  facet_wrap(~month) +\n  geom_ribbon(data = data_weekly_profiles, aes(ymin = Mean - SD, ymax = Mean + SD),\n              fill = \"firebrick2\", alpha = 0.4) +\n  geom_line(linewidth = 0.9) + \n  geom_vline(xintercept = c(96, 96*4, 96*5, 96*6), linetype = 2, linewidth = 0.8) +\n  theme(title = element_text(size = 14),\n        axis.text = element_text(size = 10),\n        axis.title = element_text(size = 12, face = \"bold\")) +\n  labs(title = \"Mean weekly profile +- deviation (SD)\") +\n  labs(x = \"Time\", y = \"Load (kW)\")\nWe can see that consumption patterns change over months‚Ä¶\nI will show you also an average daily pattern by month.\ndata_household_consumption[, day_period := 1:.N, by = .(Date = date(Date_Time))]\ndata_daily_profiles <- copy(data_household_consumption[, .(Mean = mean(Load),\n                                                            SD = sd(Load)),\n                                                        by = .(month, day_period)])\n \nggplot(data_daily_profiles, aes(x = day_period, Mean)) + \n  facet_wrap(~month) +\n  geom_ribbon(data = data_daily_profiles, aes(ymin = Mean - SD, ymax = Mean + SD),\n              fill = \"firebrick2\", alpha = 0.4) +\n  geom_line(linewidth = 0.9) + \n  theme(title = element_text(size = 14),\n        axis.text = element_text(size = 10),\n        axis.title = element_text(size = 12, face = \"bold\")) +\n  labs(title = \"Mean daily profile +- deviation (SD)\") +\n  labs(x = \"Time\", y = \"Load (kW)\")\nIt‚Äôs obvious that during winter months there is different behavior of electricity consumption, and much more consumed electricity because of heating needs.\nsmooth and ADAM\nThe main goal of this blog post is to try experimentally multistep losses in\nETS model\nand compare them against classical losses (MSE and MAE).\nIn\nadam\nfunction we have several possibilities for multistep losses, check in here: https://openforecast.org/adam/multistepLosses.html.\nI will use these in experiments:\nMSEh, MAEh\nTMSE ‚Äì Trace MSE\nGTMSE ‚Äì Geometric Trace MSE\nMSCE ‚Äì Mean Squared Cumulative Error\nGPL ‚Äì General Predictive Likelihood\nLet‚Äôs try\nadam\non 4 weeks sample of data.\nwin_days <- 28 # 4 weeks\nall_dates <- data_household_consumption[, unique(date(Date_Time))]\ndata_household_consumption_4weeks <- copy(data_household_consumption[Date_Time >= all_dates[1] & Date_Time < all_dates[win_days + 1]])\nFirstly, I will try only the additive ETS model with classic MSE loss.\nadam_model_classic <- adam(data = data_household_consumption_4weeks[, Load],\n                           model = \"XXX\",\n                           lags = c(1, 96, 96*7),\n                           loss = \"MSE\",\n                           h = 96,\n                           holdout = T,\n                           ic = \"AICc\",\n                           initial = \"back\")\n \nadam_model_classic\n## Time elapsed: 0.99 seconds\n## Model estimated using adam() function: ETS(ANA)[96, 672]\n## Distribution assumed in the model: Normal\n## Loss function type: MSE; Loss function value: 0.2732\n## Persistence vector g:\n##  alpha gamma1 gamma2 \n## 0.5246 0.0000 0.0791 \n## \n## Sample size: 2592\n## Number of estimated parameters: 3\n## Number of degrees of freedom: 2589\n## Information criteria:\n##      AIC     AICc      BIC     BICc \n## 3998.436 3998.445 4016.017 4016.053 \n## \n## Forecast errors:\n## ME: 0.041; MAE: 0.534; RMSE: 0.726\n## sCE: 425.014%; Asymmetry: -15.5%; sMAE: 57.586%; sMSE: 61.353%\n## MASE: 1.304; RMSSE: 1.2; rMAE: 0.845; rRMSE: 0.897\nplot(adam_model_classic, which = 7)\nWe can see that\nANA\nmodel is selected, so additive on level and season components, trend component was not used.\nLet‚Äôs try multistep loss - GPL.\nadam_model_multistep <- adam(data = data_household_consumption_4weeks[, Load],\n                             model = \"XXX\",\n                             lags = c(1, 96, 96*7),\n                             loss = \"GPL\",\n                             h = 96,\n                             holdout = T,\n                             ic = \"AICc\",\n                             initial = \"back\")\n \nadam_model_multistep\n## Time elapsed: 25.37 seconds\n## Model estimated using adam() function: ETS(ANA)[96, 672]\n## Distribution assumed in the model: Normal\n## Loss function type: GPL; Loss function value: -135.4807\n## Persistence vector g:\n##  alpha gamma1 gamma2 \n## 0.0004 0.0000 0.0884 \n## \n## Sample size: 2592\n## Number of estimated parameters: 3\n## Number of degrees of freedom: 2589\n## Information criteria:\n##      AIC     AICc      BIC     BICc \n## 3703.800 3703.809 3721.380 3721.417 \n## \n## Forecast errors:\n## ME: 0.075; MAE: 0.526; RMSE: 0.732\n## sCE: 778.763%; Asymmetry: -7.8%; sMAE: 56.684%; sMSE: 62.332%\n## MASE: 1.284; RMSSE: 1.21; rMAE: 0.832; rRMSE: 0.904\nplot(adam_model_multistep, which = 7)\nAlso, ANA was selected.\nBetter AICc and MAE with multistep loss, but RMSE was worse than in the MSE case.\nLet‚Äôs see on whole dataset‚Ä¶\nPrediction simulations\nI will roll predictions day by day with a whole 1-day prediction horizon with a train-set of length 28 days (\nwin_days\n) on our household consumption data -> so 214 predictions will be made.\nI will use all multistep losses mentioned above, MAE, MSE, and STL+ARIMA, STL+ETS methods from the\nforecast\npackage as benchmarks (as was used also in my previous blog post about\nbootstrapping\n).\nYou can check\nR script\nhow to simulate and compute this in parallel on my GitHub repository.\nNow, let‚Äôs see generated predictions.\nload(\"_rmd/data_household_predictions.Rdata\")\ndata_predictions_all\n##                   Date_Time Load_real Load_predicted    model\n##      1: 2023-01-30 00:00:00     0.274     -0.7679345  STL+ETS\n##      2: 2023-01-30 00:15:00     0.363     -0.7959601  STL+ETS\n##      3: 2023-01-30 00:30:00     0.342     -0.7443988  STL+ETS\n##      4: 2023-01-30 00:45:00     0.391     -0.8461708  STL+ETS\n##      5: 2023-01-30 01:00:00     0.230     -0.8708582  STL+ETS\n##     ---                                                      \n## 205436: 2023-08-31 22:45:00     0.080      0.4030999 ADAM-GPL\n## 205437: 2023-08-31 23:00:00     0.039      0.2060178 ADAM-GPL\n## 205438: 2023-08-31 23:15:00     0.087      0.2489654 ADAM-GPL\n## 205439: 2023-08-31 23:30:00     0.080      0.1719251 ADAM-GPL\n## 205440: 2023-08-31 23:45:00     0.040      0.4710469 ADAM-GPL\nI will compute forecasting evaluation metrics as RMSE, MAE, and\nMAAPE\n.\ndata_predictions_all[, .(RMSE = rmse(Load_real, Load_predicted),\n                         MAE = mae(Load_real, Load_predicted),\n                         MAAPE = maape(Load_real, Load_predicted)),\n                     by = .(model)][order(MAE)]\n##          model      RMSE       MAE    MAAPE\n##  1:  ADAM-MSCE 0.7645952 0.5588521 77.19498\n##  2:  ADAM-MAEh 0.7768230 0.5596729 75.60581\n##  3:  ADAM-TMSE 0.7703792 0.5622345 77.03544\n##  4:  ADAM-MSEh 0.7713433 0.5623389 76.99981\n##  5: ADAM-GTMSE 0.7704186 0.5626845 77.15252\n##  6:   ADAM-GPL 0.7704062 0.5629255 77.17820\n##  7:   ADAM-MAE 0.8690460 0.6133876 78.18846\n##  8:   ADAM-MSE 0.8701501 0.6152678 78.22642\n##  9:  STL+ARIMA 0.8751174 0.6159549 78.60016\n## 10:    STL+ETS 0.9403524 0.6500436 76.13823\nWe can see that all multistep losses are better than benchmark methods! That‚Äôs very nice results!\nFor further analysis, we will check only the two best multistep losses and two benchmarks.\nmodels_sub <- c(\"ADAM-MSCE\", \"ADAM-MAEh\", \"ADAM-MAE\", \"STL+ARIMA\")\nLet‚Äôs see all generated prediction graphs with selected models:\nggplot(data_predictions_all[model %in% models_sub], aes(Date_Time, Load_predicted)) +\n  facet_wrap(~model, ncol = 1) +\n  geom_line(data = data_predictions_all[model %in% models_sub],\n            aes(Date_Time, Load_real),\n            color = \"black\") +\n  geom_line(color = \"firebrick1\") +\n  theme_bw()\nThere is a much lower variance of predictions with multistep losses ETS than in benchmarks, which‚Äôs interesting, but in nature of these losses logical also -> again check https://openforecast.org/adam/multistepLosses.html#multistepLosses.\nModels diagnostics\nNext, I will try to demonstrate how to compare results from multiple models and analyze them with model diagnostic methods, which are also described in\nadam forecasting book\n.\nLet‚Äôs create residuals and abs(residuals) columns for investigation purposes.\ndata_predictions_all[, Error := Load_real - Load_predicted]\ndata_predictions_all[, AE := abs(Error)]\nLet‚Äôs see the boxplot of absolute errors:\nggplot(data_predictions_all[model %in% models_sub],\n       aes(model, AE, fill = model)) +\n  geom_boxplot() +\n  theme_bw()\nWe see narrower IQR with multistep losses, that‚Äôs a good feature.\nPredictions vs real values scatter plot:\nggplot(data_predictions_all[model %in% models_sub],\n       aes(Load_predicted, Load_real)) +\n  facet_wrap(~model, scales = \"fixed\", ncol = 2) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", \n              linetype = \"dashed\", linewidth = 1) +\n  geom_smooth(alpha = 0.4) +\n  labs(title = \"Predicted vs Real values\") +\n  theme_bw()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nThat is not very nice behavior in general, actually :), there are a lot of below-zero predictions (but fewer with multistep losses!) and no high-load predictions with multistep losses.\nResiduals vs real values scatter plot:\nggplot(data_predictions_all[model %in% models_sub],\n       aes(Error, Load_real)) +\n  facet_wrap(~model, scales = \"fixed\") +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  xlab(\"resid (predicted_value - Load_real)\") +\n  labs(title = \"Residual vs Real values\") +\n  theme_bw()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nAgain, it is not a very nice plot üôÇ the conclusions are very similar to with previous one.\nHeteroscedasticity check - so real values vs. absolute errors.\nggplot(data_predictions_all[model %in% models_sub],\n       aes(Load_real, AE)) +\n  facet_wrap(~model, scales = \"fixed\") +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.6) +\n  ylab(\"abs(resid)\") +\n  labs(title = \"heteroscedasticity check\") + \n  theme_bw()\n## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\nOur models have no heteroscedasticity at all, so absolute errors are not similarly distributed around real values distribution.\nLet‚Äôs see assumptions of normally distributed errors - Q-Q plot check:\nggplot(data_predictions_all[model %in% models_sub],\n       aes(sample = Error, group = model)) +\n  facet_wrap(~model, scales = \"fixed\") +\n  stat_qq() + stat_qq_line() +\n  ylab(\"resid (predicted_value - Load_real)\") +\n  theme_bw()\nWe can see a nicer Q-Q plot with multistep losses - values are sliding on the optimal line much more than with benchmarks.\nThe last check is about the assumption that multistep forecasting errors have zero mean (will check it hourly - not by 15 minutes):\ndata_predictions_all[, Hour := lubridate::hour(Date_Time)]\n \nggplot(data_predictions_all[model %in% models_sub],\n       aes(Hour, Error, group = Hour)) +\n  facet_wrap(~model, scales = \"free_y\", ncol = 1) +\n  geom_boxplot(alpha = 0.7, color = \"black\", fill = \"gray\") +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_point(data = data_predictions_all[model %in% models_sub,\n                                         .(Error = mean(Error)),\n                                         by = .(model, Hour)],\n             aes(Hour, Error, group = Hour), color = \"firebrick2\") +\n  xlab(\"Hour (horizon)\") +\n  ylab(\"resid (predicted_value - Load_real)\") +\n  labs(title = \"Multistep forecast errors have zero mean check\") + \n  theme_bw()\nAll models are quite around zero, so good.\nSummary\nIn this blog post, I showed that multistep losses in the ETS model have a great impact on forecast behavior:\nlower variance,\nand more accurate forecasts,\nin our case of 1 day ahead household electricity consumption forecasting.\nIn general, household electricity consumption forecasting with ETS is not satisfactory as seen with model diagnostic methods that were showed -> higher values of consumption load are not well covered, etc.\nIn the future, I would like to show the possibility of improving forecasting accuracy with some ensemble methods, even with an ensemble of ‚Äúweak‚Äù models such as ETS.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nPeter Laurinec\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Recently, during work on some forecasting task at PowereX, I stumbled upon an interesting improvement in time series forecasting modeling. In general, in regression modeling, we use evaluation/loss metrics based on one step ahead (one value) horizon. B...",
      "meta_keywords": null,
      "og_description": "Recently, during work on some forecasting task at PowereX, I stumbled upon an interesting improvement in time series forecasting modeling. In general, in regression modeling, we use evaluation/loss metrics based on one step ahead (one value) horizon. B...",
      "og_image": "https://petolau.github.io/images/post_14/unnamed-chunk-3-1.png",
      "og_title": "Multistep horizon loss optimized forecasting with ADAM | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 11.4,
      "sitemap_lastmod": "2023-10-26T00:00:00+00:00",
      "twitter_description": "Recently, during work on some forecasting task at PowereX, I stumbled upon an interesting improvement in time series forecasting modeling. In general, in regression modeling, we use evaluation/loss metrics based on one step ahead (one value) horizon. B...",
      "twitter_title": "Multistep horizon loss optimized forecasting with ADAM | R-bloggers",
      "url": "https://www.r-bloggers.com/2023/10/multistep-horizon-loss-optimized-forecasting-with-adam/",
      "word_count": 2272
    }
  }
}