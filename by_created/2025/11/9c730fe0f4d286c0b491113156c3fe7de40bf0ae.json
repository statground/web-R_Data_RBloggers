{
  "id": "9c730fe0f4d286c0b491113156c3fe7de40bf0ae",
  "url": "https://www.r-bloggers.com/2025/05/impact-of-encoding/",
  "created_at_utc": "2025-11-22T19:58:33Z",
  "data": null,
  "raw_original": {
    "uuid": "e729d8ff-243d-4fe9-9817-d210f928cec0",
    "created_at": "2025-11-22 19:58:33",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/05/impact-of-encoding/",
      "crawled_at": "2025-11-22T10:48:34.905918",
      "external_links": [
        {
          "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-impact-encoding/",
          "text": "mlr-org"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://github.com/mlr-org/mlr3website/",
          "text": "GitHub"
        },
        {
          "href": "https://dl.acm.org/doi/10.1145/2939672.2939785",
          "text": "Chen and Guestrin, 2016"
        },
        {
          "href": "https://dl.acm.org/doi/10.1145/507533.507538",
          "text": "Micci-Barreca 2001"
        },
        {
          "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-impact-encoding/",
          "text": "mlr-org"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Impact of Encoding | R-bloggers",
      "images": [],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/giuseppe-casalicchio/",
          "text": "Giuseppe Casalicchio"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392887 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Impact of Encoding</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 21, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/giuseppe-casalicchio/\">Giuseppe Casalicchio</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-impact-encoding/\"> mlr-org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n<strong>JavaScript is required to unlock solutions.</strong><br/>\n    Please enable JavaScript and reload the page,<br/>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" rel=\"nofollow\" target=\"_blank\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n<section class=\"level1\" id=\"goal\">\n<h1>Goal</h1>\n<p>Apply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem.</p>\n</section>\n<section class=\"level1\" id=\"house-prices-in-king-county\">\n<h1>House Prices in King county</h1>\n<p>In this exercise, we want to model house sale prices in King county in the state of Washington, USA.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>set.seed(124)\nlibrary(mlr3verse)\nlibrary(\"mlr3tuningspaces\")\ndata(\"kc_housing\", package = \"mlr3data\")</pre>\n</div>\n<p>We do some simple feature pre-processing first:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre># Transform time to numeric variable:\nlibrary(anytime)\ndates = anytime(kc_housing$date)\nkc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\"))\n# Scale prices:\nkc_housing$price = kc_housing$price / 1000\n# For this task, delete columns containing NAs:\nkc_housing[,c(13, 15)] = NULL\n# Create factor columns:\nkc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])})\n# Get an overview:\nstr(kc_housing)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>'data.frame':   21613 obs. of  18 variables:\n $ date         : num  164 221 299 221 292 ...\n $ price        : num  222 538 180 604 510 ...\n $ bedrooms     : int  3 3 2 4 3 4 3 3 3 3 ...\n $ bathrooms    : num  1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ...\n $ sqft_living  : int  1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ...\n $ sqft_lot     : int  5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ...\n $ floors       : num  1 2 1 1 1 1 2 1 1 2 ...\n $ waterfront   : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ...\n $ view         : int  0 0 0 0 0 0 0 0 0 0 ...\n $ condition    : int  3 3 3 5 3 3 3 3 3 3 ...\n $ grade        : int  7 7 6 7 8 11 7 7 7 7 ...\n $ sqft_above   : int  1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ...\n $ yr_built     : int  1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ...\n $ zipcode      : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ...\n $ lat          : num  47.5 47.7 47.7 47.5 47.6 ...\n $ long         : num  -122 -122 -122 -122 -122 ...\n $ sqft_living15: int  1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ...\n $ sqft_lot15   : int  5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ...\n - attr(*, \"index\")= int(0) </pre>\n</div>\n</div>\n</section>\n<section class=\"level1\" id=\"train-test-split\">\n<h1>Train-test Split</h1>\n<p>Before we train a model, let’s reserve some data for evaluating our model later on:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>task = as_task_regr(kc_housing, target = \"price\")\nsplit = partition(task, ratio = 0.6)\n\ntasktrain = task$clone()\ntasktrain$filter(split$train)\ntasktrain</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;TaskRegr:kc_housing&gt; (12968 x 18)\n* Target: price\n* Properties: -\n* Features (17):\n  - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15,\n    view, yr_built\n  - dbl (5): bathrooms, date, floors, lat, long\n  - fct (2): waterfront, zipcode</pre>\n</div>\n<pre>tasktest = task$clone()\ntasktest$filter(split$test)\ntasktest</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;TaskRegr:kc_housing&gt; (8645 x 18)\n* Target: price\n* Properties: -\n* Features (17):\n  - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15,\n    view, yr_built\n  - dbl (5): bathrooms, date, floors, lat, long\n  - fct (2): waterfront, zipcode</pre>\n</div>\n</div>\n</section>\n<section class=\"level1\" id=\"xgboost\">\n<h1>XGBoost</h1>\n<p>XGBoost (<a href=\"https://dl.acm.org/doi/10.1145/2939672.2939785\" rel=\"nofollow\" target=\"_blank\">Chen and Guestrin, 2016</a> is a highly performant library for gradient-boosted trees. As some other ML learners, it cannot handle categorical data, so categorical features must be encoded as numerical variables. In the King county data, there are two categorical features encoded as <code>factor</code>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>ft = task$feature_types\nft[ft[[2]] == \"factor\"]</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Key: &lt;id&gt;\n           id   type\n       &lt;char&gt; &lt;char&gt;\n1: waterfront factor\n2:    zipcode factor</pre>\n</div>\n</div>\n<p>Categorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM). For now, let’s check the cardinality of <code>waterfront</code> and <code>zipcode</code>:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>lengths(task$levels())</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>waterfront    zipcode \n         2         70 </pre>\n</div>\n</div>\n<p>Obviously, <code>waterfront</code> is a low-cardinality feature suitable for dummy (also called treatment) encoding and <code>zipcode</code> is a very high-cardinality feature. Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding.</p>\n</section>\n<section class=\"level1\" id=\"impact-encoding\">\n<h1>Impact encoding</h1>\n<p>Impact encoding (<a href=\"https://dl.acm.org/doi/10.1145/507533.507538\" rel=\"nofollow\" target=\"_blank\">Micci-Barreca 2001</a>) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:</p>\n<ol type=\"1\">\n<li>Group the target variable by the categorical feature.</li>\n<li>Compute the mean of the target variable for each group.</li>\n<li>Compute the global mean of the target variable.</li>\n<li>Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.</li>\n<li>Replace the categorical feature with the impact scores.</li>\n</ol>\n<p>Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data.</p>\n</section>\n<section class=\"level1\" id=\"exercises\">\n<h1>Exercises</h1>\n<section class=\"level2\" id=\"exercise-1-create-a-pipeline\">\n<h2 class=\"anchored\" data-anchor-id=\"exercise-1-create-a-pipeline\">Exercise 1: Create a pipeline</h2>\n<p>Create a pipeline that pre-processes each factor variable with impact encoding. The pipeline should run an <code>autotuner</code> that automatically conducts hyperparameter optimization (HPO) with an XGBoost learner that learns on the pre-processed features using random search and MSE as performance measure. You can use CV with a suitable number of folds for the resampling stragegy. For the search space, you can use <code>lts(\"regr.xgboost.default\")</code> from the <code>mlr3tuningspaces</code> package. This constructs a search space customized for Xgboost based on theoretically and empirically validated considerations on which variables to tune or not. However, you should set the parameter <code>nrounds = 100</code> for speed reasons. Further, set <code>nthread = parallel::detectCores()</code> to prepare multi-core computing later on.</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>The pipeline must be embedded in the <code>autotuner</code>: the learner supplied to the <code>autotuner</code> must include the feature preprocessing and the XGboost learner.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre># Create xgboost learner:\nxgb = lrn(...)\n\n# Set search space from mlr3tuningspaces:\nxgb_ts = ...\n\n# Set nrounds and nthread:\nxgb_ts$... = ....\nxgb_ts$... = ....\n\n# Combine xgb_ts with impact encoding:\nxgb_ts_impact = as_learner(...)\n\n# Use random search:\ntuner = tnr(...)\n\n#Autotuner pipeline component:\nat = auto_tuner(\n  tuner = ...,\n  learner = ...,\n  search_space = ...,\n  resampling = ...,\n  measure = ...,\n  term_time = ...) # Maximum allowed time in seconds.\n\n# Combine pipeline:\nglrn_xgb_impact = as_learner(...)\nglrn_xgb_impact$id = \"XGB_enc_impact\"</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-1\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-1-contents callout-collapse collapse\" id=\"callout-1\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-d002e7174d1f2db12e794334-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create xgboost learner </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>xgb <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.xgboost&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(xgb<span class="sc">$</span>param_set<span class="sc">$</span>values)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   nrounds nthread verbose
     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;
1:    1000       1       0</code></pre>
</div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set search space from mlr3tuningspaces</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>xgb_ts <span class="ot">=</span> <span class="fu">lts</span>(xgb)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(xgb_ts<span class="sc">$</span>param_set<span class="sc">$</span>values)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                   alpha colsample_bylevel colsample_bytree                                eta
                        &lt;RangeTuneToken&gt;  &lt;RangeTuneToken&gt; &lt;RangeTuneToken&gt;                   &lt;RangeTuneToken&gt;
1:                             &lt;list[3]&gt;         &lt;list[3]&gt;        &lt;list[3]&gt;                          &lt;list[3]&gt;
2: to_tune(0.001, 1000, logscale = TRUE)   to_tune(0.1, 1)  to_tune(0.1, 1) to_tune(1e-04, 1, logscale = TRUE)
                                  lambda        max_depth          nrounds nthread        subsample verbose
                        &lt;RangeTuneToken&gt; &lt;RangeTuneToken&gt; &lt;RangeTuneToken&gt;   &lt;int&gt; &lt;RangeTuneToken&gt;   &lt;int&gt;
1:                             &lt;list[3]&gt;        &lt;list[3]&gt;        &lt;list[3]&gt;       1        &lt;list[3]&gt;       0
2: to_tune(0.001, 1000, logscale = TRUE)   to_tune(1, 20) to_tune(1, 5000)       1  to_tune(0.1, 1)       0</code></pre>
</div>
<div class="sourceCode" id="cb5"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set nrounds and nthread:</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>xgb_ts<span class="sc">$</span>param_set<span class="sc">$</span>values<span class="sc">$</span>nrounds <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>xgb_ts<span class="sc">$</span>param_set<span class="sc">$</span>values<span class="sc">$</span>nthread <span class="ot">=</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine xgb_ts with impact encoding</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>xgb_ts_impact <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">&quot;encodeimpact&quot;</span>) <span class="sc">%&gt;&gt;%</span> xgb_ts)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(xgb_ts_impact<span class="sc">$</span>param_set<span class="sc">$</span>values)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   encodeimpact.smoothing encodeimpact.impute_zero                    regr.xgboost.alpha regr.xgboost.colsample_bylevel
                    &lt;num&gt;                   &lt;lgcl&gt;                      &lt;RangeTuneToken&gt;               &lt;RangeTuneToken&gt;
1:                  1e-04                    FALSE                             &lt;list[3]&gt;                      &lt;list[3]&gt;
2:                  1e-04                    FALSE to_tune(0.001, 1000, logscale = TRUE)                to_tune(0.1, 1)
   regr.xgboost.colsample_bytree                   regr.xgboost.eta                   regr.xgboost.lambda
                &lt;RangeTuneToken&gt;                   &lt;RangeTuneToken&gt;                      &lt;RangeTuneToken&gt;
1:                     &lt;list[3]&gt;                          &lt;list[3]&gt;                             &lt;list[3]&gt;
2:               to_tune(0.1, 1) to_tune(1e-04, 1, logscale = TRUE) to_tune(0.001, 1000, logscale = TRUE)
   regr.xgboost.max_depth regr.xgboost.nrounds regr.xgboost.nthread regr.xgboost.subsample regr.xgboost.verbose
         &lt;RangeTuneToken&gt;                &lt;int&gt;                &lt;int&gt;       &lt;RangeTuneToken&gt;                &lt;int&gt;
1:              &lt;list[3]&gt;                  100                   10              &lt;list[3]&gt;                    0
2:         to_tune(1, 20)                  100                   10        to_tune(0.1, 1)                    0</code></pre>
</div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use random search:</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">&quot;random_search&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create auto-tuned xgboost</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>auto_xgb_impact <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> tuner,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> xgb_ts_impact,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">2</span>),</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">&quot;regr.mse&quot;</span>),</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_time =</span> <span class="dv">20</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"exercise-2-benchmark-a-pipeline\">\n<h2 class=\"anchored\" data-anchor-id=\"exercise-2-benchmark-a-pipeline\">Exercise 2: Benchmark a pipeline</h2>\n<p>Benchmark your impact encoding pipeline from the previous task against a simple one-hot encoding pipeline that uses one-hot encoding for all factor variables. Use the same <code>autotuner</code> setup as element for both. Use two-fold CV as resampling strategy for the benchmark. Afterwards, evaluate the benchmark with MSE. Finally, assess the performance via the “untouched test set principle” by training both autotuners on <code>tasktrain</code> and evaluate their performance on <code>tasktest</code>.</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>To conduct the benchmark, use <code>benchmark(benchmark_grid(...))</code>.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<p>To conduct performance evaluation, use <code>$aggregate()</code> on the benchmark object.</p>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-2\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-2-contents callout-collapse collapse\" id=\"callout-2\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-d002e7174d1f2db12e794334-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"summary\">\n<h1>Summary</h1>\n<p>We learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments.</p>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-impact-encoding/\"> mlr-org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Impact of Encoding\nPosted on\nMay 21, 2025\nby\nGiuseppe Casalicchio\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nmlr-org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nJavaScript is required to unlock solutions.\nPlease enable JavaScript and reload the page,\nor download the source files from\nGitHub\nand run the code locally.\nGoal\nApply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem.\nHouse Prices in King county\nIn this exercise, we want to model house sale prices in King county in the state of Washington, USA.\nset.seed(124)\nlibrary(mlr3verse)\nlibrary(\"mlr3tuningspaces\")\ndata(\"kc_housing\", package = \"mlr3data\")\nWe do some simple feature pre-processing first:\n# Transform time to numeric variable:\nlibrary(anytime)\ndates = anytime(kc_housing$date)\nkc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\"))\n# Scale prices:\nkc_housing$price = kc_housing$price / 1000\n# For this task, delete columns containing NAs:\nkc_housing[,c(13, 15)] = NULL\n# Create factor columns:\nkc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])})\n# Get an overview:\nstr(kc_housing)\n'data.frame':   21613 obs. of  18 variables:\n $ date         : num  164 221 299 221 292 ...\n $ price        : num  222 538 180 604 510 ...\n $ bedrooms     : int  3 3 2 4 3 4 3 3 3 3 ...\n $ bathrooms    : num  1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ...\n $ sqft_living  : int  1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ...\n $ sqft_lot     : int  5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ...\n $ floors       : num  1 2 1 1 1 1 2 1 1 2 ...\n $ waterfront   : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ...\n $ view         : int  0 0 0 0 0 0 0 0 0 0 ...\n $ condition    : int  3 3 3 5 3 3 3 3 3 3 ...\n $ grade        : int  7 7 6 7 8 11 7 7 7 7 ...\n $ sqft_above   : int  1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ...\n $ yr_built     : int  1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ...\n $ zipcode      : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ...\n $ lat          : num  47.5 47.7 47.7 47.5 47.6 ...\n $ long         : num  -122 -122 -122 -122 -122 ...\n $ sqft_living15: int  1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ...\n $ sqft_lot15   : int  5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ...\n - attr(*, \"index\")= int(0)\nTrain-test Split\nBefore we train a model, let’s reserve some data for evaluating our model later on:\ntask = as_task_regr(kc_housing, target = \"price\")\nsplit = partition(task, ratio = 0.6)\n\ntasktrain = task$clone()\ntasktrain$filter(split$train)\ntasktrain\n<TaskRegr:kc_housing> (12968 x 18)\n* Target: price\n* Properties: -\n* Features (17):\n  - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15,\n    view, yr_built\n  - dbl (5): bathrooms, date, floors, lat, long\n  - fct (2): waterfront, zipcode\ntasktest = task$clone()\ntasktest$filter(split$test)\ntasktest\n<TaskRegr:kc_housing> (8645 x 18)\n* Target: price\n* Properties: -\n* Features (17):\n  - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15,\n    view, yr_built\n  - dbl (5): bathrooms, date, floors, lat, long\n  - fct (2): waterfront, zipcode\nXGBoost\nXGBoost (\nChen and Guestrin, 2016\nis a highly performant library for gradient-boosted trees. As some other ML learners, it cannot handle categorical data, so categorical features must be encoded as numerical variables. In the King county data, there are two categorical features encoded as\nfactor\n:\nft = task$feature_types\nft[ft[[2]] == \"factor\"]\nKey: <id>\n           id   type\n       <char> <char>\n1: waterfront factor\n2:    zipcode factor\nCategorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM). For now, let’s check the cardinality of\nwaterfront\nand\nzipcode\n:\nlengths(task$levels())\nwaterfront    zipcode \n         2         70\nObviously,\nwaterfront\nis a low-cardinality feature suitable for dummy (also called treatment) encoding and\nzipcode\nis a very high-cardinality feature. Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding.\nImpact encoding\nImpact encoding (\nMicci-Barreca 2001\n) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:\nGroup the target variable by the categorical feature.\nCompute the mean of the target variable for each group.\nCompute the global mean of the target variable.\nCompute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.\nReplace the categorical feature with the impact scores.\nImpact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data.\nExercises\nExercise 1: Create a pipeline\nCreate a pipeline that pre-processes each factor variable with impact encoding. The pipeline should run an\nautotuner\nthat automatically conducts hyperparameter optimization (HPO) with an XGBoost learner that learns on the pre-processed features using random search and MSE as performance measure. You can use CV with a suitable number of folds for the resampling stragegy. For the search space, you can use\nlts(\"regr.xgboost.default\")\nfrom the\nmlr3tuningspaces\npackage. This constructs a search space customized for Xgboost based on theoretically and empirically validated considerations on which variables to tune or not. However, you should set the parameter\nnrounds = 100\nfor speed reasons. Further, set\nnthread = parallel::detectCores()\nto prepare multi-core computing later on.\nHint 1:\nThe pipeline must be embedded in the\nautotuner\n: the learner supplied to the\nautotuner\nmust include the feature preprocessing and the XGboost learner.\nHint 2:\n# Create xgboost learner:\nxgb = lrn(...)\n\n# Set search space from mlr3tuningspaces:\nxgb_ts = ...\n\n# Set nrounds and nthread:\nxgb_ts$... = ....\nxgb_ts$... = ....\n\n# Combine xgb_ts with impact encoding:\nxgb_ts_impact = as_learner(...)\n\n# Use random search:\ntuner = tnr(...)\n\n#Autotuner pipeline component:\nat = auto_tuner(\n  tuner = ...,\n  learner = ...,\n  search_space = ...,\n  resampling = ...,\n  measure = ...,\n  term_time = ...) # Maximum allowed time in seconds.\n\n# Combine pipeline:\nglrn_xgb_impact = as_learner(...)\nglrn_xgb_impact$id = \"XGB_enc_impact\"\nSolution\nUnlock solution\nExercise 2: Benchmark a pipeline\nBenchmark your impact encoding pipeline from the previous task against a simple one-hot encoding pipeline that uses one-hot encoding for all factor variables. Use the same\nautotuner\nsetup as element for both. Use two-fold CV as resampling strategy for the benchmark. Afterwards, evaluate the benchmark with MSE. Finally, assess the performance via the “untouched test set principle” by training both autotuners on\ntasktrain\nand evaluate their performance on\ntasktest\n.\nHint 1:\nTo conduct the benchmark, use\nbenchmark(benchmark_grid(...))\n.\nHint 2:\nTo conduct performance evaluation, use\n$aggregate()\non the benchmark object.\nSolution\nUnlock solution\nSummary\nWe learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nmlr-org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Apply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem. House Prices in King county In this exercise, we want to model house sale prices in King county in the state of Washington, USA. set.seed(124) library(mlr3verse) library(\"mlr3tuningspaces\") data(\"kc_housing\", package = \"mlr3data\") We do some simple feature pre-processing first: # Transform time to numeric variable: library(anytime) dates = anytime(kc_housing$date) kc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\")) # Scale prices: kc_housing$price = kc_housing$price / 1000 # For this task, delete columns containing NAs: kc_housing[,c(13, 15)] = NULL # Create factor columns: kc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])}) # Get an overview: str(kc_housing) 'data.frame': 21613 obs. of 18 variables: $ date : num 164 221 299 221 292 ... $ price : num 222 538 180 604 510 ... $ bedrooms : int 3 3 2 4 3 4 3 3 3 3 ... $ bathrooms : num 1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ... $ sqft_living : int 1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ... $ sqft_lot : int 5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ... $ floors : num 1 2 1 1 1 1 2 1 1 2 ... $ waterfront : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ... $ view : int 0 0 0 0 0 0 0 0 0 0 ... $ condition : int 3 3 3 5 3 3 3 3 3 3 ... $ grade : int 7 7 6 7 8 11 7 7 7 7 ... $ sqft_above : int 1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ... $ yr_built : int 1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ... $ zipcode : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ... $ lat : num 47.5 47.7 47.7 47.5 47.6 ... $ long : num -122 -122 -122 -122 -122 ... $ sqft_living15: int 1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ... $ sqft_lot15 : int 5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ... - attr(*, \"index\")= int(0) Train-test Split Before we train a model, let’s reserve some data for evaluating our model later on: task = as_task_regr(kc_housing, target = \"price\") split = partition(task, ratio = 0.6) tasktrain = task$clone() tasktrain$filter(split$train) tasktrain (12968 x 18) * Target: price * Properties: - * Features (17): - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15, view, yr_built - dbl (5): bathrooms, date, floors, lat, long - fct (2): waterfront, zipcode tasktest = task$clone() tasktest$filter(split$test) tasktest (8645 x 18) * Target: price * Properties: - * Features (17): - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15, view, yr_built - dbl (5): bathrooms, date, floors, lat, long - fct (2): waterfront, zipcode XGBoost XGBoost (Chen and Guestrin, 2016 is a highly performant library for gradient-boosted trees. As some other ML learners, it cannot handle categorical data, so categorical features must be encoded as numerical variables. In the King county data, there are two categorical features encoded as factor: ft = task$feature_types ft[ft[[2]] == \"factor\"] Key: id type 1: waterfront factor 2: zipcode factor Categorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM). For now, let’s check the cardinality of waterfront and zipcode: lengths(task$levels()) waterfront zipcode 2 70 Obviously, waterfront is a low-cardinality feature suitable for dummy (also called treatment) encoding and zipcode is a very high-cardinality feature. Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. Impact encoding Impact encoding (Micci-Barreca 2001) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps: Group the target variable by the categorical feature. Compute the mean of the target variable for each group. Compute the global mean of the target variable. Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable. Replace the categorical feature with the impact scores. Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data. Exercises Exercise 1: Create a pipeline Create a pipeline that pre-processes each factor variable with impact encoding. The pipeline should run an autotuner that automatically conducts hyperparameter optimization (HPO) with an XGBoost learner that learns on the pre-processed features using random search and MSE as performance measure. You can use CV with a suitable number of folds for the resampling stragegy. For the search space, you can use lts(\"regr.xgboost.default\") from the mlr3tuningspaces package. This constructs a search space customized for Xgboost based on theoretically and empirically validated considerations on which variables to tune or not. However, you should set the parameter nrounds = 100 for speed reasons. Further, set nthread = parallel::detectCores() to prepare multi-core computing later on. Hint 1: The pipeline must be embedded in the autotuner: the learner supplied to the autotuner must include the feature preprocessing and the XGboost learner. Hint 2: # Create xgboost learner: xgb = lrn(...) # Set search space from mlr3tuningspaces: xgb_ts = ... # Set nrounds and nthread: xgb_ts$... = .... xgb_ts$... = .... # Combine xgb_ts with impact encoding: xgb_ts_impact = as_learner(...) # Use random search: tuner = tnr(...) #Autotuner pipeline component: at = auto_tuner( tuner = ..., learner = ..., search_space = ..., resampling = ..., measure = ..., term_time = ...) # Maximum allowed time in seconds. # Combine pipeline: glrn_xgb_impact = as_learner(...) glrn_xgb_impact$id = \"XGB_enc_impact\" Solution Unlock solution Exercise 2: Benchmark a pipeline Benchmark your impact encoding pipeline from the previous task against a simple one-hot encoding pipeline that uses one-hot encoding for all factor variables. Use the same autotuner setup as element for both. Use two-fold CV as resampling strategy for the benchmark. Afterwards, evaluate the benchmark with MSE. Finally, assess the performance via the “untouched test set principle” by training both autotuners on tasktrain and evaluate their performance on tasktest. Hint 1: To conduct the benchmark, use benchmark(benchmark_grid(...)). Hint 2: To conduct performance evaluation, use $aggregate() on the benchmark object. Solution Unlock solution Summary We learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments.",
      "meta_keywords": null,
      "og_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Apply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem. House Prices in King county In this exercise, we want to model house sale prices in King county in the state of Washington, USA. set.seed(124) library(mlr3verse) library(\"mlr3tuningspaces\") data(\"kc_housing\", package = \"mlr3data\") We do some simple feature pre-processing first: # Transform time to numeric variable: library(anytime) dates = anytime(kc_housing$date) kc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\")) # Scale prices: kc_housing$price = kc_housing$price / 1000 # For this task, delete columns containing NAs: kc_housing[,c(13, 15)] = NULL # Create factor columns: kc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])}) # Get an overview: str(kc_housing) 'data.frame': 21613 obs. of 18 variables: $ date : num 164 221 299 221 292 ... $ price : num 222 538 180 604 510 ... $ bedrooms : int 3 3 2 4 3 4 3 3 3 3 ... $ bathrooms : num 1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ... $ sqft_living : int 1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ... $ sqft_lot : int 5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ... $ floors : num 1 2 1 1 1 1 2 1 1 2 ... $ waterfront : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ... $ view : int 0 0 0 0 0 0 0 0 0 0 ... $ condition : int 3 3 3 5 3 3 3 3 3 3 ... $ grade : int 7 7 6 7 8 11 7 7 7 7 ... $ sqft_above : int 1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ... $ yr_built : int 1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ... $ zipcode : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ... $ lat : num 47.5 47.7 47.7 47.5 47.6 ... $ long : num -122 -122 -122 -122 -122 ... $ sqft_living15: int 1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ... $ sqft_lot15 : int 5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ... - attr(*, \"index\")= int(0) Train-test Split Before we train a model, let’s reserve some data for evaluating our model later on: task = as_task_regr(kc_housing, target = \"price\") split = partition(task, ratio = 0.6) tasktrain = task$clone() tasktrain$filter(split$train) tasktrain (12968 x 18) * Target: price * Properties: - * Features (17): - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15, view, yr_built - dbl (5): bathrooms, date, floors, lat, long - fct (2): waterfront, zipcode tasktest = task$clone() tasktest$filter(split$test) tasktest (8645 x 18) * Target: price * Properties: - * Features (17): - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15, view, yr_built - dbl (5): bathrooms, date, floors, lat, long - fct (2): waterfront, zipcode XGBoost XGBoost (Chen and Guestrin, 2016 is a highly performant library for gradient-boosted trees. As some other ML learners, it cannot handle categorical data, so categorical features must be encoded as numerical variables. In the King county data, there are two categorical features encoded as factor: ft = task$feature_types ft[ft[[2]] == \"factor\"] Key: id type 1: waterfront factor 2: zipcode factor Categorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM). For now, let’s check the cardinality of waterfront and zipcode: lengths(task$levels()) waterfront zipcode 2 70 Obviously, waterfront is a low-cardinality feature suitable for dummy (also called treatment) encoding and zipcode is a very high-cardinality feature. Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. Impact encoding Impact encoding (Micci-Barreca 2001) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps: Group the target variable by the categorical feature. Compute the mean of the target variable for each group. Compute the global mean of the target variable. Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable. Replace the categorical feature with the impact scores. Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data. Exercises Exercise 1: Create a pipeline Create a pipeline that pre-processes each factor variable with impact encoding. The pipeline should run an autotuner that automatically conducts hyperparameter optimization (HPO) with an XGBoost learner that learns on the pre-processed features using random search and MSE as performance measure. You can use CV with a suitable number of folds for the resampling stragegy. For the search space, you can use lts(\"regr.xgboost.default\") from the mlr3tuningspaces package. This constructs a search space customized for Xgboost based on theoretically and empirically validated considerations on which variables to tune or not. However, you should set the parameter nrounds = 100 for speed reasons. Further, set nthread = parallel::detectCores() to prepare multi-core computing later on. Hint 1: The pipeline must be embedded in the autotuner: the learner supplied to the autotuner must include the feature preprocessing and the XGboost learner. Hint 2: # Create xgboost learner: xgb = lrn(...) # Set search space from mlr3tuningspaces: xgb_ts = ... # Set nrounds and nthread: xgb_ts$... = .... xgb_ts$... = .... # Combine xgb_ts with impact encoding: xgb_ts_impact = as_learner(...) # Use random search: tuner = tnr(...) #Autotuner pipeline component: at = auto_tuner( tuner = ..., learner = ..., search_space = ..., resampling = ..., measure = ..., term_time = ...) # Maximum allowed time in seconds. # Combine pipeline: glrn_xgb_impact = as_learner(...) glrn_xgb_impact$id = \"XGB_enc_impact\" Solution Unlock solution Exercise 2: Benchmark a pipeline Benchmark your impact encoding pipeline from the previous task against a simple one-hot encoding pipeline that uses one-hot encoding for all factor variables. Use the same autotuner setup as element for both. Use two-fold CV as resampling strategy for the benchmark. Afterwards, evaluate the benchmark with MSE. Finally, assess the performance via the “untouched test set principle” by training both autotuners on tasktrain and evaluate their performance on tasktest. Hint 1: To conduct the benchmark, use benchmark(benchmark_grid(...)). Hint 2: To conduct performance evaluation, use $aggregate() on the benchmark object. Solution Unlock solution Summary We learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments.",
      "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
      "og_title": "Impact of Encoding | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 7.1,
      "sitemap_lastmod": null,
      "twitter_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Apply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem. House Prices in King county In this exercise, we want to model house sale prices in King county in the state of Washington, USA. set.seed(124) library(mlr3verse) library(\"mlr3tuningspaces\") data(\"kc_housing\", package = \"mlr3data\") We do some simple feature pre-processing first: # Transform time to numeric variable: library(anytime) dates = anytime(kc_housing$date) kc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\")) # Scale prices: kc_housing$price = kc_housing$price / 1000 # For this task, delete columns containing NAs: kc_housing[,c(13, 15)] = NULL # Create factor columns: kc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])}) # Get an overview: str(kc_housing) 'data.frame': 21613 obs. of 18 variables: $ date : num 164 221 299 221 292 ... $ price : num 222 538 180 604 510 ... $ bedrooms : int 3 3 2 4 3 4 3 3 3 3 ... $ bathrooms : num 1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ... $ sqft_living : int 1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ... $ sqft_lot : int 5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ... $ floors : num 1 2 1 1 1 1 2 1 1 2 ... $ waterfront : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ... $ view : int 0 0 0 0 0 0 0 0 0 0 ... $ condition : int 3 3 3 5 3 3 3 3 3 3 ... $ grade : int 7 7 6 7 8 11 7 7 7 7 ... $ sqft_above : int 1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ... $ yr_built : int 1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ... $ zipcode : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ... $ lat : num 47.5 47.7 47.7 47.5 47.6 ... $ long : num -122 -122 -122 -122 -122 ... $ sqft_living15: int 1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ... $ sqft_lot15 : int 5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ... - attr(*, \"index\")= int(0) Train-test Split Before we train a model, let’s reserve some data for evaluating our model later on: task = as_task_regr(kc_housing, target = \"price\") split = partition(task, ratio = 0.6) tasktrain = task$clone() tasktrain$filter(split$train) tasktrain (12968 x 18) * Target: price * Properties: - * Features (17): - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15, view, yr_built - dbl (5): bathrooms, date, floors, lat, long - fct (2): waterfront, zipcode tasktest = task$clone() tasktest$filter(split$test) tasktest (8645 x 18) * Target: price * Properties: - * Features (17): - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15, view, yr_built - dbl (5): bathrooms, date, floors, lat, long - fct (2): waterfront, zipcode XGBoost XGBoost (Chen and Guestrin, 2016 is a highly performant library for gradient-boosted trees. As some other ML learners, it cannot handle categorical data, so categorical features must be encoded as numerical variables. In the King county data, there are two categorical features encoded as factor: ft = task$feature_types ft[ft[[2]] == \"factor\"] Key: id type 1: waterfront factor 2: zipcode factor Categorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM). For now, let’s check the cardinality of waterfront and zipcode: lengths(task$levels()) waterfront zipcode 2 70 Obviously, waterfront is a low-cardinality feature suitable for dummy (also called treatment) encoding and zipcode is a very high-cardinality feature. Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. Impact encoding Impact encoding (Micci-Barreca 2001) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps: Group the target variable by the categorical feature. Compute the mean of the target variable for each group. Compute the global mean of the target variable. Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable. Replace the categorical feature with the impact scores. Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data. Exercises Exercise 1: Create a pipeline Create a pipeline that pre-processes each factor variable with impact encoding. The pipeline should run an autotuner that automatically conducts hyperparameter optimization (HPO) with an XGBoost learner that learns on the pre-processed features using random search and MSE as performance measure. You can use CV with a suitable number of folds for the resampling stragegy. For the search space, you can use lts(\"regr.xgboost.default\") from the mlr3tuningspaces package. This constructs a search space customized for Xgboost based on theoretically and empirically validated considerations on which variables to tune or not. However, you should set the parameter nrounds = 100 for speed reasons. Further, set nthread = parallel::detectCores() to prepare multi-core computing later on. Hint 1: The pipeline must be embedded in the autotuner: the learner supplied to the autotuner must include the feature preprocessing and the XGboost learner. Hint 2: # Create xgboost learner: xgb = lrn(...) # Set search space from mlr3tuningspaces: xgb_ts = ... # Set nrounds and nthread: xgb_ts$... = .... xgb_ts$... = .... # Combine xgb_ts with impact encoding: xgb_ts_impact = as_learner(...) # Use random search: tuner = tnr(...) #Autotuner pipeline component: at = auto_tuner( tuner = ..., learner = ..., search_space = ..., resampling = ..., measure = ..., term_time = ...) # Maximum allowed time in seconds. # Combine pipeline: glrn_xgb_impact = as_learner(...) glrn_xgb_impact$id = \"XGB_enc_impact\" Solution Unlock solution Exercise 2: Benchmark a pipeline Benchmark your impact encoding pipeline from the previous task against a simple one-hot encoding pipeline that uses one-hot encoding for all factor variables. Use the same autotuner setup as element for both. Use two-fold CV as resampling strategy for the benchmark. Afterwards, evaluate the benchmark with MSE. Finally, assess the performance via the “untouched test set principle” by training both autotuners on tasktrain and evaluate their performance on tasktest. Hint 1: To conduct the benchmark, use benchmark(benchmark_grid(...)). Hint 2: To conduct performance evaluation, use $aggregate() on the benchmark object. Solution Unlock solution Summary We learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments.",
      "twitter_title": "Impact of Encoding | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/05/impact-of-encoding/",
      "word_count": 1421
    }
  }
}