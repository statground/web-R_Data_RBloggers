{
  "id": "bf29200aca1f873cfc981285683e2236b8caa519",
  "url": "https://www.r-bloggers.com/2025/06/power-and-fragile-p-values-by-ellis2013nz/",
  "created_at_utc": "2025-11-22T19:58:25Z",
  "data": null,
  "raw_original": {
    "uuid": "c613699e-234b-4fb0-86cb-f4749434f49d",
    "created_at": "2025-11-22 19:58:25",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/06/power-and-fragile-p-values-by-ellis2013nz/",
      "crawled_at": "2025-11-22T10:47:44.762305",
      "external_links": [
        {
          "href": "https://freerangestats.info/blog/2025/06/08/power-and-fragile-p-values",
          "text": "free range statistics - R"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://journals.sagepub.com/doi/10.1177/25152459251323480",
          "text": "this article onpvalues in the psychology literature"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Replication_crisis",
          "text": "replication crisis"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Publication_bias#:~:text=This%20term%20suggests%20that%20negative,a%20bias%20in%20published%20research.",
          "text": "“file-drawer” problem"
        },
        {
          "href": "https://peerj.com/articles/1142/",
          "text": "this piece by Daniel Lakens"
        },
        {
          "href": "https://www.science.org/content/article/big-win-dubious-statistical-results-are-becoming-less-common-psychology",
          "text": "Science"
        },
        {
          "href": "https://bsky.app/profile/jbakcoleman.bsky.social/post/3lqyuqimtq22a",
          "text": "Bluesky"
        },
        {
          "href": "https://freerangestats.info/blog/2020/06/13/publication-reform",
          "text": "could be improved"
        },
        {
          "href": "https://freerangestats.info/blog/2025/06/08/power-and-fragile-p-values",
          "text": "free range statistics - R"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Power and ‘fragile’ p-values by @ellis2013nz | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/freerangestats.info/img/0295-fragile-diff.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/freerangestats.info/img/0295-fragile-n.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/free-range-statistics-r/",
          "text": "free range statistics - R"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392925 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Power and ‘fragile’ p-values by @ellis2013nz</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">June 7, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/free-range-statistics-r/\">free range statistics - R</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://freerangestats.info/blog/2025/06/08/power-and-fragile-p-values\"> free range statistics - R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><h3 id=\"do-fragile-p-values-tell-us-anything\">Do ‘fragile’ <i>p</i> values tell us anything?</h3>\n<p>I was interested recently to see <a href=\"https://journals.sagepub.com/doi/10.1177/25152459251323480\" rel=\"nofollow\" target=\"_blank\">this article on <i>p</i> values in the psychology literature</a> float across my social media feed. Paul C Bogdan makes the case that the severity of the <a href=\"https://en.wikipedia.org/wiki/Replication_crisis\" rel=\"nofollow\" target=\"_blank\">replication crisis</a> in science can be judged in part by the proportion of <i>p</i> values that are ‘fragile’,which he defines as between 0.01 and 0.05.</p>\n<p>Of course, concern at the proportion of <i>p</i> values that are ‘significant but only just’ is a stable feature of the replication crisis. One of the standing concerns with science is that researchers use questionable research practices to somehow nudge the <i>p</i> values down to just below the threshold deemed to be “signficant” evidence. Another standing concern is that researchers who might not use those practices in the analysis themselves will not publish or not be able to publish their null results, leaving a bias towards positive results in the published literature (the <a href=\"https://en.wikipedia.org/wiki/Publication_bias#:~:text=This%20term%20suggests%20that%20negative,a%20bias%20in%20published%20research.\" rel=\"nofollow\" target=\"_blank\">“file-drawer” problem</a>).</p>\n<p>Bogdan argues that for studies with 80% power (defined as 1 minus the probability of accepting the null hypothesis when there is in fact a real effect in the data), 26% of <i>p</i> values that are significant should be in this “fragile” range, based on simulations.</p>\n<p>The research Bogdan describes in the article linked above is a clever data processing exercise of published psychology literature to see what proportion of <i>p</i> values are in fact, “fragile” and how this changes over time. He finds that “From before the replication crisis (2004–2011) to today (2024), the overall percentage of significant <i>p</i> values in the fragile range has dropped from 32% to nearly 26%”. As 26% is about what we’d expect, if all the studies had power of 80%, then this is seen as good news.</p>\n<p>Is the replication crisis over? (to be fair, I don’t think Bogdan claims this last point).</p>\n<p>One of Bogdan’s own citations is <a href=\"https://peerj.com/articles/1142/\" rel=\"nofollow\" target=\"_blank\">this piece by Daniel Lakens</a>, which itself is a critique of a similar attempt at this earlier. Lakens argues “the changes in the ratio of fractions of p-values between 0.041–0.049 over the years are better explained by assuming the average power has decreased over time” rather than by changes in questionable research practices. I think I agree with Lakens on this.</p>\n<p>I just don’t think the 26% of significant <i>p</i> values to be ‘fragile’ is a solid enough benchmark to judge research pracices on.</p>\n<p>Anyway, all this intrigued me enough when it was discussed first in <a href=\"https://www.science.org/content/article/big-win-dubious-statistical-results-are-becoming-less-common-psychology\" rel=\"nofollow\" target=\"_blank\">Science</a> (as “a big win”) and then on <a href=\"https://bsky.app/profile/jbakcoleman.bsky.social/post/3lqyuqimtq22a\" rel=\"nofollow\" target=\"_blank\">Bluesky</a> for me to want to do my own simulations to see how changes in effect sizes and sample sizes would change that 26%. My hunch was 26% was based on assumptions that all studies have 80% power and (given power has to be calculated for some assumed but unobserved true effect size) that the actual difference in the real world is close to the difference assumed in making that power calculation. Both these assumptions are obviously extremely brittle, but what is the impact if they are wrong?</p>\n<p>From my rough playing out below, the impact is pretty material. We shouldn’t think that changes in the proportion of signficant <i>p</i> values that are between 0.01 and 0.05 tells us much about questionable research practices, because there is just too much else going on — pre-calculated power, how much power calculations and indeed the research that is chosen are based on a good reflection of reality, the size of differences we’re looking for, and sample sizes — confounding the whole thing.</p>\n<h3 id=\"do-your-own-research-simulations\">Do your own <s>research</s> simulations</h3>\n<p>To do this, I wrote a simple function <code>experiment</code> which draws two independent samples from two populations, all observations normally distributed. For my purposes the two sample sizes are going to be the same and the standard deviations the same in both populations; only the means differ by population. But this function is set up for a more general exploration if I’m ever motivated.</p>\n<h4 id=\"the-ideal-situation---researchers-power-calculation-matches-the-real-world\">The ideal situation – researcher’s power calculation matches the real world</h4>\n<p>With this function I first played around a bit to get a situation where the power is very close to 80%. I got this with sample sizes of 53 each and a difference in the means of the two populations of 0.55 (remembering each population has a standard distribtuion of N(0, 1)).</p>\n<p>I then checked this with a published power package, <i>Bulus, M. (2023). <code>pwrss</code>: Statistical Power and Sample Size Calculation Tools. R package version 0.3.1. https://CRAN.R-project.org/package=pwrss</i>. I’ve never used this before and just downloaded it to check I hadn’t made mistakes in my own calculations, and later I will use it to speed up some stuff.</p>\n<figure class=\"highlight\"><pre>library(pwrss)\nlibrary(tidyverse)\n\nexperiment &lt;- function(d, m1 = 0, sd1 = 1, sd2 = 1, n1 = 50, n2 = n1, seed = NULL){\n  if(!is.null(seed)){\n    set.seed(seed)\n  }\n  x1 &lt;- rnorm(n1, m1, sd1)\n  x2 &lt;- rnorm(n2 ,m1 + d, sd2)\n  t.test(x1, x2)$p.value\n}\n\nreps &lt;- 10000\nres &lt;- numeric(reps)\n\nfor(i in 1:reps){\n  res[i] &lt;- experiment(d = 0.55, n1 = 53)\n}</pre></figure>\n<p>Yes, that’s right, I’m using a <code>for</code> loop here. Why? Because it’s very readable, and very easy to write.</p>\n<p>Here’s what that gives us. My simulated power is 80%, Bulus’ package agrees with 80%, and 27% of the ‘signficant’ (at alpha = 0.05) <i>p</i> values are in the fragile range. This isn’t the same as 26% but it’s not a million miles away; it’s easy to imagine a few changes in the experiment that would lead to his 26% figure.</p>\n<pre>&gt; # power from simulation\n&gt; 1 - mean(res &gt; 0.05)\n[1] 0.7964\n&gt; \n&gt; # power from Bulus' package\n&gt; pwrss.t.2means(mu1 = 0.55, sd1 = 1, sd2 = 1, n2 = 53)\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.801 \n  n1 = 53 \n  n2 = 53 \n ------------------------------ \n Alternative = “not equal” \n Degrees of freedom = 104 \n Non-centrality parameter = 2.831 \n Type I error rate = 0.05 \n Type II error rate = 0.199 \n&gt; \n&gt; # Of those experiments that have 'significant' results, what proportion are in \n&gt; # the so-called fragile range (i.e. betwen 0.01 and 0.05)\n&gt; summ1 &lt;- mean(res &gt; 0.01 &amp; res &lt; 0.05) / mean(res &lt; 0.05)\n&gt; print(summ1)\n[1] 0.2746107\n</pre>\n<h4 id=\"changes-in-difference-and-in-sample-size\">Changes in difference and in sample size</h4>\n<p>I made some arbitrary calls in that first run — sample size about 50 observations in each group, and the difference about 0.5 standard deviations. What if I let the difference between the two populations be smaller or larger than this, and just set the number of observations to whatever is necessary to get 80% power? What change does this make to the proportion of <i>p</i> values that are ‘fragile’?</p>\n<p>It turns out it makes a <em>big</em> difference, as we see in these two charts:</p>\n<object data=\"https://freerangestats.info/img/0295-fragile-diff.svg\" type=\"image/svg+xml\" width=\"450\"><img data-lazy-src=\"https://i2.wp.com/freerangestats.info/img/0295-fragile-diff.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/freerangestats.info/img/0295-fragile-diff.png?w=450&amp;ssl=1\"/></noscript></object>\n<object data=\"https://freerangestats.info/img/0295-fragile-n.svg\" type=\"image/svg+xml\" width=\"450\"><img data-lazy-src=\"https://i2.wp.com/freerangestats.info/img/0295-fragile-n.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/freerangestats.info/img/0295-fragile-n.png?w=450&amp;ssl=1\"/></noscript></object>\n<p>These are simulations, still in the world where the researcher happens to guess the real world exactly right when they do their power calculation and determine a sample size to get 80% power. We see in the top chart that as the real world difference gets bigger, with constant power, the proportion of significant but ‘fragile’ <i>p</i> values goes up markedly. And the second chart shows the same simulations, but focusing on the variation in sample size which changes in compensation for the real world difference in populations, to maintain the same power. Bigger samples with the same power mean that you are looking for relatively smaller real world differences, and the proportion of significant <i>p</i> values that are ‘fragile’ gets smaller.</p>\n<p>Here’s the code that did these simulations:</p>\n<figure class=\"highlight\"><pre>#--------------varying difference and sample sizes---------------\npossible_diffs &lt;- 10:200 / 100 # measured in standard deviations\n\n# what sample size do we need to have 80% power\nn_for_power &lt;- sapply(possible_diffs, function(d){\n  as.numeric(pwrss.t.2means(mu1 = d, power = 0.8, verbose = FALSE)$n[1])\n})\n\nprop_fragile &lt;- numeric(length(possible_diffs))\n\n# This takes some minutes to run, could be better if parallelized or done in\n# Julia if we thought saving those minutes was important:\nfor(j in 1:length(possible_diffs)){\n  for(i in 1:reps){\n    res[i] &lt;- experiment(d = possible_diffs[j], n1 = n_for_power[j])\n  }\n  prop_fragile[j] &lt;- mean(res &gt; 0.01 &amp; res &lt; 0.05) / mean(res &lt; 0.05)\n}\n\n# Plot 1\ntibble(prop_fragile, possible_diffs) |&gt; \n  ggplot(aes(x = possible_diffs,y= prop_fragile)) +\n  geom_point()+\n  scale_y_continuous(label = percent) +\n  labs(x = \"Difference (in standard deviations) between two means\",\n       y = \"Proportion of significant p values \\nthat are between 0.01 and 0.05\",\n       title = \"Two sample tests for difference between two means with power = 80%\",\n       subtitle = \"t test for independent samples at a combination of sample size and population difference\\nneeded to give the desired power. Both populations are standard normal distributions.\")\n\n# Plot 2\ntibble(prop_fragile, n_for_power) |&gt; \n  ggplot(aes(x = n_for_power,y = prop_fragile)) +\n  geom_point() +\n  scale_x_sqrt() +\n  scale_y_continuous(label = percent) +\n  labs(x = \"Sample size needed to get 80% power for given difference of means\",\n       y = \"Proportion of significant p values \\nthat are between 0.01 and 0.05\",\n       title = \"Two sample tests for difference between two means with power = 80%\",\n       subtitle = \"t test for independent samples at a combination of sample size and population difference\\nneeded to give the desired power. Both populations are standard normal distributions.\")</pre></figure>\n<h4 id=\"relaxing-assumptions\">Relaxing assumptions</h4>\n<p>OK, so that was what we get when the power calculation was based on a true representation of the world, known before we did the experiment. Obviously this is never the case (or we’d not need to do experiments) — the actual difference between two populations might be bigger or smaller than we expected, it might actually be exactly zero, the shape and spread of the populations will differ from what we thought when we calculated the power, etc.</p>\n<p>I decided to try three simple breaks of the assumptions to see what impact they have on the 27% of <i>p</i> values that were fragile:</p>\n<ul>\n<li>The actual difference between populations is a random number, albeit on average is what is expected during the power calculation</li>\n<li>the actual difference between populations is a coin flip between exactly what was expected (when the power calculation was made) and zero (ie null hypothesis turns out to be true)</li>\n<li>the actual difference between population is a coin flip between a random number with average the same as expected and zero (ie a combination of the first two scenarios)</li>\n</ul>\n<figure class=\"highlight\"><pre>#------------------when true d isn't what was expected---------------\n\nreps &lt;- 10000\nres &lt;- numeric(reps)\n\n# we are going to let the actual difference deviate from that which was used\n# in the power calculation, but say that on average the planned-for difference\n# was correct\nfor(i in 1:reps){\n  res[i] &lt;- experiment(d = rnorm(1, 0.55, 0.5), n1 = 53)\n}\n\n# \"actual\" power:\n1 - mean(res &gt; 0.05)\n\n# proportion of so-called fragile p values is much less\nsumm2 &lt;- mean(res &gt; 0.01 &amp; res &lt; 0.05) / mean(res &lt; 0.05)\n\n#---------when true d is same as expected except half the time H0 is true---------\n\nfor(i in 1:reps){\n  res[i] &lt;- experiment(d = sample(c(0, 0.55), 1), n1 = 53)\n}\n\n\n# proportion of so-called fragile p values is now *more*\nsumm3 &lt;- mean(res &gt; 0.01 &amp; res &lt; 0.05) / mean(res &lt; 0.05)\n\n#---------when true d is random, AND half the time H0 is true---------\n\nfor(i in 1:reps){\n  res[i] &lt;- experiment(d = sample(c(0, rnorm(1, 0.55, 0.5)), 1), n1 = 53)\n}\n\n\n# proportion of so-called fragile p values is now less\nsumm4 &lt;- mean(res &gt; 0.01 &amp; res &lt; 0.05) / mean(res &lt; 0.05)\n\ntibble(`Context` = c(\n  \"Difference is as expected during power calculation\",\n  \"Difference is random, but on average is as expected\",\n  \"Difference is as expected, except half the time null hypothesis is true\",\n  \"Difference is random, AND null hypothesis true half the time\"\n), `Proportion of p-values that are fragile` = c(summ1, summ2, summ3, summ4)) |&gt; \n  mutate(across(where(is.numeric), \\(x) percent(x, accuracy = 1))) </pre></figure>\n<p>That gets us these interesting results:</p>\n<table auto;=\"\" auto;\"=\"\" class=\"lightable-material\" helvetica,=\"\" margin-left:=\"\" margin-right:=\"\" pro\",=\"\" sans=\"\" sans-serif;=\"\" source=\"\" style=\"font-family: \">\n<thead>\n<tr>\n<th style=\"text-align:left;\"> Context </th>\n<th style=\"text-align:left;\"> Proportion of p-values that are fragile </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left;\"> Difference is as expected during power calculation </td>\n<td style=\"text-align:left;\"> 27% </td>\n</tr>\n<tr>\n<td style=\"text-align:left;\"> Difference is random, but on average is as expected </td>\n<td style=\"text-align:left;\"> 16% </td>\n</tr>\n<tr>\n<td style=\"text-align:left;\"> Difference is as expected, except half the time null hypothesis is true </td>\n<td style=\"text-align:left;\"> 29% </td>\n</tr>\n<tr>\n<td style=\"text-align:left;\"> Difference is random, AND null hypothesis true half the time </td>\n<td style=\"text-align:left;\"> 20% </td>\n</tr>\n</tbody>\n</table>\n<hr/>\n<p>There’s a marked variation here in what proportion of <i>p</i> values is fragile. Arguably, the fourth of these scenarios is the closest approximation to the real world (although there is a lot of debate about this, how much are exactly-zero differences really plausible?) Either this, or the other realistic scenario (‘difference is random but on average is as expected’) gives a proportion of fragile <i>p</i> values well below the 27% we saw in our base scenario.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>There’s just too many factors impacting on the proportion of <i>p</i> values that will be between 0.01 and 0.05 to assume that variations in it are either an improvement or a worsening in research practices. These things include:</p>\n<ul>\n<li>When expected differences change and sample sizes change to go with them for a given level of power, it impacts materially on the proportion of fragile <i>p</i> values we’d expect to see</li>\n<li>When the real world differs from that expected by the researcher when they did their power calculation, it impacts materially on the proportion of fragile <i>p</i> values we’d expect to see</li>\n<li>Anyway, researchers don’t all set their sample sizes to give 80% power, for various reasons, some of them good and some not so good</li>\n</ul>\n<p>Final thought — none of the above tells us whether we have a replication crisis or not, and if so if it’s getting better or getting worse. As it happens, I tend to think we do have one and that it’s very serious. I think the peer review process works very poorly and <a href=\"https://freerangestats.info/blog/2020/06/13/publication-reform\" rel=\"nofollow\" target=\"_blank\">could be improved</a>, and academic publishing in general sets up terrible — and perhaps worsening — incentives. However, I think criticism in the past decade or so has led to improvements (such as more access to reproducible code and data, more pre-registration, general raised awareness), which is consistent really with Bogdan’s substantive argument here. I just don’t think the ‘fragile’ <i>p</i> values are much evidence either way, and if we monitor them at all we should do so with great caution.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://freerangestats.info/blog/2025/06/08/power-and-fragile-p-values\"> free range statistics - R</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Power and ‘fragile’ p-values by @ellis2013nz\nPosted on\nJune 7, 2025\nby\nfree range statistics - R\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nfree range statistics - R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nDo ‘fragile’\np\nvalues tell us anything?\nI was interested recently to see\nthis article on\np\nvalues in the psychology literature\nfloat across my social media feed. Paul C Bogdan makes the case that the severity of the\nreplication crisis\nin science can be judged in part by the proportion of\np\nvalues that are ‘fragile’,which he defines as between 0.01 and 0.05.\nOf course, concern at the proportion of\np\nvalues that are ‘significant but only just’ is a stable feature of the replication crisis. One of the standing concerns with science is that researchers use questionable research practices to somehow nudge the\np\nvalues down to just below the threshold deemed to be “signficant” evidence. Another standing concern is that researchers who might not use those practices in the analysis themselves will not publish or not be able to publish their null results, leaving a bias towards positive results in the published literature (the\n“file-drawer” problem\n).\nBogdan argues that for studies with 80% power (defined as 1 minus the probability of accepting the null hypothesis when there is in fact a real effect in the data), 26% of\np\nvalues that are significant should be in this “fragile” range, based on simulations.\nThe research Bogdan describes in the article linked above is a clever data processing exercise of published psychology literature to see what proportion of\np\nvalues are in fact, “fragile” and how this changes over time. He finds that “From before the replication crisis (2004–2011) to today (2024), the overall percentage of significant\np\nvalues in the fragile range has dropped from 32% to nearly 26%”. As 26% is about what we’d expect, if all the studies had power of 80%, then this is seen as good news.\nIs the replication crisis over? (to be fair, I don’t think Bogdan claims this last point).\nOne of Bogdan’s own citations is\nthis piece by Daniel Lakens\n, which itself is a critique of a similar attempt at this earlier. Lakens argues “the changes in the ratio of fractions of p-values between 0.041–0.049 over the years are better explained by assuming the average power has decreased over time” rather than by changes in questionable research practices. I think I agree with Lakens on this.\nI just don’t think the 26% of significant\np\nvalues to be ‘fragile’ is a solid enough benchmark to judge research pracices on.\nAnyway, all this intrigued me enough when it was discussed first in\nScience\n(as “a big win”) and then on\nBluesky\nfor me to want to do my own simulations to see how changes in effect sizes and sample sizes would change that 26%. My hunch was 26% was based on assumptions that all studies have 80% power and (given power has to be calculated for some assumed but unobserved true effect size) that the actual difference in the real world is close to the difference assumed in making that power calculation. Both these assumptions are obviously extremely brittle, but what is the impact if they are wrong?\nFrom my rough playing out below, the impact is pretty material. We shouldn’t think that changes in the proportion of signficant\np\nvalues that are between 0.01 and 0.05 tells us much about questionable research practices, because there is just too much else going on — pre-calculated power, how much power calculations and indeed the research that is chosen are based on a good reflection of reality, the size of differences we’re looking for, and sample sizes — confounding the whole thing.\nDo your own\nresearch\nsimulations\nTo do this, I wrote a simple function\nexperiment\nwhich draws two independent samples from two populations, all observations normally distributed. For my purposes the two sample sizes are going to be the same and the standard deviations the same in both populations; only the means differ by population. But this function is set up for a more general exploration if I’m ever motivated.\nThe ideal situation – researcher’s power calculation matches the real world\nWith this function I first played around a bit to get a situation where the power is very close to 80%. I got this with sample sizes of 53 each and a difference in the means of the two populations of 0.55 (remembering each population has a standard distribtuion of N(0, 1)).\nI then checked this with a published power package,\nBulus, M. (2023).\npwrss\n: Statistical Power and Sample Size Calculation Tools. R package version 0.3.1. https://CRAN.R-project.org/package=pwrss\n. I’ve never used this before and just downloaded it to check I hadn’t made mistakes in my own calculations, and later I will use it to speed up some stuff.\nlibrary(pwrss)\nlibrary(tidyverse)\n\nexperiment <- function(d, m1 = 0, sd1 = 1, sd2 = 1, n1 = 50, n2 = n1, seed = NULL){\n  if(!is.null(seed)){\n    set.seed(seed)\n  }\n  x1 <- rnorm(n1, m1, sd1)\n  x2 <- rnorm(n2 ,m1 + d, sd2)\n  t.test(x1, x2)$p.value\n}\n\nreps <- 10000\nres <- numeric(reps)\n\nfor(i in 1:reps){\n  res[i] <- experiment(d = 0.55, n1 = 53)\n}\nYes, that’s right, I’m using a\nfor\nloop here. Why? Because it’s very readable, and very easy to write.\nHere’s what that gives us. My simulated power is 80%, Bulus’ package agrees with 80%, and 27% of the ‘signficant’ (at alpha = 0.05)\np\nvalues are in the fragile range. This isn’t the same as 26% but it’s not a million miles away; it’s easy to imagine a few changes in the experiment that would lead to his 26% figure.\n> # power from simulation\n> 1 - mean(res > 0.05)\n[1] 0.7964\n> \n> # power from Bulus' package\n> pwrss.t.2means(mu1 = 0.55, sd1 = 1, sd2 = 1, n2 = 53)\n Difference between Two means \n (Independent Samples t Test) \n H0: mu1 = mu2 \n HA: mu1 != mu2 \n ------------------------------ \n  Statistical power = 0.801 \n  n1 = 53 \n  n2 = 53 \n ------------------------------ \n Alternative = “not equal” \n Degrees of freedom = 104 \n Non-centrality parameter = 2.831 \n Type I error rate = 0.05 \n Type II error rate = 0.199 \n> \n> # Of those experiments that have 'significant' results, what proportion are in \n> # the so-called fragile range (i.e. betwen 0.01 and 0.05)\n> summ1 <- mean(res > 0.01 & res < 0.05) / mean(res < 0.05)\n> print(summ1)\n[1] 0.2746107\nChanges in difference and in sample size\nI made some arbitrary calls in that first run — sample size about 50 observations in each group, and the difference about 0.5 standard deviations. What if I let the difference between the two populations be smaller or larger than this, and just set the number of observations to whatever is necessary to get 80% power? What change does this make to the proportion of\np\nvalues that are ‘fragile’?\nIt turns out it makes a\nbig\ndifference, as we see in these two charts:\nThese are simulations, still in the world where the researcher happens to guess the real world exactly right when they do their power calculation and determine a sample size to get 80% power. We see in the top chart that as the real world difference gets bigger, with constant power, the proportion of significant but ‘fragile’\np\nvalues goes up markedly. And the second chart shows the same simulations, but focusing on the variation in sample size which changes in compensation for the real world difference in populations, to maintain the same power. Bigger samples with the same power mean that you are looking for relatively smaller real world differences, and the proportion of significant\np\nvalues that are ‘fragile’ gets smaller.\nHere’s the code that did these simulations:\n#--------------varying difference and sample sizes---------------\npossible_diffs <- 10:200 / 100 # measured in standard deviations\n\n# what sample size do we need to have 80% power\nn_for_power <- sapply(possible_diffs, function(d){\n  as.numeric(pwrss.t.2means(mu1 = d, power = 0.8, verbose = FALSE)$n[1])\n})\n\nprop_fragile <- numeric(length(possible_diffs))\n\n# This takes some minutes to run, could be better if parallelized or done in\n# Julia if we thought saving those minutes was important:\nfor(j in 1:length(possible_diffs)){\n  for(i in 1:reps){\n    res[i] <- experiment(d = possible_diffs[j], n1 = n_for_power[j])\n  }\n  prop_fragile[j] <- mean(res > 0.01 & res < 0.05) / mean(res < 0.05)\n}\n\n# Plot 1\ntibble(prop_fragile, possible_diffs) |> \n  ggplot(aes(x = possible_diffs,y= prop_fragile)) +\n  geom_point()+\n  scale_y_continuous(label = percent) +\n  labs(x = \"Difference (in standard deviations) between two means\",\n       y = \"Proportion of significant p values \\nthat are between 0.01 and 0.05\",\n       title = \"Two sample tests for difference between two means with power = 80%\",\n       subtitle = \"t test for independent samples at a combination of sample size and population difference\\nneeded to give the desired power. Both populations are standard normal distributions.\")\n\n# Plot 2\ntibble(prop_fragile, n_for_power) |> \n  ggplot(aes(x = n_for_power,y = prop_fragile)) +\n  geom_point() +\n  scale_x_sqrt() +\n  scale_y_continuous(label = percent) +\n  labs(x = \"Sample size needed to get 80% power for given difference of means\",\n       y = \"Proportion of significant p values \\nthat are between 0.01 and 0.05\",\n       title = \"Two sample tests for difference between two means with power = 80%\",\n       subtitle = \"t test for independent samples at a combination of sample size and population difference\\nneeded to give the desired power. Both populations are standard normal distributions.\")\nRelaxing assumptions\nOK, so that was what we get when the power calculation was based on a true representation of the world, known before we did the experiment. Obviously this is never the case (or we’d not need to do experiments) — the actual difference between two populations might be bigger or smaller than we expected, it might actually be exactly zero, the shape and spread of the populations will differ from what we thought when we calculated the power, etc.\nI decided to try three simple breaks of the assumptions to see what impact they have on the 27% of\np\nvalues that were fragile:\nThe actual difference between populations is a random number, albeit on average is what is expected during the power calculation\nthe actual difference between populations is a coin flip between exactly what was expected (when the power calculation was made) and zero (ie null hypothesis turns out to be true)\nthe actual difference between population is a coin flip between a random number with average the same as expected and zero (ie a combination of the first two scenarios)\n#------------------when true d isn't what was expected---------------\n\nreps <- 10000\nres <- numeric(reps)\n\n# we are going to let the actual difference deviate from that which was used\n# in the power calculation, but say that on average the planned-for difference\n# was correct\nfor(i in 1:reps){\n  res[i] <- experiment(d = rnorm(1, 0.55, 0.5), n1 = 53)\n}\n\n# \"actual\" power:\n1 - mean(res > 0.05)\n\n# proportion of so-called fragile p values is much less\nsumm2 <- mean(res > 0.01 & res < 0.05) / mean(res < 0.05)\n\n#---------when true d is same as expected except half the time H0 is true---------\n\nfor(i in 1:reps){\n  res[i] <- experiment(d = sample(c(0, 0.55), 1), n1 = 53)\n}\n\n# proportion of so-called fragile p values is now *more*\nsumm3 <- mean(res > 0.01 & res < 0.05) / mean(res < 0.05)\n\n#---------when true d is random, AND half the time H0 is true---------\n\nfor(i in 1:reps){\n  res[i] <- experiment(d = sample(c(0, rnorm(1, 0.55, 0.5)), 1), n1 = 53)\n}\n\n# proportion of so-called fragile p values is now less\nsumm4 <- mean(res > 0.01 & res < 0.05) / mean(res < 0.05)\n\ntibble(`Context` = c(\n  \"Difference is as expected during power calculation\",\n  \"Difference is random, but on average is as expected\",\n  \"Difference is as expected, except half the time null hypothesis is true\",\n  \"Difference is random, AND null hypothesis true half the time\"\n), `Proportion of p-values that are fragile` = c(summ1, summ2, summ3, summ4)) |> \n  mutate(across(where(is.numeric), \\(x) percent(x, accuracy = 1)))\nThat gets us these interesting results:\nContext\nProportion of p-values that are fragile\nDifference is as expected during power calculation\n27%\nDifference is random, but on average is as expected\n16%\nDifference is as expected, except half the time null hypothesis is true\n29%\nDifference is random, AND null hypothesis true half the time\n20%\nThere’s a marked variation here in what proportion of\np\nvalues is fragile. Arguably, the fourth of these scenarios is the closest approximation to the real world (although there is a lot of debate about this, how much are exactly-zero differences really plausible?) Either this, or the other realistic scenario (‘difference is random but on average is as expected’) gives a proportion of fragile\np\nvalues well below the 27% we saw in our base scenario.\nConclusion\nThere’s just too many factors impacting on the proportion of\np\nvalues that will be between 0.01 and 0.05 to assume that variations in it are either an improvement or a worsening in research practices. These things include:\nWhen expected differences change and sample sizes change to go with them for a given level of power, it impacts materially on the proportion of fragile\np\nvalues we’d expect to see\nWhen the real world differs from that expected by the researcher when they did their power calculation, it impacts materially on the proportion of fragile\np\nvalues we’d expect to see\nAnyway, researchers don’t all set their sample sizes to give 80% power, for various reasons, some of them good and some not so good\nFinal thought — none of the above tells us whether we have a replication crisis or not, and if so if it’s getting better or getting worse. As it happens, I tend to think we do have one and that it’s very serious. I think the peer review process works very poorly and\ncould be improved\n, and academic publishing in general sets up terrible — and perhaps worsening — incentives. However, I think criticism in the past decade or so has led to improvements (such as more access to reproducible code and data, more pre-registration, general raised awareness), which is consistent really with Bogdan’s substantive argument here. I just don’t think the ‘fragile’\np\nvalues are much evidence either way, and if we monitor them at all we should do so with great caution.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nfree range statistics - R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Do ‘fragile’ p values tell us anything? I was interested recently to see this article on p values in the psychology literature float across my social media feed. Paul C Bogdan makes the case that the severity of the replication crisis in science can be...",
      "meta_keywords": null,
      "og_description": "Do ‘fragile’ p values tell us anything? I was interested recently to see this article on p values in the psychology literature float across my social media feed. Paul C Bogdan makes the case that the severity of the replication crisis in science can be...",
      "og_image": "https://freerangestats.info/img/0295-fragile-diff.png",
      "og_title": "Power and ‘fragile’ p-values by @ellis2013nz | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 12.7,
      "sitemap_lastmod": null,
      "twitter_description": "Do ‘fragile’ p values tell us anything? I was interested recently to see this article on p values in the psychology literature float across my social media feed. Paul C Bogdan makes the case that the severity of the replication crisis in science can be...",
      "twitter_title": "Power and ‘fragile’ p-values by @ellis2013nz | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/06/power-and-fragile-p-values-by-ellis2013nz/",
      "word_count": 2544
    }
  }
}