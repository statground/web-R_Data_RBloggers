{
  "id": "7a264c84d211c36f9783041b81162d4022b8a0cc",
  "url": "https://www.r-bloggers.com/2009/09/power-analysis-for-mixed-effect-models-in-r/",
  "created_at_utc": "2025-11-17T20:40:22Z",
  "data": null,
  "raw_original": {
    "uuid": "732109b6-7035-4296-bf86-4e89d62bf0b1",
    "created_at": "2025-11-17 20:40:22",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2009/09/power-analysis-for-mixed-effect-models-in-r/",
      "crawled_at": "2025-11-17T10:25:21.658627",
      "external_links": [
        {
          "href": "https://toddjobe.blogspot.com/2009/09/power-analysis-for-mixed-effect-models.html",
          "text": "Computational Ecology"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "http://www.stat.columbia.edu/~gelman/blog/",
          "text": "blog"
        },
        {
          "href": "https://toddjobe.blogspot.com/2009/09/power-analysis-for-mixed-effect-models.html",
          "text": "Computational Ecology"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Power Analysis for mixed-effect models in R | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0001.png?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0001.png?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0002.png?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0002.png?w=578"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/toddjobe/",
          "text": "toddjobe"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection#b3c7dcd7d7d9dcd1d6f3c6ddd09dd6d7c6",
          "text": null
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-45068 post type-post status-publish format-standard hentry\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Power Analysis for mixed-effect models in R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">September 18, 2009</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/toddjobe/\">toddjobe</a></span>  in Uncategorized | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<p class=\"syndicated-attribution\"><!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://toddjobe.blogspot.com/2009/09/power-analysis-for-mixed-effect-models.html\"> Computational Ecology</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div></p>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.0--><p>The power of a statistical test is the probability that a null\nhypothesis will be rejected when the alternative hypothesis is true.\nIn lay terms, power is your ability to refine or “prove” your\nexpectations from the data you collect.  The most frequent motivation\nfor estimating the power of a study is to figure out what sample size\nwill be needed to observe a treatment effect.  Given a set of pilot\ndata or some other estimate of the variation in a sample, we can use\npower analysis to inform how much additional data we should collect.\n</p>\n<p>\nI recently did a power analysis on a set of pilot data for a long-term\nmonitoring study of the US National Park Service.  I thought I would\nshare some of the things I learned and a bit of R code for others that\nmight need to do something like this.  If you aren’t into power\nanalysis, the code below may still be useful as examples of how to use\nthe error handling functions in R (<code>withCallingHandlers</code>,\n<code>withRestarts</code>), parallel programming using the <code>snow</code>\npackage, and linear mixed effect regression using <code>nlme</code>.  If you\nhave any suggestions for improvement or if I got something wrong on\nthe analysis, I’d love to hear from you.\n</p><div class=\"outline-2\" id=\"outline-container-1\">\n<h2 id=\"sec-1\"><span class=\"section-number-2\">1</span> The Study </h2>\n<div class=\"outline-text-2\" id=\"text-1\">\n<p>The study system was cobblebars along the Cumberland river in Big\nSouth Fork National Park (Kentucky and Tennessee, United States).\nCobblebars are typically dominated by grassy vegetation that include\ndisjunct tall-grass prairie species.  It is hypothesized that woody\nspecies will encroach onto cobblebars if they are not seasonally\nscoured by floods.  The purpose of the NPS sampling was to observe\nchanges in woody cover through time.  The study design consisted of\ntwo-stages of clustering: the first being cobblebars, and the second\nbeing transects within cobblebars.  The response variable was the\npercentage of the transect that was woody vegetation.  Because of\nthe clustered design, the inferential model for this study design\nhas mixed-effects.  I used a simple varying intercept\nmodel:\n</p>\n<p>\n<img class=\"jetpack-lazy-image\" data-lazy-src=\"https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0001.png?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0001.png?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0001.png?w=578\"/></noscript>\n</p>\n<p>\nwhere <i>y</i> is the percent of each transect in woody vegetation sampled\n<i>n</i> times within <i>J</i> cobblebars, each with <i>K</i> transects.  The\nparameter of inference for the purpose of monitoring change in woody\nvegetation through time is <i>β</i>, the rate at which cover changes as\na function of time.  <i>α</i>, <i>γ</i>, <i>σ<sup>2</sup><sub>γ</sub></i>, and\n<i>σ<sup>2</sup><sub>y</sub></i> are hyper-parameters that describe the hierarchical\nvariance structure inherent in the clustered sampling design.\n</p>\n<p>\nBelow is the function code used I used to regress the pilot data.  It\nshould be noted that with this function you can log or logit transform\nthe response variable (percentage of transect that is woody).  I put\nthis in because the responses are proportions (0,1) and errors should\ntechnically follow a beta-distribution.  Log and logit transforms with\nGaussian errors could approximate this. I ran all the models with\ntransformed and untransformed response, and the results did not vary\nat all.  So, I stuck with untransformed responses:\n</p>\n<pre>Model &lt;- function(x = cobblebars,\n  type = c(\"normal\",\"log\",\"logit\")){\n  ## Transforms\n  if (type[1] == \"log\")\n    x$prop.woody &lt;- log(x$prop.woody)\n  else if (type[1] == \"logit\")\n    x$prop.woody &lt;- log(x$prop.woody / (1 - x$prop.woody))\n\n  mod &lt;- lme(prop.woody ~ year,\n             data = x,\n             random = ~ 1 | cobblebar/transect,\n             na.action = na.omit,\n             control = lmeControl(opt = \"optim\",\n               maxIter = 800, msMaxIter = 800)\n             )\n  mod$type &lt;- type[1]\n\n  return(mod)\n}\n</pre>\n<p>\nHere are the results from this regression of the pilot data:\n</p>\n<pre>Linear mixed-effects model fit by REML\n Data: x \n        AIC       BIC   logLik\n  -134.4319 -124.1297 72.21595\n\nRandom effects:\n Formula: ~1 | cobblebar\n        (Intercept)\nStdDev:  0.03668416\n\n Formula: ~1 | transect %in% cobblebar\n        (Intercept)   Residual\nStdDev:  0.02625062 0.05663784\n\nFixed effects: prop.woody ~ year \n                  Value  Std.Error DF   t-value p-value\n(Intercept)  0.12966667 0.01881983 29  6.889896  0.0000\nyear        -0.00704598 0.01462383 29 -0.481815  0.6336\n Correlation: \n     (Intr)\nyear -0.389\n\nNumber of Observations: 60\nNumber of Groups: \n              cobblebar transect %in% cobblebar \n                      6                      30 \n</pre>\n</div>\n</div>\n<div class=\"outline-2\" id=\"outline-container-2\">\n<h2 id=\"sec-2\"><span class=\"section-number-2\">2</span> We don't learn about power analysis and complex models </h2>\n<div class=\"outline-text-2\" id=\"text-2\">\n<p>When I decided upon the inferential model the first thing that\noccurred to me was that I never learned in any statistics course I\nhad taken how to do such a power analysis on a multi-level model.\nI've taken more statistics courses than I'd like to count and taught\nmy own statistics courses for undergrads and graduate students, and\nthe only exposure to power analysis that I had was in the context of\nsimple t-tests or ANOVA.  You learn about it in your first 2\nstatistics courses, then it rarely if ever comes up again until you\nactually need it.\n</p>\n<p>\nI was, however, able to find a great resource on power analysis from\na Bayesian perspective in the excellent book \"Data Analysis Using\nRegression and Multilevel/Hierarchical Models\" by Andrew Gelman and\nJennifer Hill.  Andrew Gelman has thought and debated about power\nanalysis and you can get more from his <a href=\"http://www.stat.columbia.edu/~gelman/blog/\" rel=\"nofollow\" target=\"_blank\">blog</a>.  The approach in the\nbook is a simulation-based one and I have adopted it for this\nanalysis.\n</p>\n</div>\n</div>\n<div class=\"outline-2\" id=\"outline-container-3\">\n<h2 id=\"sec-3\"><span class=\"section-number-2\">3</span> Analysis Procedure </h2>\n<div class=\"outline-text-2\" id=\"text-3\">\n<p>For the current analysis we needed to know three things: effect\nsize, sample size, and estimates of population variance. We set\neffect size beforehand.  In this context, the parameter of interest\nis the rate of change in woody cover through time <i>β</i>, and\neffect size is simply how large or small a value of <i>β</i> you want\nto distinguish with a regression.  Sample size is also set <i>a   priori</i>. In the analysis we want to vary sample size by varying the\nnumber of cobblebars, the number of transects per cobblebar or the\nnumber of years the study is conducted.  \n</p>\n<p>\nThe population variance cannot be known precisely, and this is where\nthe pilot data come in.  By regressing the pilot data using the\nmodel we can obtain estimates of all the different components of the\nvariance (cobblebars, transects within cobblebars, and the residual\nvariance).  Below is the R function that will return all the\nhyperparameters (and <i>β</i>) from the regression:\n</p>\n<pre>GetHyperparam&lt;-function(x,b=NULL){\n  ## Get the hyperparameters from the mixed effect model\n  fe &lt;- fixef(x)\n  \n  if(is.null(b))\n    b&lt;-fe[2] # use the data effect size if not supplied\n\n  mu.a &lt;- fe[1] \n\n  vc &lt;- VarCorr(x)\n  sigma.y &lt;- as.numeric(vc[5, 2]) # Residual StdDev\n  sigma.a &lt;- as.numeric(vc[2, 2]) # Cobblebar StdDev\n  sigma.g &lt;- as.numeric(vc[4, 2]) # Cobblebar:transect StdDev\n\n  hp&lt;-c(b, mu.a, sigma.y, sigma.a, sigma.g)\n  names(hp)&lt;-c(\"b\", \"mu.a\", \"sigma.y\", \"sigma.a\", \"sigma.g\")\n  return(hp)\n}\n</pre>\n<p>\nTo calculate power we to regress the simulated data in the same way we\ndid the pilot data, and check for a significant <i>β</i>.  Since\noptimization is done using numeric methods there is always the chance\nthat the optimization will not work.  So, we make sure the regression\non the fake data catches and recovers from all errors.  The solution\nfor error recovery is to simply try the regression on a new set of\nfake data.  This function is a pretty good example of using the R\nerror handling function <code>withCallingHandlers</code> and\n<code>withRestarts</code>.\n</p>\n<pre>fakeModWithRestarts &lt;- function(m.o, n = 100,  ...){\n  ## A Fake Model\n  withCallingHandlers({\n    i &lt;- 0\n    mod &lt;- NULL\n    while (i &lt; n &amp; is.null(mod)){\n      mod &lt;- withRestarts({\n        f &lt;- fake(m.orig = m.o, transform = F, ...)\n        return(update(m.o, data = f))\n      },\n      rs = function(){\n        i &lt;&lt;- i + 1\n        return(NULL)\n      })\n    }\n    if(is.null(mod))\n      warning(\"ExceededIterations\")\n    return(mod)\n  },\n  error = function(e){\n    invokeRestart(\"rs\")\n  },\n  warning = function(w){\n    if(w$message == \"ExceededIterations\")\n      cat(\"\\n\", w$message, \"\\n\")\n    else\n      invokeRestart(\"rs\")\n  })\n}\n</pre>\n<p>\nTo calculate the power of a particular design we run\n<code>fakeModWithRestarts</code> 1000 times and look at the proportion of\nsignificant <i>β</i> values:\n</p>\n<pre>dt.power &lt;- function (m, n.sims = 1000, alpha=0.05, ...){\n  ## Calculate power for a particular sampling design\n  signif&lt;-rep(NA, n.sims)\n  for(i in 1:n.sims){\n    lme.power &lt;- fakeModWithRestarts(m.o = m, ...)\n    if(!is.null(lme.power))\n      signif[i] &lt;- summary(lme.power)$tTable[2, 5] &lt; alpha\n  }\n  power &lt;- mean(signif, na.rm = T)\n  return(power)\n}\n</pre>\n<p>\nFinally, we want to perform this analysis on many different sampling\ndesigns.  In my case I did all combinations of set of effect sizes,\ncobblebars, transects, and years.  So, I generated the appropriate designs:\n</p>\n<pre>factoredDesign &lt;- function(Elevs = 0.2/c(1,5,10,20),\n                           Nlevs = seq(2, 10, by = 2),\n                           Jlevs = seq(4, 10, by = 2),\n                           Klevs = c(3, 5, 7), ...){\n  ## Generates factored series of sampling designs for simulation\n  ## of data that follow a particular model.\n  ## Inputs:\n  ##   Elevs - vector of effect sizes for the slope parameter.\n  ##   Nlevs - vector of number of years to sample.\n  ##   Jlevs - vector of number of cobblebars to sample.\n  ##   Klevs - vector of number of transects to sample.\n  ## Results:\n  ##   Data frame with where columns are the factors and\n  ##   rows are the designs.\n\n  # Level lengths\n  lE &lt;- length(Elevs)\n  lN &lt;- length(Nlevs)\n  lJ &lt;- length(Jlevs)\n  lK &lt;- length(Klevs)\n\n  # Generate repeated vectors for each factor\n  E &lt;- rep(Elevs, each = lN*lJ*lK)\n  N &lt;- rep(rep(Nlevs, each = lJ*lK), times = lE)\n  J &lt;- rep(rep(Jlevs, each = lK), times = lE*lN)\n  K &lt;- rep(Klevs, times = lE*lN*lJ)\n  \n  return(data.frame(E, N, J, K))\n}\n</pre>\n<p>\nOnce we know our effect sizes, the different sample sizes we want,\nand the estimates of population variance we can generate simulated\ndataset that are similar to the pilot data.  To calculate power we\nsimply simulate a large number of dataset and calculate the\nproportion of slopes, <i>β</i> that are significantly different from\nzero (p-value &lt; 0.05).  This procedure is repeated for all the\neffect sizes and sample sizes of interest. Here is the code for\ngenerating a simulated dataset. It also does the work of doing the\ninverse transform of the response variables if necessary.\n</p>\n<pre>fake &lt;- function(N = 2, J = 6, K = 5, b = NULL, m.orig = mod,\n                 transform = TRUE, ...){\n  ## Simulated Data for power analysis\n  ## N = Number of years\n  ## J = Number of cobblebars\n  ## K = Number of transects within cobblebars\n  year &lt;- rep(0:(N-1), each = J*K)\n  cobblebar &lt;- factor(rep(rep(1:J, each = K), times = N))\n  transect &lt;- factor(rep(1:K, times = N*J))\n\n  ## Simulated parameters\n  hp&lt;-GetHyperparam(x=m.orig)\n  if(is.null(b))\n    b &lt;- hp['b']\n  g &lt;- rnorm(J*K, 0, hp['sigma.g'])\n  a &lt;- rnorm(J*K, hp['mu.a'] + g, hp['sigma.a'])\n  \n  ## Simulated responses\n  eta &lt;- rnorm(J*K*N, a + b * year, hp['sigma.y'])\n  if (transform){\n    if (m.orig$type == \"normal\"){\n      y &lt;- eta\n      y[y &gt; 1] &lt;- 1 # Fix any boundary problems.\n      y[y &lt; 0] &lt;- 0\n    }\n    else if (m.orig$type == \"log\"){\n      y &lt;- exp(eta)\n      y[y &gt; 1] &lt;- 1\n    }\n    else if (m.orig$type == \"logit\")\n      y &lt;- exp(eta) / (1 + exp(eta))\n  }\n  else{\n    y &lt;- eta\n  }\n  \n  return(data.frame(prop.woody = y, year, transect, cobblebar))\n}\n</pre>\n<p>\nThen I performed the power calculations on each of these designs.  This\ncould take a long time, so I set this procedure to use parallel processing\nif needed.  Note that I had to re-~source~ the file with all the\nnecessary functions for each processor.\n</p>\n<pre>powerAnalysis &lt;- function(parallel = T, ...){\n  ## Full Power Analysis\n  \n  ## Parallel\n  if(parallel){\n    closeAllConnections()\n    cl &lt;- makeCluster(7, type = \"SOCK\")\n    on.exit(closeAllConnections())\n    clusterEvalQ(cl, source(\"cobblebars2.r\"))\n  }\n  \n  ## The simulations\n  dat &lt;- factoredDesign(...)\n\n  if (parallel){\n    dat$power &lt;- parRapply(cl, dat, function(x,...){\n      dt.power(N = x[2], J = x[3], K = x[4], b = x[1], ...)\n    }, ...)\n  } else {\n    dat$power &lt;- apply(dat, 1, function(x, ...){\n      dt.power(N = x[2], J = x[3], K = x[4], b = x[1], ...)\n    }, ...)\n  }\n\n  return(dat)\n}\n</pre>\n<p>\nThe output of the <code>powerAnalysis</code> function is a data frame with\ncolumns for the power and all the sample design settings.  So, I wrote\na custom plotting function for this data frame:\n</p>\n<pre>plotPower &lt;- function(dt){\n  xyplot(power~N|J*K, data = dt, groups = E,\n         panel = function(...){panel.xyplot(...)\n                               panel.abline(h = 0.8, lty = 2)},\n         type = c(\"p\", \"l\"),\n         xlab = \"sampling years\",\n         ylab = \"power\",\n         strip = strip.custom(var.name = c(\"C\", \"T\"),\n           strip.levels = c(T, T)),\n         auto.key = T\n         )\n}\n</pre>\n<p>\nBelow is the figure for the cobblebar power analysis.  I won't go into\ndetail on what the results mean since I am concerned here with\nillustrating the technique and the R code.  Obviously, as the number of\ncobblebars and transects per year increase, so does power.  And, as\nthe effect size increases, observing it with a test is easier.\n</p>\n<p>\n<img class=\"jetpack-lazy-image\" data-lazy-src=\"https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0002.png?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0002.png?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0002.png?w=578\"/></noscript>\n</p>\n</div>\n</div>\n<div id=\"postamble\">\n<p class=\"author\"> Author: Todd Jobe\n<a href=\"/cdn-cgi/l/email-protection#b3c7dcd7d7d9dcd1d6f3c6ddd09dd6d7c6\" rel=\"nofollow\" target=\"_blank\"><toddjobe@unc.edu></toddjobe@unc.edu></a>\n</p>\n<p class=\"date\"> Date: 2009-09-18 Fri</p>\n<p class=\"creator\">HTML generated by org-mode 6.30trans in emacs 22</p>\n</div>\n\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.0-->\n<p class=\"syndicated-attribution\"><div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://toddjobe.blogspot.com/2009/09/power-analysis-for-mixed-effect-models.html\"> Computational Ecology</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div></p> </div>\n</article>",
      "main_text": "Power Analysis for mixed-effect models in R\nPosted on\nSeptember 18, 2009\nby\ntoddjobe\nin Uncategorized | 0 Comments\n[This article was first published on\nComputational Ecology\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nThe power of a statistical test is the probability that a null\nhypothesis will be rejected when the alternative hypothesis is true.\nIn lay terms, power is your ability to refine or “prove” your\nexpectations from the data you collect.  The most frequent motivation\nfor estimating the power of a study is to figure out what sample size\nwill be needed to observe a treatment effect.  Given a set of pilot\ndata or some other estimate of the variation in a sample, we can use\npower analysis to inform how much additional data we should collect.\nI recently did a power analysis on a set of pilot data for a long-term\nmonitoring study of the US National Park Service.  I thought I would\nshare some of the things I learned and a bit of R code for others that\nmight need to do something like this.  If you aren’t into power\nanalysis, the code below may still be useful as examples of how to use\nthe error handling functions in R (\nwithCallingHandlers\n,\nwithRestarts\n), parallel programming using the\nsnow\npackage, and linear mixed effect regression using\nnlme\n.  If you\nhave any suggestions for improvement or if I got something wrong on\nthe analysis, I’d love to hear from you.\n1\nThe Study\nThe study system was cobblebars along the Cumberland river in Big\nSouth Fork National Park (Kentucky and Tennessee, United States).\nCobblebars are typically dominated by grassy vegetation that include\ndisjunct tall-grass prairie species.  It is hypothesized that woody\nspecies will encroach onto cobblebars if they are not seasonally\nscoured by floods.  The purpose of the NPS sampling was to observe\nchanges in woody cover through time.  The study design consisted of\ntwo-stages of clustering: the first being cobblebars, and the second\nbeing transects within cobblebars.  The response variable was the\npercentage of the transect that was woody vegetation.  Because of\nthe clustered design, the inferential model for this study design\nhas mixed-effects.  I used a simple varying intercept\nmodel:\nwhere\ny\nis the percent of each transect in woody vegetation sampled\nn\ntimes within\nJ\ncobblebars, each with\nK\ntransects.  The\nparameter of inference for the purpose of monitoring change in woody\nvegetation through time is\nβ\n, the rate at which cover changes as\na function of time.\nα\n,\nγ\n,\nσ\n2\nγ\n, and\nσ\n2\ny\nare hyper-parameters that describe the hierarchical\nvariance structure inherent in the clustered sampling design.\nBelow is the function code used I used to regress the pilot data.  It\nshould be noted that with this function you can log or logit transform\nthe response variable (percentage of transect that is woody).  I put\nthis in because the responses are proportions (0,1) and errors should\ntechnically follow a beta-distribution.  Log and logit transforms with\nGaussian errors could approximate this. I ran all the models with\ntransformed and untransformed response, and the results did not vary\nat all.  So, I stuck with untransformed responses:\nModel <- function(x = cobblebars,\n  type = c(\"normal\",\"log\",\"logit\")){\n  ## Transforms\n  if (type[1] == \"log\")\n    x$prop.woody <- log(x$prop.woody)\n  else if (type[1] == \"logit\")\n    x$prop.woody <- log(x$prop.woody / (1 - x$prop.woody))\n\n  mod <- lme(prop.woody ~ year,\n             data = x,\n             random = ~ 1 | cobblebar/transect,\n             na.action = na.omit,\n             control = lmeControl(opt = \"optim\",\n               maxIter = 800, msMaxIter = 800)\n             )\n  mod$type <- type[1]\n\n  return(mod)\n}\nHere are the results from this regression of the pilot data:\nLinear mixed-effects model fit by REML\n Data: x \n        AIC       BIC   logLik\n  -134.4319 -124.1297 72.21595\n\nRandom effects:\n Formula: ~1 | cobblebar\n        (Intercept)\nStdDev:  0.03668416\n\n Formula: ~1 | transect %in% cobblebar\n        (Intercept)   Residual\nStdDev:  0.02625062 0.05663784\n\nFixed effects: prop.woody ~ year \n                  Value  Std.Error DF   t-value p-value\n(Intercept)  0.12966667 0.01881983 29  6.889896  0.0000\nyear        -0.00704598 0.01462383 29 -0.481815  0.6336\n Correlation: \n     (Intr)\nyear -0.389\n\nNumber of Observations: 60\nNumber of Groups: \n              cobblebar transect %in% cobblebar \n                      6                      30\n2\nWe don't learn about power analysis and complex models\nWhen I decided upon the inferential model the first thing that\noccurred to me was that I never learned in any statistics course I\nhad taken how to do such a power analysis on a multi-level model.\nI've taken more statistics courses than I'd like to count and taught\nmy own statistics courses for undergrads and graduate students, and\nthe only exposure to power analysis that I had was in the context of\nsimple t-tests or ANOVA.  You learn about it in your first 2\nstatistics courses, then it rarely if ever comes up again until you\nactually need it.\nI was, however, able to find a great resource on power analysis from\na Bayesian perspective in the excellent book \"Data Analysis Using\nRegression and Multilevel/Hierarchical Models\" by Andrew Gelman and\nJennifer Hill.  Andrew Gelman has thought and debated about power\nanalysis and you can get more from his\nblog\n.  The approach in the\nbook is a simulation-based one and I have adopted it for this\nanalysis.\n3\nAnalysis Procedure\nFor the current analysis we needed to know three things: effect\nsize, sample size, and estimates of population variance. We set\neffect size beforehand.  In this context, the parameter of interest\nis the rate of change in woody cover through time\nβ\n, and\neffect size is simply how large or small a value of\nβ\nyou want\nto distinguish with a regression.  Sample size is also set\na   priori\n. In the analysis we want to vary sample size by varying the\nnumber of cobblebars, the number of transects per cobblebar or the\nnumber of years the study is conducted.\nThe population variance cannot be known precisely, and this is where\nthe pilot data come in.  By regressing the pilot data using the\nmodel we can obtain estimates of all the different components of the\nvariance (cobblebars, transects within cobblebars, and the residual\nvariance).  Below is the R function that will return all the\nhyperparameters (and\nβ\n) from the regression:\nGetHyperparam<-function(x,b=NULL){\n  ## Get the hyperparameters from the mixed effect model\n  fe <- fixef(x)\n  \n  if(is.null(b))\n    b<-fe[2] # use the data effect size if not supplied\n\n  mu.a <- fe[1] \n\n  vc <- VarCorr(x)\n  sigma.y <- as.numeric(vc[5, 2]) # Residual StdDev\n  sigma.a <- as.numeric(vc[2, 2]) # Cobblebar StdDev\n  sigma.g <- as.numeric(vc[4, 2]) # Cobblebar:transect StdDev\n\n  hp<-c(b, mu.a, sigma.y, sigma.a, sigma.g)\n  names(hp)<-c(\"b\", \"mu.a\", \"sigma.y\", \"sigma.a\", \"sigma.g\")\n  return(hp)\n}\nTo calculate power we to regress the simulated data in the same way we\ndid the pilot data, and check for a significant\nβ\n.  Since\noptimization is done using numeric methods there is always the chance\nthat the optimization will not work.  So, we make sure the regression\non the fake data catches and recovers from all errors.  The solution\nfor error recovery is to simply try the regression on a new set of\nfake data.  This function is a pretty good example of using the R\nerror handling function\nwithCallingHandlers\nand\nwithRestarts\n.\nfakeModWithRestarts <- function(m.o, n = 100,  ...){\n  ## A Fake Model\n  withCallingHandlers({\n    i <- 0\n    mod <- NULL\n    while (i < n & is.null(mod)){\n      mod <- withRestarts({\n        f <- fake(m.orig = m.o, transform = F, ...)\n        return(update(m.o, data = f))\n      },\n      rs = function(){\n        i <<- i + 1\n        return(NULL)\n      })\n    }\n    if(is.null(mod))\n      warning(\"ExceededIterations\")\n    return(mod)\n  },\n  error = function(e){\n    invokeRestart(\"rs\")\n  },\n  warning = function(w){\n    if(w$message == \"ExceededIterations\")\n      cat(\"\\n\", w$message, \"\\n\")\n    else\n      invokeRestart(\"rs\")\n  })\n}\nTo calculate the power of a particular design we run\nfakeModWithRestarts\n1000 times and look at the proportion of\nsignificant\nβ\nvalues:\ndt.power <- function (m, n.sims = 1000, alpha=0.05, ...){\n  ## Calculate power for a particular sampling design\n  signif<-rep(NA, n.sims)\n  for(i in 1:n.sims){\n    lme.power <- fakeModWithRestarts(m.o = m, ...)\n    if(!is.null(lme.power))\n      signif[i] <- summary(lme.power)$tTable[2, 5] < alpha\n  }\n  power <- mean(signif, na.rm = T)\n  return(power)\n}\nFinally, we want to perform this analysis on many different sampling\ndesigns.  In my case I did all combinations of set of effect sizes,\ncobblebars, transects, and years.  So, I generated the appropriate designs:\nfactoredDesign <- function(Elevs = 0.2/c(1,5,10,20),\n                           Nlevs = seq(2, 10, by = 2),\n                           Jlevs = seq(4, 10, by = 2),\n                           Klevs = c(3, 5, 7), ...){\n  ## Generates factored series of sampling designs for simulation\n  ## of data that follow a particular model.\n  ## Inputs:\n  ##   Elevs - vector of effect sizes for the slope parameter.\n  ##   Nlevs - vector of number of years to sample.\n  ##   Jlevs - vector of number of cobblebars to sample.\n  ##   Klevs - vector of number of transects to sample.\n  ## Results:\n  ##   Data frame with where columns are the factors and\n  ##   rows are the designs.\n\n  # Level lengths\n  lE <- length(Elevs)\n  lN <- length(Nlevs)\n  lJ <- length(Jlevs)\n  lK <- length(Klevs)\n\n  # Generate repeated vectors for each factor\n  E <- rep(Elevs, each = lN*lJ*lK)\n  N <- rep(rep(Nlevs, each = lJ*lK), times = lE)\n  J <- rep(rep(Jlevs, each = lK), times = lE*lN)\n  K <- rep(Klevs, times = lE*lN*lJ)\n  \n  return(data.frame(E, N, J, K))\n}\nOnce we know our effect sizes, the different sample sizes we want,\nand the estimates of population variance we can generate simulated\ndataset that are similar to the pilot data.  To calculate power we\nsimply simulate a large number of dataset and calculate the\nproportion of slopes,\nβ\nthat are significantly different from\nzero (p-value < 0.05).  This procedure is repeated for all the\neffect sizes and sample sizes of interest. Here is the code for\ngenerating a simulated dataset. It also does the work of doing the\ninverse transform of the response variables if necessary.\nfake <- function(N = 2, J = 6, K = 5, b = NULL, m.orig = mod,\n                 transform = TRUE, ...){\n  ## Simulated Data for power analysis\n  ## N = Number of years\n  ## J = Number of cobblebars\n  ## K = Number of transects within cobblebars\n  year <- rep(0:(N-1), each = J*K)\n  cobblebar <- factor(rep(rep(1:J, each = K), times = N))\n  transect <- factor(rep(1:K, times = N*J))\n\n  ## Simulated parameters\n  hp<-GetHyperparam(x=m.orig)\n  if(is.null(b))\n    b <- hp['b']\n  g <- rnorm(J*K, 0, hp['sigma.g'])\n  a <- rnorm(J*K, hp['mu.a'] + g, hp['sigma.a'])\n  \n  ## Simulated responses\n  eta <- rnorm(J*K*N, a + b * year, hp['sigma.y'])\n  if (transform){\n    if (m.orig$type == \"normal\"){\n      y <- eta\n      y[y > 1] <- 1 # Fix any boundary problems.\n      y[y < 0] <- 0\n    }\n    else if (m.orig$type == \"log\"){\n      y <- exp(eta)\n      y[y > 1] <- 1\n    }\n    else if (m.orig$type == \"logit\")\n      y <- exp(eta) / (1 + exp(eta))\n  }\n  else{\n    y <- eta\n  }\n  \n  return(data.frame(prop.woody = y, year, transect, cobblebar))\n}\nThen I performed the power calculations on each of these designs.  This\ncould take a long time, so I set this procedure to use parallel processing\nif needed.  Note that I had to re-~source~ the file with all the\nnecessary functions for each processor.\npowerAnalysis <- function(parallel = T, ...){\n  ## Full Power Analysis\n  \n  ## Parallel\n  if(parallel){\n    closeAllConnections()\n    cl <- makeCluster(7, type = \"SOCK\")\n    on.exit(closeAllConnections())\n    clusterEvalQ(cl, source(\"cobblebars2.r\"))\n  }\n  \n  ## The simulations\n  dat <- factoredDesign(...)\n\n  if (parallel){\n    dat$power <- parRapply(cl, dat, function(x,...){\n      dt.power(N = x[2], J = x[3], K = x[4], b = x[1], ...)\n    }, ...)\n  } else {\n    dat$power <- apply(dat, 1, function(x, ...){\n      dt.power(N = x[2], J = x[3], K = x[4], b = x[1], ...)\n    }, ...)\n  }\n\n  return(dat)\n}\nThe output of the\npowerAnalysis\nfunction is a data frame with\ncolumns for the power and all the sample design settings.  So, I wrote\na custom plotting function for this data frame:\nplotPower <- function(dt){\n  xyplot(power~N|J*K, data = dt, groups = E,\n         panel = function(...){panel.xyplot(...)\n                               panel.abline(h = 0.8, lty = 2)},\n         type = c(\"p\", \"l\"),\n         xlab = \"sampling years\",\n         ylab = \"power\",\n         strip = strip.custom(var.name = c(\"C\", \"T\"),\n           strip.levels = c(T, T)),\n         auto.key = T\n         )\n}\nBelow is the figure for the cobblebar power analysis.  I won't go into\ndetail on what the results mean since I am concerned here with\nillustrating the technique and the R code.  Obviously, as the number of\ncobblebars and transects per year increase, so does power.  And, as\nthe effect size increases, observing it with a test is easier.\nAuthor: Todd Jobe\nDate: 2009-09-18 Fri\nHTML generated by org-mode 6.30trans in emacs 22\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nComputational Ecology\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "The power of a statistical test is the probability that a null hypothesis will be rejected when the alternative hypothesis is true. In lay terms, power is your ability to refine or \"prove\" your expectations from the data you collect. The most frequent...",
      "meta_keywords": null,
      "og_description": "The power of a statistical test is the probability that a null hypothesis will be rejected when the alternative hypothesis is true. In lay terms, power is your ability to refine or \"prove\" your expectations from the data you collect. The most frequent...",
      "og_image": "https://toddjobe.blogspot.com/2009/09/ltxpng/powerAnalysis_0001.png",
      "og_title": "Power Analysis for mixed-effect models in R | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 11,
      "sitemap_lastmod": "2009-09-18T19:39:23+00:00",
      "twitter_description": "The power of a statistical test is the probability that a null hypothesis will be rejected when the alternative hypothesis is true. In lay terms, power is your ability to refine or \"prove\" your expectations from the data you collect. The most frequent...",
      "twitter_title": "Power Analysis for mixed-effect models in R | R-bloggers",
      "url": "https://www.r-bloggers.com/2009/09/power-analysis-for-mixed-effect-models-in-r/",
      "word_count": 2206
    }
  }
}