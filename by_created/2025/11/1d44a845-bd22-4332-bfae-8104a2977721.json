{
  "uuid": "1d44a845-bd22-4332-bfae-8104a2977721",
  "created_at": "2025-11-22 19:59:22",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/01/thinking-about-covariates-in-an-analysis-of-an-rct/",
    "crawled_at": "2025-11-22T10:54:56.705779",
    "external_links": [
      {
        "href": "https://www.rdatagen.net/post/2025-01-28-handling-covariates-in-an-analysis-of-an-rct/",
        "text": "ouR data generation"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703",
        "text": "paper"
      },
      {
        "href": "https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1296",
        "text": "paper"
      },
      {
        "href": "https://www.rdatagen.net/post/2025-01-28-handling-covariates-in-an-analysis-of-an-rct/",
        "text": "ouR data generation"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Thinking about covariates in an analysis of an RCT | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/keith-goldfeld/",
        "text": "Keith Goldfeld"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-390200 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Thinking about covariates in an analysis of an RCT</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">January 27, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/keith-goldfeld/\">Keith Goldfeld</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.rdatagen.net/post/2025-01-28-handling-covariates-in-an-analysis-of-an-rct/\"> ouR data generation</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>I was recently discussing the analytic plan for a randomized controlled trial (RCT) with a clinical collaborator when she asked whether it’s appropriate to adjust for pre-specified baseline covariates. This question is so interesting because it touches on fundamental issues of inference—both causal and statistical. What is the target estimand in an RCT—that is, what effect are we actually measuring? What do we hope to learn from the specific sample recruited for the trial (i.e., how can the findings be analyzed in a way that enhances generalizability)? What underlying assumptions about replicability, resampling, and uncertainty inform the arguments for and against covariate adjustment? These are big questions, which won’t necessarily be answered here, but need to be kept in mind when thinking about the merits of covariate adjustment</p>\n<p>Some researchers resist covariate adjustment in the primary analysis, concerned that it might complicate interpretability or limit transparency. Others might like the straightforward clarity and simplicity of the randomized comparison. But perhaps the biggest issue that people have with covariate adjustment is a longstanding concern that flexible modeling could turn into a fishing expedition—searching for covariates that yield the most favorable effect estimate.</p>\n<p>After that conversation with my colleague, I revisited a 1994 <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703\" rel=\"nofollow\" target=\"_blank\">paper</a> by Stephen Senn, which argues that rather than checking for chance covariate imbalances before making adjustments, “the practical statistician will do well to establish beforehand a limited list of covariates deemed useful and fit them regardless. Such a strategy will <em>usually lead to a gain in power</em>, has no adverse effect on unconditional size and controls conditional size with respect to the covariates identified.” A subsequent <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1296\" rel=\"nofollow\" target=\"_blank\">paper</a> by Pocock et al. reinforces this perspective. Although they note that “experience shows that for most clinical trials, analyses which adjust for baseline covariates are in close agreement with the simpler unadjusted treatment comparisons”, they argue adjusting for covariates can be justified if it helps: (1) achieve the most appropriate p-value for treatment differences, (2) provide unbiased estimates, and (3) improve precision.</p>\n<p>Motivated by Pocock et al., I created some simulations to explore the operational characteristics of covariate adjustment that I’m sharing here. I’ve been distracted more recently with paper writing and manuscript editing, so I am happy to get back to a little <code>R</code> coding.</p>\n<div class=\"section level3\" id=\"simulations\">\n<h3>Simulations</h3>\n<p>To get things started, here are the <code>R</code> packages used in the simulations.</p>\n<pre>library(simstudy)\nlibrary(data.table)\nlibrary(stargazer)\nlibrary(parallel)</pre>\n<div class=\"section level4\" id=\"data-definitions\">\n<h4>Data definitions</h4>\n<p>I am using two sets of data definitions here, splitting up the creation of baseline covariates (<span class=\"math inline\">\\(x_1\\)</span> and <span class=\"math inline\">\\(x_2\\)</span>) and group assignment from the outcome <span class=\"math inline\">\\(y\\)</span>. We are assuming that <span class=\"math inline\">\\(y\\)</span> has a Gaussian (normal) distribution.</p>\n<pre>def_c &lt;- \n  defData(varname = \"x1\", formula = 0, variance = \"..s_x1^2\", dist = \"normal\") |&gt;\n  defData(varname = \"x2\", formula = 0, variance = \"..s_x2^2\", dist = \"normal\") |&gt;\n  defData(varname = \"A\", formula = \"1;1\", dist = \"trtAssign\")\n\ndef_y &lt;- defDataAdd(\n  varname = \"y\", \n  formula = \"5 + ..delta * A + ..b1 * x1 + ..b2 * x2\", \n  variance = \"..s_y^2\", \n  dist = \"normal\"\n)</pre>\n</div>\n<div class=\"section level4\" id=\"initial-parammeters\">\n<h4>Initial parammeters</h4>\n<p>Here are the parameters used in the data generation. <span class=\"math inline\">\\(x_1\\)</span> is highly correlated with the outcome <span class=\"math inline\">\\(y\\)</span>, whereas <span class=\"math inline\">\\(x_2\\)</span> is not. In the first set of simulations, we are assuming a true effect size <span class=\"math inline\">\\(\\delta = 5\\)</span>.</p>\n<pre>s_x1 &lt;- 8\ns_x2 &lt;- 9\ns_y &lt;- 12\nb1 &lt;- 1.50\nb2 &lt;- 0.0\ndelta &lt;- 5</pre>\n</div>\n<div class=\"section level4\" id=\"single-data-set-generation\">\n<h4>Single data set generation</h4>\n<p>L’Ecuyer’s Combined Multiple Recursive Generator (CMRG) random number generator is being used here, because the replication of the data (and the analyses) are done using a parallel process to speed things up a bit.</p>\n<pre>RNGkind(\"L'Ecuyer-CMRG\")\nset.seed(55)\n\ndc &lt;- genData(250, def_c) \ndd &lt;- addColumns(def_y, dc)\n\nhead(dd)\n## Key: &lt;id&gt;\n##       id         x1          x2     A          y\n##    &lt;int&gt;      &lt;num&gt;       &lt;num&gt; &lt;int&gt;      &lt;num&gt;\n## 1:     1  3.4075117 -1.66327988     0  8.1461927\n## 2:     2 -0.3040474 -5.60073657     0 -1.7577859\n## 3:     3 -4.4460516  1.20189340     1 -1.6324336\n## 4:     4  0.6834332  0.09974478     1  9.5938736\n## 5:     5 -4.6324773  7.85745373     0 -0.4782144\n## 6:     6 -6.5650815  0.49812462     0 -3.4082489</pre>\n<p>For this single data set, we can see that the means for <span class=\"math inline\">\\(x_1\\)</span> within each group are slightly different, while the means for <span class=\"math inline\">\\(x_2\\)</span> are more similar.</p>\n<pre>dd[, .(mu_x1 = mean(x1), mu_x2 = mean(x2)), keyby = A]\n## Key: &lt;A&gt;\n##        A      mu_x1       mu_x2\n##    &lt;int&gt;      &lt;num&gt;       &lt;num&gt;\n## 1:     0  0.3960784 -0.36510184\n## 2:     1 -0.4821337  0.08564994</pre>\n<p>These differences are confirmed by calculating the standardized imbalance <span class=\"math inline\">\\(Z_x\\)</span> and the standardized difference <span class=\"math inline\">\\(d\\)</span>. (The difference between <span class=\"math inline\">\\(Z_x\\)</span> and <span class=\"math inline\">\\(d\\)</span> is that <span class=\"math inline\">\\(Z_x\\)</span> has an adjustment for the group sample sizes.)</p>\n<pre>calc_diff &lt;- function(dx, rx, v) {\n  \n  mean_diff &lt;- dx[get(rx)==1, mean(get(v))] - dx[get(rx)==0, mean(get(v))]\n  s_pooled &lt;- sqrt(\n    (dx[get(rx)==1, (.N - 1) * var(get(v))] + \n       dx[get(rx)==0, (.N - 1) * var(get(v))] ) / dx[, .N - 2])\n  \n  Z_x &lt;- mean_diff / ( s_pooled * sqrt(1/dx[get(rx)==0, .N] + 1/dx[get(rx)==1, .N]) )\n  d &lt;- mean_diff / s_pooled\n  \n  return(list(Z_x = Z_x, d = d))\n  \n  }\n\ncalc_diff(dd, \"A\", \"x1\")\n## $Z_x\n## [1] -0.9424664\n## \n## $d\n## [1] -0.1192136\ncalc_diff(dd, \"A\", \"x2\")\n## $Z_x\n## [1] 0.3862489\n## \n## $d\n## [1] 0.04885705</pre>\n<p>As designed, <span class=\"math inline\">\\(x_1\\)</span> is strongly correlated with the outcome <span class=\"math inline\">\\(y\\)</span>, whereas <span class=\"math inline\">\\(x_2\\)</span> is not.</p>\n<pre>dd[, .(rho_x1.y = cor(x1, y), rho_x2.y = cor(x2, y))]\n##     rho_x1.y     rho_x2.y\n##        &lt;num&gt;        &lt;num&gt;\n## 1: 0.6807215 -0.003174757</pre>\n</div>\n<div class=\"section level4\" id=\"model-estimation\">\n<h4>Model estimation</h4>\n<p>We fit four models to this data: (1) no adjustment for the covariates, (2) adjusting for <span class=\"math inline\">\\(x_1\\)</span> alone, (3) adjusting for <span class=\"math inline\">\\(x_2\\)</span> alone, and (4) adjusting for both covariates.</p>\n<pre>model_1 &lt;- lm(data = dd, formula = y ~ A)\nmodel_2 &lt;- lm(data = dd, formula = y ~ A + x1)\nmodel_3 &lt;- lm(data = dd, formula = y ~ A + x2)\nmodel_4 &lt;- lm(data = dd, formula = y ~ A + x1 + x2)</pre>\n<p>Two key takeaways from this single data set are that (1) since <span class=\"math inline\">\\(x_1\\)</span> is a (albeit weak) confounder, failing to adjust for the covariate leads to an underestimation of the treatment effect (due to the (small) negative correlation of <span class=\"math inline\">\\(x_1\\)</span> and <span class=\"math inline\">\\(A\\)</span>), and (2) since <span class=\"math inline\">\\(x_1\\)</span> is so highly correlated with <span class=\"math inline\">\\(y\\)</span>, the models that adjust for <span class=\"math inline\">\\(x_1\\)</span> have lower standard errors for the treatment effect estimate (around 2.0 for models 1 and 3, and closer to 1.5 for models 2 and 4).</p>\n<pre>## \n## ============================================\n##            (1)      (2)      (3)      (4)   \n## --------------------------------------------\n## A         3.040   4.367***  3.044   4.367***\n##          (2.039)  (1.480)  (2.044)  (1.484) \n##                                             \n## x1                1.512***          1.512***\n##                   (0.101)           (0.101) \n##                                             \n## x2                          -0.010   0.001  \n##                            (0.111)  (0.081) \n##                                             \n## Constant 6.237*** 5.638*** 6.234*** 5.639***\n##          (1.442)  (1.046)  (1.446)  (1.048) \n##                                             \n## ============================================\n## ============================================\n## </pre>\n</div>\n<div class=\"section level4\" id=\"operating-characteristics-based-on-replicated-data-sets\">\n<h4>Operating characteristics (based on replicated data sets)</h4>\n<p>In order to understand the relative merits of the different modeling approaches, we need to replicate multiple data sets under the same set of assumptions used to generate the single data set. We will generate 2000 data sets and estimate all four models for each data set. For each replication, we use the function <code>est_ancova</code> to calculate a one-sided p-value. We will keep track of the point estimate, the standard error estimate, and the p-value for each iteration.</p>\n<pre>est_ancova &lt;- function(dx, vars) {\n\n  formula &lt;- as.formula(paste(\"y ~\", paste(vars, collapse = \" + \")))\n  model &lt;- lm(data = dx, formula = formula)\n  \n  coef_summary &lt;- summary(model)$coefficients[\"A\", ]\n  t_stat &lt;- coef_summary[\"t value\"]\n  \n  p_value &lt;- pt(t_stat, df = model$df.residual, lower.tail = FALSE)\n  ests &lt;- data.table(t(coef_summary[1:2]), p_value)\n  setnames(ests, c(\"est\", \"se\", \"pval\"))\n  \n  return(ests)\n\n}\n\nreplicate &lt;- function() {\n  \n  dc &lt;- genData(250, def_c) \n  dd &lt;- addColumns(def_y, dc)\n  \n  est_1 &lt;- est_ancova(dd, vars = \"A\")\n  est_2 &lt;- est_ancova(dd, vars = c(\"A\", \"x1\"))\n  est_3 &lt;- est_ancova(dd, vars = c(\"A\", \"x2\"))\n  est_4 &lt;- est_ancova(dd, vars = c(\"A\", \"x1\", \"x2\"))\n  \n  return(list(est_1 = est_1, est_2 = est_2, est_3 = est_3, est_4 = est_4))\n  \n}\nres &lt;- mclapply(1:2000, function(x) replicate())</pre>\n<p>All four models yield relatively unbiased estimates, though the models that adjust for <span class=\"math inline\">\\(x_1\\)</span> (the potential confounder) result in reduced bias relative to those that do not. However, the clear advantage of models 2 and 4 (those that adjust for <span class=\"math inline\">\\(x_1\\)</span>) is the reduced variance of the treatment effect estimator:</p>\n<pre>get.field &lt;- function(x, field) {\n  data.table(t(sapply(x, function(x) x[[field]]) ))\n}\n\nests &lt;- rbindlist(lapply(res, function(x) get.field(x, \"est\")))\nsapply(ests, function(x) c(bias = mean(x) - delta, var = var(x)))\n##            est_1       est_2       est_3         est_4\n## bias -0.05513569 -0.00157402 -0.05263476 -0.0002470811\n## var   4.51844704  2.33883113  4.55788040  2.3507163302</pre>\n<p>The reduction in variance translates directly to increased power for the models that adjust for <span class=\"math inline\">\\(x_1\\)</span>, from about 63% to 90%. This seems like a pretty good reason to adjust for a baseline covariate that (a) you collect, and (b) is highly correlated with the outcome.</p>\n<pre>pvals &lt;- rbindlist(lapply(res, function(x) get.field(x, \"pval\")))\nsapply(pvals, function(x) c(mean(x &lt; 0.025))) \n##  est_1  est_2  est_3  est_4 \n## 0.6255 0.9055 0.6275 0.9035</pre>\n</div>\n<div class=\"section level4\" id=\"exploring-type-1-error-rates\">\n<h4>Exploring Type 1 error rates</h4>\n<p>The flip side of statistical power is the Type 1 error - the probability of concluding that there is a treatment effect when in fact there is no treatment effect. We can assess this by setting <span class=\"math inline\">\\(\\delta = 0\\)</span> and running another large number of replications. If we do this 2,000 times by generating a completely new data set each time, we see that the observed error rates are close to 0.025 for all the models, though the models that adjust for <span class=\"math inline\">\\(x_1\\)</span> are closer to the theoretical value.</p>\n<pre>delta &lt;- 0\n\nres &lt;- mclapply(1:2000, function(x) replicate())\n\npvals &lt;- rbindlist(lapply(res, function(x) get.field(x, \"pval\")))\nsapply(pvals, function(x) c(mean(x &lt; 0.025))) \n##  est_1  est_2  est_3  est_4 \n## 0.0180 0.0265 0.0185 0.0285</pre>\n<p>Both Senn and Pocock et al. suggest that a key advantage of adjusting for baseline covariates is that it helps achieve the desired error rates, particularly for one-sided tests. Assuming that <em>all possible RCTs are conducted with the same level of covariate imbalance</em>, models that include baseline covariate adjustments will yield accurate error rates. In contrast, models that do not adjust for important (highly correlated) covariates will produce deflated error rates. This occurs primarily because the standard errors of the effect estimates are systematically overestimated, reducing the likelihood of ever rejecting the null hypothesis.</p>\n<p>To mimic the requirement that the dataset is sampled conditional on a fixed level of covariate imbalance, we generate the baseline covariates and treatment assignment only once, while the outcome is generated anew for each dataset. Under this approach, covariates and treatment assignment are fixed and only the outcome for a particular unit varies across iterations. An alternative approach would be to generate a large number of datasets using the full randomization process—creating new covariate values, treatment assignments, and outcomes for each iteration. Data sets would only be analyzed if they match the pre-specified covariate imbalance level. Although this approach yields the same results as our chosen method (I confirmed with simulations not shown here), the sampling process appears overly artificial, further complicating the interpretation of the p-value.</p>\n<pre>replicate_2 &lt;- function() {\n  \n  dd &lt;- addColumns(def_y, dc)\n  \n  est_1 &lt;- est_ancova(dd, vars = \"A\")\n  est_2 &lt;- est_ancova(dd, vars = c(\"A\", \"x1\"))\n  est_3 &lt;- est_ancova(dd, vars = c(\"A\", \"x2\"))\n  est_4 &lt;- est_ancova(dd, vars = c(\"A\", \"x1\", \"x2\"))\n  \n  return(list(est_1 = est_1, est_2 = est_2, est_3 = est_3, est_4 = est_4))\n  \n}\n\ndc &lt;- genData(250, def_c) \n\nres &lt;- mclapply(1:2000, function(x) replicate_2())\n\npvals &lt;- rbindlist(lapply(res, function(x) get.field(x, \"pval\")))\nsapply(pvals, function(x) c(mean(x &lt; 0.025))) \n##  est_1  est_2  est_3  est_4 \n## 0.0075 0.0285 0.0070 0.0280</pre>\n</div>\n</div>\n<div class=\"section level3\" id=\"causal-inference-methods-for-balancing\">\n<h3>Causal inference methods for balancing</h3>\n<p>To me, the strongest argument against adjusting for baseline covariates in the analysis is the risk that investigators may appear overly eager to demonstrate the intervention’s success. Pre-specifying the analysis plan goes a long way toward alleviating such concerns. Additionally, alternative approaches from causal inference methods can further reduce reliance on outcome model assumptions. In particular, balancing methods such as inverse probability weighting (IPW) and overlapping weights (OW) can address covariate imbalances while preserving the original estimand. These techniques re-weight the sample to create balanced pseudo-populations without directly modifying the outcome model, offering a viable alternative to regression-based adjustments. They have the advantage of separating the design model from the outcome model (since the exposure and outcome models are two distinct steps). The balancing can be done before looking at the outcome data - so no risk of fishing for results. I plan on sharing simulations using these approaches sometime in the future.</p>\n<p>\n</p><p><small><font color=\"darkkhaki\">\nReferences:</font></small></p>\n<p>Senn, Stephen. “Testing for baseline balance in clinical trials.” Statistics in medicine 13, no. 17 (1994): 1715-1726.</p>\n<p>Pocock, Stuart J., Susan E. Assmann, Laura E. Enos, and Linda E. Kasten. “Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: current practice and problems.” Statistics in medicine 21, no. 19 (2002): 2917-2930.</p>\n</div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.rdatagen.net/post/2025-01-28-handling-covariates-in-an-analysis-of-an-rct/\"> ouR data generation</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Thinking about covariates in an analysis of an RCT\nPosted on\nJanuary 27, 2025\nby\nKeith Goldfeld\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nouR data generation\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nI was recently discussing the analytic plan for a randomized controlled trial (RCT) with a clinical collaborator when she asked whether it’s appropriate to adjust for pre-specified baseline covariates. This question is so interesting because it touches on fundamental issues of inference—both causal and statistical. What is the target estimand in an RCT—that is, what effect are we actually measuring? What do we hope to learn from the specific sample recruited for the trial (i.e., how can the findings be analyzed in a way that enhances generalizability)? What underlying assumptions about replicability, resampling, and uncertainty inform the arguments for and against covariate adjustment? These are big questions, which won’t necessarily be answered here, but need to be kept in mind when thinking about the merits of covariate adjustment\nSome researchers resist covariate adjustment in the primary analysis, concerned that it might complicate interpretability or limit transparency. Others might like the straightforward clarity and simplicity of the randomized comparison. But perhaps the biggest issue that people have with covariate adjustment is a longstanding concern that flexible modeling could turn into a fishing expedition—searching for covariates that yield the most favorable effect estimate.\nAfter that conversation with my colleague, I revisited a 1994\npaper\nby Stephen Senn, which argues that rather than checking for chance covariate imbalances before making adjustments, “the practical statistician will do well to establish beforehand a limited list of covariates deemed useful and fit them regardless. Such a strategy will\nusually lead to a gain in power\n, has no adverse effect on unconditional size and controls conditional size with respect to the covariates identified.” A subsequent\npaper\nby Pocock et al. reinforces this perspective. Although they note that “experience shows that for most clinical trials, analyses which adjust for baseline covariates are in close agreement with the simpler unadjusted treatment comparisons”, they argue adjusting for covariates can be justified if it helps: (1) achieve the most appropriate p-value for treatment differences, (2) provide unbiased estimates, and (3) improve precision.\nMotivated by Pocock et al., I created some simulations to explore the operational characteristics of covariate adjustment that I’m sharing here. I’ve been distracted more recently with paper writing and manuscript editing, so I am happy to get back to a little\nR\ncoding.\nSimulations\nTo get things started, here are the\nR\npackages used in the simulations.\nlibrary(simstudy)\nlibrary(data.table)\nlibrary(stargazer)\nlibrary(parallel)\nData definitions\nI am using two sets of data definitions here, splitting up the creation of baseline covariates (\n\\(x_1\\)\nand\n\\(x_2\\)\n) and group assignment from the outcome\n\\(y\\)\n. We are assuming that\n\\(y\\)\nhas a Gaussian (normal) distribution.\ndef_c <- \n  defData(varname = \"x1\", formula = 0, variance = \"..s_x1^2\", dist = \"normal\") |>\n  defData(varname = \"x2\", formula = 0, variance = \"..s_x2^2\", dist = \"normal\") |>\n  defData(varname = \"A\", formula = \"1;1\", dist = \"trtAssign\")\n\ndef_y <- defDataAdd(\n  varname = \"y\", \n  formula = \"5 + ..delta * A + ..b1 * x1 + ..b2 * x2\", \n  variance = \"..s_y^2\", \n  dist = \"normal\"\n)\nInitial parammeters\nHere are the parameters used in the data generation.\n\\(x_1\\)\nis highly correlated with the outcome\n\\(y\\)\n, whereas\n\\(x_2\\)\nis not. In the first set of simulations, we are assuming a true effect size\n\\(\\delta = 5\\)\n.\ns_x1 <- 8\ns_x2 <- 9\ns_y <- 12\nb1 <- 1.50\nb2 <- 0.0\ndelta <- 5\nSingle data set generation\nL’Ecuyer’s Combined Multiple Recursive Generator (CMRG) random number generator is being used here, because the replication of the data (and the analyses) are done using a parallel process to speed things up a bit.\nRNGkind(\"L'Ecuyer-CMRG\")\nset.seed(55)\n\ndc <- genData(250, def_c) \ndd <- addColumns(def_y, dc)\n\nhead(dd)\n## Key: <id>\n##       id         x1          x2     A          y\n##    <int>      <num>       <num> <int>      <num>\n## 1:     1  3.4075117 -1.66327988     0  8.1461927\n## 2:     2 -0.3040474 -5.60073657     0 -1.7577859\n## 3:     3 -4.4460516  1.20189340     1 -1.6324336\n## 4:     4  0.6834332  0.09974478     1  9.5938736\n## 5:     5 -4.6324773  7.85745373     0 -0.4782144\n## 6:     6 -6.5650815  0.49812462     0 -3.4082489\nFor this single data set, we can see that the means for\n\\(x_1\\)\nwithin each group are slightly different, while the means for\n\\(x_2\\)\nare more similar.\ndd[, .(mu_x1 = mean(x1), mu_x2 = mean(x2)), keyby = A]\n## Key: <A>\n##        A      mu_x1       mu_x2\n##    <int>      <num>       <num>\n## 1:     0  0.3960784 -0.36510184\n## 2:     1 -0.4821337  0.08564994\nThese differences are confirmed by calculating the standardized imbalance\n\\(Z_x\\)\nand the standardized difference\n\\(d\\)\n. (The difference between\n\\(Z_x\\)\nand\n\\(d\\)\nis that\n\\(Z_x\\)\nhas an adjustment for the group sample sizes.)\ncalc_diff <- function(dx, rx, v) {\n  \n  mean_diff <- dx[get(rx)==1, mean(get(v))] - dx[get(rx)==0, mean(get(v))]\n  s_pooled <- sqrt(\n    (dx[get(rx)==1, (.N - 1) * var(get(v))] + \n       dx[get(rx)==0, (.N - 1) * var(get(v))] ) / dx[, .N - 2])\n  \n  Z_x <- mean_diff / ( s_pooled * sqrt(1/dx[get(rx)==0, .N] + 1/dx[get(rx)==1, .N]) )\n  d <- mean_diff / s_pooled\n  \n  return(list(Z_x = Z_x, d = d))\n  \n  }\n\ncalc_diff(dd, \"A\", \"x1\")\n## $Z_x\n## [1] -0.9424664\n## \n## $d\n## [1] -0.1192136\ncalc_diff(dd, \"A\", \"x2\")\n## $Z_x\n## [1] 0.3862489\n## \n## $d\n## [1] 0.04885705\nAs designed,\n\\(x_1\\)\nis strongly correlated with the outcome\n\\(y\\)\n, whereas\n\\(x_2\\)\nis not.\ndd[, .(rho_x1.y = cor(x1, y), rho_x2.y = cor(x2, y))]\n##     rho_x1.y     rho_x2.y\n##        <num>        <num>\n## 1: 0.6807215 -0.003174757\nModel estimation\nWe fit four models to this data: (1) no adjustment for the covariates, (2) adjusting for\n\\(x_1\\)\nalone, (3) adjusting for\n\\(x_2\\)\nalone, and (4) adjusting for both covariates.\nmodel_1 <- lm(data = dd, formula = y ~ A)\nmodel_2 <- lm(data = dd, formula = y ~ A + x1)\nmodel_3 <- lm(data = dd, formula = y ~ A + x2)\nmodel_4 <- lm(data = dd, formula = y ~ A + x1 + x2)\nTwo key takeaways from this single data set are that (1) since\n\\(x_1\\)\nis a (albeit weak) confounder, failing to adjust for the covariate leads to an underestimation of the treatment effect (due to the (small) negative correlation of\n\\(x_1\\)\nand\n\\(A\\)\n), and (2) since\n\\(x_1\\)\nis so highly correlated with\n\\(y\\)\n, the models that adjust for\n\\(x_1\\)\nhave lower standard errors for the treatment effect estimate (around 2.0 for models 1 and 3, and closer to 1.5 for models 2 and 4).\n## \n## ============================================\n##            (1)      (2)      (3)      (4)   \n## --------------------------------------------\n## A         3.040   4.367***  3.044   4.367***\n##          (2.039)  (1.480)  (2.044)  (1.484) \n##                                             \n## x1                1.512***          1.512***\n##                   (0.101)           (0.101) \n##                                             \n## x2                          -0.010   0.001  \n##                            (0.111)  (0.081) \n##                                             \n## Constant 6.237*** 5.638*** 6.234*** 5.639***\n##          (1.442)  (1.046)  (1.446)  (1.048) \n##                                             \n## ============================================\n## ============================================\n##\nOperating characteristics (based on replicated data sets)\nIn order to understand the relative merits of the different modeling approaches, we need to replicate multiple data sets under the same set of assumptions used to generate the single data set. We will generate 2000 data sets and estimate all four models for each data set. For each replication, we use the function\nest_ancova\nto calculate a one-sided p-value. We will keep track of the point estimate, the standard error estimate, and the p-value for each iteration.\nest_ancova <- function(dx, vars) {\n\n  formula <- as.formula(paste(\"y ~\", paste(vars, collapse = \" + \")))\n  model <- lm(data = dx, formula = formula)\n  \n  coef_summary <- summary(model)$coefficients[\"A\", ]\n  t_stat <- coef_summary[\"t value\"]\n  \n  p_value <- pt(t_stat, df = model$df.residual, lower.tail = FALSE)\n  ests <- data.table(t(coef_summary[1:2]), p_value)\n  setnames(ests, c(\"est\", \"se\", \"pval\"))\n  \n  return(ests)\n\n}\n\nreplicate <- function() {\n  \n  dc <- genData(250, def_c) \n  dd <- addColumns(def_y, dc)\n  \n  est_1 <- est_ancova(dd, vars = \"A\")\n  est_2 <- est_ancova(dd, vars = c(\"A\", \"x1\"))\n  est_3 <- est_ancova(dd, vars = c(\"A\", \"x2\"))\n  est_4 <- est_ancova(dd, vars = c(\"A\", \"x1\", \"x2\"))\n  \n  return(list(est_1 = est_1, est_2 = est_2, est_3 = est_3, est_4 = est_4))\n  \n}\nres <- mclapply(1:2000, function(x) replicate())\nAll four models yield relatively unbiased estimates, though the models that adjust for\n\\(x_1\\)\n(the potential confounder) result in reduced bias relative to those that do not. However, the clear advantage of models 2 and 4 (those that adjust for\n\\(x_1\\)\n) is the reduced variance of the treatment effect estimator:\nget.field <- function(x, field) {\n  data.table(t(sapply(x, function(x) x[[field]]) ))\n}\n\nests <- rbindlist(lapply(res, function(x) get.field(x, \"est\")))\nsapply(ests, function(x) c(bias = mean(x) - delta, var = var(x)))\n##            est_1       est_2       est_3         est_4\n## bias -0.05513569 -0.00157402 -0.05263476 -0.0002470811\n## var   4.51844704  2.33883113  4.55788040  2.3507163302\nThe reduction in variance translates directly to increased power for the models that adjust for\n\\(x_1\\)\n, from about 63% to 90%. This seems like a pretty good reason to adjust for a baseline covariate that (a) you collect, and (b) is highly correlated with the outcome.\npvals <- rbindlist(lapply(res, function(x) get.field(x, \"pval\")))\nsapply(pvals, function(x) c(mean(x < 0.025))) \n##  est_1  est_2  est_3  est_4 \n## 0.6255 0.9055 0.6275 0.9035\nExploring Type 1 error rates\nThe flip side of statistical power is the Type 1 error - the probability of concluding that there is a treatment effect when in fact there is no treatment effect. We can assess this by setting\n\\(\\delta = 0\\)\nand running another large number of replications. If we do this 2,000 times by generating a completely new data set each time, we see that the observed error rates are close to 0.025 for all the models, though the models that adjust for\n\\(x_1\\)\nare closer to the theoretical value.\ndelta <- 0\n\nres <- mclapply(1:2000, function(x) replicate())\n\npvals <- rbindlist(lapply(res, function(x) get.field(x, \"pval\")))\nsapply(pvals, function(x) c(mean(x < 0.025))) \n##  est_1  est_2  est_3  est_4 \n## 0.0180 0.0265 0.0185 0.0285\nBoth Senn and Pocock et al. suggest that a key advantage of adjusting for baseline covariates is that it helps achieve the desired error rates, particularly for one-sided tests. Assuming that\nall possible RCTs are conducted with the same level of covariate imbalance\n, models that include baseline covariate adjustments will yield accurate error rates. In contrast, models that do not adjust for important (highly correlated) covariates will produce deflated error rates. This occurs primarily because the standard errors of the effect estimates are systematically overestimated, reducing the likelihood of ever rejecting the null hypothesis.\nTo mimic the requirement that the dataset is sampled conditional on a fixed level of covariate imbalance, we generate the baseline covariates and treatment assignment only once, while the outcome is generated anew for each dataset. Under this approach, covariates and treatment assignment are fixed and only the outcome for a particular unit varies across iterations. An alternative approach would be to generate a large number of datasets using the full randomization process—creating new covariate values, treatment assignments, and outcomes for each iteration. Data sets would only be analyzed if they match the pre-specified covariate imbalance level. Although this approach yields the same results as our chosen method (I confirmed with simulations not shown here), the sampling process appears overly artificial, further complicating the interpretation of the p-value.\nreplicate_2 <- function() {\n  \n  dd <- addColumns(def_y, dc)\n  \n  est_1 <- est_ancova(dd, vars = \"A\")\n  est_2 <- est_ancova(dd, vars = c(\"A\", \"x1\"))\n  est_3 <- est_ancova(dd, vars = c(\"A\", \"x2\"))\n  est_4 <- est_ancova(dd, vars = c(\"A\", \"x1\", \"x2\"))\n  \n  return(list(est_1 = est_1, est_2 = est_2, est_3 = est_3, est_4 = est_4))\n  \n}\n\ndc <- genData(250, def_c) \n\nres <- mclapply(1:2000, function(x) replicate_2())\n\npvals <- rbindlist(lapply(res, function(x) get.field(x, \"pval\")))\nsapply(pvals, function(x) c(mean(x < 0.025))) \n##  est_1  est_2  est_3  est_4 \n## 0.0075 0.0285 0.0070 0.0280\nCausal inference methods for balancing\nTo me, the strongest argument against adjusting for baseline covariates in the analysis is the risk that investigators may appear overly eager to demonstrate the intervention’s success. Pre-specifying the analysis plan goes a long way toward alleviating such concerns. Additionally, alternative approaches from causal inference methods can further reduce reliance on outcome model assumptions. In particular, balancing methods such as inverse probability weighting (IPW) and overlapping weights (OW) can address covariate imbalances while preserving the original estimand. These techniques re-weight the sample to create balanced pseudo-populations without directly modifying the outcome model, offering a viable alternative to regression-based adjustments. They have the advantage of separating the design model from the outcome model (since the exposure and outcome models are two distinct steps). The balancing can be done before looking at the outcome data - so no risk of fishing for results. I plan on sharing simulations using these approaches sometime in the future.\nReferences:\nSenn, Stephen. “Testing for baseline balance in clinical trials.” Statistics in medicine 13, no. 17 (1994): 1715-1726.\nPocock, Stuart J., Susan E. Assmann, Laura E. Enos, and Linda E. Kasten. “Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: current practice and problems.” Statistics in medicine 21, no. 19 (2002): 2917-2930.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nouR data generation\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "I was recently discussing the analytic plan for a randomized controlled trial (RCT) with a clinical collaborator when she asked whether it’s appropriate to adjust for pre-specified baseline covariates. This question is so interesting because it touc...",
    "meta_keywords": null,
    "og_description": "I was recently discussing the analytic plan for a randomized controlled trial (RCT) with a clinical collaborator when she asked whether it’s appropriate to adjust for pre-specified baseline covariates. This question is so interesting because it touc...",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "Thinking about covariates in an analysis of an RCT | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 11.4,
    "sitemap_lastmod": null,
    "twitter_description": "I was recently discussing the analytic plan for a randomized controlled trial (RCT) with a clinical collaborator when she asked whether it’s appropriate to adjust for pre-specified baseline covariates. This question is so interesting because it touc...",
    "twitter_title": "Thinking about covariates in an analysis of an RCT | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/01/thinking-about-covariates-in-an-analysis-of-an-rct/",
    "word_count": 2275
  }
}