{
  "id": "2eeb9c8952ed5582e6e1e0273a14a18b6aee56f7",
  "url": "https://www.r-bloggers.com/2025/05/from-complete-separation-to-maximum-likelihood-estimation-in-logistic-regresion-a-note-to-myself/",
  "created_at_utc": "2025-11-22T19:58:35Z",
  "data": null,
  "raw_original": {
    "uuid": "d72a72e7-948f-4346-8416-d82ef0a5e8f8",
    "created_at": "2025-11-22 19:58:35",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/05/from-complete-separation-to-maximum-likelihood-estimation-in-logistic-regresion-a-note-to-myself/",
      "crawled_at": "2025-11-22T10:48:52.251074",
      "external_links": [
        {
          "href": "https://www.kenkoonwong.com/blog/mle/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#objectives",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#sepration",
          "text": "A Complete Separation"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#code",
          "text": "Let‚Äôs Code"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#basics",
          "text": "Wait, How Do We Even Estimate The Coefficient?"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#link",
          "text": "Link Function"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#prob",
          "text": "Probability Function"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#likefn",
          "text": "Construct Likelihood function"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#loglik",
          "text": "Log Likelihood Function"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#derivative",
          "text": "Derivative of Log Likelihood Function With Respect to B0, B1"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#hessian",
          "text": "Hessian Matrix"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#optim",
          "text": "Let‚Äôs Inspect Complete Separation with Optim"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#thought",
          "text": "Final Thoughts"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#lessons",
          "text": "Lessons Learnt"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#separation",
          "text": null
        },
        {
          "href": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logistic-regression-and-what-are-some-strategies-to-deal-with-the-issue/",
          "text": "Dive deeper"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#code",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#basics",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#link",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#prob",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#likelihood",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#likfn",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#derivative",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#intercept",
          "text": null
        },
        {
          "href": "https://www.math.wustl.edu/~freiwald/131derivativetable.pdf",
          "text": "chain rule, quotient rule"
        },
        {
          "href": "https://en.wikipedia.org/wiki/Fisher_information",
          "text": "Fisher Information"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#hessian",
          "text": null
        },
        {
          "href": "https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound",
          "text": "Cram√©r-Rao Lower Bound (CRLB)"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#why-do-we-need-second-derivative",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#how-to-calculate-standard-error",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#lets-inspect-complete-separation-with-optim",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#thought",
          "text": null
        },
        {
          "href": "https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression",
          "text": "possible solutions"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/#lessons",
          "text": null
        },
        {
          "href": "https://www.kenkoonwong.com/blog/",
          "text": "comment or visit my other blogs"
        },
        {
          "href": "https://bsky.app/profile/kenkoonwong.bsky.social",
          "text": "BlueSky"
        },
        {
          "href": "https://twitter.com/kenkoonwong/",
          "text": "twitter"
        },
        {
          "href": "https://github.com/kenkoonwong/",
          "text": "GitHub"
        },
        {
          "href": "https://med-mastodon.com/@kenkoonwong",
          "text": "Mastodon"
        },
        {
          "href": "https://www.kenkoonwong.com/contact/",
          "text": "contact me"
        },
        {
          "href": "https://www.kenkoonwong.com/blog/mle/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "From Complete Separation To Maximum Likelihood Estimation in Logistic Regresion: A Note To Myself | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.kenkoonwong.com/blog/mle/seperate.png?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/",
          "text": "r on Everyday Is A School Day"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392472 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">From Complete Separation To Maximum Likelihood Estimation in Logistic Regresion: A Note To Myself</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 16, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/\">r on Everyday Is A School Day</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.kenkoonwong.com/blog/mle/\"> r on Everyday Is A School Day</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><blockquote>\n<p>Refreshed my rusty calculus skills lately! ü§ì Finally understand what happens during complete separation and why those coefficient SE get so extreme. The math behind maximum likelihood estimation makes more sense now! Chain rule, quotient rule, matrix inversion are crucial!</p>\n</blockquote>\n<p><img alt=\"\" data-lazy-src=\"https://i0.wp.com/www.kenkoonwong.com/blog/mle/seperate.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.kenkoonwong.com/blog/mle/seperate.png?w=578&amp;ssl=1\"/></noscript></p>\n<h2 id=\"objectives\">Objectives:\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#objectives\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#sepration\" rel=\"nofollow\" target=\"_blank\">A Complete Separation</a>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#code\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs Code</a></li>\n</ul>\n</li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#basics\" rel=\"nofollow\" target=\"_blank\">Wait, How Do We Even Estimate The Coefficient?</a>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#link\" rel=\"nofollow\" target=\"_blank\">Link Function</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#prob\" rel=\"nofollow\" target=\"_blank\">Probability Function</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#likefn\" rel=\"nofollow\" target=\"_blank\">Construct Likelihood function</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#loglik\" rel=\"nofollow\" target=\"_blank\">Log Likelihood Function</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#derivative\" rel=\"nofollow\" target=\"_blank\">Derivative of Log Likelihood Function With Respect to B0, B1</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#hessian\" rel=\"nofollow\" target=\"_blank\">Hessian Matrix</a></li>\n</ul>\n</li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#optim\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs Inspect Complete Separation with Optim</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#thought\" rel=\"nofollow\" target=\"_blank\">Final Thoughts</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#lessons\" rel=\"nofollow\" target=\"_blank\">Lessons Learnt</a></li>\n</ul>\n<h2 id=\"separation\">A Complete Separation\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#separation\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>I know this is a rather basic concept, but I just recently came across this interesting problem. Complete separation in logistic regression occurs when a predictor perfectly divides your outcomes into groups ‚Äì like if all patients over 50 developed a condition and none under 50 did. This creates a problem because the model tries to draw an impossibly steep line at that boundary, causing coefficient estimates to become unreliable and extremely large. It‚Äôs a case where perfect prediction paradoxically breaks the statistical machinery, making the model unable to properly quantify uncertainty or provide stable estimates.</p>\n<p>\n<a href=\"https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logistic-regression-and-what-are-some-strategies-to-deal-with-the-issue/\" rel=\"nofollow\" target=\"_blank\">Dive deeper</a></p>\n<h3 id=\"code\">Let‚Äôs Code\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#code\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<pre>y &lt;- c(rep(1,50),rep(0,50))\nx &lt;- c(rep(0,50),rep(1,50))\n\nmodel &lt;- glm(y~x, family=\"binomial\")\nsummary(model)\n\n## \n## Call:\n## glm(formula = y ~ x, family = \"binomial\")\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)    26.57   50363.44   0.001    1.000\n## x             -53.13   71224.73  -0.001    0.999\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1.3863e+02  on 99  degrees of freedom\n## Residual deviance: 5.8016e-10  on 98  degrees of freedom\n## AIC: 4\n## \n## Number of Fisher Scoring iterations: 25\n</pre><p>A few reminder for myself, if I see these things on a logistic regression model, think about separation:</p>\n<ul>\n<li><code>Warning: glm.fit: algorithm did not converge</code> and <code>fitted probabilities numerically 0 or 1 occurred</code></li>\n<li>Extreme value for <code>estimates</code>. Imagine trying to <code>exp</code> coefficient of x, odds ratio would 8.4141037\\times 10^{-24} which is a very very small.</li>\n<li>Enormouse standard errors</li>\n<li>Residual deviance very very small, almost 0. Indicating model fit perfectly</li>\n<li>Number of Fisher Scoring Iteration reaching max iteration, indicating the model is not converging</li>\n</ul>\n<p>Let‚Äôs take a look for a normal logistic regression model summary look like.</p>\n<pre>x &lt;- runif(100,0,1)\ny &lt;- rbinom(100, 1, plogis(2*x))\n\nmodel &lt;- glm(y~x, family=\"binomial\")\nsummary(model)\n\n## \n## Call:\n## glm(formula = y ~ x, family = \"binomial\")\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)   0.3970     0.4343   0.914    0.361\n## x             1.1060     0.7822   1.414    0.157\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 118.59  on 99  degrees of freedom\n## Residual deviance: 116.55  on 98  degrees of freedom\n## AIC: 120.55\n## \n## Number of Fisher Scoring iterations: 4\n</pre><p>Look at the difference in the summary output. The estimates are much smaller, and the standard errors are reasonable. The residual deviance is also much larger, indicating that the model is not perfectly fitting the data. Also with lower iterations.</p>\n<p>This made me really curious how does <code>glm</code> find these <code>coefficients</code> to begin with? Yes, I‚Äôve heard of <code>maximum likelihood estimation</code> and I know that it uses that to find the estimate and standard error, but‚Ä¶ how does it actually do that? ü§î</p>\n<p>Also, if we have a perfect prediction, shouldn‚Äôt our standard error be very very small instead of very very big !?! Maybe the answer lies in how these coefficients are estimated!</p>\n<h2 id=\"basics\">Wait, How Do We Even Estimate The Coefficient?\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#basics\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ol>\n<li>Choose a distribution from the exponential family (binomial for logistic regression, Poisson for count data, etc.)</li>\n<li>Specify a link function that connects the linear predictor to the response variable (logit link for logistic regression)</li>\n<li>Construct the likelihood function based on the chosen distribution ‚Äì this represents the probability of observing your data given the parameters</li>\n<li>Find the maximum likelihood estimates (MLEs) by:</li>\n</ol>\n<ul>\n<li>Taking the log of the likelihood function (for mathematical convenience)</li>\n<li>Finding the derivatives with respect to each coefficient</li>\n<li>Setting these derivatives equal to zero</li>\n<li>Solving for the coefficients that maximize the likelihood</li>\n</ul>\n<ol start=\"5\">\n<li>Since most GLMs don‚Äôt have closed-form solutions, the model uses iterative numerical methods (like Fisher Scoring or Newton-Raphson) to converge to the maximum likelihood estimates</li>\n</ol>\n<p>Alright, let‚Äôs go through step by step. As for the first step, we‚Äôre going to choose a binomial distribution. For simplicity, our model only has intercept and one predictor.</p>\n<h3 id=\"link\">Link function\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#link\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>\\begin{gather}\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 x_{1i} = \\text{z}\n\\end{gather}</p>\n<p>this is basically the equation for log odds, and we will use <code>z</code></p>\n<h3 id=\"prob\">Probability function\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#prob\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>\\begin{gather}\np_i = \\frac{1}{1 + e^{-z}} \\\n= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\n\\end{gather}</p>\n<h3 id=\"likelihood\">Construct the likelihood function\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#likelihood\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>\\begin{gather}\nL(\\boldsymbol{\\beta_0, \\beta_1}) = \\prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i} \\\n\\end{gather}</p>\n<p>Notice that if <code>y_i</code> is 1, the first term will be 1 and the second term will be 0. If <code>y_i</code> is 0, the first term will be 0 and the second term will be 1. This is very convenient! And the likelihood function is a product of the probabilities of each observation. We essentially want to maximize this likelihood function. We want to find the coefficients that make the observed data most probable.</p>\n<h3 id=\"likfn\">Log-likelihood function\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#likfn\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>$$\n\\begin{gather}\n\\ln L(\\boldsymbol{\\beta_0, \\beta_1}) \\\n= \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n\\end{gather}\n$$</p>\n<p>The reason for taking the log is that it turns products into sums, which are easier to work with mathematically. Also notice that we use natural log here.</p>\n<h3 id=\"derivative\">Derivative With Respect To Coefficient\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#derivative\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>Let‚Äôs look at <code>b0</code> the intercept first. The final answer should be:\n$$\n\\begin{gather}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( y_i ‚Äì p_i \\right)\n\\end{gather}\n$$</p>\n<p>As for <code>b1</code> the coefficient of the predictor, the final answer should be:\n$$\n\\begin{gather}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\left( y_i ‚Äì p_i \\right) x_{1i}\n\\end{gather}\n$$</p>\n<p>Let‚Äôs try to practice our mathematical muscles and try to proof both of these equations.</p>\n<h4 id=\"intercept\">Intercept\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#intercept\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\begin{gather}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right] \\newline\n= \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_0} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right] \\newline\n= \\sum_{i=1}^{n} \\left[ y_i \\frac{1}{p_i} \\frac{\\partial p_i}{\\partial \\beta_0} + (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial (1-p_i)}{\\partial \\beta_0} \\right]\n\\end{gather}\n$$</p>\n<p>Since there are 2 parts, let‚Äôs work on the left part first.\n$$\n\\begin{gather}\ny_i \\frac{\\partial}{\\partial \\beta_0} \\ln(p_i) \\newline\n= y_i \\frac{1}{p_i} \\frac{\\partial p_i}{\\partial \\beta_0} \\newline\n= y_i \\frac{1}{p_i} \\frac{\\partial}{\\partial \\beta_0} \\left( \\frac{1}{1 + e^{-z}} \\right) \\newline\n= y_i \\frac{1}{p_i} \\frac{\\partial}{\\partial \\beta_0} \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right)\n\\end{gather}\n$$</p>\n<p>Alright, let‚Äôs focus on the derivative of the probability function. We can use the quotient rule here.\n$$\n\\begin{gather}\n\\frac{\\partial}{\\partial \\beta_0} \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right) \\newline\n= -\\frac{(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) \\cdot \\frac{\\partial}{\\partial \\beta_0} 1 ‚Äì 1 \\cdot \\frac{\\partial}{\\partial \\beta_0} (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\newline\n= -\\frac{(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) \\cdot 0 ‚Äì 1 \\cdot  (0 + e^{-(\\beta_0 + \\beta_1 x_{1i})} \\cdot -1)} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\newline\n= \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2}\n\\end{gather}\n$$</p>\n<p>Let‚Äôs put all the left part together.</p>\n<p>$$\n\\begin{gather}\ny_i \\frac{1}{p_i} \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= y_i (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= y_i \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right)\n\\end{gather}\n$$</p>\n<p>Alright, now let‚Äôs work on the right part.\n$$\n\\begin{gather}\n(1-y_i) \\frac{\\partial}{\\partial \\beta_0} \\ln(1-p_i) \\newline\n= (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial (1-p_i)}{\\partial \\beta_0} \\newline\n= (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial}{\\partial \\beta_0} \\left( 1 ‚Äì \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right) \\newline\n= (1-y_i) \\frac{1}{1-p_i} \\left( 0 ‚Äì \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= (1-y_i) \\frac{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}{e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\left( -\\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= -\\frac{1-y_i}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\n\\end{gather}\n$$</p>\n<p>A note to myself, for line 26, we used the answer on line 22 to replace the derivative.</p>\n<p>Okay, still with me? Let‚Äôs put them all together!\n$$\n\\begin{gather}\n\\sum_{i=1}^{n} \\left[ y_i \\frac{1}{p_i} \\frac{\\partial p_i}{\\partial \\beta_0} + (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial (1-p_i)}{\\partial \\beta_0} \\right] \\newline\n= \\sum_{i=1}^{n} \\left[  y_i \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right) + \\frac{-(1-y_i)}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})} y_i ‚Äì 1 + y_i }{1+e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ \\frac{y_i (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) ‚Äì 1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ \\frac{y_i (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) }{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} ‚Äì \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ y_i ‚Äì p_i\\right]\n\\end{gather}\n$$</p>\n<p>YES !!! WE DID IT !!! It was a really good time to refresh on my calculus. Especially on \n<a href=\"https://www.math.wustl.edu/~freiwald/131derivativetable.pdf\" rel=\"nofollow\" target=\"_blank\">chain rule, quotient rule</a>. As for the coefficient of <code>x1</code> it‚Äôs essentially the same process as above, and we will get to the derivative mentioned earlier if we work it out. We will spare that process on this blog.</p>\n<p>Since we know the derivative of the log-likelihood function, let‚Äôs create a gradient descent function to find the coefficients. Let‚Äôs code it!</p>\n<pre># Custom gradient descent function for logistic regression\nlogistic_gradient_descent &lt;- function(x, y, learning_rate = 0.01, max_iter = 100000, tol = 1e-6) {\n  # Initialize parameters\n  beta0 &lt;- 0\n  beta1 &lt;- 0\n  \n  # Store history for tracking convergence\n  history &lt;- matrix(0, nrow = max_iter, ncol = 2)\n  \n  for (i in 1:max_iter) {\n    # Calculate predicted probabilities with current parameters\n    linear_pred &lt;- beta0 + beta1 * x\n    p &lt;- 1 / (1 + exp(-linear_pred))\n    \n    # Calculate gradients (derivatives of log-likelihood)\n    grad_beta0 &lt;- sum(y - p)\n    grad_beta1 &lt;- sum((y - p) * x)\n    \n    # Store current parameters\n    history[i, ] &lt;- c(beta0, beta1)\n    \n    # Update parameters using gradient ascent (since we want to maximize log-likelihood)\n    beta0_new &lt;- beta0 + learning_rate * grad_beta0\n    beta1_new &lt;- beta1 + learning_rate * grad_beta1\n    \n    # Check for convergence\n    if (abs(beta0_new - beta0) &lt; tol &amp;&amp; abs(beta1_new - beta1) &lt; tol) {\n      # Trim history to actual iterations used\n      history &lt;- history[1:i, ]\n      break\n    }\n    \n    # Update parameters\n    beta0 &lt;- beta0_new\n    beta1 &lt;- beta1_new\n  }\n  \n  # Calculate final log-likelihood\n  linear_pred &lt;- beta0 + beta1 * x\n  p &lt;- 1 / (1 + exp(-linear_pred))\n  log_lik &lt;- sum(y * log(p) + (1 - y) * log(1 - p))\n  \n  # Return results\n  return(list(\n    par = c(beta0, beta1),\n    iterations = nrow(history),\n    convergence = if(i &lt; max_iter) 0 else 1,\n    history = history,\n    log_likelihood = log_lik\n  ))\n}\n\n# Run the custom gradient descent function\nn &lt;- 1000\nx &lt;- runif(n, 0, 1)\ny &lt;- rbinom(n, 1, plogis(0.5 + 2*x))\n\nresult_custom &lt;- logistic_gradient_descent(x, y)\n\n# Print results\ncat(paste0(\"Final parameter estimates:\\n\",\n             \"Beta0: \", round(result_custom$par[1], 4), \" (True: 0.5)\\n\",\n             \"Beta1: \", round(result_custom$par[2], 4), \" (True: 2)\\n\",\n             \"Converged in \", result_custom$iterations, \" iterations\\n\",\n             \"Final log-likelihood: \", round(result_custom$log_likelihood, 4)))\n\n## Final parameter estimates:\n## Beta0: 0.4953 (True: 0.5)\n## Beta1: 2.0957 (True: 2)\n## Converged in 121 iterations\n## Final log-likelihood: -464.5161\n</pre><p>Not too shabby! Alternatively you can just use likelihood function and <code>optim</code> to find the coefficients like so.</p>\n<pre>n &lt;- 1000\nx &lt;- runif(n,0,1)\ny &lt;- rbinom(n, 1, plogis(0.5 + 2*x))\n\nlog_likelihood &lt;- function(param, x, y) {\n  beta0 &lt;- param[1]\n  beta1 &lt;- param[2]\n  p &lt;- 1 / (1 + exp(-(beta0 + beta1 * x)))\n  return(-sum(y * log(p) + (1 - y) * log(1 - p)))\n}\n\ngradient &lt;- function(param, x, y) {\n  beta0 &lt;- param[1]\n  beta1 &lt;- param[2]\n  p &lt;- 1 / (1 + exp(-(beta0 + beta1 * x)))\n\n  # Negated derivatives (since we're minimizing)\n  d_beta0 &lt;- -sum(y - p)\n  d_beta1 &lt;- -sum((y - p) * x)\n\n  return(c(d_beta0, d_beta1))\n}\n\n(result &lt;- optim(\n  par = c(0, 0), \n  fn = log_likelihood, \n  gr = gradient,\n  x = x, \n  y = y, \n  method = \"BFGS\",\n  hessian = TRUE\n))\n\n## $par\n## [1] 0.5332625 2.1409545\n## \n## $value\n## [1] 447.1439\n## \n## $counts\n## function gradient \n##       24       11 \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\n## \n## $hessian\n##           [,1]     [,2]\n## [1,] 140.56298 56.77555\n## [2,]  56.77555 33.25137\n</pre><p>OK, something we need to note here is that the <code>optim</code> function minimizes the function, while we want to maximize the log-likelihood function. So we need to negate the log-likelihood function and the gradient, hence you see the return of <code>-</code> in both the logistic regression function and gradient. This means that the original Hessian matrix should be multiplied by <code>-</code>.</p>\n<pre>(hessian_matrix &lt;- -result$hessian)\n\n##            [,1]      [,2]\n## [1,] -140.56298 -56.77555\n## [2,]  -56.77555 -33.25137\n</pre><p>Now, because according to \n<a href=\"https://en.wikipedia.org/wiki/Fisher_information\" rel=\"nofollow\" target=\"_blank\">Fisher Information</a>:</p>\n<p>$$\n\\begin{gather}\nI(\\theta) = -E\\left[\\frac{\\partial^2 \\ln L(\\theta)}{\\partial \\theta^2}\\right]\n\\end{gather}\n$$\nThe <code>\\(I(\\theta)\\)</code> here represents <code>Fisher Information</code>, not identity matrix. We see that there is a <code>-</code> in front of the equation, hence the ultimate <code>Hessian matrix</code> calculated by <code>optim</code> is actually the <code>Fisher Information</code>.</p>\n<p>The interesting thing too is this works without actually specifying the gradient. OK, now we‚Äôve estimated the coefficients, what about the standard error? And also what‚Äôs Hessian matrix you say? Let‚Äôs take a look!</p>\n<h3 id=\"hessian\">Hessian Matrix\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#hessian\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>A Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. For a function f(x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô) with n variables, the Hessian is an n√ón matrix where each element Hij is the second partial derivative of f with respect to xi and xj.</p>\n<p>For simplicity, we will use the log likelihood function <code>\\(\\ln L(\\beta_0,\\beta_1)\\)</code> , and it would be second derivative of the log-likelihood function with respect to the coefficients <code>\\(\\beta_0 , \\beta_1\\)</code>.</p>\n<p>$$\n\\begin{bmatrix}\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0^2} &amp; \\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} \\newline\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} &amp; \\frac{\\partial^2 \\ln L}{\\partial \\beta_1^2}\n\\end{bmatrix}\n$$</p>\n<p>Now, why do we need <code>Hessian Matrix</code> and <code>Fisher Information</code> here? The \n<a href=\"https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound\" rel=\"nofollow\" target=\"_blank\">Cram√©r-Rao Lower Bound (CRLB)</a> establishes a fundamental limit on estimation precision in statistics. For any unbiased estimator of a parameter <code>\\(\\theta\\)</code>, the variance cannot be smaller than the reciprocal of the Fisher information, <code>\\(\\frac{1}{I(\\theta)}\\)</code>. This bound quantifies the best possible performance achievable by any estimation procedure, making it a cornerstone of statistical theory. Remarkably, MLEs are asymptotically efficient, meaning they achieve this minimum variance as sample size increases, with <code>\\(Var(\\hat \\theta)\\)</code> approaching <code>\\(I(\\theta)^{-1}\\)</code> as <code>n</code> approaches infinity.</p>\n<h4 id=\"why-do-we-need-second-derivative\">Why Do We Need Second Derivative?\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#why-do-we-need-second-derivative\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>Second derivatives are essential in MLE for 3 primary reasons: they help confirm whether critical points are indeed maxima (rather than minima or saddle points); they quantify the curvature of the log-likelihood function, with steeper curves indicating greater precision in parameter estimates; and they provide the foundation for calculating standard errors and confidence intervals through the variance-covariance matrix (derived from the negative inverse of the Hessian).</p>\n<h4 id=\"how-to-calculate-standard-error\">How To Calculate Standard Error?\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#how-to-calculate-standard-error\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\text{Var}(\\hat{\\theta}) = I(\\theta)^{-1} \\\n= -\n\\begin{bmatrix}\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0^2} &amp; \\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} \\newline\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} &amp; \\frac{\\partial^2 \\ln L}{\\partial \\beta_1^2}\n\\end{bmatrix}\n^{-1}\n$$\nLet‚Äôs denote the Fisher Information matrix as, <code>\\(I\\)</code></p>\n<p>$$\nI =\n\\begin{bmatrix}\na &amp; b \\newline\nb &amp; c\n\\end{bmatrix}\n$$\n$$\n\\text{where}\\ a = -\\frac{\\partial^2 \\ln L}{\\partial \\beta_0^2} ,\nb = -\\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} ,\nc = -\\frac{\\partial^2 \\ln L}{\\partial \\beta_1^2}\n$$\n<br/>\n$$\n\\text{The determinant is: } \\det(I) = ac - b^2\n$$</p>\n<p>$$\n\\text{The inverse matrix is: } I^{-1} = \\frac{1}{det(I)}\n\\begin{bmatrix}\nc &amp; -b \\newline\n-b &amp; a\n\\end{bmatrix}\n$$\n$$\n= \\frac{1}{ac - b^2}\n\\begin{bmatrix}\nc &amp; -b \\newline\n-b &amp; a\n\\end{bmatrix}\n$$\n<br/></p>\n<p>$$\n\\text{Therefore: } \\text{Var}(\\hat{\\theta}) = \\frac{1}{ac - b^2}\\begin{bmatrix}\nc &amp; -b \\newline\n-b &amp; a\n\\end{bmatrix}\n$$</p>\n<p>$$\n\\text{Standard errors: } \\\\ SE(\\hat{\\beta}_0) = \\sqrt{\\frac{c}{ac - b^2}} \\\\ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{a}{ac - b^2}}\n$$</p>\n<p>The code to perform above is as simple as:</p>\n<pre>sqrt(diag(solve(result$hessian)))\n\n## [1] 0.1514097 0.3113037\n</pre><p>Let‚Äôs see if it‚Äôs close to <code>glm</code></p>\n<pre>summary(glm(y~x, family=\"binomial\"))$coefficients[,2]\n\n## (Intercept)           x \n##   0.1514081   0.3112891\n</pre><p>there you go !!!</p>\n<h2 id=\"lets-inspect-complete-separation-with-optim\">Let‚Äôs Inspect Complete Separation With <code>optim</code>\n<a href=\"https://www.kenkoonwong.com/blog/mle/#lets-inspect-complete-separation-with-optim\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<pre>y &lt;- c(rep(1,50),rep(0,50))\nx &lt;- c(rep(0,50),rep(1,50))\n\n(result &lt;- optim(\n  par = c(0, 0), \n  fn = log_likelihood, \n  gr = gradient,\n  x = x, \n  y = y, \n  method = \"BFGS\",\n  hessian = TRUE\n))\n\n## $par\n## [1]  25 -50\n## \n## $value\n## [1] 1.388795e-09\n## \n## $counts\n## function gradient \n##       14        3 \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\n## \n## $hessian\n##              [,1]         [,2]\n## [1,] 1.388287e-09 6.943973e-10\n## [2,] 6.943973e-10 6.943973e-10\n\ncat(paste0(\"Final parameter estimates:\\n\",\n             \"Beta0: \", round(result$par[1], 4), \"\\n\",\n             \"Beta1: \", round(result$par[2], 4), \"\\n\",\n             \"Converged in \", result$counts[[1]], \" iterations\\n\"\n           ))\n\n## Final parameter estimates:\n## Beta0: 25\n## Beta1: -50\n## Converged in 14 iterations\n\nvar &lt;- diag(solve(result$hessian))\nse &lt;- sqrt(var)\n\ncat(paste0(\"Standard errors:\\n\",\n             \"Beta0: \", round(se[1], 4), \"\\n\",\n             \"Beta1: \", round(se[2], 4), \"\\n\"\n))\n\n## Standard errors:\n## Beta0: 37962.5062\n## Beta1: 53677.2729\n</pre><p>Let‚Äôs compare it with <code>glm</code> again</p>\n<pre>glm_model &lt;- glm(y ~ x, family = binomial())\nsummary(glm_model)\n\n## \n## Call:\n## glm(formula = y ~ x, family = binomial())\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)    26.57   50363.44   0.001    1.000\n## x             -53.13   71224.73  -0.001    0.999\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1.3863e+02  on 99  degrees of freedom\n## Residual deviance: 5.8016e-10  on 98  degrees of freedom\n## AIC: 4\n## \n## Number of Fisher Scoring iterations: 25\n</pre><p>Wow, quite similar results !!! Interesting note to myself, if we use <code>x &lt;- seq(1,100,1)</code> instead of binary, for <code>optim</code> our SE won‚Äôt be as big, it‚Äôs qctually soooo much smaller than <code>glm</code>. Maybe because of the optimization method used, since <code>glm</code> uses <code>IWLS</code> if I‚Äôm not mistaken.</p>\n<h2 id=\"thought\">Final Thoughts\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#thought\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>Wow, trying to figure out some of these fundamentals are really refreshing!</li>\n<li>It makes calculating MLE for other GLM much easier!</li>\n<li>We got to practice our calculus, which is quite cool! Lots of tries! Lots of errors!</li>\n<li>If we face complete separation from real data, here are some \n<a href=\"https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\" rel=\"nofollow\" target=\"_blank\">possible solutions</a></li>\n<li>Striving to be perfect is not a natural thing for the language of the universe</li>\n</ul>\n<h2 id=\"lessons\">Lessons Learnt\n  <a href=\"https://www.kenkoonwong.com/blog/mle/#lessons\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>A complete separation in logistic regression, symptoms are non-convergence, extreme estimates, and large standard errors, very small residual deviance</li>\n<li>learnt maximum likelihood estimation for logistic regression</li>\n<li>practiced calculus, refreshed on chain rule, quotient rule, matrix inverse</li>\n<li>learnt how to use <code>optim</code></li>\n<li>learnt about Hessian matrix and Fisher information</li>\n<li>learnt about Cram√©r-Rao Lower Bound (CRLB) and its significance in MLE</li>\n<li>for some reason <code>\\\\</code> does not work as well as <code>\\\\\\</code> or <code>\\newline</code> in latex when convert <code>rmd</code> to <code>md</code></li>\n</ul>\n<p>If you like this article:</p>\n<ul>\n<li>please feel free to send me a \n<a href=\"https://www.kenkoonwong.com/blog/\" rel=\"nofollow\" target=\"_blank\">comment or visit my other blogs</a></li>\n<li>please feel free to follow me on \n<a href=\"https://bsky.app/profile/kenkoonwong.bsky.social\" rel=\"nofollow\" target=\"_blank\">BlueSky</a>, \n<a href=\"https://twitter.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">twitter</a>, \n<a href=\"https://github.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">GitHub</a> or \n<a href=\"https://med-mastodon.com/@kenkoonwong\" rel=\"nofollow\" target=\"_blank\">Mastodon</a></li>\n<li>if you would like collaborate please feel free to \n<a href=\"https://www.kenkoonwong.com/contact/\" rel=\"nofollow\" target=\"_blank\">contact me</a></li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.kenkoonwong.com/blog/mle/\"> r on Everyday Is A School Day</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "From Complete Separation To Maximum Likelihood Estimation in Logistic Regresion: A Note To Myself\nPosted on\nMay 16, 2025\nby\nr on Everyday Is A School Day\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nr on Everyday Is A School Day\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nRefreshed my rusty calculus skills lately! ü§ì Finally understand what happens during complete separation and why those coefficient SE get so extreme. The math behind maximum likelihood estimation makes more sense now! Chain rule, quotient rule, matrix inversion are crucial!\nObjectives:\nA Complete Separation\nLet‚Äôs Code\nWait, How Do We Even Estimate The Coefficient?\nLink Function\nProbability Function\nConstruct Likelihood function\nLog Likelihood Function\nDerivative of Log Likelihood Function With Respect to B0, B1\nHessian Matrix\nLet‚Äôs Inspect Complete Separation with Optim\nFinal Thoughts\nLessons Learnt\nA Complete Separation\nI know this is a rather basic concept, but I just recently came across this interesting problem. Complete separation in logistic regression occurs when a predictor perfectly divides your outcomes into groups ‚Äì like if all patients over 50 developed a condition and none under 50 did. This creates a problem because the model tries to draw an impossibly steep line at that boundary, causing coefficient estimates to become unreliable and extremely large. It‚Äôs a case where perfect prediction paradoxically breaks the statistical machinery, making the model unable to properly quantify uncertainty or provide stable estimates.\nDive deeper\nLet‚Äôs Code\ny <- c(rep(1,50),rep(0,50))\nx <- c(rep(0,50),rep(1,50))\n\nmodel <- glm(y~x, family=\"binomial\")\nsummary(model)\n\n## \n## Call:\n## glm(formula = y ~ x, family = \"binomial\")\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)    26.57   50363.44   0.001    1.000\n## x             -53.13   71224.73  -0.001    0.999\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1.3863e+02  on 99  degrees of freedom\n## Residual deviance: 5.8016e-10  on 98  degrees of freedom\n## AIC: 4\n## \n## Number of Fisher Scoring iterations: 25\nA few reminder for myself, if I see these things on a logistic regression model, think about separation:\nWarning: glm.fit: algorithm did not converge\nand\nfitted probabilities numerically 0 or 1 occurred\nExtreme value for\nestimates\n. Imagine trying to\nexp\ncoefficient of x, odds ratio would 8.4141037\\times 10^{-24} which is a very very small.\nEnormouse standard errors\nResidual deviance very very small, almost 0. Indicating model fit perfectly\nNumber of Fisher Scoring Iteration reaching max iteration, indicating the model is not converging\nLet‚Äôs take a look for a normal logistic regression model summary look like.\nx <- runif(100,0,1)\ny <- rbinom(100, 1, plogis(2*x))\n\nmodel <- glm(y~x, family=\"binomial\")\nsummary(model)\n\n## \n## Call:\n## glm(formula = y ~ x, family = \"binomial\")\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)   0.3970     0.4343   0.914    0.361\n## x             1.1060     0.7822   1.414    0.157\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 118.59  on 99  degrees of freedom\n## Residual deviance: 116.55  on 98  degrees of freedom\n## AIC: 120.55\n## \n## Number of Fisher Scoring iterations: 4\nLook at the difference in the summary output. The estimates are much smaller, and the standard errors are reasonable. The residual deviance is also much larger, indicating that the model is not perfectly fitting the data. Also with lower iterations.\nThis made me really curious how does\nglm\nfind these\ncoefficients\nto begin with? Yes, I‚Äôve heard of\nmaximum likelihood estimation\nand I know that it uses that to find the estimate and standard error, but‚Ä¶ how does it actually do that? ü§î\nAlso, if we have a perfect prediction, shouldn‚Äôt our standard error be very very small instead of very very big !?! Maybe the answer lies in how these coefficients are estimated!\nWait, How Do We Even Estimate The Coefficient?\nChoose a distribution from the exponential family (binomial for logistic regression, Poisson for count data, etc.)\nSpecify a link function that connects the linear predictor to the response variable (logit link for logistic regression)\nConstruct the likelihood function based on the chosen distribution ‚Äì this represents the probability of observing your data given the parameters\nFind the maximum likelihood estimates (MLEs) by:\nTaking the log of the likelihood function (for mathematical convenience)\nFinding the derivatives with respect to each coefficient\nSetting these derivatives equal to zero\nSolving for the coefficients that maximize the likelihood\nSince most GLMs don‚Äôt have closed-form solutions, the model uses iterative numerical methods (like Fisher Scoring or Newton-Raphson) to converge to the maximum likelihood estimates\nAlright, let‚Äôs go through step by step. As for the first step, we‚Äôre going to choose a binomial distribution. For simplicity, our model only has intercept and one predictor.\nLink function\n\\begin{gather}\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 x_{1i} = \\text{z}\n\\end{gather}\nthis is basically the equation for log odds, and we will use\nz\nProbability function\n\\begin{gather}\np_i = \\frac{1}{1 + e^{-z}} \\\n= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\n\\end{gather}\nConstruct the likelihood function\n\\begin{gather}\nL(\\boldsymbol{\\beta_0, \\beta_1}) = \\prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i} \\\n\\end{gather}\nNotice that if\ny_i\nis 1, the first term will be 1 and the second term will be 0. If\ny_i\nis 0, the first term will be 0 and the second term will be 1. This is very convenient! And the likelihood function is a product of the probabilities of each observation. We essentially want to maximize this likelihood function. We want to find the coefficients that make the observed data most probable.\nLog-likelihood function\n$$\n\\begin{gather}\n\\ln L(\\boldsymbol{\\beta_0, \\beta_1}) \\\n= \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n\\end{gather}\n$$\nThe reason for taking the log is that it turns products into sums, which are easier to work with mathematically. Also notice that we use natural log here.\nDerivative With Respect To Coefficient\nLet‚Äôs look at\nb0\nthe intercept first. The final answer should be:\n$$\n\\begin{gather}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( y_i ‚Äì p_i \\right)\n\\end{gather}\n$$\nAs for\nb1\nthe coefficient of the predictor, the final answer should be:\n$$\n\\begin{gather}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\left( y_i ‚Äì p_i \\right) x_{1i}\n\\end{gather}\n$$\nLet‚Äôs try to practice our mathematical muscles and try to proof both of these equations.\nIntercept\n$$\n\\begin{gather}\n\\frac{\\partial \\ln L(\\boldsymbol{\\beta_0, \\beta_1})}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right] \\newline\n= \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_0} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right] \\newline\n= \\sum_{i=1}^{n} \\left[ y_i \\frac{1}{p_i} \\frac{\\partial p_i}{\\partial \\beta_0} + (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial (1-p_i)}{\\partial \\beta_0} \\right]\n\\end{gather}\n$$\nSince there are 2 parts, let‚Äôs work on the left part first.\n$$\n\\begin{gather}\ny_i \\frac{\\partial}{\\partial \\beta_0} \\ln(p_i) \\newline\n= y_i \\frac{1}{p_i} \\frac{\\partial p_i}{\\partial \\beta_0} \\newline\n= y_i \\frac{1}{p_i} \\frac{\\partial}{\\partial \\beta_0} \\left( \\frac{1}{1 + e^{-z}} \\right) \\newline\n= y_i \\frac{1}{p_i} \\frac{\\partial}{\\partial \\beta_0} \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right)\n\\end{gather}\n$$\nAlright, let‚Äôs focus on the derivative of the probability function. We can use the quotient rule here.\n$$\n\\begin{gather}\n\\frac{\\partial}{\\partial \\beta_0} \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right) \\newline\n= -\\frac{(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) \\cdot \\frac{\\partial}{\\partial \\beta_0} 1 ‚Äì 1 \\cdot \\frac{\\partial}{\\partial \\beta_0} (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\newline\n= -\\frac{(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) \\cdot 0 ‚Äì 1 \\cdot  (0 + e^{-(\\beta_0 + \\beta_1 x_{1i})} \\cdot -1)} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\newline\n= \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2}\n\\end{gather}\n$$\nLet‚Äôs put all the left part together.\n$$\n\\begin{gather}\ny_i \\frac{1}{p_i} \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= y_i (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= y_i \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right)\n\\end{gather}\n$$\nAlright, now let‚Äôs work on the right part.\n$$\n\\begin{gather}\n(1-y_i) \\frac{\\partial}{\\partial \\beta_0} \\ln(1-p_i) \\newline\n= (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial (1-p_i)}{\\partial \\beta_0} \\newline\n= (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial}{\\partial \\beta_0} \\left( 1 ‚Äì \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right) \\newline\n= (1-y_i) \\frac{1}{1-p_i} \\left( 0 ‚Äì \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= (1-y_i) \\frac{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}{e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\left( -\\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {(1 + e^{-(\\beta_0 + \\beta_1 x_{1i})})^2} \\right) \\newline\n= -\\frac{1-y_i}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\n\\end{gather}\n$$\nA note to myself, for line 26, we used the answer on line 22 to replace the derivative.\nOkay, still with me? Let‚Äôs put them all together!\n$$\n\\begin{gather}\n\\sum_{i=1}^{n} \\left[ y_i \\frac{1}{p_i} \\frac{\\partial p_i}{\\partial \\beta_0} + (1-y_i) \\frac{1}{1-p_i} \\frac{\\partial (1-p_i)}{\\partial \\beta_0} \\right] \\newline\n= \\sum_{i=1}^{n} \\left[  y_i \\left( \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})}} {1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} \\right) + \\frac{-(1-y_i)}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ \\frac{e^{-(\\beta_0 + \\beta_1 x_{1i})} y_i ‚Äì 1 + y_i }{1+e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ \\frac{y_i (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) ‚Äì 1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ \\frac{y_i (1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}) }{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}} ‚Äì \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1i})}}\\right] \\newline\n= \\sum_{i=1}^{n} \\left[ y_i ‚Äì p_i\\right]\n\\end{gather}\n$$\nYES !!! WE DID IT !!! It was a really good time to refresh on my calculus. Especially on\nchain rule, quotient rule\n. As for the coefficient of\nx1\nit‚Äôs essentially the same process as above, and we will get to the derivative mentioned earlier if we work it out. We will spare that process on this blog.\nSince we know the derivative of the log-likelihood function, let‚Äôs create a gradient descent function to find the coefficients. Let‚Äôs code it!\n# Custom gradient descent function for logistic regression\nlogistic_gradient_descent <- function(x, y, learning_rate = 0.01, max_iter = 100000, tol = 1e-6) {\n  # Initialize parameters\n  beta0 <- 0\n  beta1 <- 0\n  \n  # Store history for tracking convergence\n  history <- matrix(0, nrow = max_iter, ncol = 2)\n  \n  for (i in 1:max_iter) {\n    # Calculate predicted probabilities with current parameters\n    linear_pred <- beta0 + beta1 * x\n    p <- 1 / (1 + exp(-linear_pred))\n    \n    # Calculate gradients (derivatives of log-likelihood)\n    grad_beta0 <- sum(y - p)\n    grad_beta1 <- sum((y - p) * x)\n    \n    # Store current parameters\n    history[i, ] <- c(beta0, beta1)\n    \n    # Update parameters using gradient ascent (since we want to maximize log-likelihood)\n    beta0_new <- beta0 + learning_rate * grad_beta0\n    beta1_new <- beta1 + learning_rate * grad_beta1\n    \n    # Check for convergence\n    if (abs(beta0_new - beta0) < tol && abs(beta1_new - beta1) < tol) {\n      # Trim history to actual iterations used\n      history <- history[1:i, ]\n      break\n    }\n    \n    # Update parameters\n    beta0 <- beta0_new\n    beta1 <- beta1_new\n  }\n  \n  # Calculate final log-likelihood\n  linear_pred <- beta0 + beta1 * x\n  p <- 1 / (1 + exp(-linear_pred))\n  log_lik <- sum(y * log(p) + (1 - y) * log(1 - p))\n  \n  # Return results\n  return(list(\n    par = c(beta0, beta1),\n    iterations = nrow(history),\n    convergence = if(i < max_iter) 0 else 1,\n    history = history,\n    log_likelihood = log_lik\n  ))\n}\n\n# Run the custom gradient descent function\nn <- 1000\nx <- runif(n, 0, 1)\ny <- rbinom(n, 1, plogis(0.5 + 2*x))\n\nresult_custom <- logistic_gradient_descent(x, y)\n\n# Print results\ncat(paste0(\"Final parameter estimates:\\n\",\n             \"Beta0: \", round(result_custom$par[1], 4), \" (True: 0.5)\\n\",\n             \"Beta1: \", round(result_custom$par[2], 4), \" (True: 2)\\n\",\n             \"Converged in \", result_custom$iterations, \" iterations\\n\",\n             \"Final log-likelihood: \", round(result_custom$log_likelihood, 4)))\n\n## Final parameter estimates:\n## Beta0: 0.4953 (True: 0.5)\n## Beta1: 2.0957 (True: 2)\n## Converged in 121 iterations\n## Final log-likelihood: -464.5161\nNot too shabby! Alternatively you can just use likelihood function and\noptim\nto find the coefficients like so.\nn <- 1000\nx <- runif(n,0,1)\ny <- rbinom(n, 1, plogis(0.5 + 2*x))\n\nlog_likelihood <- function(param, x, y) {\n  beta0 <- param[1]\n  beta1 <- param[2]\n  p <- 1 / (1 + exp(-(beta0 + beta1 * x)))\n  return(-sum(y * log(p) + (1 - y) * log(1 - p)))\n}\n\ngradient <- function(param, x, y) {\n  beta0 <- param[1]\n  beta1 <- param[2]\n  p <- 1 / (1 + exp(-(beta0 + beta1 * x)))\n\n  # Negated derivatives (since we're minimizing)\n  d_beta0 <- -sum(y - p)\n  d_beta1 <- -sum((y - p) * x)\n\n  return(c(d_beta0, d_beta1))\n}\n\n(result <- optim(\n  par = c(0, 0), \n  fn = log_likelihood, \n  gr = gradient,\n  x = x, \n  y = y, \n  method = \"BFGS\",\n  hessian = TRUE\n))\n\n## $par\n## [1] 0.5332625 2.1409545\n## \n## $value\n## [1] 447.1439\n## \n## $counts\n## function gradient \n##       24       11 \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\n## \n## $hessian\n##           [,1]     [,2]\n## [1,] 140.56298 56.77555\n## [2,]  56.77555 33.25137\nOK, something we need to note here is that the\noptim\nfunction minimizes the function, while we want to maximize the log-likelihood function. So we need to negate the log-likelihood function and the gradient, hence you see the return of\n-\nin both the logistic regression function and gradient. This means that the original Hessian matrix should be multiplied by\n-\n.\n(hessian_matrix <- -result$hessian)\n\n##            [,1]      [,2]\n## [1,] -140.56298 -56.77555\n## [2,]  -56.77555 -33.25137\nNow, because according to\nFisher Information\n:\n$$\n\\begin{gather}\nI(\\theta) = -E\\left[\\frac{\\partial^2 \\ln L(\\theta)}{\\partial \\theta^2}\\right]\n\\end{gather}\n$$\nThe\n\\(I(\\theta)\\)\nhere represents\nFisher Information\n, not identity matrix. We see that there is a\n-\nin front of the equation, hence the ultimate\nHessian matrix\ncalculated by\noptim\nis actually the\nFisher Information\n.\nThe interesting thing too is this works without actually specifying the gradient. OK, now we‚Äôve estimated the coefficients, what about the standard error? And also what‚Äôs Hessian matrix you say? Let‚Äôs take a look!\nHessian Matrix\nA Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. For a function f(x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x‚Çô) with n variables, the Hessian is an n√ón matrix where each element Hij is the second partial derivative of f with respect to xi and xj.\nFor simplicity, we will use the log likelihood function\n\\(\\ln L(\\beta_0,\\beta_1)\\)\n, and it would be second derivative of the log-likelihood function with respect to the coefficients\n\\(\\beta_0 , \\beta_1\\)\n.\n$$\n\\begin{bmatrix}\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0^2} & \\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} \\newline\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} & \\frac{\\partial^2 \\ln L}{\\partial \\beta_1^2}\n\\end{bmatrix}\n$$\nNow, why do we need\nHessian Matrix\nand\nFisher Information\nhere? The\nCram√©r-Rao Lower Bound (CRLB)\nestablishes a fundamental limit on estimation precision in statistics. For any unbiased estimator of a parameter\n\\(\\theta\\)\n, the variance cannot be smaller than the reciprocal of the Fisher information,\n\\(\\frac{1}{I(\\theta)}\\)\n. This bound quantifies the best possible performance achievable by any estimation procedure, making it a cornerstone of statistical theory. Remarkably, MLEs are asymptotically efficient, meaning they achieve this minimum variance as sample size increases, with\n\\(Var(\\hat \\theta)\\)\napproaching\n\\(I(\\theta)^{-1}\\)\nas\nn\napproaches infinity.\nWhy Do We Need Second Derivative?\nSecond derivatives are essential in MLE for 3 primary reasons: they help confirm whether critical points are indeed maxima (rather than minima or saddle points); they quantify the curvature of the log-likelihood function, with steeper curves indicating greater precision in parameter estimates; and they provide the foundation for calculating standard errors and confidence intervals through the variance-covariance matrix (derived from the negative inverse of the Hessian).\nHow To Calculate Standard Error?\n$$\n\\text{Var}(\\hat{\\theta}) = I(\\theta)^{-1} \\\n= -\n\\begin{bmatrix}\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0^2} & \\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} \\newline\n\\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} & \\frac{\\partial^2 \\ln L}{\\partial \\beta_1^2}\n\\end{bmatrix}\n^{-1}\n$$\nLet‚Äôs denote the Fisher Information matrix as,\n\\(I\\)\n$$\nI =\n\\begin{bmatrix}\na & b \\newline\nb & c\n\\end{bmatrix}\n$$\n$$\n\\text{where}\\ a = -\\frac{\\partial^2 \\ln L}{\\partial \\beta_0^2} ,\nb = -\\frac{\\partial^2 \\ln L}{\\partial \\beta_0 \\partial \\beta_1} ,\nc = -\\frac{\\partial^2 \\ln L}{\\partial \\beta_1^2}\n$$\n$$\n\\text{The determinant is: } \\det(I) = ac - b^2\n$$\n$$\n\\text{The inverse matrix is: } I^{-1} = \\frac{1}{det(I)}\n\\begin{bmatrix}\nc & -b \\newline\n-b & a\n\\end{bmatrix}\n$$\n$$\n= \\frac{1}{ac - b^2}\n\\begin{bmatrix}\nc & -b \\newline\n-b & a\n\\end{bmatrix}\n$$\n$$\n\\text{Therefore: } \\text{Var}(\\hat{\\theta}) = \\frac{1}{ac - b^2}\\begin{bmatrix}\nc & -b \\newline\n-b & a\n\\end{bmatrix}\n$$\n$$\n\\text{Standard errors: } \\\\ SE(\\hat{\\beta}_0) = \\sqrt{\\frac{c}{ac - b^2}} \\\\ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{a}{ac - b^2}}\n$$\nThe code to perform above is as simple as:\nsqrt(diag(solve(result$hessian)))\n\n## [1] 0.1514097 0.3113037\nLet‚Äôs see if it‚Äôs close to\nglm\nsummary(glm(y~x, family=\"binomial\"))$coefficients[,2]\n\n## (Intercept)           x \n##   0.1514081   0.3112891\nthere you go !!!\nLet‚Äôs Inspect Complete Separation With\noptim\ny <- c(rep(1,50),rep(0,50))\nx <- c(rep(0,50),rep(1,50))\n\n(result <- optim(\n  par = c(0, 0), \n  fn = log_likelihood, \n  gr = gradient,\n  x = x, \n  y = y, \n  method = \"BFGS\",\n  hessian = TRUE\n))\n\n## $par\n## [1]  25 -50\n## \n## $value\n## [1] 1.388795e-09\n## \n## $counts\n## function gradient \n##       14        3 \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\n## \n## $hessian\n##              [,1]         [,2]\n## [1,] 1.388287e-09 6.943973e-10\n## [2,] 6.943973e-10 6.943973e-10\n\ncat(paste0(\"Final parameter estimates:\\n\",\n             \"Beta0: \", round(result$par[1], 4), \"\\n\",\n             \"Beta1: \", round(result$par[2], 4), \"\\n\",\n             \"Converged in \", result$counts[[1]], \" iterations\\n\"\n           ))\n\n## Final parameter estimates:\n## Beta0: 25\n## Beta1: -50\n## Converged in 14 iterations\n\nvar <- diag(solve(result$hessian))\nse <- sqrt(var)\n\ncat(paste0(\"Standard errors:\\n\",\n             \"Beta0: \", round(se[1], 4), \"\\n\",\n             \"Beta1: \", round(se[2], 4), \"\\n\"\n))\n\n## Standard errors:\n## Beta0: 37962.5062\n## Beta1: 53677.2729\nLet‚Äôs compare it with\nglm\nagain\nglm_model <- glm(y ~ x, family = binomial())\nsummary(glm_model)\n\n## \n## Call:\n## glm(formula = y ~ x, family = binomial())\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)\n## (Intercept)    26.57   50363.44   0.001    1.000\n## x             -53.13   71224.73  -0.001    0.999\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1.3863e+02  on 99  degrees of freedom\n## Residual deviance: 5.8016e-10  on 98  degrees of freedom\n## AIC: 4\n## \n## Number of Fisher Scoring iterations: 25\nWow, quite similar results !!! Interesting note to myself, if we use\nx <- seq(1,100,1)\ninstead of binary, for\noptim\nour SE won‚Äôt be as big, it‚Äôs qctually soooo much smaller than\nglm\n. Maybe because of the optimization method used, since\nglm\nuses\nIWLS\nif I‚Äôm not mistaken.\nFinal Thoughts\nWow, trying to figure out some of these fundamentals are really refreshing!\nIt makes calculating MLE for other GLM much easier!\nWe got to practice our calculus, which is quite cool! Lots of tries! Lots of errors!\nIf we face complete separation from real data, here are some\npossible solutions\nStriving to be perfect is not a natural thing for the language of the universe\nLessons Learnt\nA complete separation in logistic regression, symptoms are non-convergence, extreme estimates, and large standard errors, very small residual deviance\nlearnt maximum likelihood estimation for logistic regression\npracticed calculus, refreshed on chain rule, quotient rule, matrix inverse\nlearnt how to use\noptim\nlearnt about Hessian matrix and Fisher information\nlearnt about Cram√©r-Rao Lower Bound (CRLB) and its significance in MLE\nfor some reason\n\\\\\ndoes not work as well as\n\\\\\\\nor\n\\newline\nin latex when convert\nrmd\nto\nmd\nIf you like this article:\nplease feel free to send me a\ncomment or visit my other blogs\nplease feel free to follow me on\nBlueSky\n,\ntwitter\n,\nGitHub\nor\nMastodon\nif you would like collaborate please feel free to\ncontact me\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nr on Everyday Is A School Day\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Refreshed my rusty calculus skills lately! ü§ì Finally understand what happens during complete separation and why those coefficient SE get so extreme. The math behind maximum likelihood estimation makes more sense now! Chain rule, quotient rule, matrix ...",
      "meta_keywords": null,
      "og_description": "Refreshed my rusty calculus skills lately! ü§ì Finally understand what happens during complete separation and why those coefficient SE get so extreme. The math behind maximum likelihood estimation makes more sense now! Chain rule, quotient rule, matrix ...",
      "og_image": "https://www.kenkoonwong.com/blog/mle/seperate.png",
      "og_title": "From Complete Separation To Maximum Likelihood Estimation in Logistic Regresion: A Note To Myself | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 17.5,
      "sitemap_lastmod": null,
      "twitter_description": "Refreshed my rusty calculus skills lately! ü§ì Finally understand what happens during complete separation and why those coefficient SE get so extreme. The math behind maximum likelihood estimation makes more sense now! Chain rule, quotient rule, matrix ...",
      "twitter_title": "From Complete Separation To Maximum Likelihood Estimation in Logistic Regresion: A Note To Myself | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/05/from-complete-separation-to-maximum-likelihood-estimation-in-logistic-regresion-a-note-to-myself/",
      "word_count": 3492
    }
  }
}