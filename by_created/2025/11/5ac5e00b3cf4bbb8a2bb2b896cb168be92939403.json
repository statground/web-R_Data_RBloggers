{
  "id": "5ac5e00b3cf4bbb8a2bb2b896cb168be92939403",
  "url": "https://www.r-bloggers.com/2025/06/more-on-power-and-fragile-p-values-by-ellis2013nz/",
  "created_at_utc": "2025-11-22T19:58:21Z",
  "data": null,
  "raw_original": {
    "uuid": "f7a5f634-3108-4929-a68f-77131d15f3bc",
    "created_at": "2025-11-22 19:58:21",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/06/more-on-power-and-fragile-p-values-by-ellis2013nz/",
      "crawled_at": "2025-11-22T10:47:22.348521",
      "external_links": [
        {
          "href": "https://freerangestats.info/blog/2025/06/14/more-on-fragile-p-values",
          "text": "free range statistics - R"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://freerangestats.info/blog/2025/06/14/more-on-fragile-p-values",
          "text": "free range statistics - R"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "More on power and ‘fragile’ p-values by @ellis2013nz | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/freerangestats.info/img/0296-lines.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/freerangestats.info/img/0296-heatmap.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/free-range-statistics-r/",
          "text": "free range statistics - R"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-393040 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">More on power and ‘fragile’ p-values by @ellis2013nz</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">June 13, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/free-range-statistics-r/\">free range statistics - R</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://freerangestats.info/blog/2025/06/14/more-on-fragile-p-values\"> free range statistics - R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p>Last week I posted about the proportion of <i>p</i> values that will be ‘fragile’ under various types of scientific experiments or other inferences. The proportion of <i>p</i> values that is fragile had been defined as the proportion that are between 0.01 and 005. I was reacting to the question of whether it’s a good thing that the proportion fragile in the psychology literature seems to have declined over a decade or two from about 32% to about 26%. I had concluded that this didn’t really tell us much about progress in response to the replication crisis.</p>\n<p>There was some good discussion on BlueSky when I posted on this and I would now qualify my views a little. The key points that people made that I hadn’t adequately taken into account in my presentation were:</p>\n<ul>\n<li>When scientists determine a suitable sample size by a power calculation, they are most often basing it on the sample size needed to give a certain amount of power for the “minimum difference we want to detect”, rather than the difference they are actually expecting</li>\n<li>One of my key plots showed an apparent increase in fragile <i>p</i> values even with constant power when the planned-for difference between means was large — when what was actually happening was probably an artefact of the tiny sample sizes in this very artificial situation (to be honest, still not sure as I write this exactly what was happening, but am pretty satisfied that it’s not terribly important whatever it is, and certainly doesn’t deserve to be my main plot on the topic)</li>\n<li>More general thinking about “if this decrease in ‘fragile’ <i>p</i> values isn’t showing something positive, then what is it showing?” which gives me a slightly more nuanced view.</li>\n</ul>\n<p>This made me decide I should look more systematically into the difference between “planned for” difference between two samples and the “actual” difference in the population. I had done some simulations of random variation of the actual difference from that planned for, but not systematic and comprehensive enough.</p>\n<p>What I’ve done now is systematically compare every combination of a sampling strategy to give 80% power for “minimum detectable differences” from 0.05 to 0.60 (12 different values, spaced 0.05 apart), and actual differences of mean between 0.0 and 1.0 (11 different values, spaced 0.10 apart). With 10,000 simulations at each combination we have 12 x 11 x 10,000 = 1.32 million simulations</p>\n<object data=\"https://freerangestats.info/img/0296-lines.svg\" type=\"image/svg+xml\" width=\"450\"><img data-lazy-src=\"https://i0.wp.com/freerangestats.info/img/0296-lines.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/freerangestats.info/img/0296-lines.png?w=450&amp;ssl=1\"/></noscript></object>\n<p>We can see that many reasonable combinations of “planned for” and “actual” differences between the two populations give fragile proportions of <i>p</i> values that are quite different from 26%. In particular, in the situation where the actual difference is more than the “minimum detectable difference” that the sample size would have been calculated for 80% power (which is probably what most researchers are aiming for), the proportion fragile quickly gets well below 26%.</p>\n<p>That proportion ranges from 80% when the real difference is zero (i.e. the null hypothesis is true) and the <i>p</i> value distribution is uniform over [0,1]; to well below 10% when the sample size is ample for high (much greater than 80%) power, sufficient to pick up the real difference between the two populations.</p>\n<p>I think this is better at presenting the issues than my blog last week. Here’s how I think about this issue now:</p>\n<ul>\n<li>Around 26% of <i>p</i> values will indeed be ‘fragile’ when the sample size has been set to give 80% power on the basis of a detectable difference between two populations which is indeed roughly what the actual difference turns out to be.</li>\n<li>In general, a proportion of fragile <i>p</i> values that is higher than this suggests that experiments are under-powered by design, or decent designs with 80% power turned out to be based on minimum-detectalbe differences that are often not actual differences in reality, or something else sinister is going on.</li>\n<li>If in fact many or most experiments are based on realities where the actual difference between means is more than the minimum-detectable difference the samples size was chosen for at 80% power, we’d expect a proportion of <i>p</i> values that are fragile to be somewhat less than 26%.</li>\n<li>Taking these together, it <em>is</em> reasonable to say that <i>p</i> values declining from 32% to 26% is a good thing; but that 26% probably is still a bit too high and should not be appreciated to be a very artificial benchmark; and that we can’t be sure what’s driving the decline.</li>\n</ul>\n<p>Here’s the R code to run those 1.32 million simulations, making using of the <code>foreach</code> and <code>doParallel</code> R packages for parallel computing to speed things up a bit:</p>\n<figure class=\"highlight\"><pre>library(pwrss)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(foreach)\nlibrary(doParallel)\n\n#' Function to run a two sample experiment with t test on difference of means\n#' @returns a single p value\n#' @param d difference in means of the two populations that samples are drawn\n#'   from. If sd1 and sd2 are both 1, then d is a proportion of that sd and\n#'   everything is scaleless.\nexperiment &lt;- function(d, m1 = 0, sd1 = 1, sd2 = 1, n1 = 50, n2 = n1, seed = NULL){\n  if(!is.null(seed)){\n    set.seed(seed)\n  }\n  x1 &lt;- rnorm(n1, m1, sd1)\n  x2 &lt;- rnorm(n2, m1 + d, sd2)\n  t.test(x1, x2)$p.value\n}\n\n\n#--------------varying difference and sample sizes---------------\npd1 &lt;- tibble(planned_diff = seq(from = 0.05, to = 0.60, by = 0.05)) |&gt; \n  # what sample size do we need to have 80% power, based on that planned\n  # \"minimum difference to detect\"?:\n  mutate(n_for_power = sapply(planned_diff, function(d){\n    as.numeric(pwrss.t.2means(mu1 = d, power = 0.8, verbose = FALSE)$n[1])\n  }))\n\n# the actual differences, which can be from zero to much bigger than the minimum\n# we planned power to detect:\npd2 &lt;- tibble(actual_diff = seq(from = 0, to = 1.0, by = 0.1))\n\n# Number of simulations we do for each combination of planned power (based on a\n# given 'minimum difference to detect') and actual power (based on the real\n# difference). when the real difference is zero in particular, there will only\n# be 1/20 of reps that come up with a 'significant' difference, so 10000 reps in\n# total gives us a sample of 500 signficant tests to get the proportion that are\n# fragile from, so still not huge. If I'd bothered I could have changed the\n# number of reps to do for each combination based on some number that's really\n# needed, but I didn't bother.:\nreps_each &lt;- 10000\n\n# combine the planned-for and actual differences in a data frame with a row\n# for each repeated sim we are going to do:\ndata &lt;- expand_grid(pd1, pd2) |&gt; \n  mutate(link = 1) |&gt; \n  full_join(tibble(link = 1,\n                   rep = 1:reps_each),\n            relationship = \"many-to-many\", by = \"link\") |&gt; \n  select(-link) |&gt; \n  mutate(p = NA)\n\n\nprint(glue(\"Running {nrow(data)} simulations. This will take a while.\"))\n\n# set up parallel processing cluster\ncluster &lt;- makeCluster(7) \nregisterDoParallel(cluster)\n\nclusterEvalQ(cluster, {\n  library(foreach)\n})\n\nclusterExport(cluster, c(\"data\", \"experiment\"))\n\nresults &lt;- foreach(i = 1:nrow(data), .combine = rbind) %dopar% {\n  set.seed(i)\n  \n  # the row of data for just this simulation:\n  d &lt;- data[i, ]\n  \n  # perform the simulation and capture the p value:\n  d$p &lt;- experiment(d = d$actual_diff, \n             n1 = d$n_for_power,\n             seed = d$rep)\n  \n  # return the result as a row of data, which will be rbinded into a single data\n  # frame of all the parallel processes:\n  return(d)\n}\n\nstopCluster(cluster)\n\n#--------------summarise and present results--------------------------\n\n# Summarise and calculate the proportions:\nres &lt;- results |&gt; \n  group_by(planned_diff, n_for_power, actual_diff) |&gt; \n  summarise(number_sig = sum(p &lt; 0.05),\n            prop_fragile = sum(p &gt; 0.01 &amp; p &lt; 0.05) / number_sig)</pre></figure>\n<p>Here’s how I draw two plots to summarise that. I have the line plot shown above, and a heatmap that is shown below the code. Overall I think the line plot is easier and clearer to read.</p>\n<figure class=\"highlight\"><pre># Line chart plot:\nres |&gt; \n  mutate(pd_lab = glue(\"80% power planned for diff of {planned_diff}\")) |&gt; \n  ggplot(aes(x = actual_diff, y = prop_fragile)) +\n  facet_wrap(~pd_lab) +\n  geom_vline(aes(xintercept = planned_diff), colour = \"steelblue\") +\n  geom_hline(yintercept = 0.26, colour = \"orange\") +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(label = percent) +\n  labs(y = \"Proportion of significant &lt;i&gt;p&lt;/i&gt; values that are between 0.01 and 0.05\",\n       x = \"Actual difference (in standard deviations)\",\n       subtitle = \"Vertical blue line shows where the actual difference equals the minimum difference to detect that the 80% power calculation was based upon.\nHorizontal orange line shows the observed average proportion of 'fragile' p values in the recent psychology literature.\",\n       title = \"Fragility of p values in relation to actual and planned differences in a two sample t test.\")\n\n\n# Heatmap:\nres |&gt; \n  ggplot(aes(x = actual_diff, y = as.ordered(planned_diff), fill = prop_fragile)) +\n  geom_tile() +\n  geom_tile(data = filter(res, prop_fragile &gt; 0.25 &amp; prop_fragile &lt; 0.31),\n            fill = \"white\", alpha = 0.1, colour = \"white\", linewidth = 2) +\n  scale_fill_viridis_c(label = percent, direction = -1) +\n  theme(panel.grid.minor = element_blank()) +\n  labs(y= \"Smallest detectable difference for 80% power\",\n       x = \"Actual difference (in standard deviations)\",\n       fill = \"Proportion of significant p values that are between 0.01 and 0.05:\",\n       subtitle = \"Sample size is based on 80% power for the difference on the vertical axis. White boxes indicate where the proportion of fragile significant p values is between 25% and 31%.\",\n       title = \"Fragility of p values in relation to actual and planned differences in a two sample t test.\")</pre></figure>\n<p>Here’s the heatmap we get from that. It’s prettier but I think not actually as clear as the simpler, faceted line plot.</p>\n<object data=\"https://freerangestats.info/img/0296-heatmap.svg\" type=\"image/svg+xml\" width=\"450\"><img data-lazy-src=\"https://i2.wp.com/freerangestats.info/img/0296-heatmap.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/freerangestats.info/img/0296-heatmap.png?w=450&amp;ssl=1\"/></noscript></object>\n<p>That’s all for now. I don’t think there’s any big implications of this. Just a better understanding of what proportion of <i>p</i> values we might expect to be in this fragile range and what impacts on it.</p>\n<p>I’m planning on looking at some real life data, on a completely different topic, in my next post.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://freerangestats.info/blog/2025/06/14/more-on-fragile-p-values\"> free range statistics - R</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "More on power and ‘fragile’ p-values by @ellis2013nz\nPosted on\nJune 13, 2025\nby\nfree range statistics - R\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nfree range statistics - R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nLast week I posted about the proportion of\np\nvalues that will be ‘fragile’ under various types of scientific experiments or other inferences. The proportion of\np\nvalues that is fragile had been defined as the proportion that are between 0.01 and 005. I was reacting to the question of whether it’s a good thing that the proportion fragile in the psychology literature seems to have declined over a decade or two from about 32% to about 26%. I had concluded that this didn’t really tell us much about progress in response to the replication crisis.\nThere was some good discussion on BlueSky when I posted on this and I would now qualify my views a little. The key points that people made that I hadn’t adequately taken into account in my presentation were:\nWhen scientists determine a suitable sample size by a power calculation, they are most often basing it on the sample size needed to give a certain amount of power for the “minimum difference we want to detect”, rather than the difference they are actually expecting\nOne of my key plots showed an apparent increase in fragile\np\nvalues even with constant power when the planned-for difference between means was large — when what was actually happening was probably an artefact of the tiny sample sizes in this very artificial situation (to be honest, still not sure as I write this exactly what was happening, but am pretty satisfied that it’s not terribly important whatever it is, and certainly doesn’t deserve to be my main plot on the topic)\nMore general thinking about “if this decrease in ‘fragile’\np\nvalues isn’t showing something positive, then what is it showing?” which gives me a slightly more nuanced view.\nThis made me decide I should look more systematically into the difference between “planned for” difference between two samples and the “actual” difference in the population. I had done some simulations of random variation of the actual difference from that planned for, but not systematic and comprehensive enough.\nWhat I’ve done now is systematically compare every combination of a sampling strategy to give 80% power for “minimum detectable differences” from 0.05 to 0.60 (12 different values, spaced 0.05 apart), and actual differences of mean between 0.0 and 1.0 (11 different values, spaced 0.10 apart). With 10,000 simulations at each combination we have 12 x 11 x 10,000 = 1.32 million simulations\nWe can see that many reasonable combinations of “planned for” and “actual” differences between the two populations give fragile proportions of\np\nvalues that are quite different from 26%. In particular, in the situation where the actual difference is more than the “minimum detectable difference” that the sample size would have been calculated for 80% power (which is probably what most researchers are aiming for), the proportion fragile quickly gets well below 26%.\nThat proportion ranges from 80% when the real difference is zero (i.e. the null hypothesis is true) and the\np\nvalue distribution is uniform over [0,1]; to well below 10% when the sample size is ample for high (much greater than 80%) power, sufficient to pick up the real difference between the two populations.\nI think this is better at presenting the issues than my blog last week. Here’s how I think about this issue now:\nAround 26% of\np\nvalues will indeed be ‘fragile’ when the sample size has been set to give 80% power on the basis of a detectable difference between two populations which is indeed roughly what the actual difference turns out to be.\nIn general, a proportion of fragile\np\nvalues that is higher than this suggests that experiments are under-powered by design, or decent designs with 80% power turned out to be based on minimum-detectalbe differences that are often not actual differences in reality, or something else sinister is going on.\nIf in fact many or most experiments are based on realities where the actual difference between means is more than the minimum-detectable difference the samples size was chosen for at 80% power, we’d expect a proportion of\np\nvalues that are fragile to be somewhat less than 26%.\nTaking these together, it\nis\nreasonable to say that\np\nvalues declining from 32% to 26% is a good thing; but that 26% probably is still a bit too high and should not be appreciated to be a very artificial benchmark; and that we can’t be sure what’s driving the decline.\nHere’s the R code to run those 1.32 million simulations, making using of the\nforeach\nand\ndoParallel\nR packages for parallel computing to speed things up a bit:\nlibrary(pwrss)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(foreach)\nlibrary(doParallel)\n\n#' Function to run a two sample experiment with t test on difference of means\n#' @returns a single p value\n#' @param d difference in means of the two populations that samples are drawn\n#'   from. If sd1 and sd2 are both 1, then d is a proportion of that sd and\n#'   everything is scaleless.\nexperiment <- function(d, m1 = 0, sd1 = 1, sd2 = 1, n1 = 50, n2 = n1, seed = NULL){\n  if(!is.null(seed)){\n    set.seed(seed)\n  }\n  x1 <- rnorm(n1, m1, sd1)\n  x2 <- rnorm(n2, m1 + d, sd2)\n  t.test(x1, x2)$p.value\n}\n\n#--------------varying difference and sample sizes---------------\npd1 <- tibble(planned_diff = seq(from = 0.05, to = 0.60, by = 0.05)) |> \n  # what sample size do we need to have 80% power, based on that planned\n  # \"minimum difference to detect\"?:\n  mutate(n_for_power = sapply(planned_diff, function(d){\n    as.numeric(pwrss.t.2means(mu1 = d, power = 0.8, verbose = FALSE)$n[1])\n  }))\n\n# the actual differences, which can be from zero to much bigger than the minimum\n# we planned power to detect:\npd2 <- tibble(actual_diff = seq(from = 0, to = 1.0, by = 0.1))\n\n# Number of simulations we do for each combination of planned power (based on a\n# given 'minimum difference to detect') and actual power (based on the real\n# difference). when the real difference is zero in particular, there will only\n# be 1/20 of reps that come up with a 'significant' difference, so 10000 reps in\n# total gives us a sample of 500 signficant tests to get the proportion that are\n# fragile from, so still not huge. If I'd bothered I could have changed the\n# number of reps to do for each combination based on some number that's really\n# needed, but I didn't bother.:\nreps_each <- 10000\n\n# combine the planned-for and actual differences in a data frame with a row\n# for each repeated sim we are going to do:\ndata <- expand_grid(pd1, pd2) |> \n  mutate(link = 1) |> \n  full_join(tibble(link = 1,\n                   rep = 1:reps_each),\n            relationship = \"many-to-many\", by = \"link\") |> \n  select(-link) |> \n  mutate(p = NA)\n\nprint(glue(\"Running {nrow(data)} simulations. This will take a while.\"))\n\n# set up parallel processing cluster\ncluster <- makeCluster(7) \nregisterDoParallel(cluster)\n\nclusterEvalQ(cluster, {\n  library(foreach)\n})\n\nclusterExport(cluster, c(\"data\", \"experiment\"))\n\nresults <- foreach(i = 1:nrow(data), .combine = rbind) %dopar% {\n  set.seed(i)\n  \n  # the row of data for just this simulation:\n  d <- data[i, ]\n  \n  # perform the simulation and capture the p value:\n  d$p <- experiment(d = d$actual_diff, \n             n1 = d$n_for_power,\n             seed = d$rep)\n  \n  # return the result as a row of data, which will be rbinded into a single data\n  # frame of all the parallel processes:\n  return(d)\n}\n\nstopCluster(cluster)\n\n#--------------summarise and present results--------------------------\n\n# Summarise and calculate the proportions:\nres <- results |> \n  group_by(planned_diff, n_for_power, actual_diff) |> \n  summarise(number_sig = sum(p < 0.05),\n            prop_fragile = sum(p > 0.01 & p < 0.05) / number_sig)\nHere’s how I draw two plots to summarise that. I have the line plot shown above, and a heatmap that is shown below the code. Overall I think the line plot is easier and clearer to read.\n# Line chart plot:\nres |> \n  mutate(pd_lab = glue(\"80% power planned for diff of {planned_diff}\")) |> \n  ggplot(aes(x = actual_diff, y = prop_fragile)) +\n  facet_wrap(~pd_lab) +\n  geom_vline(aes(xintercept = planned_diff), colour = \"steelblue\") +\n  geom_hline(yintercept = 0.26, colour = \"orange\") +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(label = percent) +\n  labs(y = \"Proportion of significant <i>p</i> values that are between 0.01 and 0.05\",\n       x = \"Actual difference (in standard deviations)\",\n       subtitle = \"Vertical blue line shows where the actual difference equals the minimum difference to detect that the 80% power calculation was based upon.\nHorizontal orange line shows the observed average proportion of 'fragile' p values in the recent psychology literature.\",\n       title = \"Fragility of p values in relation to actual and planned differences in a two sample t test.\")\n\n# Heatmap:\nres |> \n  ggplot(aes(x = actual_diff, y = as.ordered(planned_diff), fill = prop_fragile)) +\n  geom_tile() +\n  geom_tile(data = filter(res, prop_fragile > 0.25 & prop_fragile < 0.31),\n            fill = \"white\", alpha = 0.1, colour = \"white\", linewidth = 2) +\n  scale_fill_viridis_c(label = percent, direction = -1) +\n  theme(panel.grid.minor = element_blank()) +\n  labs(y= \"Smallest detectable difference for 80% power\",\n       x = \"Actual difference (in standard deviations)\",\n       fill = \"Proportion of significant p values that are between 0.01 and 0.05:\",\n       subtitle = \"Sample size is based on 80% power for the difference on the vertical axis. White boxes indicate where the proportion of fragile significant p values is between 25% and 31%.\",\n       title = \"Fragility of p values in relation to actual and planned differences in a two sample t test.\")\nHere’s the heatmap we get from that. It’s prettier but I think not actually as clear as the simpler, faceted line plot.\nThat’s all for now. I don’t think there’s any big implications of this. Just a better understanding of what proportion of\np\nvalues we might expect to be in this fragile range and what impacts on it.\nI’m planning on looking at some real life data, on a completely different topic, in my next post.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nfree range statistics - R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Last week I posted about the proportion of p values that will be ‘fragile’ under various types of scientific experiments or other inferences. The proportion of p values that is fragile had been defined as the proportion that are between 0.01 and 005. I...",
      "meta_keywords": null,
      "og_description": "Last week I posted about the proportion of p values that will be ‘fragile’ under various types of scientific experiments or other inferences. The proportion of p values that is fragile had been defined as the proportion that are between 0.01 and 005. I...",
      "og_image": "https://freerangestats.info/img/0296-lines.png",
      "og_title": "More on power and ‘fragile’ p-values by @ellis2013nz | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 8.9,
      "sitemap_lastmod": null,
      "twitter_description": "Last week I posted about the proportion of p values that will be ‘fragile’ under various types of scientific experiments or other inferences. The proportion of p values that is fragile had been defined as the proportion that are between 0.01 and 005. I...",
      "twitter_title": "More on power and ‘fragile’ p-values by @ellis2013nz | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/06/more-on-power-and-fragile-p-values-by-ellis2013nz/",
      "word_count": 1776
    }
  }
}