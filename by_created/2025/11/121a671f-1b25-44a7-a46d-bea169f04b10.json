{
  "uuid": "121a671f-1b25-44a7-a46d-bea169f04b10",
  "created_at": "2025-11-22 19:58:23",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/06/from-math-to-code-building-gam-with-penalty-functions-from-scratch/",
    "crawled_at": "2025-11-22T10:47:30.675681",
    "external_links": [
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/",
        "text": "r on Everyday Is A School Day"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#objectives",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#linear",
        "text": "Equation of Linear Regression"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#penalty",
        "text": "Add Penalty Parameter"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#beta",
        "text": "Proof of Derivate with Respect to Beta Coefficients"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#optimal",
        "text": "Find Optimal Lambda"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#code",
        "text": "Let‚Äôs Code"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#compare",
        "text": "Compare Our Custom Function vs mgcv::gam"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#idsky",
        "text": "Applying this to Real Data"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#lessons",
        "text": "Lessons Learnt"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#the-end-in-mind",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#linear",
        "text": null
      },
      {
        "href": "https://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf",
        "text": "Matrix Derivates Cheat Sheet"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#penalty",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#how-do-we-get-penalty-matrix-s",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/",
        "text": "cox deboor recursion"
      },
      {
        "href": "https://en.wikipedia.org/wiki/Riemann_sum",
        "text": "Riemann‚Äôs sum"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#beta",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#lambda",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#gcv",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#optimal",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#code",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#penalty-matrix",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#find-the-minimum-gcv",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#find-beta-coefficients",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#compare",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#idsky",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#improvement",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/#lessons",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/",
        "text": "comment or visit my other blogs"
      },
      {
        "href": "https://bsky.app/profile/kenkoonwong.bsky.social",
        "text": "BlueSky"
      },
      {
        "href": "https://twitter.com/kenkoonwong/",
        "text": "twitter"
      },
      {
        "href": "https://github.com/kenkoonwong/",
        "text": "GitHub"
      },
      {
        "href": "https://med-mastodon.com/@kenkoonwong",
        "text": "Mastodon"
      },
      {
        "href": "https://www.kenkoonwong.com/contact/",
        "text": "contact me"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/gam-penalty/",
        "text": "r on Everyday Is A School Day"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "From Math to Code: Building GAM with Penalty Functions From Scratch | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-1-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-3-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-5-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-6-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-6-2.png?w=450&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/",
        "text": "r on Everyday Is A School Day"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-393002 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">From Math to Code: Building GAM with Penalty Functions From Scratch</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">June 10, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/\">r on Everyday Is A School Day</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.kenkoonwong.com/blog/gam-penalty/\"> r on Everyday Is A School Day</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><blockquote>\n<p>Enjoyed learning penalized GAM math. Built penalty matrices, optimized Œª using GCV, and implement our own GAM function. Confusing? Yes! Rewarding? Oh yes!</p>\n</blockquote>\n<p>We dove into the engine of what made basis spline work on our last blog, now let‚Äôs add to that and see how we can further improve the generalized additive model with penalty. Our previous linear regression model has a downside, it may overfit. Let‚Äôs see how we can minimize that with penalty function.</p>\n<h2 id=\"objectives\">Objectives\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#objectives\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#linear\" rel=\"nofollow\" target=\"_blank\">Equation of Linear Regression</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#penalty\" rel=\"nofollow\" target=\"_blank\">Add Penalty Parameter</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#beta\" rel=\"nofollow\" target=\"_blank\">Proof of Derivate with Respect to Beta Coefficients</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#optimal\" rel=\"nofollow\" target=\"_blank\">Find Optimal Lambda</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#code\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs Code</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#compare\" rel=\"nofollow\" target=\"_blank\">Compare Our Custom Function vs mgcv::gam</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#idsky\" rel=\"nofollow\" target=\"_blank\">Applying this to Real Data</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#lessons\" rel=\"nofollow\" target=\"_blank\">Lessons Learnt</a></li>\n</ul>\n<h4 id=\"the-end-in-mind\">The End In Mind\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#the-end-in-mind\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>Keep this at the back of our mind, our end goal is to understand this\n$$\n\\min (|| y ‚Äì \\beta x ||^2 + \\lambda \\beta^T S \\beta)\n$$\nLet‚Äôs split this up to two parts. The linear equation which is the left side, and the penalty equation which is the right.</p>\n<h2 id=\"linear\">Equation of Linear Regression\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#linear\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Let‚Äôs look the first equation, it‚Äôs basically ordinary least square.</p>\n<p>$$\ny = x \\beta + \\varepsilon \\\\\ny ‚Äì x \\beta = \\varepsilon\n$$</p>\n<p>$$\n\\text{In order to ensure no negative values, we square the error} \\\\\n(y ‚Äì x \\beta)^2 = \\varepsilon^{2}\n$$</p>\n<p>$$\n\\text{We then want to minimize the error squared} \\\\\n\\min \\varepsilon^{2} = \\min (y ‚Äì x \\beta)^2\n$$\n$$\n\\text{Sum over all error squared} \\\\\n= \\min \\sum (y ‚Äì x \\beta)^2 \\\\\n= (y ‚Äì x \\beta)^T (y ‚Äì x \\beta) \\text{ which is the math trick for sum of squares}\n$$\n$$\n\\text{Expand the equation} \\\\\n= y^T y ‚Äì y^T (x \\beta) ‚Äì (x \\beta)^T y + (x \\beta)^T (x \\beta) \\\\\n= y^T y ‚Äì 2 (x^T \\beta)^T y + \\beta^T (x^T x) \\beta\n$$\nIf you‚Äôre like me rusty in math, you might be wondering how did we get from the 2nd last equation to the last equation. Let‚Äôs break it down. Especially <code>\\(-2 (x^T \\beta)^T y + \\beta^T (x^T x) \\beta\\)</code></p>\n<p>$$\n-y^{T} (x \\beta) ‚Äì (x \\beta)^{T} y \\\\\n= -y^{T} (x \\beta) ‚Äì \\beta^{T} x^{T} y \\\\\n\\text{because } s = s^{T} \\text{ if scalar and both }  y^{T} (x \\beta) \\text{ and } \\beta^{T} x^{T} y \\text{ are scalars}   \\\\\n\\text{meaning} ‚Äì \\beta^{T} x^{T} y ‚Äì \\beta^{T} x^{T} y \\\\\n= -2 (x^T \\beta)^{T} y\n$$</p>\n<p>Alright, as for the last term, we can see that <code>\\((x \\beta)^T (x \\beta)\\)</code> is the same as <code>\\(\\beta^T (x^T x) \\beta\\)</code> because of the property of transpose.</p>\n<p>$$\n(x \\beta)^T (x \\beta) \\\\\n= \\beta^T x^T x \\beta\n$$\n\n<a href=\"https://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf\" rel=\"nofollow\" target=\"_blank\">Matrix Derivates Cheat Sheet</a></p>\n<p>Fantastic! Now let‚Äôs plug these into the original equation.\n$$\n\\min \\sum (y ‚Äì x \\beta)^2 = \\min (y^T y ‚Äì 2 x^T \\beta^T y + \\beta^T x^T x \\beta)\n$$</p>\n<h2 id=\"penalty\">Add Penalty Parameter\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#penalty\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Now we have the equation for linear regression, let‚Äôs add a penalty parameter <code>\\(\\lambda\\)</code> to it. This is to prevent overfitting. The other part of the equation is the penalty term, which is <code>\\(\\beta^T S \\beta\\)</code> where <code>\\(S\\)</code> is a penalty matrix.\n$$\n\\lambda \\beta^T S \\beta\n$$\nWhere <code>\\(S\\)</code> is a penalty matrix, and <code>\\(\\lambda\\)</code> is a penalty parameter.</p>\n<h4 id=\"how-do-we-get-penalty-matrix-s\">How Do We Get Penalty Matrix (S)?\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#how-do-we-get-penalty-matrix-s\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>The penalty matrix <code>\\(S\\)</code> is typically derived from the second derivative of the basis functions. In the case of B-splines, it can be calculated as a difference matrix that captures the curvature of the spline. The second-order difference matrix is often used, which penalizes the second derivative of the spline function.</p>\n<p>But how did we get to <code>\\(\\beta^{T}S\\beta\\)</code>? Now, beta <code>\\(\\beta\\)</code> and <code>\\(B\\)</code> look similar, it was confusing for me to when we have all these equations. Let‚Äôs temporarily change <code>\\(\\beta\\)</code> (<code>Beta</code>, aka coefficients we want to estiamte) to <code>c</code> as <code>coefficients</code>, and maintain <code>\\(B\\)</code> as the B-spline basis functions. üëç So we have <code>\\(c^{T}Sc\\)</code>, and <code>S</code> as <code>penalty matrix</code>.</p>\n<p>OK. Let‚Äôs assume that this is some <code>f(x)</code> function where <code>x</code> is the grid point. And we essentially want to find the second derivative of the B-spline basis functions at these grid points. The second derivative is denoted as <code>\\(f''(x)\\)</code>. We want to square it to be positive definite, so we have <code>\\(f''(x)^2\\)</code>.</p>\n<p>So, what <code>f(x)</code> are we trying to penalize here? You got it, the B-spline basis functions. We want to penalize the second derivative of the B-spline basis functions. Let‚Äôs refresh our memory on the B-spline basis functions. The B-spline basis functions are defined as follows:</p>\n<p>$$\nf(x) = c_1B_1(x) + c_2B_2(x) + ‚Ä¶ + c_nB_n(x)\n$$</p>\n<p>Let‚Äôs just take <code>3</code> basis function just for exercise.</p>\n<p>$$\nf(x) = c_1B_1(x) + c_2B_2(x) + c_3B_3(x)\n$$</p>\n<p>We want to square the second derivative of this function, so we have:\n$$\nf‚Äô‚Äô(x) = c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x)\n$$\nNext, for positive definiteness, we square the second derivative:\n$$\nf‚Äô‚Äô(x)^2 = (c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x))^2\n$$\nNow, we want to integrate this over the domain of the B-spline basis functions.\n$$\n\\int f‚Äô‚Äô(x)^2 dx = \\int (c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x))^2 dx\n$$\nOK, now it becomes a tad scary. Let‚Äôs ignore the integration for now and try to expand the quadratic term:\n$$\n(c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x))^2 \\\\\n= c_1^2 B_1‚Äô‚Äô(x)^2 + c_2^2 B_2‚Äô‚Äô(x)^2 + c_3^2 B_3‚Äô‚Äô(x)^2 \\\\ + 2c_1c_2B_1‚Äô‚Äô(x)B_2‚Äô‚Äô(x) + 2c_1c_3B_1‚Äô‚Äô(x)B_3‚Äô‚Äô(x) + 2c_2c_3B_2‚Äô‚Äô(x)B_3‚Äô‚Äô(x) \\\\\n\\text{which eventually leads to} \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) \\\\\n$$\nStill with me? Now let‚Äôs plug this back into our integration:\n$$\n\\int f‚Äô‚Äô(x)^2 dx = \\int \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx\n$$\nWe‚Äôll come back to the <code>\\(c^{T}Sc\\)</code> proof. Now, let‚Äôs define the penalty matrix <code>S</code> as follows:\n$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx\n$$</p>\n<p>OK, how do we then calculate or estimate <code>\\(B_i''(x)\\)</code> ? We don‚Äôt have a formula or anything to get to the second derivative of the B-spline basis functions directly. Instead, we will use numerical integration to approximate the second derivative at the grid points. But how !?! In come, <code>Taylor series approxiation</code>, again!</p>\n<p>The second derivative of a function at a point can be approximated using finite differences as follows using a general <code>f(x)</code>:\n$$\n\\text{foreward approximation} \\\\\nf(x+h) \\approx f(x) + f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2 \\\\\n\\text{backward approximation} \\\\\nf(x-h) \\approx f(x) ‚Äì f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2\n$$\nNow, there is a mathematical trickery where we can use the forward and backward approximation to get to the second derivative by adding both together:\n$$\n\\text{adding both together} \\\\\nf(x+h) + f(x-h)\n= (f(x) + f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2) + (f(x) ‚Äì f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2) \\\\\nf(x+h) + f(x-h) = 2f(x) + f‚Äô‚Äô(x)h^2 \\\\\nf‚Äô‚Äô(x)h^2 = f(x+h) + f(x-h) ‚Äì 2f(x) \\\\\nf‚Äô‚Äô(x) = \\frac{f(x+h) + f(x-h) ‚Äì 2f(x)}{h^2}\n$$</p>\n<p>We basically just turn the <code>f</code> to <code>B</code> to go back to our <code>S</code> matrix (penalty matrix)\n$$\nB_i‚Äô‚Äô(x) = \\frac{B_i(x+h) + B_i(x-h) ‚Äì 2B_i(x)}{h^2}\n$$\nThis is the same for <code>\\(B_j''\\)</code>.</p>\n<p>Now, we can plug this back into our penalty matrix <code>S</code>:\n$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\int \\frac{B_i(x+h) + B_i(x-h) ‚Äì 2B_i(x)}{h^2} \\cdot \\frac{B_j(x+h) + B_j(x-h) ‚Äì 2B_j(x)}{h^2} dx\n$$</p>\n<p>In this case we will use numerical integration to compute the penalty matrix. Meaning, we will use a grid of 100 evenly separated points to evaluate the B-spline basis functions and their second derivatives, and then compute the penalty matrix <code>S</code>. We have to ensure NOT to use the first and last point since we need to use both forward and backward approximation to get to the second derivative.</p>\n<p>Example, let‚Äôs say we have 5 evenly separated points (this is very small, but just for illustration) and our min point is -5, max is 5, which means <code>x = [-5, -2.5, 0, 2.5, 5]</code> as our grid points. <code>h</code> here is our step size.\n$$\nh = \\frac{5 ‚Äì (-5)}{5 ‚Äì 1} = \\frac{10}{4} = 2.5\n$$\nNow let‚Äôs look at what <code>\\(B_i\\)</code> (pay attention, this is NOT the 2nd derivative) looks like with 3 basis function (columns), and 5 grid points (rows), these can be calculated from \n<a href=\"https://www.kenkoonwong.com/blog/bspline/\" rel=\"nofollow\" target=\"_blank\">cox deboor recursion</a>, but you can just use <code>spliness::bs(x=c(-5,-2.5,0,2.5,5))</code>:\n$$\n<code>\\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\\\ 0.42 &amp; 0.14 &amp; 0.02 \\\\\\ 0.38 &amp; 0.38 &amp; 0.13 \\\\\\ 0.14 &amp; 0.42 &amp; 0.42 \\\\\\ 0 &amp; 0 &amp; 1  \\end{bmatrix}</code>\n$$\nNow we want to then use this formula <code>\\(\\frac{B_i(x+h) + B_i(x-h) - 2B_i(x)}{h^2}\\)</code> to create <code>\\(B_i''(x)\\)</code> (the second derivative of the B-spline basis functions). Note that we don‚Äôt want to look at <code>x=-5 and x=5</code>.</p>\n<p>Let‚Äôs look at <code>\\(B_1\\)</code> and <code>\\(x=-2.5\\)</code> as an example:</p>\n<p>$$\nB_1‚Äô‚Äô(x=-2.5) = \\frac{B_1(x+2.5) + B_1(x-2.5) ‚Äì 2B_1(x)}{2.5^2} \\\\\n= \\frac{B_1(x=0) + B_1(x=-5) ‚Äì 2B_1(x=-2.5)}{2.5^2} \\\\\n= \\frac{0.38 + 0 ‚Äì 2(0.42)}{2.5^2} \\\\\n= \\frac{0.38 ‚Äì 0.84}{6.25}\n$$\nAlright, we don‚Äôt have to get the result, the above is just an example on how to use the formula. Now let‚Äôs construct our <code>\\(D\\)</code> matrix, which is a matrix of <code>\\(B_i''(x)\\)</code> with the above formula like so:</p>\n<p>$$\nD \\\\\n= \\begin{bmatrix}\n\\frac{B_1(x_2+h) + B_1(x_2-h) ‚Äì 2B_1(x_2)}{h^2} &amp; \\frac{B_2(x_2+h) + B_2(x_2-h) ‚Äì 2B_2(x_2)}{h^2} &amp; \\frac{B_3(x_2+h) + B_3(x_2-h) ‚Äì 2B_3(x_2)}{h^2} \\\\\n\\frac{B_1(x_3+h) + B_1(x_3-h) ‚Äì 2B_1(x_3)}{h^2} &amp; \\frac{B_2(x_3+h) + B_2(x_3-h) ‚Äì 2B_2(x_3)}{h^2} &amp; \\frac{B_3(x_3+h) + B_3(x_3-h) ‚Äì 2B_3(x_3)}{h^2} \\\\\n\\frac{B_1(x_4+h) + B_1(x_4-h) ‚Äì 2B_1(x_4)}{h^2} &amp; \\frac{B_2(x_4+h) + B_2(x_4-h) ‚Äì 2B_2(x_4)}{h^2} &amp; \\frac{B_3(x_4+h) + B_3(x_4-h) ‚Äì 2B_3(x_4)}{h^2}\n\\end{bmatrix}`\n$$</p>\n<p>Notie that we don‚Äôt have to calculate the first and last row since we don‚Äôt have the forward and backward approximation for those points.</p>\n<p>Still with me? Let‚Äôs refresh on where we digressed. This <code>\\(S[i,j] = \\int B_i''(x)B_j''(x) dx\\)</code> !!! How on earth do we integrate this? We don‚Äôt have to! We can use numerical integration to approximate the integral. In this case, we can use the trapezoidal rule or \n<a href=\"https://en.wikipedia.org/wiki/Riemann_sum\" rel=\"nofollow\" target=\"_blank\">Riemann‚Äôs sum</a> to estimate.</p>\n<p>$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\cdot h \\\\\n= D^{T} D \\cdot h \\\\\n\\text{Because } \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\text{ is essentially } D^{T} D\n$$\nHere is why <code>\\(\\sum_{k=1}^{n} B_i''(x_k)B_j''(x_k)\\)</code> is essentially <code>\\(D^{T} D\\)</code>:\n$$\n\\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\\\\n= \\begin{bmatrix}\nB_1‚Äô‚Äô(x_2) &amp; B_1‚Äô‚Äô(x_3) &amp; B_1‚Äô‚Äô(x_4) \\\\\nB_2‚Äô‚Äô(x_2) &amp; B_2‚Äô‚Äô(x_3) &amp; B_2‚Äô‚Äô(x_4) \\\\\nB_3‚Äô‚Äô(x_2) &amp; B_3‚Äô‚Äô(x_3) &amp; B_3‚Äô‚Äô(x_4)\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nB_1‚Äô‚Äô(x_2) &amp; B_2‚Äô‚Äô(x_2) &amp; B_3‚Äô‚Äô(x_2) \\\\\nB_1‚Äô‚Äô(x_3) &amp; B_2‚Äô‚Äô(x_3) &amp; B_3‚Äô‚Äô(x_3) \\\\\nB_1‚Äô‚Äô(x_4) &amp; B_2‚Äô‚Äô(x_4) &amp; B_3‚Äô‚Äô(x_4)\n\\end{bmatrix} \\\\\n= D^{T} D\n$$\nThis <code>\\(D^{T} D \\cdot h\\)</code> is ultimately our <code>\\(S\\)</code> matrix, which is the penalty matrix.</p>\n<p>OK, now let‚Äôs bring this equation back to our original equation:\n$$\n\\int f‚Äô‚Äô(x)^2 dx = \\int \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j] \\\\\n= c^{T} S c\n$$</p>\n<p>Now you may ask how on earth did we go from <code>\\(\\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j]\\)</code> to <code>\\(c^{T} S c\\)</code></p>\n<p>Let‚Äôs set an example:\n$$\nc = \\begin{bmatrix}\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix} ,\nS = \\begin{bmatrix}\nS_{1,1} &amp; S_{1,2} &amp; S_{1,3} \\\\\nS_{2,1} &amp; S_{2,2} &amp; S_{2,3} \\\\\nS_{3,1} &amp; S_{3,2} &amp; S_{3,3}\n\\end{bmatrix}\n$$\nNow we know that <code>\\(\\sum\\)</code> in coding is basically a <code>for loop</code>, hence <code>\\(\\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j]\\)</code> means we have a nested for loop on another for loop. If we were to expand this it will look like</p>\n<p>$$\n\\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j] \\\\\n= c_1c_1S_{1,1} + c_1c_2S_{1,2} + c_1c_3S_{1,3} + \\\\\nc_2c_1S_{2,1} + c_2c_2S_{2,2} + c_2c_3S_{2,3} + \\\\\nc_3c_1S_{3,1} + c_3c_2S_{3,2} + c_3c_3S_{3,3}\n$$\nHow we then turn these into <code>\\(c^{T}Sc\\)</code> is the following magic matrix operation.</p>\n<ol>\n<li>Rearrange</li>\n</ol>\n<p>$$\nc_1(S_{1,1}c_1 + S_{1,2}c_2 + S_{1,3}c_3) + \\\\\nc_2(S_{2,1}c_1 + S_{2,2}c_2 + S_{2,3}c_3) + \\\\\nc_3(S_{3,1}c_1 + S_{3,2}c_2 + S_{3,3}c_3)\n$$</p>\n<ol start=\"2\">\n<li>Notice how we can use matrix multiplication to express this as a dot product:</li>\n</ol>\n<p>$$\n`\\begin{bmatrix}\nS_{1,1}c_1 + S_{1,2}c_2 + S_{1,3}c_3 \\\\\nS_{2,1}c_1 + S_{2,2}c_2 + S_{2,3}c_3 \\\\\nS_{3,1}c_1 + S_{3,2}c_2 + S_{3,3}c_3\n\\end{bmatrix} = \\begin{bmatrix}\nS_{1,1} &amp; S_{1,2} &amp; S_{1,3} \\\\\nS_{2,1} &amp; S_{2,2} &amp; S_{2,3} \\\\\nS_{3,1} &amp; S_{3,2} &amp; S_{3,3}\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix} \\\\\n= S \\cdot c \\\\\n\\text{In order to get to } c^{T}Sc \\text{ we need to transpose } c \\text{, hence} \\\\\nc^{T}Sc = \\begin{bmatrix}\nc_1 &amp; c_2 &amp; c_3 \\end{bmatrix} \\cdot \\begin{bmatrix}\nS_{1,1} &amp; S_{1,2} &amp; S_{1,3} \\\\\nS_{2,1} &amp; S_{2,2} &amp; S_{2,3} \\\\\nS_{3,1} &amp; S_{3,2} &amp; S_{3,3}\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix} \\\\\n= c_1(S_{1,1}c_1 + S_{1,2}c_2 + S_{1,3}c_3) + \\\\\nc_2(S_{2,1}c_1 + S_{2,2}c_2 + S_{2,3}c_3) + \\\\\nc_3(S_{3,1}c_1 + S_{3,2}c_2 + S_{3,3}c_3) \\\\\n\\text{which is essentially a scalar!}\n$$</p>\n<p>Phew, all those math gymnastics and acrobatics! Worked out a sweat, don‚Äôt they? Now, remember we switch <code>\\(\\beta\\)</code> to <code>c</code>, these are coefficients, so that we don‚Äôt confuse <code>\\(\\beta\\)</code> (beta) with <code>B</code>, our basis function? OK, now that basis function is long ‚Äúgone‚Äù, we will then turn <code>\\(c^{T}Sc\\)</code> back to <code>\\(\\beta^{T}S\\beta\\)</code> for consistency with general notation.</p>\n<p>And there you have it! We have derived the penalty term <code>\\(\\beta^{T}S\\beta\\)</code> in the generalized additive model with B-spline basis functions.</p>\n<h2 id=\"beta\">Proof of Derivate with Respect to Beta Coefficients\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#beta\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Now that we have the penalty term, let‚Äôs take the derivative of the entire equation with respect to <code>\\(\\beta\\)</code>:</p>\n<p>$$\n\\min (|| y ‚Äì \\beta x ||^2 + \\lambda \\beta^T S \\beta) \\\\\n= \\min (y^T y ‚Äì 2 x^T \\beta^T y + \\beta^T x^T x \\beta + \\lambda \\beta^T S \\beta)\n$$</p>\n<p>Taking the derivative with respect to <code>\\(\\beta\\)</code> gives us:\n$$\n\\frac{\\partial}{\\partial \\beta} (y^T y ‚Äì 2 x^T \\beta^T y + \\beta^T x^T x \\beta + \\lambda \\beta^T S \\beta) \\\\\n= 0 -2 x^T y + 2 x^T x \\beta + 2 \\lambda S \\beta\n$$\nSetting the derivative to zero gives us the normal equations:\n$$\n0 ‚Äì 2 x^T y + 2 x^T x \\beta + 2 \\lambda S \\beta \\\\\n2 x^T y = 2 x^T x \\beta + 2 \\lambda S \\beta \\\\\nx^T y = x^T x \\beta + \\lambda S \\beta \\\\\n\\beta (x^T x + \\lambda S) = x^T y \\\\\n\\beta = (x^T x + \\lambda S)^{-1} x^T y\n$$\nOK. We went through ALL those trouble to find out the equation to find <code>beta</code> coefficients for After all these, what on earth is <code>lambda</code> !?!?!</p>\n<h2 id=\"lambda\">What is Lambda\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#lambda\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p><code>Lambda</code> is a penalty parameter that controls the amount of regularization applied to the model. It is a hyperparameter that you can tune to find the optimal balance between fitting the data and preventing overfitting. A larger <code>lambda</code> value will result in more regularization, while a smaller value will result in less regularization. <code>lambda</code> = 0 means no penalty, it is equivalent to ordinary least squares regression. Once we found the optimal <code>lambda</code>, we can plug that number in to calculate our <code>beta</code>, how cool! Now how to we find the optimal <code>lambda</code>?</p>\n<h3 id=\"gcv\">Generalized Cross Validated (GCV)\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#gcv\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>To find the optimal <code>lambda</code>, we can use Generalized Cross Validation (GCV). GCV is a method that estimates the prediction error of a model by minimizing the GCV score. The GCV score is defined as follows:</p>\n<p>$$\n\\text{GCV} = \\frac{|| y ‚Äì \\hat{y} ||^2}{(1 ‚Äì \\text{trace}(H))^2}\n$$</p>\n<p>Where <code>\\(\\hat{y}\\)</code> is the predicted value, and <code>\\(H\\)</code> is the hat matrix. The trace of the hat matrix is the sum of the diagonal elements of the hat matrix, which is the number of parameters in the model. The GCV score is a measure of how well the model fits the data, and we want to minimize this score to find the optimal <code>lambda</code>.</p>\n<h3 id=\"optimal\">Find Optimal Lambda\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#optimal\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<p>In other words:  <br/>\nStep 1. Calculate <code>H</code> (hat matrix)</p>\n<p>$$\nH = X(X^T X + \\lambda S)^{-1} X^T\n$$</p>\n<p>Step 2. Calculate <code>\\(\\hat{y}\\)</code> (predicted value)</p>\n<p>$$\n\\hat{y} = H y\n$$</p>\n<p>Step 3. Calculate RSS</p>\n<p>$$\nRSS = || y ‚Äì \\hat{y} ||^2\n$$\nStep 4. Calculate GCV score</p>\n<p>$$\n\\text{GCV}(\\lambda) = \\frac{n \\times RSS}{(n ‚Äì \\text{trace}(H))^2}\n$$\nStep 5. Find the optimal <code>lambda</code> that minimizes the GCV score. Et viola!</p>\n<h2 id=\"code\">Let‚Äôs Code\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#code\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Now, we did A LOT of math there, let‚Äôs bring down the essential formulae as a reminder. But first, let‚Äôs simulate some non-linear data!</p>\n<pre>library(tidyverse)\nlibrary(splines)\nlibrary(mgcv)\n\nset.seed(123)\nn &lt;- 1000\nz &lt;- runif(n,-5,5)\ny &lt;- sin(z) + rnorm(n,0.1)\ndf &lt;- tibble(z = z, y = y)\n\ndf |&gt;\n  ggplot(aes(x = z, y = y)) +\n  geom_point(alpha=0.5)\n</pre><img data-lazy-src=\"https://i0.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-1-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-1-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>This should be pretty straight forward. We‚Äôve seen this before in our previous blog.</p>\n<h4 id=\"penalty-matrix\">Penalty Matrix\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#penalty-matrix\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\cdot h \\\\\n= D^{T} D \\cdot h \\\\\n\\text{Because } \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\text{ is essentially } D^{T} D\n$$</p>\n<pre>penalty_matrix &lt;- function(z_vals, n_basis, n_grid = 100) {\n  # Step 1: Create fine grid over the domain\n  z_min &lt;- min(z_vals)\n  z_max &lt;- max(z_vals)\n  z_grid &lt;- seq(z_min, z_max, length.out = n_grid)\n  h &lt;- z_grid[2] - z_grid[1]\n\n  # Step 2: Evaluate B-spline basis functions on the grid\n  X_grid &lt;- bs(z_grid, df = n_basis, intercept = TRUE)\n\n  # Step 3: Compute second derivatives using finite differences; Also known as Bi\" or Bj\" here (Basis, not beta), or D\n  D &lt;- matrix(0, n_grid - 2, n_basis)\n\n  for (i in 2:(n_grid - 1)) {\n    for (j in 1:n_basis) {\n      D[i-1, j] &lt;- (X_grid[i-1, j] - 2*X_grid[i, j] + X_grid[i+1, j]) / h^2\n    }\n  }\n\n  # Step 4: Compute penalty matrix S[i,j] = ‚à´ B''·µ¢(x) B''‚±º(x) dx\n  S &lt;- t(D) %*% D * h\n\n  return(S)\n}\n</pre><p>The above is our penalty matrix, basically to calculate <code>S</code>.</p>\n<h4 id=\"find-the-minimum-gcv\">Find the Minimum GCV\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#find-the-minimum-gcv\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\text{GCV}(\\lambda) = \\frac{n \\times RSS}{(n - \\text{trace}(H))^2}\n$$</p>\n<pre># y &lt;- df_bs$n\n# z &lt;- seq(1,length(df_bs$date),1)\nn_basis &lt;- 10\nx &lt;- bs(z, df = n_basis, intercept=T)\n# s &lt;- penalty_matrix(n_basis)\ns &lt;- penalty_matrix(z,n_basis=n_basis)\nlambda_vec &lt;- seq(0,10,0.01)\n\n\ngcv &lt;- numeric(length(lambda_vec))\n# GCV function\nfor (i in 1:length(lambda_vec)) {\n  lambda &lt;- lambda_vec[i]\n  \n  # 1. Calculate H = x(x·µÄx + ŒªS)‚Åª¬πx·µÄ\n  H &lt;- function(lambda, x, s) {\n    # Calculate H\n    H &lt;- x %*% solve(t(x) %*% x + lambda * s) %*% t(x)\n    return(H)\n  }\n  \n  h &lt;- H(lambda,x, s)\n  \n  # 2. Calculate ≈∑ = Hy  \n  y_hat &lt;- h %*% y\n  \n  # 3. Calculate RSS = ||y - ≈∑||¬≤\n  rss &lt;- sum((y-y_hat)^2)\n  \n  # 4. Calculate GCV(Œª) = (n √ó RSS) / (n - tr(H))¬≤\n  tr_H &lt;- sum(diag(h))\n  gcv_value &lt;- (n * rss) / ((n - tr_H)^2)\n  \n  gcv[i] &lt;- gcv_value\n}\n\ntibble(lambda=lambda_vec, gcv=gcv) |&gt;\n  ggplot(aes(x=lambda, y=gcv)) +\n  geom_line() +\n  labs(title=\"GCV vs Lambda\", x=\"Lambda\", y=\"GCV\") +\n  theme_minimal()\n</pre><img data-lazy-src=\"https://i1.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-3-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-3-1.png?w=450&amp;ssl=1\"/></noscript>\n<pre>## optimal lambda\n(opt_lambda &lt;- lambda_vec[which(gcv==min(gcv))])\n\n## [1] 0.6\n\n(min_gcv &lt;- min(gcv_value))\n\n## [1] 1.018823\n</pre><p>Alright, the plot shows GCV value vs lambda. We essentially want to find lambda with the lowest GCV value. <code>opt_lambda</code> is the optimal lambda value. We then use this number to calculate our <code>beta coefficients</code></p>\n<h4 id=\"find-beta-coefficients\">Find Beta Coefficients\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#find-beta-coefficients\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>$$\n\\beta = (x^T x + \\lambda S)^{-1} x^T y \\\\\n$$</p>\n<pre>(beta &lt;- solve(t(x) %*% x + opt_lambda * s) %*% t(x) %*% y)\n\n##          [,1]\n## 1   1.2375133\n## 2   1.1800055\n## 3   0.8037137\n## 4  -1.2787422\n## 5  -0.6854527\n## 6   1.0479672\n## 7   1.3246798\n## 8  -0.4118710\n## 9  -1.2195539\n## 10 -0.8501258\n</pre>\n<h2 id=\"compare\">Compare Our Custom Function vs mgcv::gam\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#compare\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<pre># using mgcv::gam\nmodel_gam &lt;- gam(y~s(z, k=n_basis, bs = \"bs\"))\n\n# turn our calculated beta to vector\nbeta &lt;- as.vector(beta)\n\n# Plotting the results\ntibble(z, y) |&gt;\n  ggplot(aes(z, y)) +\n  geom_point(alpha=0.2) +\n  geom_line(aes(x = z, y = x %*% beta), color = \"blue\", size=2, alpha=0.5) +\n  geom_line(aes(x = z, y = predict(model_gam)), color = \"red\", size = 2, alpha=0.5) +\n  labs(x = \"z\", y = \"y\") +\n  ggtitle(label = \"GAM Fit with Custom Penalty Matrix and mgcv Package\") +\n  theme_minimal()\n</pre><img data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-5-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-5-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Wow, look at that! The blue line is our custom function, and the red line is the <code>mgcv::gam</code> function. They look pretty similar, right? As you can see we have <code>blue</code> for our custom gam and <code>red</code> as mgcv::gam. We get a purple color!</p>\n<h2 id=\"idsky\">Let‚Äôs Try A Real Dataset, Our Very Own #IDsky\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#idsky\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Now let‚Äôs try a different dataset, let‚Äôs look at Bluesky #IDsky posts counts</p>\n<pre># Load the dataset\nload(\"df_bs.rda\")\n\n# Some data wrangling\ny &lt;- df_bs$n\nz &lt;- seq(1,length(df_bs$date),1)\n\n# Create B-spline basis functions\nn_basis &lt;- 20\nx &lt;- bs(z, df = n_basis, intercept=T)\ns &lt;- penalty_matrix(z,n_basis=n_basis,n_grid = 200)\n\n# find optimal lambda\nlambda_vec &lt;- seq(44,48,0.001)\n\ngcv &lt;- numeric(length(lambda_vec))\n\n# GCV function\nfor (i in 1:length(lambda_vec)) {\n  lambda &lt;- lambda_vec[i]\n  \n  # 1. Calculate H = x(x·µÄx + ŒªS)‚Åª¬πx·µÄ\n  H &lt;- function(lambda, x, s) {\n    # Calculate H\n    H &lt;- x %*% solve(t(x) %*% x + lambda * s) %*% t(x)\n    return(H)\n  }\n  \n  h &lt;- H(lambda,x, s)\n  \n  # 2. Calculate ≈∑ = Hy  \n  y_hat &lt;- h %*% y\n  \n  # 3. Calculate RSS = ||y - ≈∑||¬≤\n  rss &lt;- sum((y-y_hat)^2)\n  \n  # 4. Calculate GCV(Œª) = (n √ó RSS) / (n - tr(H))¬≤\n  tr_H &lt;- sum(diag(h))\n  gcv_value &lt;- (n * rss) / ((n - tr_H)^2)\n  \n  gcv[i] &lt;- gcv_value\n}\n\ntibble(lambda=lambda_vec, gcv=gcv) |&gt;\n  ggplot(aes(x=lambda, y=gcv)) +\n  geom_line() +\n  labs(title=\"GCV vs Lambda\", x=\"Lambda\", y=\"GCV\") +\n  theme_minimal()\n</pre><img data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\"/></noscript>\n<pre>## optimal lambda\n(opt_lambda &lt;- lambda_vec[which(gcv==min(gcv))])\n\n## [1] 45.505\n\n(min_gcv &lt;- min(gcv_value))\n\n## [1] 129.3409\n\n(beta &lt;- solve(t(x) %*% x + opt_lambda * s) %*% t(x) %*% y)\n\n##          [,1]\n## 1    3.444439\n## 2    6.746956\n## 3   -3.840163\n## 4   15.139798\n## 5   -5.688893\n## 6   32.633905\n## 7  -31.848643\n## 8   75.342451\n## 9  192.032407\n## 10  44.874593\n## 11  48.429720\n## 12  65.626734\n## 13  54.892071\n## 14  86.819061\n## 15  53.498207\n## 16  55.391699\n## 17  67.752137\n## 18  21.012682\n## 19  77.276639\n## 20  33.333471\n\n# compare mgcv::gam\nmodel_gam &lt;- gam(y~s(z, k=n_basis, bs = \"bs\"))\nbeta &lt;- as.vector(beta)\n\n# visualize\ntibble(z, y) |&gt;\n  ggplot(aes(z, y)) +\n  geom_point(alpha=0.2) +\n  geom_line(aes(x = z, y = x %*% beta), color = \"blue\", size=2, alpha=0.5) +\n  geom_line(aes(x = z, y = predict(model_gam)), color = \"red\", size = 2, alpha=0.5) +\n  labs(x = \"z\", y = \"y\") +\n  ggtitle(label = \"GAM Fit with Custom Penalty Matrix and mgcv Package\") +\n  theme_minimal()\n</pre><img data-lazy-src=\"https://i1.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-6-2.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-6-2.png?w=450&amp;ssl=1\"/></noscript>\n<p>Look at that ! They are quite similar, aren‚Äôt they? Yes, we did it!!! PHew, that was a lot of math and coding!</p>\n<h2 id=\"improvement\">Opportunities for Improvement\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#improvement\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>Need to learn how to do the same for REML</li>\n</ul>\n<h2 id=\"lessons\">Lessons Learnt\n  <a href=\"https://www.kenkoonwong.com/blog/gam-penalty/#lessons\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>learnt lots of lots of math behind GAM with penalty</li>\n<li>learnt how to derive the penalty matrix</li>\n<li>refresh on matrix operations, calculus, and algebra</li>\n</ul>\n<p>If you like this article:</p>\n<ul>\n<li>please feel free to send me a \n<a href=\"https://www.kenkoonwong.com/blog/\" rel=\"nofollow\" target=\"_blank\">comment or visit my other blogs</a></li>\n<li>please feel free to follow me on \n<a href=\"https://bsky.app/profile/kenkoonwong.bsky.social\" rel=\"nofollow\" target=\"_blank\">BlueSky</a>, \n<a href=\"https://twitter.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">twitter</a>, \n<a href=\"https://github.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">GitHub</a> or \n<a href=\"https://med-mastodon.com/@kenkoonwong\" rel=\"nofollow\" target=\"_blank\">Mastodon</a></li>\n<li>if you would like collaborate please feel free to \n<a href=\"https://www.kenkoonwong.com/contact/\" rel=\"nofollow\" target=\"_blank\">contact me</a></li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.kenkoonwong.com/blog/gam-penalty/\"> r on Everyday Is A School Day</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "From Math to Code: Building GAM with Penalty Functions From Scratch\nPosted on\nJune 10, 2025\nby\nr on Everyday Is A School Day\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nr on Everyday Is A School Day\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nEnjoyed learning penalized GAM math. Built penalty matrices, optimized Œª using GCV, and implement our own GAM function. Confusing? Yes! Rewarding? Oh yes!\nWe dove into the engine of what made basis spline work on our last blog, now let‚Äôs add to that and see how we can further improve the generalized additive model with penalty. Our previous linear regression model has a downside, it may overfit. Let‚Äôs see how we can minimize that with penalty function.\nObjectives\nEquation of Linear Regression\nAdd Penalty Parameter\nProof of Derivate with Respect to Beta Coefficients\nFind Optimal Lambda\nLet‚Äôs Code\nCompare Our Custom Function vs mgcv::gam\nApplying this to Real Data\nLessons Learnt\nThe End In Mind\nKeep this at the back of our mind, our end goal is to understand this\n$$\n\\min (|| y ‚Äì \\beta x ||^2 + \\lambda \\beta^T S \\beta)\n$$\nLet‚Äôs split this up to two parts. The linear equation which is the left side, and the penalty equation which is the right.\nEquation of Linear Regression\nLet‚Äôs look the first equation, it‚Äôs basically ordinary least square.\n$$\ny = x \\beta + \\varepsilon \\\\\ny ‚Äì x \\beta = \\varepsilon\n$$\n$$\n\\text{In order to ensure no negative values, we square the error} \\\\\n(y ‚Äì x \\beta)^2 = \\varepsilon^{2}\n$$\n$$\n\\text{We then want to minimize the error squared} \\\\\n\\min \\varepsilon^{2} = \\min (y ‚Äì x \\beta)^2\n$$\n$$\n\\text{Sum over all error squared} \\\\\n= \\min \\sum (y ‚Äì x \\beta)^2 \\\\\n= (y ‚Äì x \\beta)^T (y ‚Äì x \\beta) \\text{ which is the math trick for sum of squares}\n$$\n$$\n\\text{Expand the equation} \\\\\n= y^T y ‚Äì y^T (x \\beta) ‚Äì (x \\beta)^T y + (x \\beta)^T (x \\beta) \\\\\n= y^T y ‚Äì 2 (x^T \\beta)^T y + \\beta^T (x^T x) \\beta\n$$\nIf you‚Äôre like me rusty in math, you might be wondering how did we get from the 2nd last equation to the last equation. Let‚Äôs break it down. Especially\n\\(-2 (x^T \\beta)^T y + \\beta^T (x^T x) \\beta\\)\n$$\n-y^{T} (x \\beta) ‚Äì (x \\beta)^{T} y \\\\\n= -y^{T} (x \\beta) ‚Äì \\beta^{T} x^{T} y \\\\\n\\text{because } s = s^{T} \\text{ if scalar and both }  y^{T} (x \\beta) \\text{ and } \\beta^{T} x^{T} y \\text{ are scalars}   \\\\\n\\text{meaning} ‚Äì \\beta^{T} x^{T} y ‚Äì \\beta^{T} x^{T} y \\\\\n= -2 (x^T \\beta)^{T} y\n$$\nAlright, as for the last term, we can see that\n\\((x \\beta)^T (x \\beta)\\)\nis the same as\n\\(\\beta^T (x^T x) \\beta\\)\nbecause of the property of transpose.\n$$\n(x \\beta)^T (x \\beta) \\\\\n= \\beta^T x^T x \\beta\n$$\nMatrix Derivates Cheat Sheet\nFantastic! Now let‚Äôs plug these into the original equation.\n$$\n\\min \\sum (y ‚Äì x \\beta)^2 = \\min (y^T y ‚Äì 2 x^T \\beta^T y + \\beta^T x^T x \\beta)\n$$\nAdd Penalty Parameter\nNow we have the equation for linear regression, let‚Äôs add a penalty parameter\n\\(\\lambda\\)\nto it. This is to prevent overfitting. The other part of the equation is the penalty term, which is\n\\(\\beta^T S \\beta\\)\nwhere\n\\(S\\)\nis a penalty matrix.\n$$\n\\lambda \\beta^T S \\beta\n$$\nWhere\n\\(S\\)\nis a penalty matrix, and\n\\(\\lambda\\)\nis a penalty parameter.\nHow Do We Get Penalty Matrix (S)?\nThe penalty matrix\n\\(S\\)\nis typically derived from the second derivative of the basis functions. In the case of B-splines, it can be calculated as a difference matrix that captures the curvature of the spline. The second-order difference matrix is often used, which penalizes the second derivative of the spline function.\nBut how did we get to\n\\(\\beta^{T}S\\beta\\)\n? Now, beta\n\\(\\beta\\)\nand\n\\(B\\)\nlook similar, it was confusing for me to when we have all these equations. Let‚Äôs temporarily change\n\\(\\beta\\)\n(\nBeta\n, aka coefficients we want to estiamte) to\nc\nas\ncoefficients\n, and maintain\n\\(B\\)\nas the B-spline basis functions. üëç So we have\n\\(c^{T}Sc\\)\n, and\nS\nas\npenalty matrix\n.\nOK. Let‚Äôs assume that this is some\nf(x)\nfunction where\nx\nis the grid point. And we essentially want to find the second derivative of the B-spline basis functions at these grid points. The second derivative is denoted as\n\\(f''(x)\\)\n. We want to square it to be positive definite, so we have\n\\(f''(x)^2\\)\n.\nSo, what\nf(x)\nare we trying to penalize here? You got it, the B-spline basis functions. We want to penalize the second derivative of the B-spline basis functions. Let‚Äôs refresh our memory on the B-spline basis functions. The B-spline basis functions are defined as follows:\n$$\nf(x) = c_1B_1(x) + c_2B_2(x) + ‚Ä¶ + c_nB_n(x)\n$$\nLet‚Äôs just take\n3\nbasis function just for exercise.\n$$\nf(x) = c_1B_1(x) + c_2B_2(x) + c_3B_3(x)\n$$\nWe want to square the second derivative of this function, so we have:\n$$\nf‚Äô‚Äô(x) = c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x)\n$$\nNext, for positive definiteness, we square the second derivative:\n$$\nf‚Äô‚Äô(x)^2 = (c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x))^2\n$$\nNow, we want to integrate this over the domain of the B-spline basis functions.\n$$\n\\int f‚Äô‚Äô(x)^2 dx = \\int (c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x))^2 dx\n$$\nOK, now it becomes a tad scary. Let‚Äôs ignore the integration for now and try to expand the quadratic term:\n$$\n(c_1B_1‚Äô‚Äô(x) + c_2B_2‚Äô‚Äô(x) + c_3B_3‚Äô‚Äô(x))^2 \\\\\n= c_1^2 B_1‚Äô‚Äô(x)^2 + c_2^2 B_2‚Äô‚Äô(x)^2 + c_3^2 B_3‚Äô‚Äô(x)^2 \\\\ + 2c_1c_2B_1‚Äô‚Äô(x)B_2‚Äô‚Äô(x) + 2c_1c_3B_1‚Äô‚Äô(x)B_3‚Äô‚Äô(x) + 2c_2c_3B_2‚Äô‚Äô(x)B_3‚Äô‚Äô(x) \\\\\n\\text{which eventually leads to} \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) \\\\\n$$\nStill with me? Now let‚Äôs plug this back into our integration:\n$$\n\\int f‚Äô‚Äô(x)^2 dx = \\int \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx\n$$\nWe‚Äôll come back to the\n\\(c^{T}Sc\\)\nproof. Now, let‚Äôs define the penalty matrix\nS\nas follows:\n$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx\n$$\nOK, how do we then calculate or estimate\n\\(B_i''(x)\\)\n? We don‚Äôt have a formula or anything to get to the second derivative of the B-spline basis functions directly. Instead, we will use numerical integration to approximate the second derivative at the grid points. But how !?! In come,\nTaylor series approxiation\n, again!\nThe second derivative of a function at a point can be approximated using finite differences as follows using a general\nf(x)\n:\n$$\n\\text{foreward approximation} \\\\\nf(x+h) \\approx f(x) + f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2 \\\\\n\\text{backward approximation} \\\\\nf(x-h) \\approx f(x) ‚Äì f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2\n$$\nNow, there is a mathematical trickery where we can use the forward and backward approximation to get to the second derivative by adding both together:\n$$\n\\text{adding both together} \\\\\nf(x+h) + f(x-h)\n= (f(x) + f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2) + (f(x) ‚Äì f‚Äô(x)h + \\frac{f‚Äô‚Äô(x)}{2}h^2) \\\\\nf(x+h) + f(x-h) = 2f(x) + f‚Äô‚Äô(x)h^2 \\\\\nf‚Äô‚Äô(x)h^2 = f(x+h) + f(x-h) ‚Äì 2f(x) \\\\\nf‚Äô‚Äô(x) = \\frac{f(x+h) + f(x-h) ‚Äì 2f(x)}{h^2}\n$$\nWe basically just turn the\nf\nto\nB\nto go back to our\nS\nmatrix (penalty matrix)\n$$\nB_i‚Äô‚Äô(x) = \\frac{B_i(x+h) + B_i(x-h) ‚Äì 2B_i(x)}{h^2}\n$$\nThis is the same for\n\\(B_j''\\)\n.\nNow, we can plug this back into our penalty matrix\nS\n:\n$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\int \\frac{B_i(x+h) + B_i(x-h) ‚Äì 2B_i(x)}{h^2} \\cdot \\frac{B_j(x+h) + B_j(x-h) ‚Äì 2B_j(x)}{h^2} dx\n$$\nIn this case we will use numerical integration to compute the penalty matrix. Meaning, we will use a grid of 100 evenly separated points to evaluate the B-spline basis functions and their second derivatives, and then compute the penalty matrix\nS\n. We have to ensure NOT to use the first and last point since we need to use both forward and backward approximation to get to the second derivative.\nExample, let‚Äôs say we have 5 evenly separated points (this is very small, but just for illustration) and our min point is -5, max is 5, which means\nx = [-5, -2.5, 0, 2.5, 5]\nas our grid points.\nh\nhere is our step size.\n$$\nh = \\frac{5 ‚Äì (-5)}{5 ‚Äì 1} = \\frac{10}{4} = 2.5\n$$\nNow let‚Äôs look at what\n\\(B_i\\)\n(pay attention, this is NOT the 2nd derivative) looks like with 3 basis function (columns), and 5 grid points (rows), these can be calculated from\ncox deboor recursion\n, but you can just use\nspliness::bs(x=c(-5,-2.5,0,2.5,5))\n:\n$$\n\\begin{bmatrix} 0 & 0 & 0 \\\\\\ 0.42 & 0.14 & 0.02 \\\\\\ 0.38 & 0.38 & 0.13 \\\\\\ 0.14 & 0.42 & 0.42 \\\\\\ 0 & 0 & 1  \\end{bmatrix}\n$$\nNow we want to then use this formula\n\\(\\frac{B_i(x+h) + B_i(x-h) - 2B_i(x)}{h^2}\\)\nto create\n\\(B_i''(x)\\)\n(the second derivative of the B-spline basis functions). Note that we don‚Äôt want to look at\nx=-5 and x=5\n.\nLet‚Äôs look at\n\\(B_1\\)\nand\n\\(x=-2.5\\)\nas an example:\n$$\nB_1‚Äô‚Äô(x=-2.5) = \\frac{B_1(x+2.5) + B_1(x-2.5) ‚Äì 2B_1(x)}{2.5^2} \\\\\n= \\frac{B_1(x=0) + B_1(x=-5) ‚Äì 2B_1(x=-2.5)}{2.5^2} \\\\\n= \\frac{0.38 + 0 ‚Äì 2(0.42)}{2.5^2} \\\\\n= \\frac{0.38 ‚Äì 0.84}{6.25}\n$$\nAlright, we don‚Äôt have to get the result, the above is just an example on how to use the formula. Now let‚Äôs construct our\n\\(D\\)\nmatrix, which is a matrix of\n\\(B_i''(x)\\)\nwith the above formula like so:\n$$\nD \\\\\n= \\begin{bmatrix}\n\\frac{B_1(x_2+h) + B_1(x_2-h) ‚Äì 2B_1(x_2)}{h^2} & \\frac{B_2(x_2+h) + B_2(x_2-h) ‚Äì 2B_2(x_2)}{h^2} & \\frac{B_3(x_2+h) + B_3(x_2-h) ‚Äì 2B_3(x_2)}{h^2} \\\\\n\\frac{B_1(x_3+h) + B_1(x_3-h) ‚Äì 2B_1(x_3)}{h^2} & \\frac{B_2(x_3+h) + B_2(x_3-h) ‚Äì 2B_2(x_3)}{h^2} & \\frac{B_3(x_3+h) + B_3(x_3-h) ‚Äì 2B_3(x_3)}{h^2} \\\\\n\\frac{B_1(x_4+h) + B_1(x_4-h) ‚Äì 2B_1(x_4)}{h^2} & \\frac{B_2(x_4+h) + B_2(x_4-h) ‚Äì 2B_2(x_4)}{h^2} & \\frac{B_3(x_4+h) + B_3(x_4-h) ‚Äì 2B_3(x_4)}{h^2}\n\\end{bmatrix}`\n$$\nNotie that we don‚Äôt have to calculate the first and last row since we don‚Äôt have the forward and backward approximation for those points.\nStill with me? Let‚Äôs refresh on where we digressed. This\n\\(S[i,j] = \\int B_i''(x)B_j''(x) dx\\)\n!!! How on earth do we integrate this? We don‚Äôt have to! We can use numerical integration to approximate the integral. In this case, we can use the trapezoidal rule or\nRiemann‚Äôs sum\nto estimate.\n$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\cdot h \\\\\n= D^{T} D \\cdot h \\\\\n\\text{Because } \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\text{ is essentially } D^{T} D\n$$\nHere is why\n\\(\\sum_{k=1}^{n} B_i''(x_k)B_j''(x_k)\\)\nis essentially\n\\(D^{T} D\\)\n:\n$$\n\\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\\\\n= \\begin{bmatrix}\nB_1‚Äô‚Äô(x_2) & B_1‚Äô‚Äô(x_3) & B_1‚Äô‚Äô(x_4) \\\\\nB_2‚Äô‚Äô(x_2) & B_2‚Äô‚Äô(x_3) & B_2‚Äô‚Äô(x_4) \\\\\nB_3‚Äô‚Äô(x_2) & B_3‚Äô‚Äô(x_3) & B_3‚Äô‚Äô(x_4)\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nB_1‚Äô‚Äô(x_2) & B_2‚Äô‚Äô(x_2) & B_3‚Äô‚Äô(x_2) \\\\\nB_1‚Äô‚Äô(x_3) & B_2‚Äô‚Äô(x_3) & B_3‚Äô‚Äô(x_3) \\\\\nB_1‚Äô‚Äô(x_4) & B_2‚Äô‚Äô(x_4) & B_3‚Äô‚Äô(x_4)\n\\end{bmatrix} \\\\\n= D^{T} D\n$$\nThis\n\\(D^{T} D \\cdot h\\)\nis ultimately our\n\\(S\\)\nmatrix, which is the penalty matrix.\nOK, now let‚Äôs bring this equation back to our original equation:\n$$\n\\int f‚Äô‚Äô(x)^2 dx = \\int \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j] \\\\\n= c^{T} S c\n$$\nNow you may ask how on earth did we go from\n\\(\\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j]\\)\nto\n\\(c^{T} S c\\)\nLet‚Äôs set an example:\n$$\nc = \\begin{bmatrix}\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix} ,\nS = \\begin{bmatrix}\nS_{1,1} & S_{1,2} & S_{1,3} \\\\\nS_{2,1} & S_{2,2} & S_{2,3} \\\\\nS_{3,1} & S_{3,2} & S_{3,3}\n\\end{bmatrix}\n$$\nNow we know that\n\\(\\sum\\)\nin coding is basically a\nfor loop\n, hence\n\\(\\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j]\\)\nmeans we have a nested for loop on another for loop. If we were to expand this it will look like\n$$\n\\sum_{i}^{n} \\sum_{j}^{n} c_ic_j S[i,j] \\\\\n= c_1c_1S_{1,1} + c_1c_2S_{1,2} + c_1c_3S_{1,3} + \\\\\nc_2c_1S_{2,1} + c_2c_2S_{2,2} + c_2c_3S_{2,3} + \\\\\nc_3c_1S_{3,1} + c_3c_2S_{3,2} + c_3c_3S_{3,3}\n$$\nHow we then turn these into\n\\(c^{T}Sc\\)\nis the following magic matrix operation.\nRearrange\n$$\nc_1(S_{1,1}c_1 + S_{1,2}c_2 + S_{1,3}c_3) + \\\\\nc_2(S_{2,1}c_1 + S_{2,2}c_2 + S_{2,3}c_3) + \\\\\nc_3(S_{3,1}c_1 + S_{3,2}c_2 + S_{3,3}c_3)\n$$\nNotice how we can use matrix multiplication to express this as a dot product:\n$$\n`\\begin{bmatrix}\nS_{1,1}c_1 + S_{1,2}c_2 + S_{1,3}c_3 \\\\\nS_{2,1}c_1 + S_{2,2}c_2 + S_{2,3}c_3 \\\\\nS_{3,1}c_1 + S_{3,2}c_2 + S_{3,3}c_3\n\\end{bmatrix} = \\begin{bmatrix}\nS_{1,1} & S_{1,2} & S_{1,3} \\\\\nS_{2,1} & S_{2,2} & S_{2,3} \\\\\nS_{3,1} & S_{3,2} & S_{3,3}\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix} \\\\\n= S \\cdot c \\\\\n\\text{In order to get to } c^{T}Sc \\text{ we need to transpose } c \\text{, hence} \\\\\nc^{T}Sc = \\begin{bmatrix}\nc_1 & c_2 & c_3 \\end{bmatrix} \\cdot \\begin{bmatrix}\nS_{1,1} & S_{1,2} & S_{1,3} \\\\\nS_{2,1} & S_{2,2} & S_{2,3} \\\\\nS_{3,1} & S_{3,2} & S_{3,3}\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nc_1 \\\\\nc_2 \\\\\nc_3\n\\end{bmatrix} \\\\\n= c_1(S_{1,1}c_1 + S_{1,2}c_2 + S_{1,3}c_3) + \\\\\nc_2(S_{2,1}c_1 + S_{2,2}c_2 + S_{2,3}c_3) + \\\\\nc_3(S_{3,1}c_1 + S_{3,2}c_2 + S_{3,3}c_3) \\\\\n\\text{which is essentially a scalar!}\n$$\nPhew, all those math gymnastics and acrobatics! Worked out a sweat, don‚Äôt they? Now, remember we switch\n\\(\\beta\\)\nto\nc\n, these are coefficients, so that we don‚Äôt confuse\n\\(\\beta\\)\n(beta) with\nB\n, our basis function? OK, now that basis function is long ‚Äúgone‚Äù, we will then turn\n\\(c^{T}Sc\\)\nback to\n\\(\\beta^{T}S\\beta\\)\nfor consistency with general notation.\nAnd there you have it! We have derived the penalty term\n\\(\\beta^{T}S\\beta\\)\nin the generalized additive model with B-spline basis functions.\nProof of Derivate with Respect to Beta Coefficients\nNow that we have the penalty term, let‚Äôs take the derivative of the entire equation with respect to\n\\(\\beta\\)\n:\n$$\n\\min (|| y ‚Äì \\beta x ||^2 + \\lambda \\beta^T S \\beta) \\\\\n= \\min (y^T y ‚Äì 2 x^T \\beta^T y + \\beta^T x^T x \\beta + \\lambda \\beta^T S \\beta)\n$$\nTaking the derivative with respect to\n\\(\\beta\\)\ngives us:\n$$\n\\frac{\\partial}{\\partial \\beta} (y^T y ‚Äì 2 x^T \\beta^T y + \\beta^T x^T x \\beta + \\lambda \\beta^T S \\beta) \\\\\n= 0 -2 x^T y + 2 x^T x \\beta + 2 \\lambda S \\beta\n$$\nSetting the derivative to zero gives us the normal equations:\n$$\n0 ‚Äì 2 x^T y + 2 x^T x \\beta + 2 \\lambda S \\beta \\\\\n2 x^T y = 2 x^T x \\beta + 2 \\lambda S \\beta \\\\\nx^T y = x^T x \\beta + \\lambda S \\beta \\\\\n\\beta (x^T x + \\lambda S) = x^T y \\\\\n\\beta = (x^T x + \\lambda S)^{-1} x^T y\n$$\nOK. We went through ALL those trouble to find out the equation to find\nbeta\ncoefficients for After all these, what on earth is\nlambda\n!?!?!\nWhat is Lambda\nLambda\nis a penalty parameter that controls the amount of regularization applied to the model. It is a hyperparameter that you can tune to find the optimal balance between fitting the data and preventing overfitting. A larger\nlambda\nvalue will result in more regularization, while a smaller value will result in less regularization.\nlambda\n= 0 means no penalty, it is equivalent to ordinary least squares regression. Once we found the optimal\nlambda\n, we can plug that number in to calculate our\nbeta\n, how cool! Now how to we find the optimal\nlambda\n?\nGeneralized Cross Validated (GCV)\nTo find the optimal\nlambda\n, we can use Generalized Cross Validation (GCV). GCV is a method that estimates the prediction error of a model by minimizing the GCV score. The GCV score is defined as follows:\n$$\n\\text{GCV} = \\frac{|| y ‚Äì \\hat{y} ||^2}{(1 ‚Äì \\text{trace}(H))^2}\n$$\nWhere\n\\(\\hat{y}\\)\nis the predicted value, and\n\\(H\\)\nis the hat matrix. The trace of the hat matrix is the sum of the diagonal elements of the hat matrix, which is the number of parameters in the model. The GCV score is a measure of how well the model fits the data, and we want to minimize this score to find the optimal\nlambda\n.\nFind Optimal Lambda\nIn other words:\nStep 1. Calculate\nH\n(hat matrix)\n$$\nH = X(X^T X + \\lambda S)^{-1} X^T\n$$\nStep 2. Calculate\n\\(\\hat{y}\\)\n(predicted value)\n$$\n\\hat{y} = H y\n$$\nStep 3. Calculate RSS\n$$\nRSS = || y ‚Äì \\hat{y} ||^2\n$$\nStep 4. Calculate GCV score\n$$\n\\text{GCV}(\\lambda) = \\frac{n \\times RSS}{(n ‚Äì \\text{trace}(H))^2}\n$$\nStep 5. Find the optimal\nlambda\nthat minimizes the GCV score. Et viola!\nLet‚Äôs Code\nNow, we did A LOT of math there, let‚Äôs bring down the essential formulae as a reminder. But first, let‚Äôs simulate some non-linear data!\nlibrary(tidyverse)\nlibrary(splines)\nlibrary(mgcv)\n\nset.seed(123)\nn <- 1000\nz <- runif(n,-5,5)\ny <- sin(z) + rnorm(n,0.1)\ndf <- tibble(z = z, y = y)\n\ndf |>\n  ggplot(aes(x = z, y = y)) +\n  geom_point(alpha=0.5)\nThis should be pretty straight forward. We‚Äôve seen this before in our previous blog.\nPenalty Matrix\n$$\nS[i,j] = \\int B_i‚Äô‚Äô(x)B_j‚Äô‚Äô(x) dx \\\\\n= \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\cdot h \\\\\n= D^{T} D \\cdot h \\\\\n\\text{Because } \\sum_{k=1}^{n} B_i‚Äô‚Äô(x_k)B_j‚Äô‚Äô(x_k) \\text{ is essentially } D^{T} D\n$$\npenalty_matrix <- function(z_vals, n_basis, n_grid = 100) {\n  # Step 1: Create fine grid over the domain\n  z_min <- min(z_vals)\n  z_max <- max(z_vals)\n  z_grid <- seq(z_min, z_max, length.out = n_grid)\n  h <- z_grid[2] - z_grid[1]\n\n  # Step 2: Evaluate B-spline basis functions on the grid\n  X_grid <- bs(z_grid, df = n_basis, intercept = TRUE)\n\n  # Step 3: Compute second derivatives using finite differences; Also known as Bi\" or Bj\" here (Basis, not beta), or D\n  D <- matrix(0, n_grid - 2, n_basis)\n\n  for (i in 2:(n_grid - 1)) {\n    for (j in 1:n_basis) {\n      D[i-1, j] <- (X_grid[i-1, j] - 2*X_grid[i, j] + X_grid[i+1, j]) / h^2\n    }\n  }\n\n  # Step 4: Compute penalty matrix S[i,j] = ‚à´ B''·µ¢(x) B''‚±º(x) dx\n  S <- t(D) %*% D * h\n\n  return(S)\n}\nThe above is our penalty matrix, basically to calculate\nS\n.\nFind the Minimum GCV\n$$\n\\text{GCV}(\\lambda) = \\frac{n \\times RSS}{(n - \\text{trace}(H))^2}\n$$\n# y <- df_bs$n\n# z <- seq(1,length(df_bs$date),1)\nn_basis <- 10\nx <- bs(z, df = n_basis, intercept=T)\n# s <- penalty_matrix(n_basis)\ns <- penalty_matrix(z,n_basis=n_basis)\nlambda_vec <- seq(0,10,0.01)\n\ngcv <- numeric(length(lambda_vec))\n# GCV function\nfor (i in 1:length(lambda_vec)) {\n  lambda <- lambda_vec[i]\n  \n  # 1. Calculate H = x(x·µÄx + ŒªS)‚Åª¬πx·µÄ\n  H <- function(lambda, x, s) {\n    # Calculate H\n    H <- x %*% solve(t(x) %*% x + lambda * s) %*% t(x)\n    return(H)\n  }\n  \n  h <- H(lambda,x, s)\n  \n  # 2. Calculate ≈∑ = Hy  \n  y_hat <- h %*% y\n  \n  # 3. Calculate RSS = ||y - ≈∑||¬≤\n  rss <- sum((y-y_hat)^2)\n  \n  # 4. Calculate GCV(Œª) = (n √ó RSS) / (n - tr(H))¬≤\n  tr_H <- sum(diag(h))\n  gcv_value <- (n * rss) / ((n - tr_H)^2)\n  \n  gcv[i] <- gcv_value\n}\n\ntibble(lambda=lambda_vec, gcv=gcv) |>\n  ggplot(aes(x=lambda, y=gcv)) +\n  geom_line() +\n  labs(title=\"GCV vs Lambda\", x=\"Lambda\", y=\"GCV\") +\n  theme_minimal()\n## optimal lambda\n(opt_lambda <- lambda_vec[which(gcv==min(gcv))])\n\n## [1] 0.6\n\n(min_gcv <- min(gcv_value))\n\n## [1] 1.018823\nAlright, the plot shows GCV value vs lambda. We essentially want to find lambda with the lowest GCV value.\nopt_lambda\nis the optimal lambda value. We then use this number to calculate our\nbeta coefficients\nFind Beta Coefficients\n$$\n\\beta = (x^T x + \\lambda S)^{-1} x^T y \\\\\n$$\n(beta <- solve(t(x) %*% x + opt_lambda * s) %*% t(x) %*% y)\n\n##          [,1]\n## 1   1.2375133\n## 2   1.1800055\n## 3   0.8037137\n## 4  -1.2787422\n## 5  -0.6854527\n## 6   1.0479672\n## 7   1.3246798\n## 8  -0.4118710\n## 9  -1.2195539\n## 10 -0.8501258\nCompare Our Custom Function vs mgcv::gam\n# using mgcv::gam\nmodel_gam <- gam(y~s(z, k=n_basis, bs = \"bs\"))\n\n# turn our calculated beta to vector\nbeta <- as.vector(beta)\n\n# Plotting the results\ntibble(z, y) |>\n  ggplot(aes(z, y)) +\n  geom_point(alpha=0.2) +\n  geom_line(aes(x = z, y = x %*% beta), color = \"blue\", size=2, alpha=0.5) +\n  geom_line(aes(x = z, y = predict(model_gam)), color = \"red\", size = 2, alpha=0.5) +\n  labs(x = \"z\", y = \"y\") +\n  ggtitle(label = \"GAM Fit with Custom Penalty Matrix and mgcv Package\") +\n  theme_minimal()\nWow, look at that! The blue line is our custom function, and the red line is the\nmgcv::gam\nfunction. They look pretty similar, right? As you can see we have\nblue\nfor our custom gam and\nred\nas mgcv::gam. We get a purple color!\nLet‚Äôs Try A Real Dataset, Our Very Own #IDsky\nNow let‚Äôs try a different dataset, let‚Äôs look at Bluesky #IDsky posts counts\n# Load the dataset\nload(\"df_bs.rda\")\n\n# Some data wrangling\ny <- df_bs$n\nz <- seq(1,length(df_bs$date),1)\n\n# Create B-spline basis functions\nn_basis <- 20\nx <- bs(z, df = n_basis, intercept=T)\ns <- penalty_matrix(z,n_basis=n_basis,n_grid = 200)\n\n# find optimal lambda\nlambda_vec <- seq(44,48,0.001)\n\ngcv <- numeric(length(lambda_vec))\n\n# GCV function\nfor (i in 1:length(lambda_vec)) {\n  lambda <- lambda_vec[i]\n  \n  # 1. Calculate H = x(x·µÄx + ŒªS)‚Åª¬πx·µÄ\n  H <- function(lambda, x, s) {\n    # Calculate H\n    H <- x %*% solve(t(x) %*% x + lambda * s) %*% t(x)\n    return(H)\n  }\n  \n  h <- H(lambda,x, s)\n  \n  # 2. Calculate ≈∑ = Hy  \n  y_hat <- h %*% y\n  \n  # 3. Calculate RSS = ||y - ≈∑||¬≤\n  rss <- sum((y-y_hat)^2)\n  \n  # 4. Calculate GCV(Œª) = (n √ó RSS) / (n - tr(H))¬≤\n  tr_H <- sum(diag(h))\n  gcv_value <- (n * rss) / ((n - tr_H)^2)\n  \n  gcv[i] <- gcv_value\n}\n\ntibble(lambda=lambda_vec, gcv=gcv) |>\n  ggplot(aes(x=lambda, y=gcv)) +\n  geom_line() +\n  labs(title=\"GCV vs Lambda\", x=\"Lambda\", y=\"GCV\") +\n  theme_minimal()\n## optimal lambda\n(opt_lambda <- lambda_vec[which(gcv==min(gcv))])\n\n## [1] 45.505\n\n(min_gcv <- min(gcv_value))\n\n## [1] 129.3409\n\n(beta <- solve(t(x) %*% x + opt_lambda * s) %*% t(x) %*% y)\n\n##          [,1]\n## 1    3.444439\n## 2    6.746956\n## 3   -3.840163\n## 4   15.139798\n## 5   -5.688893\n## 6   32.633905\n## 7  -31.848643\n## 8   75.342451\n## 9  192.032407\n## 10  44.874593\n## 11  48.429720\n## 12  65.626734\n## 13  54.892071\n## 14  86.819061\n## 15  53.498207\n## 16  55.391699\n## 17  67.752137\n## 18  21.012682\n## 19  77.276639\n## 20  33.333471\n\n# compare mgcv::gam\nmodel_gam <- gam(y~s(z, k=n_basis, bs = \"bs\"))\nbeta <- as.vector(beta)\n\n# visualize\ntibble(z, y) |>\n  ggplot(aes(z, y)) +\n  geom_point(alpha=0.2) +\n  geom_line(aes(x = z, y = x %*% beta), color = \"blue\", size=2, alpha=0.5) +\n  geom_line(aes(x = z, y = predict(model_gam)), color = \"red\", size = 2, alpha=0.5) +\n  labs(x = \"z\", y = \"y\") +\n  ggtitle(label = \"GAM Fit with Custom Penalty Matrix and mgcv Package\") +\n  theme_minimal()\nLook at that ! They are quite similar, aren‚Äôt they? Yes, we did it!!! PHew, that was a lot of math and coding!\nOpportunities for Improvement\nNeed to learn how to do the same for REML\nLessons Learnt\nlearnt lots of lots of math behind GAM with penalty\nlearnt how to derive the penalty matrix\nrefresh on matrix operations, calculus, and algebra\nIf you like this article:\nplease feel free to send me a\ncomment or visit my other blogs\nplease feel free to follow me on\nBlueSky\n,\ntwitter\n,\nGitHub\nor\nMastodon\nif you would like collaborate please feel free to\ncontact me\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nr on Everyday Is A School Day\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "Enjoyed learning penalized GAM math. Built penalty matrices, optimized Œª using GCV, and implement our own GAM function. Confusing? Yes! Rewarding? Oh yes! We dove into the engine of what made basis spline work on our last blog, now let‚Äôs add to...",
    "meta_keywords": null,
    "og_description": "Enjoyed learning penalized GAM math. Built penalty matrices, optimized Œª using GCV, and implement our own GAM function. Confusing? Yes! Rewarding? Oh yes! We dove into the engine of what made basis spline work on our last blog, now let‚Äôs add to...",
    "og_image": "https://www.kenkoonwong.com/blog/gam-penalty/index_files/figure-html/unnamed-chunk-1-1.png",
    "og_title": "From Math to Code: Building GAM with Penalty Functions From Scratch | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 21.4,
    "sitemap_lastmod": null,
    "twitter_description": "Enjoyed learning penalized GAM math. Built penalty matrices, optimized Œª using GCV, and implement our own GAM function. Confusing? Yes! Rewarding? Oh yes! We dove into the engine of what made basis spline work on our last blog, now let‚Äôs add to...",
    "twitter_title": "From Math to Code: Building GAM with Penalty Functions From Scratch | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/06/from-math-to-code-building-gam-with-penalty-functions-from-scratch/",
    "word_count": 4273
  }
}