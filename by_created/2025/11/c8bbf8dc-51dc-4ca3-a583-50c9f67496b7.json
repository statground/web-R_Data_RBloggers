{
  "uuid": "c8bbf8dc-51dc-4ca3-a583-50c9f67496b7",
  "created_at": "2025-11-22 19:58:19",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/06/building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers/",
    "crawled_at": "2025-11-22T10:47:12.371343",
    "external_links": [
      {
        "href": "https://blog.ephorie.de/building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers?utm_source=rss&utm_medium=rss&utm_campaign=building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers",
        "text": "R-Bloggers ‚Äì Learning Machines"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://blog.ephorie.de/understanding-the-magic-of-neural-networks",
        "text": "Neural Networks"
      },
      {
        "href": "https://blog.ephorie.de/create-texts-with-a-markov-chain-text-generator-and-what-this-has-to-do-with-chatgpt",
        "text": "Markov chains"
      },
      {
        "href": "https://blog.ephorie.de/attention-what-lies-at-the-core-of-chatgpt",
        "text": "Word embeddings"
      },
      {
        "href": "https://blog.ephorie.de/attention-what-lies-at-the-core-of-chatgpt",
        "text": "Self-attention"
      },
      {
        "href": "http://paulo-jorente.de/text/alice_oz.txt",
        "text": "alice_oz.txt"
      },
      {
        "href": "https://blog.ephorie.de/create-texts-with-a-markov-chain-text-generator-and-what-this-has-to-do-with-chatgpt",
        "text": "Markov chain example"
      },
      {
        "href": "https://blog.ephorie.de/attention-what-lies-at-the-core-of-chatgpt",
        "text": "simple 3√ó3 example"
      },
      {
        "href": "https://blog.ephorie.de/create-texts-with-a-markov-chain-text-generator-and-what-this-has-to-do-with-chatgpt",
        "text": "Markov chain post"
      },
      {
        "href": "https://blog.ephorie.de/understanding-the-magic-of-neural-networks",
        "text": "Understanding the Magic of Neural Networks"
      },
      {
        "href": "https://blog.ephorie.de/building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers?utm_source=rss&utm_medium=rss&utm_campaign=building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers",
        "text": "R-Bloggers ‚Äì Learning Machines"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers! | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-300x160.png?resize=300%2C160&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/learning-machines/",
        "text": "Learning Machines"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-393089 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers!</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">June 16, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/learning-machines/\">Learning Machines</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://blog.ephorie.de/building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers\"> R-Bloggers ‚Äì Learning Machines</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><p><img alt=\"\" class=\"alignleft size-medium wp-image-6971\" data-lazy-sizes=\"(max-width: 300px) 85vw, 300px\" data-lazy-src=\"https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-300x160.png?resize=300%2C160&amp;ssl=1\" data-recalc-dims=\"1\" decoding=\"async\" height=\"160\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-300x160.png?resize=300%2C160&amp;ssl=1 300w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-840x447.png 840w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-768x409.png 768w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-1200x638.png 1200w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235.png 1357w\" width=\"300\"/><noscript><img alt=\"\" class=\"alignleft size-medium wp-image-6971\" data-recalc-dims=\"1\" decoding=\"async\" height=\"160\" loading=\"lazy\" sizes=\"(max-width: 300px) 85vw, 300px\" src=\"https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-300x160.png?resize=300%2C160&amp;ssl=1\" srcset_temp=\"https://i1.wp.com/blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-300x160.png?resize=300%2C160&amp;ssl=1 300w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-840x447.png 840w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-768x409.png 768w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-1200x638.png 1200w, https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235.png 1357w\" width=\"300\"/></noscript><br/>\nRemember our journey so far? We started with simple <em>Markov chains</em> showing how statistical word prediction works, then dove into the core concepts of word <em>embeddings</em>, <em>self-attention</em>, and <em>next word prediction</em>. Now, it‚Äôs time for the grand finale: if you want to build your own working <em>transformer</em> language model in R, read on!<br/>\n<span id=\"more-6967\"></span></p>\n<p>You will say, no way!?! But yes, according to the mantra that you have only understood what you have built yourself from scratch, we will create a mini-ChatGPT that learns to write like ‚ÄúAlice in Wonderland‚Äù and the ‚ÄúWizard of Oz‚Äù!</p>\n<h2>The Secret Sauce: Bringing It All Together</h2>\n<p>What we‚Äôve learned so far:</p>\n<ul>\n<li><strong><a href=\"https://blog.ephorie.de/understanding-the-magic-of-neural-networks\" rel=\"nofollow\" target=\"_blank\">Neural Networks</a></strong> build a representation of the world based on their training data</li>\n<li><strong><a href=\"https://blog.ephorie.de/create-texts-with-a-markov-chain-text-generator-and-what-this-has-to-do-with-chatgpt\" rel=\"nofollow\" target=\"_blank\">Markov chains</a></strong> showed us that text generation is fundamentally about predicting the next word</li>\n<li><strong><a href=\"https://blog.ephorie.de/attention-what-lies-at-the-core-of-chatgpt\" rel=\"nofollow\" target=\"_blank\">Word embeddings</a></strong> convert words into numerical vectors that capture meaning</li>\n<li><strong><a href=\"https://blog.ephorie.de/attention-what-lies-at-the-core-of-chatgpt\" rel=\"nofollow\" target=\"_blank\">Self-attention</a></strong> lets the model focus on relevant words when making predictions</li>\n</ul>\n<p>A <em>transformer</em> combines ALL of these concepts into one powerful architecture. Think of it as a sophisticated Markov chain that doesn‚Äôt just look at the previous few words, but can attend to any word in the entire context, understanding relationships and patterns across the whole text!</p>\n<h2>From Theory to Practice: The R Implementation</h2>\n<p>Let‚Äôs build a complete transformer step by step, using the same <a href=\"http://paulo-jorente.de/text/alice_oz.txt\" rel=\"nofollow\" target=\"_blank\">alice_oz.txt</a> file from our <a href=\"https://blog.ephorie.de/create-texts-with-a-markov-chain-text-generator-and-what-this-has-to-do-with-chatgpt\" rel=\"nofollow\" target=\"_blank\">Markov chain example</a>:</p>\n<h3>Step 1: Word-Level Tokenization</h3>\n<pre>\nlibrary(torch) # install from CRAN\n\n# Create word-level tokenizer\ncreate_tokenizer &lt;- function(text) {\n  text &lt;- tolower(text)\n  words &lt;- unlist(strsplit(text, \"\\\\s+\"))\n  words &lt;- words[words != \"\"]\n  \n  unique_words &lt;- sort(unique(words))\n  vocab &lt;- c(\"&lt;start&gt;\", \"&lt;end&gt;\", unique_words)\n  \n  word_to_idx &lt;- setNames(seq_along(vocab), vocab)\n  idx_to_word &lt;- setNames(vocab, seq_along(vocab))\n  \n  list(word_to_idx = word_to_idx, idx_to_word = idx_to_word, vocab_size = length(vocab))\n}\n</pre>\n<p>Unlike our Markov chain that worked with fixed N-grams, this tokenizer prepares words for our transformer to process entire sequences.</p>\n<h3>Step 2: Self-Attention</h3>\n<pre>\ntransformer_layer &lt;- nn_module(\n  initialize = function(d_model, n_heads) {\n    self$d_model &lt;- d_model\n    self$n_heads &lt;- n_heads\n    self$d_k &lt;- d_model %/% n_heads\n    \n    # The Q, K, V matrices for the attention mechanism\n    self$w_q &lt;- nn_linear(d_model, d_model, bias = FALSE)\n    self$w_k &lt;- nn_linear(d_model, d_model, bias = FALSE)  \n    self$w_v &lt;- nn_linear(d_model, d_model, bias = FALSE)\n    self$w_o &lt;- nn_linear(d_model, d_model)\n    \n    # Feed-forward neural network\n    self$ff &lt;- nn_sequential(\n      nn_linear(d_model, d_model * 4),\n      nn_relu(),\n      nn_linear(d_model * 4, d_model)\n    )\n    \n    self$ln1 &lt;- nn_layer_norm(d_model)\n    self$ln2 &lt;- nn_layer_norm(d_model)\n    self$dropout &lt;- nn_dropout(0.1)\n  },\n  \n  forward = function(x, mask = NULL) {\n    # Multi-head self-attention (exactly like our simple example, but multi-headed!)\n    batch_size &lt;- x$size(1)\n    seq_len &lt;- x$size(2)\n    \n    q &lt;- self$w_q(x)$view(c(batch_size, seq_len, self$n_heads, self$d_k))$transpose(2, 3)\n    k &lt;- self$w_k(x)$view(c(batch_size, seq_len, self$n_heads, self$d_k))$transpose(2, 3)\n    v &lt;- self$w_v(x)$view(c(batch_size, seq_len, self$n_heads, self$d_k))$transpose(2, 3)\n    \n    # Scaled dot-product attention\n    scores &lt;- torch_matmul(q, k$transpose(-2, -1)) / sqrt(self$d_k)\n    \n    if (!is.null(mask)) {\n      scores &lt;- scores + mask$unsqueeze(1)$unsqueeze(1)\n    }\n    \n    attn_weights &lt;- nnf_softmax(scores, dim = -1)\n    attn_output &lt;- torch_matmul(attn_weights, v)\n    \n    # Combine heads and apply output projection\n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_len, self$d_model))\n    attn_output &lt;- self$w_o(attn_output)\n    \n    # Residual connection and layer norm\n    x &lt;- self$ln1(x + self$dropout(attn_output))\n    \n    # Feed-forward\n    ff_output &lt;- self$ff(x)\n    x &lt;- self$ln2(x + self$dropout(ff_output))\n    \n    x\n  }\n)\n</pre>\n<p>This is our <em>self-attention mechanism</em> in action! Just like in our <a href=\"https://blog.ephorie.de/attention-what-lies-at-the-core-of-chatgpt\" rel=\"nofollow\" target=\"_blank\">simple 3√ó3 example</a>, but now it works with entire sequences and multiple attention heads.</p>\n<h3>Step 3: The Transformer Language Model</h3>\n<pre>\ntoy_llm &lt;- nn_module(\n  initialize = function(vocab_size, d_model = 256, n_heads = 8, n_layers = 4) {\n    # Word embeddings (remember our love/is/wonderful example?)\n    self$token_embedding &lt;- nn_embedding(vocab_size, d_model)\n    self$pos_encoding &lt;- create_positional_encoding(512, d_model, \"cpu\")\n    \n    # Stack of transformer layers\n    self$transformer_layer_1 &lt;- transformer_layer(d_model, n_heads)\n    if (n_layers &gt;= 2) self$transformer_layer_2 &lt;- transformer_layer(d_model, n_heads)\n    if (n_layers &gt;= 3) self$transformer_layer_3 &lt;- transformer_layer(d_model, n_heads)\n    if (n_layers &gt;= 4) self$transformer_layer_4 &lt;- transformer_layer(d_model, n_heads)\n    self$n_layers &lt;- n_layers\n    \n    # Output projection (back to vocabulary)\n    self$ln_f &lt;- nn_layer_norm(d_model)\n    self$lm_head &lt;- nn_linear(d_model, vocab_size)\n    self$dropout &lt;- nn_dropout(0.1)\n  },\n  \n  forward = function(x) {\n    seq_len &lt;- x$size(2)\n    \n    # Causal mask (no peeking at future words!)\n    mask &lt;- torch_triu(torch_ones(seq_len, seq_len, device = x$device), diagonal = 1)\n    mask &lt;- mask$masked_fill(mask == 1, -Inf)\n    \n    # Token embeddings + positional encoding\n    x &lt;- self$token_embedding(x) * sqrt(self$d_model)\n    pos_enc &lt;- self$pos_encoding[1:seq_len, ]$to(device = x$device)\n    x &lt;- x + pos_enc\n    x &lt;- self$dropout(x)\n    \n    # Pass through transformer layers\n    x &lt;- self$transformer_layer_1(x, mask)\n    if (self$n_layers &gt;= 2) x &lt;- self$transformer_layer_2(x, mask)\n    if (self$n_layers &gt;= 3) x &lt;- self$transformer_layer_3(x, mask)\n    if (self$n_layers &gt;= 4) x &lt;- self$transformer_layer_4(x, mask)\n    \n    # Final layer norm and projection to vocabulary\n    x &lt;- self$ln_f(x)\n    logits &lt;- self$lm_head(x)\n    \n    logits\n  }\n)\n</pre>\n<p>This is the core of the LLM, the <em>transformer</em>. This <em>neural network</em> architecture makes use of all of the above concepts, like <em>embeddings</em>, <em>attention</em>, and <em>next word prediction</em>!</p>\n<h2>Training Our Mini-ChatGPT</h2>\n<p>Now comes the magic ‚Äì training our transformer on Alice in Wonderland and the Wizard of Oz:</p>\n<pre>\n# Load the same text from our Markov chain example\ntxt &lt;- readLines(url(\"http://paulo-jorente.de/text/alice_oz.txt\"), warn = FALSE)\ntraining_text &lt;- paste(txt, collapse = \" \")\ntraining_text &lt;- gsub(\"[^a-zA-Z0-9 .,!?;:-]\", \"\", training_text)\ntraining_text &lt;- tolower(training_text)\n\n# Create tokenizer and model\ntokenizer &lt;- create_tokenizer(training_text)\nmodel &lt;- toy_llm(vocab_size = tokenizer$vocab_size, d_model = 256, n_heads = 8, n_layers = 4)\n\n# Train the model (this is where the magic happens!)\ntrain_model(model, training_text, tokenizer, epochs = 1500, seq_len = 32, batch_size = 4)\n</pre>\n<h2>The Results</h2>\n<p>After training, our mini-transformer produces text like this:</p>\n<blockquote><p>\n<strong>Prompt ‚Äòalice‚Äô:</strong> alice looked down at them, and considered a little before she was going to shrink in the time and round the</p>\n<p><strong>Prompt ‚Äòthe queen‚Äô:</strong> the queen said to the executioner: fetch her here. and the executioner went off like an arrow. the cats head began fading</p>\n<p><strong>Prompt ‚Äòdown the‚Äô:</strong> down the chimney, and she said to herself now i can do no more, whatever happens. what will become of me? luckily\n</p></blockquote>\n<p>Compare this to our original Markov chain output:</p>\n<blockquote><p>anxious returned the Scarecrow It is such an uncomfortable feeling to know one is a crow or a man After the crows had gone I thought this over and decided</p></blockquote>\n<p>The transformer has learned:</p>\n<ul>\n<li><strong>Character names and relationships</strong> (duchess, mock turtle, gryphon, queen of hearts, scarecrow, wizard)</li>\n<li><strong>Story context and scenarios</strong> (Alice‚Äôs wonderland, Dorothy‚Äôs journey to Oz, dialogue patterns)</li>\n<li><strong>Proper grammar and sentence structure</strong></li>\n<li><strong>The whimsical, narrative style of both Carroll and Baum‚Äôs writing</strong></li>\n</ul>\n<h2>The Transformer Advantage</h2>\n<p>Unlike our Markov chain that only looked at the previous 2-3 words, our transformer can:</p>\n<ul>\n<li><strong>See the entire context</strong> of hundreds of words</li>\n<li><strong>Understand long-range dependencies</strong> (like who ‚Äúshe‚Äù refers to)</li>\n<li><strong>Learn complex grammar and style patterns</strong></li>\n<li><strong>Generate coherent narratives</strong>, not just word-by-word predictions</li>\n</ul>\n<h2>From Toy to Production</h2>\n<p>What we built is essentially a miniature version of ChatGPT! The same principles scale up:</p>\n<ul>\n<li><strong>Industry-level GPTs have up to hundreds of billions of parameters (weights)</strong> (our model has 6 million)</li>\n<li><strong>GPTs train on hundreds of terabytes</strong> (we used two books)</li>\n<li><strong>GPTs often train for months on high performance clusters</strong> (this one takes less than 15 minutes on a standard computer with some GPU)</li>\n<li><strong>Production models use sophisticated tokenizers</strong> (we used simple word splitting)</li>\n</ul>\n<p>But the core architecture? <strong>Exactly the same!</strong></p>\n<h2>The Unreasonable Effectiveness of Transformers</h2>\n<p>What‚Äôs truly remarkable is that this simple architecture ‚Äì predicting the next word using self-attention ‚Äì gives rise to seemingly intelligent behavior. Our tiny model learned:</p>\n<ul>\n<li><strong>Grammar rules</strong> (without being taught grammar)</li>\n<li><strong>Character relationships</strong> (without being told who‚Äôs who)</li>\n<li><strong>Story structure</strong> (without understanding ‚Äúplot‚Äù)</li>\n<li><strong>Writing style</strong> (without lessons in literature)</li>\n</ul>\n<p>All from the simple task of ‚Äúpredict the next word‚Äù!</p>\n<p>Isn‚Äôt it fascinating that so much apparently intelligent behavior emerges from statistical text prediction? As we saw in our <a href=\"https://blog.ephorie.de/create-texts-with-a-markov-chain-text-generator-and-what-this-has-to-do-with-chatgpt\" rel=\"nofollow\" target=\"_blank\">Markov chain post</a>, <em>‚Äúmany tasks that demand human-level intelligence can obviously be reduced to some form of (statistical) text prediction with a sufficiently performant model!‚Äù</em></p>\n<p>To give you an intuition, why using a neural network architecture for this is so powerful: we have already seen that neural networks build a representation of their world, a <em>world model</em> (see: <a href=\"https://blog.ephorie.de/understanding-the-magic-of-neural-networks\" rel=\"nofollow\" target=\"_blank\">Understanding the Magic of Neural Networks</a>). In this case, imagine a detective story which ends with ‚ÄúAnd now it was clear, the murderer was‚Ä¶‚Äù: to sensibly predict the next (and last) word the neural network really must have <em>understood </em>the story in some sense!</p>\n<h2>Next Steps: The Adventure Continues!</h2>\n<p>You‚Äôve now built your own language model using the same principles as ChatGPT! Next, we could experimenting with:</p>\n<ul>\n<li><strong>Different texts</strong> (Shakespeare? Scientific papers? Your own writing?)</li>\n<li><strong>Larger models</strong> (more layers, bigger embeddings)</li>\n<li><strong>Different hyperparameters</strong></li>\n<li><strong>Various generation strategies</strong> (temperature, top-k sampling)</li>\n</ul>\n<p>Remember: we‚Äôve just implemented the core technology behind the AI revolution. From Markov chains to attention mechanisms to transformers ‚Äì you‚Äôve mastered the journey from simple statistics to artificial intelligence!</p>\n<p>The next time someone asks you ‚ÄúHow does ChatGPT work?‚Äù, you can confidently say: ‚ÄúLet me show you‚Ä¶‚Äù and build one from scratch (or show this post üòâ )!</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://blog.ephorie.de/building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers\"> R-Bloggers ‚Äì Learning Machines</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers!\nPosted on\nJune 16, 2025\nby\nLearning Machines\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR-Bloggers ‚Äì Learning Machines\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nRemember our journey so far? We started with simple\nMarkov chains\nshowing how statistical word prediction works, then dove into the core concepts of word\nembeddings\n,\nself-attention\n, and\nnext word prediction\n. Now, it‚Äôs time for the grand finale: if you want to build your own working\ntransformer\nlanguage model in R, read on!\nYou will say, no way!?! But yes, according to the mantra that you have only understood what you have built yourself from scratch, we will create a mini-ChatGPT that learns to write like ‚ÄúAlice in Wonderland‚Äù and the ‚ÄúWizard of Oz‚Äù!\nThe Secret Sauce: Bringing It All Together\nWhat we‚Äôve learned so far:\nNeural Networks\nbuild a representation of the world based on their training data\nMarkov chains\nshowed us that text generation is fundamentally about predicting the next word\nWord embeddings\nconvert words into numerical vectors that capture meaning\nSelf-attention\nlets the model focus on relevant words when making predictions\nA\ntransformer\ncombines ALL of these concepts into one powerful architecture. Think of it as a sophisticated Markov chain that doesn‚Äôt just look at the previous few words, but can attend to any word in the entire context, understanding relationships and patterns across the whole text!\nFrom Theory to Practice: The R Implementation\nLet‚Äôs build a complete transformer step by step, using the same\nalice_oz.txt\nfile from our\nMarkov chain example\n:\nStep 1: Word-Level Tokenization\nlibrary(torch) # install from CRAN\n\n# Create word-level tokenizer\ncreate_tokenizer <- function(text) {\n  text <- tolower(text)\n  words <- unlist(strsplit(text, \"\\\\s+\"))\n  words <- words[words != \"\"]\n  \n  unique_words <- sort(unique(words))\n  vocab <- c(\"<start>\", \"<end>\", unique_words)\n  \n  word_to_idx <- setNames(seq_along(vocab), vocab)\n  idx_to_word <- setNames(vocab, seq_along(vocab))\n  \n  list(word_to_idx = word_to_idx, idx_to_word = idx_to_word, vocab_size = length(vocab))\n}\nUnlike our Markov chain that worked with fixed N-grams, this tokenizer prepares words for our transformer to process entire sequences.\nStep 2: Self-Attention\ntransformer_layer <- nn_module(\n  initialize = function(d_model, n_heads) {\n    self$d_model <- d_model\n    self$n_heads <- n_heads\n    self$d_k <- d_model %/% n_heads\n    \n    # The Q, K, V matrices for the attention mechanism\n    self$w_q <- nn_linear(d_model, d_model, bias = FALSE)\n    self$w_k <- nn_linear(d_model, d_model, bias = FALSE)  \n    self$w_v <- nn_linear(d_model, d_model, bias = FALSE)\n    self$w_o <- nn_linear(d_model, d_model)\n    \n    # Feed-forward neural network\n    self$ff <- nn_sequential(\n      nn_linear(d_model, d_model * 4),\n      nn_relu(),\n      nn_linear(d_model * 4, d_model)\n    )\n    \n    self$ln1 <- nn_layer_norm(d_model)\n    self$ln2 <- nn_layer_norm(d_model)\n    self$dropout <- nn_dropout(0.1)\n  },\n  \n  forward = function(x, mask = NULL) {\n    # Multi-head self-attention (exactly like our simple example, but multi-headed!)\n    batch_size <- x$size(1)\n    seq_len <- x$size(2)\n    \n    q <- self$w_q(x)$view(c(batch_size, seq_len, self$n_heads, self$d_k))$transpose(2, 3)\n    k <- self$w_k(x)$view(c(batch_size, seq_len, self$n_heads, self$d_k))$transpose(2, 3)\n    v <- self$w_v(x)$view(c(batch_size, seq_len, self$n_heads, self$d_k))$transpose(2, 3)\n    \n    # Scaled dot-product attention\n    scores <- torch_matmul(q, k$transpose(-2, -1)) / sqrt(self$d_k)\n    \n    if (!is.null(mask)) {\n      scores <- scores + mask$unsqueeze(1)$unsqueeze(1)\n    }\n    \n    attn_weights <- nnf_softmax(scores, dim = -1)\n    attn_output <- torch_matmul(attn_weights, v)\n    \n    # Combine heads and apply output projection\n    attn_output <- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_len, self$d_model))\n    attn_output <- self$w_o(attn_output)\n    \n    # Residual connection and layer norm\n    x <- self$ln1(x + self$dropout(attn_output))\n    \n    # Feed-forward\n    ff_output <- self$ff(x)\n    x <- self$ln2(x + self$dropout(ff_output))\n    \n    x\n  }\n)\nThis is our\nself-attention mechanism\nin action! Just like in our\nsimple 3√ó3 example\n, but now it works with entire sequences and multiple attention heads.\nStep 3: The Transformer Language Model\ntoy_llm <- nn_module(\n  initialize = function(vocab_size, d_model = 256, n_heads = 8, n_layers = 4) {\n    # Word embeddings (remember our love/is/wonderful example?)\n    self$token_embedding <- nn_embedding(vocab_size, d_model)\n    self$pos_encoding <- create_positional_encoding(512, d_model, \"cpu\")\n    \n    # Stack of transformer layers\n    self$transformer_layer_1 <- transformer_layer(d_model, n_heads)\n    if (n_layers >= 2) self$transformer_layer_2 <- transformer_layer(d_model, n_heads)\n    if (n_layers >= 3) self$transformer_layer_3 <- transformer_layer(d_model, n_heads)\n    if (n_layers >= 4) self$transformer_layer_4 <- transformer_layer(d_model, n_heads)\n    self$n_layers <- n_layers\n    \n    # Output projection (back to vocabulary)\n    self$ln_f <- nn_layer_norm(d_model)\n    self$lm_head <- nn_linear(d_model, vocab_size)\n    self$dropout <- nn_dropout(0.1)\n  },\n  \n  forward = function(x) {\n    seq_len <- x$size(2)\n    \n    # Causal mask (no peeking at future words!)\n    mask <- torch_triu(torch_ones(seq_len, seq_len, device = x$device), diagonal = 1)\n    mask <- mask$masked_fill(mask == 1, -Inf)\n    \n    # Token embeddings + positional encoding\n    x <- self$token_embedding(x) * sqrt(self$d_model)\n    pos_enc <- self$pos_encoding[1:seq_len, ]$to(device = x$device)\n    x <- x + pos_enc\n    x <- self$dropout(x)\n    \n    # Pass through transformer layers\n    x <- self$transformer_layer_1(x, mask)\n    if (self$n_layers >= 2) x <- self$transformer_layer_2(x, mask)\n    if (self$n_layers >= 3) x <- self$transformer_layer_3(x, mask)\n    if (self$n_layers >= 4) x <- self$transformer_layer_4(x, mask)\n    \n    # Final layer norm and projection to vocabulary\n    x <- self$ln_f(x)\n    logits <- self$lm_head(x)\n    \n    logits\n  }\n)\nThis is the core of the LLM, the\ntransformer\n. This\nneural network\narchitecture makes use of all of the above concepts, like\nembeddings\n,\nattention\n, and\nnext word prediction\n!\nTraining Our Mini-ChatGPT\nNow comes the magic ‚Äì training our transformer on Alice in Wonderland and the Wizard of Oz:\n# Load the same text from our Markov chain example\ntxt <- readLines(url(\"http://paulo-jorente.de/text/alice_oz.txt\"), warn = FALSE)\ntraining_text <- paste(txt, collapse = \" \")\ntraining_text <- gsub(\"[^a-zA-Z0-9 .,!?;:-]\", \"\", training_text)\ntraining_text <- tolower(training_text)\n\n# Create tokenizer and model\ntokenizer <- create_tokenizer(training_text)\nmodel <- toy_llm(vocab_size = tokenizer$vocab_size, d_model = 256, n_heads = 8, n_layers = 4)\n\n# Train the model (this is where the magic happens!)\ntrain_model(model, training_text, tokenizer, epochs = 1500, seq_len = 32, batch_size = 4)\nThe Results\nAfter training, our mini-transformer produces text like this:\nPrompt ‚Äòalice‚Äô:\nalice looked down at them, and considered a little before she was going to shrink in the time and round the\nPrompt ‚Äòthe queen‚Äô:\nthe queen said to the executioner: fetch her here. and the executioner went off like an arrow. the cats head began fading\nPrompt ‚Äòdown the‚Äô:\ndown the chimney, and she said to herself now i can do no more, whatever happens. what will become of me? luckily\nCompare this to our original Markov chain output:\nanxious returned the Scarecrow It is such an uncomfortable feeling to know one is a crow or a man After the crows had gone I thought this over and decided\nThe transformer has learned:\nCharacter names and relationships\n(duchess, mock turtle, gryphon, queen of hearts, scarecrow, wizard)\nStory context and scenarios\n(Alice‚Äôs wonderland, Dorothy‚Äôs journey to Oz, dialogue patterns)\nProper grammar and sentence structure\nThe whimsical, narrative style of both Carroll and Baum‚Äôs writing\nThe Transformer Advantage\nUnlike our Markov chain that only looked at the previous 2-3 words, our transformer can:\nSee the entire context\nof hundreds of words\nUnderstand long-range dependencies\n(like who ‚Äúshe‚Äù refers to)\nLearn complex grammar and style patterns\nGenerate coherent narratives\n, not just word-by-word predictions\nFrom Toy to Production\nWhat we built is essentially a miniature version of ChatGPT! The same principles scale up:\nIndustry-level GPTs have up to hundreds of billions of parameters (weights)\n(our model has 6 million)\nGPTs train on hundreds of terabytes\n(we used two books)\nGPTs often train for months on high performance clusters\n(this one takes less than 15 minutes on a standard computer with some GPU)\nProduction models use sophisticated tokenizers\n(we used simple word splitting)\nBut the core architecture?\nExactly the same!\nThe Unreasonable Effectiveness of Transformers\nWhat‚Äôs truly remarkable is that this simple architecture ‚Äì predicting the next word using self-attention ‚Äì gives rise to seemingly intelligent behavior. Our tiny model learned:\nGrammar rules\n(without being taught grammar)\nCharacter relationships\n(without being told who‚Äôs who)\nStory structure\n(without understanding ‚Äúplot‚Äù)\nWriting style\n(without lessons in literature)\nAll from the simple task of ‚Äúpredict the next word‚Äù!\nIsn‚Äôt it fascinating that so much apparently intelligent behavior emerges from statistical text prediction? As we saw in our\nMarkov chain post\n,\n‚Äúmany tasks that demand human-level intelligence can obviously be reduced to some form of (statistical) text prediction with a sufficiently performant model!‚Äù\nTo give you an intuition, why using a neural network architecture for this is so powerful: we have already seen that neural networks build a representation of their world, a\nworld model\n(see:\nUnderstanding the Magic of Neural Networks\n). In this case, imagine a detective story which ends with ‚ÄúAnd now it was clear, the murderer was‚Ä¶‚Äù: to sensibly predict the next (and last) word the neural network really must have\nunderstood\nthe story in some sense!\nNext Steps: The Adventure Continues!\nYou‚Äôve now built your own language model using the same principles as ChatGPT! Next, we could experimenting with:\nDifferent texts\n(Shakespeare? Scientific papers? Your own writing?)\nLarger models\n(more layers, bigger embeddings)\nDifferent hyperparameters\nVarious generation strategies\n(temperature, top-k sampling)\nRemember: we‚Äôve just implemented the core technology behind the AI revolution. From Markov chains to attention mechanisms to transformers ‚Äì you‚Äôve mastered the journey from simple statistics to artificial intelligence!\nThe next time someone asks you ‚ÄúHow does ChatGPT work?‚Äù, you can confidently say: ‚ÄúLet me show you‚Ä¶‚Äù and build one from scratch (or show this post üòâ )!\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR-Bloggers ‚Äì Learning Machines\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "Remember our journey so far? We started with simple Markov chains showing how statistical word prediction works, then dove into the core concepts of word embeddings, self-attention, and next word prediction. Now, it‚Äôs time for the grand finale: if you want to build your own working transformer language model in R, read on! You will ‚Ä¶ Continue reading \"Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers!\"",
    "meta_keywords": null,
    "og_description": "Remember our journey so far? We started with simple Markov chains showing how statistical word prediction works, then dove into the core concepts of word embeddings, self-attention, and next word prediction. Now, it‚Äôs time for the grand finale: if you want to build your own working transformer language model in R, read on! You will ‚Ä¶ Continue reading \"Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers!\"",
    "og_image": "https://blog.ephorie.de/wp-content/uploads/2025/06/R-GPT-e1749980509235-300x160.png",
    "og_title": "Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers! | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 8.2,
    "sitemap_lastmod": null,
    "twitter_description": "Remember our journey so far? We started with simple Markov chains showing how statistical word prediction works, then dove into the core concepts of word embeddings, self-attention, and next word prediction. Now, it‚Äôs time for the grand finale: if you want to build your own working transformer language model in R, read on! You will ‚Ä¶ Continue reading \"Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers!\"",
    "twitter_title": "Building Your Own Mini-ChatGPT with R: From Markov Chains to Transformers! | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/06/building-your-own-mini-chatgpt-with-r-from-markov-chains-to-transformers/",
    "word_count": 1642
  }
}