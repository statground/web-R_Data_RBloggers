{
  "uuid": "91c02b30-fc4c-438c-809f-982c8b2ba166",
  "created_at": "2025-11-22 19:59:03",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/03/how-to-get-your-llm-model-to-run-and-interpret-r-code/",
    "crawled_at": "2025-11-22T10:52:24.891841",
    "external_links": [
      {
        "href": "http://www.seascapemodels.org/rstats/2025/03/17/LLMs-in-R-tool-use.html",
        "text": "Bluecology blog"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://ellmer.tidyverse.org/articles/tool-calling.html",
        "text": "There’s a good example of how tools don’t work and do work in the\nellmer\ndocumentation."
      },
      {
        "href": "https://imos-animaltracking.github.io/remora/index.html",
        "text": "Theremorapackage lets you extract ocean data from the IMOS BlueLink\ndatabase"
      },
      {
        "href": "https://imos-animaltracking.github.io/remora/index.html",
        "text": "Note thatremoraisn’t on cran, so you will need to install it from\ngithub."
      },
      {
        "href": "https://imos-animaltracking.github.io/remora/articles/extractBlue.html",
        "text": "For a more detailed example see the remora\ndocumentation"
      },
      {
        "href": "http://www.seascapemodels.org/rstats/2025/03/17/LLMs-in-R-tool-use.html",
        "text": "Bluecology blog"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "How to get your LLM model to run and interpret R code | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/bluecology-blog/",
        "text": "Bluecology blog"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-391314 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">How to get your LLM model to run and interpret R code</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">March 16, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/bluecology-blog/\">Bluecology blog</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"http://www.seascapemodels.org/rstats/2025/03/17/LLMs-in-R-tool-use.html\"> Bluecology blog</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><h1 id=\"how-to-get-your-llm-model-to-run-and-interpret-r-code\">How to get your LLM model to run and interpret R code</h1>\n<p>Tools let AI models run code and access other applications. This can be\na tool on your computer (like an R function), or access and API.</p>\n<p>By creating tools you can give your LLM access to databases online (via\ntheir APIs) or let them run code on your computer, interpret the output\nand use it in their responses.</p>\n<p>There are many ways to use tools. So far many tool examples are for\ncomputer programmers. There are some generally useful tools out there\n(like creating files or searching the web). However, I wanted to see if\nI could make a tool that was specifically useful in my research field.</p>\n<p>In this tutorial I’ll look at using them to get ocean data from a public database that has an API.</p>\n<h2 id=\"how-tools-work\">How tools work</h2>\n<p>The basic example of a tool is getting the current date and time. Then\nwhen you ask for the time, instead of hallucinating a time and date, the\nmodel can use a tool to get an accurate current time and date.</p>\n<p>Tools are defined using a Model Context Protocol, which is a\nstandardized structure for connecting natural language models to\nprogrammatic tools (which have all the normal programming rules like\nbeing case-sensitive, syntax appropriate to the language etc).</p>\n<p>Its hard to find clear explanations of how they work, but here’s what I\nunderstand:</p>\n<p>The tool is ‘registered’ with the model. This registering is just a\nprompt to the model, but it is done with a specific structure that\ndefines the tool, its parameters, outputs and use cases. This way the\nmodel knows how to write code to run the tool.</p>\n<p>Then when you chat with the model it might interpret that tool use would\nbe helpful. Like if you ask the time and it ‘knows’ about a time-date\ntool, it might use it. It writes the appropriate code to use the tool,\nsends that back to your computer (or to the API), where the code is\nevaluated. The output is then sent back to the model. The model\ninterprets this into the final response it will give you.</p>\n<p><a href=\"https://ellmer.tidyverse.org/articles/tool-calling.html\" rel=\"nofollow\" target=\"_blank\">There’s a good example of how tools don’t work and do work in the\nellmer\ndocumentation.</a></p>\n<p>Let’s try a more interesting tool than the usual example of getting date\nand time.</p>\n<h2 id=\"use-case\">Use case</h2>\n<p><a href=\"https://imos-animaltracking.github.io/remora/index.html\" rel=\"nofollow\" target=\"_blank\">The <code>remora</code> package lets you extract ocean data from the IMOS BlueLink\ndatabase</a>. We\ncan give an LLM access to this data by creating a tool that uses\n<code>remora</code> to extract the data.</p>\n<h2 id=\"remora\">remora</h2>\n<p>We’ll start by making some data and testing out <code>remora</code>. <a href=\"https://imos-animaltracking.github.io/remora/index.html\" rel=\"nofollow\" target=\"_blank\">Note that\n<code>remora</code> isn’t on cran, so you will need to install it from\ngithub.</a></p>\n<p>What we do below is download ocean temperatures for some made up data.\nWe just need coordinates and dates.</p>\n<p><a href=\"https://imos-animaltracking.github.io/remora/articles/extractBlue.html\" rel=\"nofollow\" target=\"_blank\">For a more detailed example see the remora\ndocumentation</a></p>\n<pre>library(remora)\nlibrary(tidyverse)\nlibrary(raster)\n\n# Create a dataframe of coordinates near Tasmania with different dates in 2021\ntas_dat &lt;- data.frame(\n  datetime = ymd_hms(paste(\n        c(\"2021-02-15\", \"2021-02-15\", \"2021-02-15\", \n                   \"2021-02-15\", \"2021-02-15\"),\n                   \"10:00:00\")),\n  X = c(147.2, 148.5, 146.8, 145.3, 144.7),  # Longitudes around Tasmania\n  Y = c(-42.5, -41.8, -43.2, -42.9, -41.5)\n)\nwrite.csv(tas_dat, \"tas_dat.csv\", row.names = FALSE)\n</pre>\n<p>Extracting the SST data at a given depth is simple:</p>\n<pre>tas_temp &lt;- \n    extractBlue(df = tas_dat,\n                X = \"X\", \n                Y = \"Y\", \n                datetime = \"datetime\", \n                env_var = \"BRAN_temp\",\n                extract_depth = 30,\n                verbose = TRUE,\n                full_timeperiod = FALSE)\n</pre>\n<h2 id=\"defining-the-tool\">Defining the tool</h2>\n<p>Now we know how to use <code>remora</code> to get the data, we can define a tool so\nthe LLM can do it.</p>\n<p>We’ll use the <code>ellmer</code> package to define the tool. First step is to make\na function.</p>\n<p>The <code>ellmer</code> documentation recommends you keep the function inputs and\noutputs simple. First of all, LLMs can’t handle complex R data\nstructures (like complex lists). Second, the LLM will be consuming the\ninputs and outputs. This means it uses tokens, which cost you $ and use\nup the model’s context window (capacity for prompts).</p>\n<p>So you don’t want to have gigabytes of data going into or out of the\ntool.</p>\n<p>To get around this I’ve defined a tool that takes a path and write a\nfile. That way the LLM doesn’t ‘see’ the data, it only sees the input\nand output paths and my depth request.</p>\n<pre>#' Extracts SST data from the IMOS BlueLink database\n#'\n#' @param data_path A path to a csv dataframe\n#' @param depth The depth to extract the SST data from\n#' @return The current time in the given time zone.\nget_sst &lt;- function(data_path, depth) {\n  dat &lt;- read.csv(data_path)\n  datout &lt;- extractBlue(df = dat,\n                X = \"X\", \n                Y = \"Y\", \n                datetime = \"datetime\", \n                env_var = \"BRAN_temp\",\n                extract_depth = depth,\n                verbose = FALSE,\n                full_timeperiod = FALSE)\n  write.csv(datout, \"temp_data.csv\", row.names = FALSE)\n  return(\"File written to sst_data.csv\")\n}\n</pre>\n<p>Now just test the function works:</p>\n<pre>get_sst(\"tas_dat.csv\", 30)\n</pre>\n<p>The <code>ellmer</code> package has a <code>create_tool_def</code> that semi-automates the\nnext step based on the roxygen documentation I wrote above the function.\nHowever, this currently seems only to work if you have an openAI API\nkey.</p>\n<h2 id=\"register-the-tool-with-a-chat\">Register the tool with a chat</h2>\n<p>Start a new chat:</p>\n<pre>library(ellmer)\nchat &lt;- chat_claude(\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n</pre>\n<p>(You should probably use a strong logic model, like sonnet or GPT4.0 for\ntool use, but haiku worked fine for this simple one).</p>\n<p>Now we register the tool with the chat:</p>\n<pre>mytool &lt;- tool(\n  get_sst,\n  \"Gets SST data from the IMOS BlueLink database\",\n  data_path = type_string(\n    \"Path to a csv dataframe that has columns datetime, X, Y\",\n    required = TRUE\n  ),\n  depth = type_number(\n    \"The depth to extract the SST data from\",\n    required = TRUE\n  )\n)\n\nchat$register_tool(mytool)\n</pre>\n<p>Note the careful documentation of the tool. This is going to become\nprompts for the model, so use all your normal prompt strategies to help\nit figure out when to use the tool .</p>\n<p>We also use the <code>type_xxx</code> functions from <code>ellmer</code> to define the types\nof the inputs. This is important to ensure the model knows how to\nproperly write the code for the tool use.</p>\n<h2 id=\"using-the-tool\">Using the tool</h2>\n<p>This is the easy bit:</p>\n<pre>chat$chat(\"Can you get me SST data at 21m depth for the coordinates in tas_dat.csv?\")\n</pre>\n<p>If that works for you (as it did for me) you should have a new csv file\nin your working directory with the SST data.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>With a little bit of code you can give your LLM prompts and responses\nnew types of capabilities.</p>\n<p>My simple example could be improved by adding more parameters into the\nfunction and adding error checking and reports.</p>\n<p>If we wanted to keep going we could create an analysis tool that then\nanalyses the data from file, makes the plots, and so on.</p>\n<p>Tool definitions are based on function documentation. This means you can\ntake any existing R function and easily turn it into a tool.</p>\n<p>You do want to be careful about how much and the types of data that go\ninto tools. There are obviously security concerns (what if the model\ncreated malicious code?) as well as privacy concerns if you are working\nwith sensitive data.</p>\n<p>API costs could also blow up. <code>ellmer</code> provides a function\n<code>token_usage()</code>, to help you keep track of spending.</p>\n<p>Overall, I was excited at how easy it was to make a tool with\ninteresting applications in my field of ocean sciences.</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"http://www.seascapemodels.org/rstats/2025/03/17/LLMs-in-R-tool-use.html\"> Bluecology blog</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "How to get your LLM model to run and interpret R code\nPosted on\nMarch 16, 2025\nby\nBluecology blog\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nBluecology blog\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nHow to get your LLM model to run and interpret R code\nTools let AI models run code and access other applications. This can be\na tool on your computer (like an R function), or access and API.\nBy creating tools you can give your LLM access to databases online (via\ntheir APIs) or let them run code on your computer, interpret the output\nand use it in their responses.\nThere are many ways to use tools. So far many tool examples are for\ncomputer programmers. There are some generally useful tools out there\n(like creating files or searching the web). However, I wanted to see if\nI could make a tool that was specifically useful in my research field.\nIn this tutorial I’ll look at using them to get ocean data from a public database that has an API.\nHow tools work\nThe basic example of a tool is getting the current date and time. Then\nwhen you ask for the time, instead of hallucinating a time and date, the\nmodel can use a tool to get an accurate current time and date.\nTools are defined using a Model Context Protocol, which is a\nstandardized structure for connecting natural language models to\nprogrammatic tools (which have all the normal programming rules like\nbeing case-sensitive, syntax appropriate to the language etc).\nIts hard to find clear explanations of how they work, but here’s what I\nunderstand:\nThe tool is ‘registered’ with the model. This registering is just a\nprompt to the model, but it is done with a specific structure that\ndefines the tool, its parameters, outputs and use cases. This way the\nmodel knows how to write code to run the tool.\nThen when you chat with the model it might interpret that tool use would\nbe helpful. Like if you ask the time and it ‘knows’ about a time-date\ntool, it might use it. It writes the appropriate code to use the tool,\nsends that back to your computer (or to the API), where the code is\nevaluated. The output is then sent back to the model. The model\ninterprets this into the final response it will give you.\nThere’s a good example of how tools don’t work and do work in the\nellmer\ndocumentation.\nLet’s try a more interesting tool than the usual example of getting date\nand time.\nUse case\nThe\nremora\npackage lets you extract ocean data from the IMOS BlueLink\ndatabase\n. We\ncan give an LLM access to this data by creating a tool that uses\nremora\nto extract the data.\nremora\nWe’ll start by making some data and testing out\nremora\n.\nNote that\nremora\nisn’t on cran, so you will need to install it from\ngithub.\nWhat we do below is download ocean temperatures for some made up data.\nWe just need coordinates and dates.\nFor a more detailed example see the remora\ndocumentation\nlibrary(remora)\nlibrary(tidyverse)\nlibrary(raster)\n\n# Create a dataframe of coordinates near Tasmania with different dates in 2021\ntas_dat <- data.frame(\n  datetime = ymd_hms(paste(\n        c(\"2021-02-15\", \"2021-02-15\", \"2021-02-15\", \n                   \"2021-02-15\", \"2021-02-15\"),\n                   \"10:00:00\")),\n  X = c(147.2, 148.5, 146.8, 145.3, 144.7),  # Longitudes around Tasmania\n  Y = c(-42.5, -41.8, -43.2, -42.9, -41.5)\n)\nwrite.csv(tas_dat, \"tas_dat.csv\", row.names = FALSE)\nExtracting the SST data at a given depth is simple:\ntas_temp <- \n    extractBlue(df = tas_dat,\n                X = \"X\", \n                Y = \"Y\", \n                datetime = \"datetime\", \n                env_var = \"BRAN_temp\",\n                extract_depth = 30,\n                verbose = TRUE,\n                full_timeperiod = FALSE)\nDefining the tool\nNow we know how to use\nremora\nto get the data, we can define a tool so\nthe LLM can do it.\nWe’ll use the\nellmer\npackage to define the tool. First step is to make\na function.\nThe\nellmer\ndocumentation recommends you keep the function inputs and\noutputs simple. First of all, LLMs can’t handle complex R data\nstructures (like complex lists). Second, the LLM will be consuming the\ninputs and outputs. This means it uses tokens, which cost you $ and use\nup the model’s context window (capacity for prompts).\nSo you don’t want to have gigabytes of data going into or out of the\ntool.\nTo get around this I’ve defined a tool that takes a path and write a\nfile. That way the LLM doesn’t ‘see’ the data, it only sees the input\nand output paths and my depth request.\n#' Extracts SST data from the IMOS BlueLink database\n#'\n#' @param data_path A path to a csv dataframe\n#' @param depth The depth to extract the SST data from\n#' @return The current time in the given time zone.\nget_sst <- function(data_path, depth) {\n  dat <- read.csv(data_path)\n  datout <- extractBlue(df = dat,\n                X = \"X\", \n                Y = \"Y\", \n                datetime = \"datetime\", \n                env_var = \"BRAN_temp\",\n                extract_depth = depth,\n                verbose = FALSE,\n                full_timeperiod = FALSE)\n  write.csv(datout, \"temp_data.csv\", row.names = FALSE)\n  return(\"File written to sst_data.csv\")\n}\nNow just test the function works:\nget_sst(\"tas_dat.csv\", 30)\nThe\nellmer\npackage has a\ncreate_tool_def\nthat semi-automates the\nnext step based on the roxygen documentation I wrote above the function.\nHowever, this currently seems only to work if you have an openAI API\nkey.\nRegister the tool with a chat\nStart a new chat:\nlibrary(ellmer)\nchat <- chat_claude(\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n(You should probably use a strong logic model, like sonnet or GPT4.0 for\ntool use, but haiku worked fine for this simple one).\nNow we register the tool with the chat:\nmytool <- tool(\n  get_sst,\n  \"Gets SST data from the IMOS BlueLink database\",\n  data_path = type_string(\n    \"Path to a csv dataframe that has columns datetime, X, Y\",\n    required = TRUE\n  ),\n  depth = type_number(\n    \"The depth to extract the SST data from\",\n    required = TRUE\n  )\n)\n\nchat$register_tool(mytool)\nNote the careful documentation of the tool. This is going to become\nprompts for the model, so use all your normal prompt strategies to help\nit figure out when to use the tool .\nWe also use the\ntype_xxx\nfunctions from\nellmer\nto define the types\nof the inputs. This is important to ensure the model knows how to\nproperly write the code for the tool use.\nUsing the tool\nThis is the easy bit:\nchat$chat(\"Can you get me SST data at 21m depth for the coordinates in tas_dat.csv?\")\nIf that works for you (as it did for me) you should have a new csv file\nin your working directory with the SST data.\nConclusion\nWith a little bit of code you can give your LLM prompts and responses\nnew types of capabilities.\nMy simple example could be improved by adding more parameters into the\nfunction and adding error checking and reports.\nIf we wanted to keep going we could create an analysis tool that then\nanalyses the data from file, makes the plots, and so on.\nTool definitions are based on function documentation. This means you can\ntake any existing R function and easily turn it into a tool.\nYou do want to be careful about how much and the types of data that go\ninto tools. There are obviously security concerns (what if the model\ncreated malicious code?) as well as privacy concerns if you are working\nwith sensitive data.\nAPI costs could also blow up.\nellmer\nprovides a function\ntoken_usage()\n, to help you keep track of spending.\nOverall, I was excited at how easy it was to make a tool with\ninteresting applications in my field of ocean sciences.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nBluecology blog\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "How to get your LLM model to run and interpret R code Tools let AI models run code and access other applications. This can be a tool on your computer (like an R function), or access and API. By creating tools you can give your LLM access to databases...",
    "meta_keywords": null,
    "og_description": "How to get your LLM model to run and interpret R code Tools let AI models run code and access other applications. This can be a tool on your computer (like an R function), or access and API. By creating tools you can give your LLM access to databases...",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "How to get your LLM model to run and interpret R code | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 7,
    "sitemap_lastmod": null,
    "twitter_description": "How to get your LLM model to run and interpret R code Tools let AI models run code and access other applications. This can be a tool on your computer (like an R function), or access and API. By creating tools you can give your LLM access to databases...",
    "twitter_title": "How to get your LLM model to run and interpret R code | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/03/how-to-get-your-llm-model-to-run-and-interpret-r-code/",
    "word_count": 1394
  }
}