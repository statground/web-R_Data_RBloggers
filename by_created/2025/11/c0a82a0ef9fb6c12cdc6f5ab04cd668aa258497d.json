{
  "id": "c0a82a0ef9fb6c12cdc6f5ab04cd668aa258497d",
  "url": "https://www.r-bloggers.com/2025/10/text-analytics-in-r-with-quanteda-part-1/",
  "created_at_utc": "2025-11-22T19:57:32Z",
  "data": null,
  "raw_original": {
    "uuid": "2bf8cb11-eed4-44ad-b9a6-09ed5f4d5140",
    "created_at": "2025-11-22 19:57:32",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/10/text-analytics-in-r-with-quanteda-part-1/",
      "crawled_at": "2025-11-22T10:42:11.825221",
      "external_links": [
        {
          "href": "https://rtichoke.netlify.app/posts/text-analytics-quanteda-part1.html",
          "text": "R'tichoke"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://quanteda.io/",
          "text": "quanteda documentation"
        },
        {
          "href": "https://tutorials.quanteda.io/",
          "text": "quanteda tutorials"
        },
        {
          "href": "https://www.tidytextmining.com/",
          "text": "Text Mining with R"
        },
        {
          "href": "https://cran.r-project.org/web/views/NaturalLanguageProcessing.html",
          "text": "CRAN Task View"
        },
        {
          "href": "https://rtichoke.netlify.app/posts/text-analytics-quanteda-part1.html",
          "text": "R'tichoke"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Text Analytics in R with quanteda (Part 1) | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-12-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-13-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-14-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-15-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-16-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-17-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-17-2.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-20-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/rtichoke/",
          "text": "R'tichoke"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-395916 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Text Analytics in R with quanteda (Part 1)</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">October 6, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/rtichoke/\">R'tichoke</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://rtichoke.netlify.app/posts/text-analytics-quanteda-part1.html\"> R'tichoke</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<section class=\"level2\" id=\"required-packages\">\n<h2 class=\"anchored\" data-anchor-id=\"required-packages\">Required Packages</h2>\n<div class=\"cell\">\n<pre>library(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(DT)\nlibrary(tidytext)</pre>\n</div>\n</section>\n<section class=\"level2\" id=\"understanding-text-analytics-fundamentals\">\n<h2 class=\"anchored\" data-anchor-id=\"understanding-text-analytics-fundamentals\">Understanding Text Analytics Fundamentals</h2>\n<p>Text analytics transforms text into structured data suitable for analysis. A typical workflow looks like so:</p>\n<ol type=\"1\">\n<li><strong>Text acquisition and loading</strong>: Importing text data from various sources</li>\n<li><strong>Preprocessing</strong>: Cleaning and standardizing text</li>\n<li><strong>Tokenization</strong>: Breaking text into meaningful units (words, sentences, n-grams)</li>\n<li><strong>Document-Feature Matrix (DFM) creation</strong>: Representing text numerically</li>\n<li><strong>Analysis</strong>: Extracting insights through statistical and computational methods</li>\n</ol>\n</section>\n<section class=\"level2\" id=\"creating-a-corpus-prepping-data-for-analysis\">\n<h2 class=\"anchored\" data-anchor-id=\"creating-a-corpus-prepping-data-for-analysis\">Creating a Corpus: Prepping data for analysis</h2>\n<p>A <strong>corpus</strong> is a structured collection of texts. <code>quanteda</code> provides the <code>corpus()</code> function to create corpus objects from various data sources.</p>\n<section class=\"level3\" id=\"example-1-simple-corpus-from-character-vector\">\n<h3 class=\"anchored\" data-anchor-id=\"example-1-simple-corpus-from-character-vector\">Example 1: Simple Corpus from Character Vector</h3>\n<div class=\"cell\">\n<pre># Create a simple corpus from a character vector\ntexts &lt;- c(\n  \"The quick brown fox jumps over the lazy dog.\",\n  \"Natural language processing is fascinating and powerful.\",\n  \"Text analytics enables data-driven decision making.\",\n  \"Machine learning algorithms can analyze text at scale.\",\n  \"Data science combines statistics, programming, and domain knowledge.\"\n)\n\n# Create corpus\ncorp &lt;- corpus(texts)\n\n# Examine the corpus\nsummary(corp)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Corpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences\n text1    10     10         1\n text2     8      8         1\n text3     7      7         1\n text4     9      9         1\n text5    10     11         1</pre>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"example-2-corpus-from-data-frame\">\n<h3 class=\"anchored\" data-anchor-id=\"example-2-corpus-from-data-frame\">Example 2: Corpus from Data Frame</h3>\n<p>In practice, text data typically comes with associated metadata (e.g., author, date, category). <code>quanteda</code> handles this well:</p>\n<div class=\"cell\">\n<pre># Create a data frame with text and metadata\ntext_df &lt;- data.frame(\n  text = c(\n    \"Customer service was excellent and responsive.\",\n    \"Product quality is poor. Very disappointed.\",\n    \"Shipping was fast. Happy with my purchase.\",\n    \"Price is too high for the quality received.\",\n    \"Great value for money. Would recommend!\"\n  ),\n  rating = c(5, 2, 4, 2, 5),\n  product_category = c(\"Electronics\", \"Clothing\", \"Electronics\", \"Clothing\", \"Electronics\"),\n  review_date = as.Date(c(\"2025-01-15\", \"2025-02-20\", \"2025-03-10\", \"2025-04-05\", \"2025-05-12\")),\n  stringsAsFactors = FALSE\n)\n\ndatatable(text_df)</pre>\n<div class=\"cell-output-display\">\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-695f209d56d5555bc498\" style=\"width:100%;height:auto;\"></div>\n\n</div>\n</div>\n<div class=\"cell\">\n<pre># Create corpus from data frame\nreviews_corp &lt;- corpus(text_df, text_field = \"text\")\n\n# Examine corpus with metadata\nsummary(reviews_corp)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Corpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences rating product_category review_date\n text1     7      7         1      5      Electronics  2025-01-15\n text2     7      8         2      2         Clothing  2025-02-20\n text3     8      9         2      4      Electronics  2025-03-10\n text4     9      9         1      2         Clothing  2025-04-05\n text5     8      8         2      5      Electronics  2025-05-12</pre>\n</div>\n</div>\n<div class=\"cell\">\n<pre># Access document variables (metadata) (columns not declared as \"text_field\")\ndocvars(reviews_corp)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>  rating product_category review_date\n1      5      Electronics  2025-01-15\n2      2         Clothing  2025-02-20\n3      4      Electronics  2025-03-10\n4      2         Clothing  2025-04-05\n5      5      Electronics  2025-05-12</pre>\n</div>\n</div>\n<div class=\"cell\">\n<pre># Subset corpus by metadata\nhigh_rated &lt;- corpus_subset(reviews_corp, rating &gt;= 4)\nsummary(high_rated)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Corpus consisting of 3 documents, showing 3 documents:\n\n  Text Types Tokens Sentences rating product_category review_date\n text1     7      7         1      5      Electronics  2025-01-15\n text3     8      9         2      4      Electronics  2025-03-10\n text5     8      8         2      5      Electronics  2025-05-12</pre>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"tokenization-breaking-text-into-units\">\n<h2 class=\"anchored\" data-anchor-id=\"tokenization-breaking-text-into-units\">Tokenization: Breaking text into units</h2>\n<p>Tokenization is the process of splitting text into individual units (tokens), typically words. The <code>tokens()</code> function provides lots of tokenisation capabilities.</p>\n<section class=\"level3\" id=\"basic-tokenization\">\n<h3 class=\"anchored\" data-anchor-id=\"basic-tokenization\">Basic Tokenization</h3>\n<div class=\"cell\">\n<pre># Tokenize the reviews corpus\ntoks &lt;- tokens(reviews_corp)\n\n# View tokens from all documents\nprint(toks)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 5 documents and 3 docvars.\ntext1 :\n[1] \"Customer\"   \"service\"    \"was\"        \"excellent\"  \"and\"       \n[6] \"responsive\" \".\"         \n\ntext2 :\n[1] \"Product\"      \"quality\"      \"is\"           \"poor\"         \".\"           \n[6] \"Very\"         \"disappointed\" \".\"           \n\ntext3 :\n[1] \"Shipping\" \"was\"      \"fast\"     \".\"        \"Happy\"    \"with\"     \"my\"      \n[8] \"purchase\" \".\"       \n\ntext4 :\n[1] \"Price\"    \"is\"       \"too\"      \"high\"     \"for\"      \"the\"      \"quality\" \n[8] \"received\" \".\"       \n\ntext5 :\n[1] \"Great\"     \"value\"     \"for\"       \"money\"     \".\"         \"Would\"    \n[7] \"recommend\" \"!\"        </pre>\n</div>\n</div>\n</section>\n<section class=\"level3\" id=\"advanced-tokenization-options\">\n<h3 class=\"anchored\" data-anchor-id=\"advanced-tokenization-options\">Advanced Tokenization Options</h3>\n<p><code>quanteda</code> offers a lot of control over tokenization:</p>\n<div class=\"cell\">\n<pre># Create sample text with various elements\nsample_text &lt;- \"Dr. Smith's email is <a class=\"__cf_email__\" data-cfemail=\"1d77727573336e707469755d78657c706d7178337e7270\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>. \n                He earned $100,000 in 2024! Visit https://example.com \n                for more info. #DataScience #AI\"\n\nsample_corp &lt;- corpus(sample_text)\n\n# Different tokenization approaches\ntokens_default &lt;- tokens(sample_corp)\ntokens_no_punct &lt;- tokens(sample_corp, remove_punct = TRUE)\ntokens_no_numbers &lt;- tokens(sample_corp, remove_numbers = TRUE)\ntokens_no_symbols &lt;- tokens(sample_corp, remove_symbols = TRUE)\ntokens_lowercase &lt;- tokens(sample_corp, remove_punct = TRUE) %&gt;% tokens_tolower()\n\n# Compare results\nprint(tokens_default)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \".\"                      \"Smith's\"               \n [4] \"email\"                  \"is\"                     \"<a class=\"__cf_email__\" data-cfemail=\"264c494e4808554b4f524e66435e474b564a430845494b\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\"\n [7] \".\"                      \"He\"                     \"earned\"                \n[10] \"$\"                      \"100,000\"                \"in\"                    \n[ ... and 10 more ]</pre>\n</div>\n<pre>print(tokens_no_punct)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \"Smith's\"                \"email\"                 \n [4] \"is\"                     \"<a class=\"__cf_email__\" data-cfemail=\"3c56535452124f515548547c59445d514c5059125f5351\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\" \"He\"                    \n [7] \"earned\"                 \"$\"                      \"100,000\"               \n[10] \"in\"                     \"2024\"                   \"Visit\"                 \n[ ... and 6 more ]</pre>\n</div>\n<pre>print(tokens_no_numbers)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \".\"                      \"Smith's\"               \n [4] \"email\"                  \"is\"                     \"<a class=\"__cf_email__\" data-cfemail=\"c4aeabacaaeab7a9adb0ac84a1bca5a9b4a8a1eaa7aba9\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\"\n [7] \".\"                      \"He\"                     \"earned\"                \n[10] \"$\"                      \"in\"                     \"!\"                     \n[ ... and 8 more ]</pre>\n</div>\n<pre>print(tokens_no_symbols)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \".\"                      \"Smith's\"               \n [4] \"email\"                  \"is\"                     \"<a class=\"__cf_email__\" data-cfemail=\"157f7a7d7b3b66787c617d55706d74786579703b767a78\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\"\n [7] \".\"                      \"He\"                     \"earned\"                \n[10] \"100,000\"                \"in\"                     \"2024\"                  \n[ ... and 9 more ]</pre>\n</div>\n<pre>print(tokens_lowercase)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n [1] \"dr\"                     \"smith's\"                \"email\"                 \n [4] \"is\"                     \"<a class=\"__cf_email__\" data-cfemail=\"dfb5b0b7b1f1acb2b6abb79fbaa7beb2afb3baf1bcb0b2\" href=\"/cdn-cgi/l/email-protection\">[email protected]</a>\" \"he\"                    \n [7] \"earned\"                 \"$\"                      \"100,000\"               \n[10] \"in\"                     \"2024\"                   \"visit\"                 \n[ ... and 6 more ]</pre>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"stopword-removal\">\n<h2 class=\"anchored\" data-anchor-id=\"stopword-removal\">Stopword Removal</h2>\n<p>Stopwords are common words (e.g., “the”, “is”, “at”) that typically don’t carry significant meaning. Removing them reduces noise and improves ovrerall efficiency.</p>\n<div class=\"cell\">\n<pre># View built-in English stopwords (first 20)\nhead(stopwords(\"english\"), 20)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre> [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n[16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      </pre>\n</div>\n<pre># Count of English stopwords\nlength(stopwords(\"english\"))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 175</pre>\n</div>\n<pre># Remove stopwords from tokens\ntoks_no_stop &lt;- tokens(reviews_corp, \n                       remove_punct = TRUE,\n                       remove_numbers = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\"))\n\n# Compare with and without stopwords\nprint(tokens(reviews_corp, remove_punct = TRUE)[1])</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document and 3 docvars.\ntext1 :\n[1] \"Customer\"   \"service\"    \"was\"        \"excellent\"  \"and\"       \n[6] \"responsive\"</pre>\n</div>\n<pre>print(toks_no_stop[1])</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document and 3 docvars.\ntext1 :\n[1] \"customer\"   \"service\"    \"excellent\"  \"responsive\"</pre>\n</div>\n<pre># Count tokens before and after\nprint(ntoken(tokens(reviews_corp, remove_punct = TRUE)))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>text1 text2 text3 text4 text5 \n    6     6     7     8     6 </pre>\n</div>\n<pre>print(ntoken(toks_no_stop))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>text1 text2 text3 text4 text5 \n    4     4     4     4     4 </pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"stemming\">\n<h2 class=\"anchored\" data-anchor-id=\"stemming\">Stemming</h2>\n<p><strong>Stemming</strong> reduces words to their root form by removing suffixes (e.g., “running” → “run”).</p>\n<div class=\"cell\">\n<pre># Example text demonstrating word variations\nstem_text &lt;- c(\n  \"The running runners ran faster than expected.\",\n  \"Computing computers computed complex calculations.\",\n  \"The analyst analyzed analytical data using analysis techniques.\"\n)\n\nstem_corp &lt;- corpus(stem_text)\n\n# Tokenize\nstem_toks &lt;- tokens(stem_corp, remove_punct = TRUE) %&gt;% tokens_tolower()\n\n# Apply stemming\nstem_toks_stemmed &lt;- tokens_wordstem(stem_toks)\n\n# Compare original and stemmed\nprint(stem_toks[1])</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n[1] \"the\"      \"running\"  \"runners\"  \"ran\"      \"faster\"   \"than\"     \"expected\"</pre>\n</div>\n<pre>print(stem_toks_stemmed[1])</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Tokens consisting of 1 document.\ntext1 :\n[1] \"the\"    \"run\"    \"runner\" \"ran\"    \"faster\" \"than\"   \"expect\"</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"document-feature-matrix-dfm-numerical-representation\">\n<h2 class=\"anchored\" data-anchor-id=\"document-feature-matrix-dfm-numerical-representation\">Document-Feature Matrix (DFM): Numerical Representation</h2>\n<p>A <strong>Document-Feature Matrix (DFM)</strong> is a numerical representation of text where rows represent documents, columns represent features (typically words), and cell values indicate feature frequency in each document. This structure enables statistical analysis and machine learning applications.</p>\n<div class=\"cell\">\n<pre># Create a DFM from our reviews corpus\nreviews_dfm &lt;- reviews_corp %&gt;%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm()\n\n# Examine the DFM\nprint(reviews_dfm)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Document-feature matrix of: 5 documents, 19 features (78.95% sparse) and 3 docvars.\n       features\ndocs    customer service excellent responsive product quality poor disappointed\n  text1        1       1         1          1       0       0    0            0\n  text2        0       0         0          0       1       1    1            1\n  text3        0       0         0          0       0       0    0            0\n  text4        0       0         0          0       0       1    0            0\n  text5        0       0         0          0       0       0    0            0\n       features\ndocs    shipping fast\n  text1        0    0\n  text2        0    0\n  text3        1    1\n  text4        0    0\n  text5        0    0\n[ reached max_nfeat ... 9 more features ]</pre>\n</div>\n<pre># View DFM dimensions\nprint(dim(reviews_dfm))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1]  5 19</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"feature-statistics-understanding-word-frequencies\">\n<h2 class=\"anchored\" data-anchor-id=\"feature-statistics-understanding-word-frequencies\">Feature Statistics: Understanding Word Frequencies</h2>\n<p>Analyzing feature frequencies reveals the most important terms in the corpus.</p>\n<div class=\"cell\">\n<pre># Calculate feature frequencies\nfeat_freq &lt;- textstat_frequency(reviews_dfm)\n\n# View top features\nhead(feat_freq, 15)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>        feature frequency rank docfreq group\n1       quality         2    1       2   all\n2      customer         1    2       1   all\n3       service         1    2       1   all\n4     excellent         1    2       1   all\n5    responsive         1    2       1   all\n6       product         1    2       1   all\n7          poor         1    2       1   all\n8  disappointed         1    2       1   all\n9      shipping         1    2       1   all\n10         fast         1    2       1   all\n11        happy         1    2       1   all\n12     purchase         1    2       1   all\n13        price         1    2       1   all\n14         high         1    2       1   all\n15     received         1    2       1   all</pre>\n</div>\n<pre># Visualize top features\nfeat_freq %&gt;%\n  head(15) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 15 Most Frequent Terms\",\n       x = \"Term\",\n       y = \"Frequency\") +\n  theme_minimal()</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<section class=\"level3\" id=\"group-based-feature-analysis\">\n<h3 class=\"anchored\" data-anchor-id=\"group-based-feature-analysis\">Group-Based Feature Analysis</h3>\n<p>Analyzing features by groups (e.g., high vs. low ratings) reveals distinctive vocabulary:</p>\n<div class=\"cell\">\n<pre># Group DFM by rating category\nreviews_dfm_grouped &lt;- reviews_corp %&gt;%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = rating)\n\n# Calculate frequencies by group\nfreq_by_rating &lt;- textstat_frequency(reviews_dfm_grouped, groups = rating)\n\n# View top features for each rating\nprint(freq_by_rating %&gt;% filter(group == 5) %&gt;% head(10))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>      feature frequency rank docfreq group\n12   customer         1    1       1     5\n13    service         1    1       1     5\n14  excellent         1    1       1     5\n15 responsive         1    1       1     5\n16      great         1    1       1     5\n17      value         1    1       1     5\n18      money         1    1       1     5\n19  recommend         1    1       1     5</pre>\n</div>\n<pre>print(freq_by_rating %&gt;% filter(group == 2) %&gt;% head(10))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>       feature frequency rank docfreq group\n1      quality         2    1       1     2\n2      product         1    2       1     2\n3         poor         1    2       1     2\n4 disappointed         1    2       1     2\n5        price         1    2       1     2\n6         high         1    2       1     2\n7     received         1    2       1     2</pre>\n</div>\n<pre># Visualize comparison\nfreq_by_rating %&gt;%\n  filter(group %in% c(2, 5)) %&gt;%\n  group_by(group) %&gt;%\n  slice_max(frequency, n = 8) %&gt;%\n  ungroup() %&gt;%\n  mutate(feature = reorder_within(feature, frequency, group)) %&gt;% \n  \n  ggplot(aes(x = feature, y = frequency, fill = factor(group))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ group, scales = \"free_y\", labeller = labeller(group = c(\"2\" = \"2-Star Reviews\", \"5\" = \"5-Star Reviews\"))) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(title = \"Top Terms by Review Rating\",\n       x = \"Term\",\n       y = \"Frequency\") +\n  theme_minimal()</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-13-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-13-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"word-clouds-visual-exploration\">\n<h2 class=\"anchored\" data-anchor-id=\"word-clouds-visual-exploration\">Word Clouds: Visual Exploration</h2>\n<p>Word clouds provide intuitive visualization of term frequencies:</p>\n<div class=\"cell\">\n<pre># Create word cloud\nset.seed(123)\ntextplot_wordcloud(reviews_dfm, \n                   min_count = 1,\n                   max_words = 50,\n                   rotation = 0.25,\n                   color = RColorBrewer::brewer.pal(8, \"Dark2\"))</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-14-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-14-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"n-grams-multi-word-expressions\">\n<h2 class=\"anchored\" data-anchor-id=\"n-grams-multi-word-expressions\">N-grams: Multi-Word Expressions</h2>\n<p><strong>N-grams</strong> are contiguous sequences of n tokens. Bigrams (2-grams) and trigrams (3-grams) capture multi-word expressions and phrases that single words miss.</p>\n<div class=\"cell\">\n<pre># Create sample text for n-gram analysis\nngram_text &lt;- c(\n  \"Machine learning and artificial intelligence are transforming data science.\",\n  \"Natural language processing enables text analytics at scale.\",\n  \"Deep learning models achieve state of the art results.\",\n  \"Data science requires domain knowledge and technical skills.\",\n  \"Text mining extracts insights from unstructured data.\"\n)\n\nngram_corp &lt;- corpus(ngram_text)\n\n# Create bigrams\nbigrams &lt;- ngram_corp %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_ngrams(n = 2) %&gt;%\n  dfm()\n\n# Calculate bigram frequencies\nbigram_freq &lt;- textstat_frequency(bigrams)\nhead(bigram_freq, 15)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>                   feature frequency rank docfreq group\n1             data_science         2    1       2   all\n2         machine_learning         1    2       1   all\n3             learning_and         1    2       1   all\n4           and_artificial         1    2       1   all\n5  artificial_intelligence         1    2       1   all\n6         intelligence_are         1    2       1   all\n7         are_transforming         1    2       1   all\n8        transforming_data         1    2       1   all\n9         natural_language         1    2       1   all\n10     language_processing         1    2       1   all\n11      processing_enables         1    2       1   all\n12            enables_text         1    2       1   all\n13          text_analytics         1    2       1   all\n14            analytics_at         1    2       1   all\n15                at_scale         1    2       1   all</pre>\n</div>\n<pre># Visualize top bigrams\nbigram_freq %&gt;%\n  head(10) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = \"indianred\") +\n  coord_flip() +\n  labs(title = \"Top 10 Bigrams\",\n       x = \"Bigram\",\n       y = \"Frequency\") +\n  theme_minimal()</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-15-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-15-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<div class=\"cell\">\n<pre># Create trigrams\ntrigrams &lt;- ngram_corp %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_ngrams(n = 3) %&gt;%\n  dfm()\n\n# Calculate trigram frequencies\ntrigram_freq &lt;- textstat_frequency(trigrams)\n\n# Visualize top bigrams\ntrigram_freq %&gt;%\n  head(10) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Trigrams\",\n       x = \"Trigram\",\n       y = \"Frequency\") +\n  theme_minimal()</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-16-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-16-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"real-world-example-analyzing-customer-feedback\">\n<h2 class=\"anchored\" data-anchor-id=\"real-world-example-analyzing-customer-feedback\">Real-World Example: Analyzing Customer Feedback</h2>\n<p>Let’s try and apply these techniques to a more realistic scenario.</p>\n<div class=\"cell\">\n<pre># Create a realistic customer feedback dataset\nset.seed(456)\nfeedback_df &lt;- data.frame(\n  text = c(\n    \"Absolutely love this product! Best purchase I've made all year. Quality is outstanding.\",\n    \"Terrible experience. Product broke after one week. Customer service was unhelpful.\",\n    \"Good value for the price. Works as expected. Would buy again.\",\n    \"Shipping took forever. Product is okay but not worth the wait.\",\n    \"Amazing quality and fast delivery. Highly recommend to everyone!\",\n    \"Product description was misleading. Not what I expected at all.\",\n    \"Decent product but customer support needs improvement. Long wait times.\",\n    \"Exceeded my expectations! Great features and easy to use.\",\n    \"Poor quality control. Received damaged item. Return process was difficult.\",\n    \"Perfect! Exactly what I needed. Five stars all around.\",\n    \"Overpriced for what you get. Better alternatives available elsewhere.\",\n    \"Good product but instructions were confusing. Setup took hours.\",\n    \"Love it! Works perfectly and looks great too.\",\n    \"Not satisfied. Product feels cheap and flimsy.\",\n    \"Best customer service ever! They resolved my issue immediately.\",\n    \"Average product. Nothing special but gets the job done.\",\n    \"Fantastic! Will definitely purchase from this company again.\",\n    \"Disappointed with the quality. Expected much better.\",\n    \"Great features but battery life is poor.\",\n    \"Excellent value. Highly recommend for budget shoppers.\"\n  ),\n  rating = c(5, 1, 4, 2, 5, 1, 3, 5, 1, 5, 2, 3, 5, 2, 5, 3, 5, 2, 3, 4),\n  category = sample(c(\"Electronics\", \"Home &amp; Kitchen\", \"Clothing\"), 20, replace = TRUE),\n  helpful_votes = sample(0:50, 20, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\n# Create corpus\nfeedback_corp &lt;- corpus(feedback_df, text_field = \"text\")\n\n# Complete preprocessing pipeline\nfeedback_dfm &lt;- feedback_corp %&gt;%\n  tokens(remove_punct = TRUE, \n         remove_numbers = TRUE,\n         remove_symbols = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_wordstem() %&gt;%\n  dfm()\n\n# Analyze overall sentiment-related terms\nsentiment_terms &lt;- c(\"love\", \"best\", \"great\", \"excel\", \"amaz\", \"perfect\", \n                     \"terribl\", \"poor\", \"worst\", \"disappoint\", \"bad\")\n\nsentiment_dfm &lt;- dfm_select(feedback_dfm, pattern = sentiment_terms)\n\n# Calculate sentiment term frequencies\nsentiment_freq &lt;- textstat_frequency(sentiment_dfm)\n\n# Visualize sentiment terms\nggplot(sentiment_freq, aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(aes(fill = feature), show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Frequency of Sentiment-Related Terms\",\n       subtitle = \"Customer Feedback Analysis\",\n       x = \"Term (Stemmed)\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\n    \"love\" = \"darkgreen\", \"best\" = \"darkgreen\", \"great\" = \"darkgreen\",\n    \"excel\" = \"darkgreen\", \"amaz\" = \"darkgreen\", \"perfect\" = \"darkgreen\",\n    \"terribl\" = \"darkred\", \"poor\" = \"darkred\", \"worst\" = \"darkred\",\n    \"disappoint\" = \"darkred\", \"bad\" = \"darkred\"\n  ))</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-17-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-17-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n<pre># Compare high vs low-rated reviews\n# Create a rating category variable\nrating_category &lt;- ifelse(docvars(feedback_corp, \"rating\") &gt;= 4, \"High Rating\", \"Low Rating\")\n\nfeedback_grouped &lt;- feedback_corp %&gt;%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = rating_category)\n\n# Calculate keyness (distinctive terms)\nkeyness_stats &lt;- textstat_keyness(feedback_grouped, target = \"High Rating\")\n\n# Visualize keyness\ntextplot_keyness(keyness_stats, n = 10, color = c(\"darkgreen\", \"darkred\")) +\n  labs(title = \"Distinctive Terms: High vs Low Ratings\") +\n  theme_minimal()</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-17-2.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-17-2.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"document-similarity-analysis\">\n<h2 class=\"anchored\" data-anchor-id=\"document-similarity-analysis\">Document Similarity Analysis</h2>\n<p>Understanding document similarity is crucial for tasks like duplicate detection, document clustering, and recommendation systems:</p>\n<div class=\"cell\">\n<pre># Use the preprocessed DFM for better similarity measurement\nfeedback_dfm_clean &lt;- feedback_corp %&gt;%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_wordstem() %&gt;%\n  dfm()\n\n# Calculate document similarity using cosine similarity\ndoc_similarity &lt;- textstat_simil(feedback_dfm_clean, \n                                 method = \"cosine\",\n                                 margin = \"documents\")\n\n# Find most similar documents to first review (a positive review)\nsimilarity_df &lt;- as.data.frame(as.matrix(doc_similarity))\nsimilarity_to_doc1 &lt;- sort(as.numeric(similarity_df[1, ]), decreasing = TRUE)[2:6]  # Skip first (itself)\n\n# First review document\nas.character(feedback_corp)[1]</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>                                                                                    text1 \n\"Absolutely love this product! Best purchase I've made all year. Quality is outstanding.\" </pre>\n</div>\n<pre># Top 3 similar documents\nfor(i in 2:4) {\n  doc_idx &lt;- order(as.numeric(similarity_df[1, ]), decreasing = TRUE)[i]\n  cat(\"\\nDocument\", doc_idx, \"(Similarity:\", round(similarity_df[1, doc_idx], 3), \"):\\n\")\n  cat(as.character(feedback_corp)[doc_idx], \"\\n\")\n}</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\nDocument 6 (Similarity: 0.167 ):\nProduct description was misleading. Not what I expected at all. \n\nDocument 17 (Similarity: 0.167 ):\nFantastic! Will definitely purchase from this company again. \n\nDocument 13 (Similarity: 0.149 ):\nLove it! Works perfectly and looks great too. </pre>\n</div>\n</div>\n<p>It is worth noting that cosine similarity alone might not be enough since the top most similar document does not match the document’s sentiment.</p>\n<section class=\"level3\" id=\"sentiment-aware-similarity\">\n<h3 class=\"anchored\" data-anchor-id=\"sentiment-aware-similarity\">Sentiment-Aware Similarity</h3>\n<p>To improve similarity assessment we can incorporate sentiment. Let’s add sentiment score as a feature.</p>\n<div class=\"cell\">\n<pre># Define positive and negative sentiment lexicons\npositive_words &lt;- c(\"love\", \"best\", \"great\", \"excellent\", \"amazing\", \"perfect\", \n                    \"fantastic\", \"outstanding\", \"happy\", \"wonderful\", \"superb\")\n\nnegative_words &lt;- c(\"terrible\", \"poor\", \"worst\", \"disappointing\", \"bad\", \"awful\",\n                    \"horrible\", \"useless\", \"disappointed\", \"misleading\", \"cheap\")\n\n# Create tokens\nfeedback_toks &lt;- feedback_corp %&gt;%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\"))\n\n# Calculate sentiment scores for each document\nsentiment_scores &lt;- sapply(feedback_toks, function(doc_tokens) {\n  pos_count &lt;- sum(doc_tokens %in% positive_words)\n  neg_count &lt;- sum(doc_tokens %in% negative_words)\n  \n  # Net sentiment score\n  (pos_count - neg_count) / length(doc_tokens)\n})\n\n# Create DFM with sentiment features\nfeedback_dfm_sentiment &lt;- feedback_toks %&gt;%\n  tokens_wordstem() %&gt;%\n  dfm()\n\n# Add sentiment score as a weighted feature\n# Create a sentiment feature by replicating the sentiment score\nsentiment_feature_matrix &lt;- matrix(sentiment_scores * 10,  # Scale up for visibility\n                                   nrow = ndoc(feedback_dfm_sentiment),\n                                   ncol = 1,\n                                   dimnames = list(docnames(feedback_dfm_sentiment), \n                                                   \"SENTIMENT_SCORE\"))\n\n# Combine with original DFM\nfeedback_dfm_with_sentiment &lt;- cbind(feedback_dfm_sentiment, sentiment_feature_matrix)\n\n# Calculate similarity with sentiment\ndoc_similarity_sentiment &lt;- textstat_simil(feedback_dfm_with_sentiment,\n                                           method = \"cosine\",\n                                           margin = \"documents\")\n\n# Compare results\nsimilarity_df_sentiment &lt;- as.data.frame(as.matrix(doc_similarity_sentiment))\n\n#Standard vs Sentiment-Aware Similarity\n\n# First document\ncat(as.character(feedback_corp)[1], \"\\n\")</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Absolutely love this product! Best purchase I've made all year. Quality is outstanding. </pre>\n</div>\n<pre># Sentiment-aware similarity\nfor(i in 2:4) {\n  doc_idx &lt;- order(as.numeric(similarity_df_sentiment[1, ]), decreasing = TRUE)[i]\n  cat(\"\\nDocument\", doc_idx, \"(Similarity:\", round(similarity_df_sentiment[1, doc_idx], 3), \"):\\n\")\n  cat(as.character(feedback_corp)[doc_idx], \"\\n\")\n  cat(\"Rating:\", docvars(feedback_corp, \"rating\")[doc_idx], \n      \"| Sentiment:\", round(sentiment_scores[doc_idx], 3), \"\\n\")\n}</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\nDocument 13 (Similarity: 0.697 ):\nLove it! Works perfectly and looks great too. \nRating: 5 | Sentiment: 0.4 \n\nDocument 17 (Similarity: 0.65 ):\nFantastic! Will definitely purchase from this company again. \nRating: 5 | Sentiment: 0.25 \n\nDocument 5 (Similarity: 0.427 ):\nAmazing quality and fast delivery. Highly recommend to everyone! \nRating: 5 | Sentiment: 0.143 </pre>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" id=\"feature-co-occurrence-analysis\">\n<h2 class=\"anchored\" data-anchor-id=\"feature-co-occurrence-analysis\">Feature Co-occurrence Analysis</h2>\n<p>Understanding which words frequently appear together can help reveal semantic relationships.</p>\n<div class=\"cell\">\n<pre># Create feature co-occurrence matrix\nfcm &lt;- feedback_corp %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  fcm()\n\n# View top co-occurrences\nfeat_cooc &lt;- fcm[1:20, 1:20]\n\n# Visualize semantic network\nset.seed(123)\ntextplot_network(fcm, \n                 min_freq = 2,\n                 edge_alpha = 0.5,\n                 edge_size = 2,\n                 vertex_labelsize = 3) +\n  labs(title = \"Semantic Network of Customer Feedback\")</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-20-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-20-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"dfm-manipulation-and-transformation\">\n<h2 class=\"anchored\" data-anchor-id=\"dfm-manipulation-and-transformation\">DFM Manipulation and Transformation</h2>\n<p><code>quanteda</code> provides powerful functions for manipulating DFMs:</p>\n<div class=\"cell\">\n<pre># Trim DFM to remove rare and very common features\nfeedback_dfm_trimmed &lt;- dfm_trim(feedback_dfm,\n                                 min_termfreq = 2,  # Remove terms appearing &lt; 2 times\n                                 max_docfreq = 0.8,  # Remove terms in &gt; 80% of docs\n                                 docfreq_type = \"prop\")\n\n# Original DFM dimensions\nprint(dim(feedback_dfm))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 20 95</pre>\n</div>\n<pre># Trimmed DFM dimensions \nprint(dim(feedback_dfm_trimmed))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 20 22</pre>\n</div>\n<pre># Weight DFM using TF-IDF\nfeedback_tfidf &lt;- dfm_tfidf(feedback_dfm_trimmed)\n\n# Top features by TF-IDF\ntfidf_freq &lt;- textstat_frequency(feedback_tfidf, force = T)\nprint(head(tfidf_freq, 15))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>   feature frequency rank docfreq group\n1  product  3.183520    1       8   all\n2  qualiti  2.795880    2       4   all\n3   expect  2.795880    2       4   all\n4   custom  2.471726    4       3   all\n5    great  2.471726    4       3   all\n6     love  2.000000    6       2   all\n7     best  2.000000    6       2   all\n8  purchas  2.000000    6       2   all\n9   servic  2.000000    6       2   all\n10    good  2.000000    6       2   all\n11    valu  2.000000    6       2   all\n12    work  2.000000    6       2   all\n13    took  2.000000    6       2   all\n14    wait  2.000000    6       2   all\n15    high  2.000000    6       2   all</pre>\n</div>\n<pre># Select specific features\nservice_terms &lt;- c(\"service\", \"support\", \"help\", \"response\", \"customer\")\nservice_dfm &lt;- dfm_select(feedback_dfm, pattern = service_terms)\n\n# Frequency of service related terms\nprint(colSums(service_dfm))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>support \n      1 </pre>\n</div>\n<pre># Remove specific features\nfiltered_dfm &lt;- dfm_remove(feedback_dfm, pattern = c(\"product\", \"item\"))\n\n# Original feature count\nnfeat(feedback_dfm)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 95</pre>\n</div>\n<pre># Filtered feature count\nnfeat(filtered_dfm)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 93</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"additional-resources\">\n<h2 class=\"anchored\" data-anchor-id=\"additional-resources\">Additional Resources</h2>\n<ul>\n<li><a href=\"https://quanteda.io/\" rel=\"nofollow\" target=\"_blank\">quanteda documentation</a></li>\n<li><a href=\"https://tutorials.quanteda.io/\" rel=\"nofollow\" target=\"_blank\">quanteda tutorials</a></li>\n<li><a href=\"https://www.tidytextmining.com/\" rel=\"nofollow\" target=\"_blank\">Text Mining with R</a></li>\n<li><a href=\"https://cran.r-project.org/web/views/NaturalLanguageProcessing.html\" rel=\"nofollow\" target=\"_blank\">CRAN Task View</a></li>\n</ul>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://rtichoke.netlify.app/posts/text-analytics-quanteda-part1.html\"> R'tichoke</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Text Analytics in R with quanteda (Part 1)\nPosted on\nOctober 6, 2025\nby\nR'tichoke\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR'tichoke\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nRequired Packages\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(DT)\nlibrary(tidytext)\nUnderstanding Text Analytics Fundamentals\nText analytics transforms text into structured data suitable for analysis. A typical workflow looks like so:\nText acquisition and loading\n: Importing text data from various sources\nPreprocessing\n: Cleaning and standardizing text\nTokenization\n: Breaking text into meaningful units (words, sentences, n-grams)\nDocument-Feature Matrix (DFM) creation\n: Representing text numerically\nAnalysis\n: Extracting insights through statistical and computational methods\nCreating a Corpus: Prepping data for analysis\nA\ncorpus\nis a structured collection of texts.\nquanteda\nprovides the\ncorpus()\nfunction to create corpus objects from various data sources.\nExample 1: Simple Corpus from Character Vector\n# Create a simple corpus from a character vector\ntexts <- c(\n  \"The quick brown fox jumps over the lazy dog.\",\n  \"Natural language processing is fascinating and powerful.\",\n  \"Text analytics enables data-driven decision making.\",\n  \"Machine learning algorithms can analyze text at scale.\",\n  \"Data science combines statistics, programming, and domain knowledge.\"\n)\n\n# Create corpus\ncorp <- corpus(texts)\n\n# Examine the corpus\nsummary(corp)\nCorpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences\n text1    10     10         1\n text2     8      8         1\n text3     7      7         1\n text4     9      9         1\n text5    10     11         1\nExample 2: Corpus from Data Frame\nIn practice, text data typically comes with associated metadata (e.g., author, date, category).\nquanteda\nhandles this well:\n# Create a data frame with text and metadata\ntext_df <- data.frame(\n  text = c(\n    \"Customer service was excellent and responsive.\",\n    \"Product quality is poor. Very disappointed.\",\n    \"Shipping was fast. Happy with my purchase.\",\n    \"Price is too high for the quality received.\",\n    \"Great value for money. Would recommend!\"\n  ),\n  rating = c(5, 2, 4, 2, 5),\n  product_category = c(\"Electronics\", \"Clothing\", \"Electronics\", \"Clothing\", \"Electronics\"),\n  review_date = as.Date(c(\"2025-01-15\", \"2025-02-20\", \"2025-03-10\", \"2025-04-05\", \"2025-05-12\")),\n  stringsAsFactors = FALSE\n)\n\ndatatable(text_df)\n# Create corpus from data frame\nreviews_corp <- corpus(text_df, text_field = \"text\")\n\n# Examine corpus with metadata\nsummary(reviews_corp)\nCorpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences rating product_category review_date\n text1     7      7         1      5      Electronics  2025-01-15\n text2     7      8         2      2         Clothing  2025-02-20\n text3     8      9         2      4      Electronics  2025-03-10\n text4     9      9         1      2         Clothing  2025-04-05\n text5     8      8         2      5      Electronics  2025-05-12\n# Access document variables (metadata) (columns not declared as \"text_field\")\ndocvars(reviews_corp)\nrating product_category review_date\n1      5      Electronics  2025-01-15\n2      2         Clothing  2025-02-20\n3      4      Electronics  2025-03-10\n4      2         Clothing  2025-04-05\n5      5      Electronics  2025-05-12\n# Subset corpus by metadata\nhigh_rated <- corpus_subset(reviews_corp, rating >= 4)\nsummary(high_rated)\nCorpus consisting of 3 documents, showing 3 documents:\n\n  Text Types Tokens Sentences rating product_category review_date\n text1     7      7         1      5      Electronics  2025-01-15\n text3     8      9         2      4      Electronics  2025-03-10\n text5     8      8         2      5      Electronics  2025-05-12\nTokenization: Breaking text into units\nTokenization is the process of splitting text into individual units (tokens), typically words. The\ntokens()\nfunction provides lots of tokenisation capabilities.\nBasic Tokenization\n# Tokenize the reviews corpus\ntoks <- tokens(reviews_corp)\n\n# View tokens from all documents\nprint(toks)\nTokens consisting of 5 documents and 3 docvars.\ntext1 :\n[1] \"Customer\"   \"service\"    \"was\"        \"excellent\"  \"and\"       \n[6] \"responsive\" \".\"         \n\ntext2 :\n[1] \"Product\"      \"quality\"      \"is\"           \"poor\"         \".\"           \n[6] \"Very\"         \"disappointed\" \".\"           \n\ntext3 :\n[1] \"Shipping\" \"was\"      \"fast\"     \".\"        \"Happy\"    \"with\"     \"my\"      \n[8] \"purchase\" \".\"       \n\ntext4 :\n[1] \"Price\"    \"is\"       \"too\"      \"high\"     \"for\"      \"the\"      \"quality\" \n[8] \"received\" \".\"       \n\ntext5 :\n[1] \"Great\"     \"value\"     \"for\"       \"money\"     \".\"         \"Would\"    \n[7] \"recommend\" \"!\"\nAdvanced Tokenization Options\nquanteda\noffers a lot of control over tokenization:\n# Create sample text with various elements\nsample_text <- \"Dr. Smith's email is\n[email protected]\n. \n                He earned $100,000 in 2024! Visit https://example.com \n                for more info. #DataScience #AI\"\n\nsample_corp <- corpus(sample_text)\n\n# Different tokenization approaches\ntokens_default <- tokens(sample_corp)\ntokens_no_punct <- tokens(sample_corp, remove_punct = TRUE)\ntokens_no_numbers <- tokens(sample_corp, remove_numbers = TRUE)\ntokens_no_symbols <- tokens(sample_corp, remove_symbols = TRUE)\ntokens_lowercase <- tokens(sample_corp, remove_punct = TRUE) %>% tokens_tolower()\n\n# Compare results\nprint(tokens_default)\nTokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \".\"                      \"Smith's\"               \n [4] \"email\"                  \"is\"                     \"\n[email protected]\n\"\n [7] \".\"                      \"He\"                     \"earned\"                \n[10] \"$\"                      \"100,000\"                \"in\"                    \n[ ... and 10 more ]\nprint(tokens_no_punct)\nTokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \"Smith's\"                \"email\"                 \n [4] \"is\"                     \"\n[email protected]\n\" \"He\"                    \n [7] \"earned\"                 \"$\"                      \"100,000\"               \n[10] \"in\"                     \"2024\"                   \"Visit\"                 \n[ ... and 6 more ]\nprint(tokens_no_numbers)\nTokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \".\"                      \"Smith's\"               \n [4] \"email\"                  \"is\"                     \"\n[email protected]\n\"\n [7] \".\"                      \"He\"                     \"earned\"                \n[10] \"$\"                      \"in\"                     \"!\"                     \n[ ... and 8 more ]\nprint(tokens_no_symbols)\nTokens consisting of 1 document.\ntext1 :\n [1] \"Dr\"                     \".\"                      \"Smith's\"               \n [4] \"email\"                  \"is\"                     \"\n[email protected]\n\"\n [7] \".\"                      \"He\"                     \"earned\"                \n[10] \"100,000\"                \"in\"                     \"2024\"                  \n[ ... and 9 more ]\nprint(tokens_lowercase)\nTokens consisting of 1 document.\ntext1 :\n [1] \"dr\"                     \"smith's\"                \"email\"                 \n [4] \"is\"                     \"\n[email protected]\n\" \"he\"                    \n [7] \"earned\"                 \"$\"                      \"100,000\"               \n[10] \"in\"                     \"2024\"                   \"visit\"                 \n[ ... and 6 more ]\nStopword Removal\nStopwords are common words (e.g., “the”, “is”, “at”) that typically don’t carry significant meaning. Removing them reduces noise and improves ovrerall efficiency.\n# View built-in English stopwords (first 20)\nhead(stopwords(\"english\"), 20)\n[1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n[16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"\n# Count of English stopwords\nlength(stopwords(\"english\"))\n[1] 175\n# Remove stopwords from tokens\ntoks_no_stop <- tokens(reviews_corp, \n                       remove_punct = TRUE,\n                       remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\"))\n\n# Compare with and without stopwords\nprint(tokens(reviews_corp, remove_punct = TRUE)[1])\nTokens consisting of 1 document and 3 docvars.\ntext1 :\n[1] \"Customer\"   \"service\"    \"was\"        \"excellent\"  \"and\"       \n[6] \"responsive\"\nprint(toks_no_stop[1])\nTokens consisting of 1 document and 3 docvars.\ntext1 :\n[1] \"customer\"   \"service\"    \"excellent\"  \"responsive\"\n# Count tokens before and after\nprint(ntoken(tokens(reviews_corp, remove_punct = TRUE)))\ntext1 text2 text3 text4 text5 \n    6     6     7     8     6\nprint(ntoken(toks_no_stop))\ntext1 text2 text3 text4 text5 \n    4     4     4     4     4\nStemming\nStemming\nreduces words to their root form by removing suffixes (e.g., “running” → “run”).\n# Example text demonstrating word variations\nstem_text <- c(\n  \"The running runners ran faster than expected.\",\n  \"Computing computers computed complex calculations.\",\n  \"The analyst analyzed analytical data using analysis techniques.\"\n)\n\nstem_corp <- corpus(stem_text)\n\n# Tokenize\nstem_toks <- tokens(stem_corp, remove_punct = TRUE) %>% tokens_tolower()\n\n# Apply stemming\nstem_toks_stemmed <- tokens_wordstem(stem_toks)\n\n# Compare original and stemmed\nprint(stem_toks[1])\nTokens consisting of 1 document.\ntext1 :\n[1] \"the\"      \"running\"  \"runners\"  \"ran\"      \"faster\"   \"than\"     \"expected\"\nprint(stem_toks_stemmed[1])\nTokens consisting of 1 document.\ntext1 :\n[1] \"the\"    \"run\"    \"runner\" \"ran\"    \"faster\" \"than\"   \"expect\"\nDocument-Feature Matrix (DFM): Numerical Representation\nA\nDocument-Feature Matrix (DFM)\nis a numerical representation of text where rows represent documents, columns represent features (typically words), and cell values indicate feature frequency in each document. This structure enables statistical analysis and machine learning applications.\n# Create a DFM from our reviews corpus\nreviews_dfm <- reviews_corp %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  dfm()\n\n# Examine the DFM\nprint(reviews_dfm)\nDocument-feature matrix of: 5 documents, 19 features (78.95% sparse) and 3 docvars.\n       features\ndocs    customer service excellent responsive product quality poor disappointed\n  text1        1       1         1          1       0       0    0            0\n  text2        0       0         0          0       1       1    1            1\n  text3        0       0         0          0       0       0    0            0\n  text4        0       0         0          0       0       1    0            0\n  text5        0       0         0          0       0       0    0            0\n       features\ndocs    shipping fast\n  text1        0    0\n  text2        0    0\n  text3        1    1\n  text4        0    0\n  text5        0    0\n[ reached max_nfeat ... 9 more features ]\n# View DFM dimensions\nprint(dim(reviews_dfm))\n[1]  5 19\nFeature Statistics: Understanding Word Frequencies\nAnalyzing feature frequencies reveals the most important terms in the corpus.\n# Calculate feature frequencies\nfeat_freq <- textstat_frequency(reviews_dfm)\n\n# View top features\nhead(feat_freq, 15)\nfeature frequency rank docfreq group\n1       quality         2    1       2   all\n2      customer         1    2       1   all\n3       service         1    2       1   all\n4     excellent         1    2       1   all\n5    responsive         1    2       1   all\n6       product         1    2       1   all\n7          poor         1    2       1   all\n8  disappointed         1    2       1   all\n9      shipping         1    2       1   all\n10         fast         1    2       1   all\n11        happy         1    2       1   all\n12     purchase         1    2       1   all\n13        price         1    2       1   all\n14         high         1    2       1   all\n15     received         1    2       1   all\n# Visualize top features\nfeat_freq %>%\n  head(15) %>%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 15 Most Frequent Terms\",\n       x = \"Term\",\n       y = \"Frequency\") +\n  theme_minimal()\nGroup-Based Feature Analysis\nAnalyzing features by groups (e.g., high vs. low ratings) reveals distinctive vocabulary:\n# Group DFM by rating category\nreviews_dfm_grouped <- reviews_corp %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  dfm() %>%\n  dfm_group(groups = rating)\n\n# Calculate frequencies by group\nfreq_by_rating <- textstat_frequency(reviews_dfm_grouped, groups = rating)\n\n# View top features for each rating\nprint(freq_by_rating %>% filter(group == 5) %>% head(10))\nfeature frequency rank docfreq group\n12   customer         1    1       1     5\n13    service         1    1       1     5\n14  excellent         1    1       1     5\n15 responsive         1    1       1     5\n16      great         1    1       1     5\n17      value         1    1       1     5\n18      money         1    1       1     5\n19  recommend         1    1       1     5\nprint(freq_by_rating %>% filter(group == 2) %>% head(10))\nfeature frequency rank docfreq group\n1      quality         2    1       1     2\n2      product         1    2       1     2\n3         poor         1    2       1     2\n4 disappointed         1    2       1     2\n5        price         1    2       1     2\n6         high         1    2       1     2\n7     received         1    2       1     2\n# Visualize comparison\nfreq_by_rating %>%\n  filter(group %in% c(2, 5)) %>%\n  group_by(group) %>%\n  slice_max(frequency, n = 8) %>%\n  ungroup() %>%\n  mutate(feature = reorder_within(feature, frequency, group)) %>% \n  \n  ggplot(aes(x = feature, y = frequency, fill = factor(group))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ group, scales = \"free_y\", labeller = labeller(group = c(\"2\" = \"2-Star Reviews\", \"5\" = \"5-Star Reviews\"))) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(title = \"Top Terms by Review Rating\",\n       x = \"Term\",\n       y = \"Frequency\") +\n  theme_minimal()\nWord Clouds: Visual Exploration\nWord clouds provide intuitive visualization of term frequencies:\n# Create word cloud\nset.seed(123)\ntextplot_wordcloud(reviews_dfm, \n                   min_count = 1,\n                   max_words = 50,\n                   rotation = 0.25,\n                   color = RColorBrewer::brewer.pal(8, \"Dark2\"))\nN-grams: Multi-Word Expressions\nN-grams\nare contiguous sequences of n tokens. Bigrams (2-grams) and trigrams (3-grams) capture multi-word expressions and phrases that single words miss.\n# Create sample text for n-gram analysis\nngram_text <- c(\n  \"Machine learning and artificial intelligence are transforming data science.\",\n  \"Natural language processing enables text analytics at scale.\",\n  \"Deep learning models achieve state of the art results.\",\n  \"Data science requires domain knowledge and technical skills.\",\n  \"Text mining extracts insights from unstructured data.\"\n)\n\nngram_corp <- corpus(ngram_text)\n\n# Create bigrams\nbigrams <- ngram_corp %>%\n  tokens(remove_punct = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_ngrams(n = 2) %>%\n  dfm()\n\n# Calculate bigram frequencies\nbigram_freq <- textstat_frequency(bigrams)\nhead(bigram_freq, 15)\nfeature frequency rank docfreq group\n1             data_science         2    1       2   all\n2         machine_learning         1    2       1   all\n3             learning_and         1    2       1   all\n4           and_artificial         1    2       1   all\n5  artificial_intelligence         1    2       1   all\n6         intelligence_are         1    2       1   all\n7         are_transforming         1    2       1   all\n8        transforming_data         1    2       1   all\n9         natural_language         1    2       1   all\n10     language_processing         1    2       1   all\n11      processing_enables         1    2       1   all\n12            enables_text         1    2       1   all\n13          text_analytics         1    2       1   all\n14            analytics_at         1    2       1   all\n15                at_scale         1    2       1   all\n# Visualize top bigrams\nbigram_freq %>%\n  head(10) %>%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = \"indianred\") +\n  coord_flip() +\n  labs(title = \"Top 10 Bigrams\",\n       x = \"Bigram\",\n       y = \"Frequency\") +\n  theme_minimal()\n# Create trigrams\ntrigrams <- ngram_corp %>%\n  tokens(remove_punct = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_ngrams(n = 3) %>%\n  dfm()\n\n# Calculate trigram frequencies\ntrigram_freq <- textstat_frequency(trigrams)\n\n# Visualize top bigrams\ntrigram_freq %>%\n  head(10) %>%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Trigrams\",\n       x = \"Trigram\",\n       y = \"Frequency\") +\n  theme_minimal()\nReal-World Example: Analyzing Customer Feedback\nLet’s try and apply these techniques to a more realistic scenario.\n# Create a realistic customer feedback dataset\nset.seed(456)\nfeedback_df <- data.frame(\n  text = c(\n    \"Absolutely love this product! Best purchase I've made all year. Quality is outstanding.\",\n    \"Terrible experience. Product broke after one week. Customer service was unhelpful.\",\n    \"Good value for the price. Works as expected. Would buy again.\",\n    \"Shipping took forever. Product is okay but not worth the wait.\",\n    \"Amazing quality and fast delivery. Highly recommend to everyone!\",\n    \"Product description was misleading. Not what I expected at all.\",\n    \"Decent product but customer support needs improvement. Long wait times.\",\n    \"Exceeded my expectations! Great features and easy to use.\",\n    \"Poor quality control. Received damaged item. Return process was difficult.\",\n    \"Perfect! Exactly what I needed. Five stars all around.\",\n    \"Overpriced for what you get. Better alternatives available elsewhere.\",\n    \"Good product but instructions were confusing. Setup took hours.\",\n    \"Love it! Works perfectly and looks great too.\",\n    \"Not satisfied. Product feels cheap and flimsy.\",\n    \"Best customer service ever! They resolved my issue immediately.\",\n    \"Average product. Nothing special but gets the job done.\",\n    \"Fantastic! Will definitely purchase from this company again.\",\n    \"Disappointed with the quality. Expected much better.\",\n    \"Great features but battery life is poor.\",\n    \"Excellent value. Highly recommend for budget shoppers.\"\n  ),\n  rating = c(5, 1, 4, 2, 5, 1, 3, 5, 1, 5, 2, 3, 5, 2, 5, 3, 5, 2, 3, 4),\n  category = sample(c(\"Electronics\", \"Home & Kitchen\", \"Clothing\"), 20, replace = TRUE),\n  helpful_votes = sample(0:50, 20, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\n# Create corpus\nfeedback_corp <- corpus(feedback_df, text_field = \"text\")\n\n# Complete preprocessing pipeline\nfeedback_dfm <- feedback_corp %>%\n  tokens(remove_punct = TRUE, \n         remove_numbers = TRUE,\n         remove_symbols = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  tokens_wordstem() %>%\n  dfm()\n\n# Analyze overall sentiment-related terms\nsentiment_terms <- c(\"love\", \"best\", \"great\", \"excel\", \"amaz\", \"perfect\", \n                     \"terribl\", \"poor\", \"worst\", \"disappoint\", \"bad\")\n\nsentiment_dfm <- dfm_select(feedback_dfm, pattern = sentiment_terms)\n\n# Calculate sentiment term frequencies\nsentiment_freq <- textstat_frequency(sentiment_dfm)\n\n# Visualize sentiment terms\nggplot(sentiment_freq, aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(aes(fill = feature), show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Frequency of Sentiment-Related Terms\",\n       subtitle = \"Customer Feedback Analysis\",\n       x = \"Term (Stemmed)\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\n    \"love\" = \"darkgreen\", \"best\" = \"darkgreen\", \"great\" = \"darkgreen\",\n    \"excel\" = \"darkgreen\", \"amaz\" = \"darkgreen\", \"perfect\" = \"darkgreen\",\n    \"terribl\" = \"darkred\", \"poor\" = \"darkred\", \"worst\" = \"darkred\",\n    \"disappoint\" = \"darkred\", \"bad\" = \"darkred\"\n  ))\n# Compare high vs low-rated reviews\n# Create a rating category variable\nrating_category <- ifelse(docvars(feedback_corp, \"rating\") >= 4, \"High Rating\", \"Low Rating\")\n\nfeedback_grouped <- feedback_corp %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  dfm() %>%\n  dfm_group(groups = rating_category)\n\n# Calculate keyness (distinctive terms)\nkeyness_stats <- textstat_keyness(feedback_grouped, target = \"High Rating\")\n\n# Visualize keyness\ntextplot_keyness(keyness_stats, n = 10, color = c(\"darkgreen\", \"darkred\")) +\n  labs(title = \"Distinctive Terms: High vs Low Ratings\") +\n  theme_minimal()\nDocument Similarity Analysis\nUnderstanding document similarity is crucial for tasks like duplicate detection, document clustering, and recommendation systems:\n# Use the preprocessed DFM for better similarity measurement\nfeedback_dfm_clean <- feedback_corp %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  tokens_wordstem() %>%\n  dfm()\n\n# Calculate document similarity using cosine similarity\ndoc_similarity <- textstat_simil(feedback_dfm_clean, \n                                 method = \"cosine\",\n                                 margin = \"documents\")\n\n# Find most similar documents to first review (a positive review)\nsimilarity_df <- as.data.frame(as.matrix(doc_similarity))\nsimilarity_to_doc1 <- sort(as.numeric(similarity_df[1, ]), decreasing = TRUE)[2:6]  # Skip first (itself)\n\n# First review document\nas.character(feedback_corp)[1]\ntext1 \n\"Absolutely love this product! Best purchase I've made all year. Quality is outstanding.\"\n# Top 3 similar documents\nfor(i in 2:4) {\n  doc_idx <- order(as.numeric(similarity_df[1, ]), decreasing = TRUE)[i]\n  cat(\"\\nDocument\", doc_idx, \"(Similarity:\", round(similarity_df[1, doc_idx], 3), \"):\\n\")\n  cat(as.character(feedback_corp)[doc_idx], \"\\n\")\n}\nDocument 6 (Similarity: 0.167 ):\nProduct description was misleading. Not what I expected at all. \n\nDocument 17 (Similarity: 0.167 ):\nFantastic! Will definitely purchase from this company again. \n\nDocument 13 (Similarity: 0.149 ):\nLove it! Works perfectly and looks great too.\nIt is worth noting that cosine similarity alone might not be enough since the top most similar document does not match the document’s sentiment.\nSentiment-Aware Similarity\nTo improve similarity assessment we can incorporate sentiment. Let’s add sentiment score as a feature.\n# Define positive and negative sentiment lexicons\npositive_words <- c(\"love\", \"best\", \"great\", \"excellent\", \"amazing\", \"perfect\", \n                    \"fantastic\", \"outstanding\", \"happy\", \"wonderful\", \"superb\")\n\nnegative_words <- c(\"terrible\", \"poor\", \"worst\", \"disappointing\", \"bad\", \"awful\",\n                    \"horrible\", \"useless\", \"disappointed\", \"misleading\", \"cheap\")\n\n# Create tokens\nfeedback_toks <- feedback_corp %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\"))\n\n# Calculate sentiment scores for each document\nsentiment_scores <- sapply(feedback_toks, function(doc_tokens) {\n  pos_count <- sum(doc_tokens %in% positive_words)\n  neg_count <- sum(doc_tokens %in% negative_words)\n  \n  # Net sentiment score\n  (pos_count - neg_count) / length(doc_tokens)\n})\n\n# Create DFM with sentiment features\nfeedback_dfm_sentiment <- feedback_toks %>%\n  tokens_wordstem() %>%\n  dfm()\n\n# Add sentiment score as a weighted feature\n# Create a sentiment feature by replicating the sentiment score\nsentiment_feature_matrix <- matrix(sentiment_scores * 10,  # Scale up for visibility\n                                   nrow = ndoc(feedback_dfm_sentiment),\n                                   ncol = 1,\n                                   dimnames = list(docnames(feedback_dfm_sentiment), \n                                                   \"SENTIMENT_SCORE\"))\n\n# Combine with original DFM\nfeedback_dfm_with_sentiment <- cbind(feedback_dfm_sentiment, sentiment_feature_matrix)\n\n# Calculate similarity with sentiment\ndoc_similarity_sentiment <- textstat_simil(feedback_dfm_with_sentiment,\n                                           method = \"cosine\",\n                                           margin = \"documents\")\n\n# Compare results\nsimilarity_df_sentiment <- as.data.frame(as.matrix(doc_similarity_sentiment))\n\n#Standard vs Sentiment-Aware Similarity\n\n# First document\ncat(as.character(feedback_corp)[1], \"\\n\")\nAbsolutely love this product! Best purchase I've made all year. Quality is outstanding.\n# Sentiment-aware similarity\nfor(i in 2:4) {\n  doc_idx <- order(as.numeric(similarity_df_sentiment[1, ]), decreasing = TRUE)[i]\n  cat(\"\\nDocument\", doc_idx, \"(Similarity:\", round(similarity_df_sentiment[1, doc_idx], 3), \"):\\n\")\n  cat(as.character(feedback_corp)[doc_idx], \"\\n\")\n  cat(\"Rating:\", docvars(feedback_corp, \"rating\")[doc_idx], \n      \"| Sentiment:\", round(sentiment_scores[doc_idx], 3), \"\\n\")\n}\nDocument 13 (Similarity: 0.697 ):\nLove it! Works perfectly and looks great too. \nRating: 5 | Sentiment: 0.4 \n\nDocument 17 (Similarity: 0.65 ):\nFantastic! Will definitely purchase from this company again. \nRating: 5 | Sentiment: 0.25 \n\nDocument 5 (Similarity: 0.427 ):\nAmazing quality and fast delivery. Highly recommend to everyone! \nRating: 5 | Sentiment: 0.143\nFeature Co-occurrence Analysis\nUnderstanding which words frequently appear together can help reveal semantic relationships.\n# Create feature co-occurrence matrix\nfcm <- feedback_corp %>%\n  tokens(remove_punct = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  fcm()\n\n# View top co-occurrences\nfeat_cooc <- fcm[1:20, 1:20]\n\n# Visualize semantic network\nset.seed(123)\ntextplot_network(fcm, \n                 min_freq = 2,\n                 edge_alpha = 0.5,\n                 edge_size = 2,\n                 vertex_labelsize = 3) +\n  labs(title = \"Semantic Network of Customer Feedback\")\nDFM Manipulation and Transformation\nquanteda\nprovides powerful functions for manipulating DFMs:\n# Trim DFM to remove rare and very common features\nfeedback_dfm_trimmed <- dfm_trim(feedback_dfm,\n                                 min_termfreq = 2,  # Remove terms appearing < 2 times\n                                 max_docfreq = 0.8,  # Remove terms in > 80% of docs\n                                 docfreq_type = \"prop\")\n\n# Original DFM dimensions\nprint(dim(feedback_dfm))\n[1] 20 95\n# Trimmed DFM dimensions \nprint(dim(feedback_dfm_trimmed))\n[1] 20 22\n# Weight DFM using TF-IDF\nfeedback_tfidf <- dfm_tfidf(feedback_dfm_trimmed)\n\n# Top features by TF-IDF\ntfidf_freq <- textstat_frequency(feedback_tfidf, force = T)\nprint(head(tfidf_freq, 15))\nfeature frequency rank docfreq group\n1  product  3.183520    1       8   all\n2  qualiti  2.795880    2       4   all\n3   expect  2.795880    2       4   all\n4   custom  2.471726    4       3   all\n5    great  2.471726    4       3   all\n6     love  2.000000    6       2   all\n7     best  2.000000    6       2   all\n8  purchas  2.000000    6       2   all\n9   servic  2.000000    6       2   all\n10    good  2.000000    6       2   all\n11    valu  2.000000    6       2   all\n12    work  2.000000    6       2   all\n13    took  2.000000    6       2   all\n14    wait  2.000000    6       2   all\n15    high  2.000000    6       2   all\n# Select specific features\nservice_terms <- c(\"service\", \"support\", \"help\", \"response\", \"customer\")\nservice_dfm <- dfm_select(feedback_dfm, pattern = service_terms)\n\n# Frequency of service related terms\nprint(colSums(service_dfm))\nsupport \n      1\n# Remove specific features\nfiltered_dfm <- dfm_remove(feedback_dfm, pattern = c(\"product\", \"item\"))\n\n# Original feature count\nnfeat(feedback_dfm)\n[1] 95\n# Filtered feature count\nnfeat(filtered_dfm)\n[1] 93\nAdditional Resources\nquanteda documentation\nquanteda tutorials\nText Mining with R\nCRAN Task View\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR'tichoke\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Required Packages library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(readr) library(dplyr) library(ggplot2) library(stringr) library(DT) library(tidytext) Understanding Text Analytics Fundamentals Text analytic...",
      "meta_keywords": null,
      "og_description": "Required Packages library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(readr) library(dplyr) library(ggplot2) library(stringr) library(DT) library(tidytext) Understanding Text Analytics Fundamentals Text analytic...",
      "og_image": "https://rtichoke.netlify.app/posts/text-analytics-quanteda-part1_files/figure-html/unnamed-chunk-12-1.png",
      "og_title": "Text Analytics in R with quanteda (Part 1) | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 16.5,
      "sitemap_lastmod": null,
      "twitter_description": "Required Packages library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(readr) library(dplyr) library(ggplot2) library(stringr) library(DT) library(tidytext) Understanding Text Analytics Fundamentals Text analytic...",
      "twitter_title": "Text Analytics in R with quanteda (Part 1) | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/10/text-analytics-in-r-with-quanteda-part-1/",
      "word_count": 3308
    }
  }
}