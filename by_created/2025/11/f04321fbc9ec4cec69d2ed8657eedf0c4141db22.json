{
  "id": "f04321fbc9ec4cec69d2ed8657eedf0c4141db22",
  "url": "https://www.r-bloggers.com/2009/11/the-top-scores-for-canabalt-take-2/",
  "created_at_utc": "2025-11-17T20:39:58Z",
  "data": null,
  "raw_original": {
    "uuid": "fa271f4f-114b-44f4-890e-bb85b3d81bb7",
    "created_at": "2025-11-17 20:39:58",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2009/11/the-top-scores-for-canabalt-take-2/",
      "crawled_at": "2025-11-17T10:16:54.493733",
      "external_links": [
        {
          "href": "http://www.johnmyleswhite.com/notebook/2009/11/15/the-top-scores-for-canabalt-take-2/",
          "text": "John Myles White: Die Sudelbücher » Statistics"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "http://www.johnmyleswhite.com/content/data_sets/canabalt_data.csv",
          "text": "here."
        },
        {
          "href": "http://www.johnmyleswhite.com/notebook/2009/11/15/the-top-scores-for-canabalt-take-2/",
          "text": "John Myles White: Die Sudelbücher » Statistics"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "The Top Scores for Canabalt, Take 2 | R-bloggers",
      "images": [
        {
          "alt": "Canabalt Score Distribution.png",
          "base64": null,
          "src": "https://i1.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Distribution11.png?w=450"
        },
        {
          "alt": "Canabalt Score Distribution.png",
          "base64": null,
          "src": "https://i1.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Distribution11.png?w=450"
        },
        {
          "alt": "Canabalt Score Poisson Simulation.png",
          "base64": null,
          "src": "https://i0.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Poisson-Simulation1.png?w=450"
        },
        {
          "alt": "Canabalt Score Poisson Simulation.png",
          "base64": null,
          "src": "https://i0.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Poisson-Simulation1.png?w=450"
        },
        {
          "alt": "Canabalt Score Discretized Normal Simulation.png",
          "base64": null,
          "src": "https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Discretized-Normal-Simulation1.png?w=450"
        },
        {
          "alt": "Canabalt Score Discretized Normal Simulation.png",
          "base64": null,
          "src": "https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Discretized-Normal-Simulation1.png?w=450"
        },
        {
          "alt": "Poisson_Small.jpg",
          "base64": null,
          "src": "https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Poisson_Small1.jpg?resize=309%2C472"
        },
        {
          "alt": "Poisson_Small.jpg",
          "base64": null,
          "src": "https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Poisson_Small1.jpg?resize=309%2C472"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/john-myles-white/",
          "text": "John Myles White"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/tag/statistics/",
          "text": "statistics"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-245 post type-post status-publish format-standard hentry category-r-bloggers tag-statistics\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">The Top Scores for Canabalt, Take 2</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 15, 2009</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/john-myles-white/\">John Myles White</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<p class=\"syndicated-attribution\"><!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"http://www.johnmyleswhite.com/notebook/2009/11/15/the-top-scores-for-canabalt-take-2/\"> John Myles White: Die Sudelbücher » Statistics</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div></p>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.0--><h3>Introduction</h3>\n<p>As promised on Thursday, here’s my second pass at a statistical analysis of Canabalt scores. There are some useful results I’ll present right at the start, and then there are some results that are more or less worthless, except that working through my own mistakes helped me to think more clearly about statistical modeling in general and about Poisson models for discrete data in particular.</p>\n<h3>Rethinking the Problem</h3>\n<p>First, let’s review the reason why I proposed analyzing the scores in the first place. My goal was to determine whether or not it was reasonable to assume that learned skill really influences your score on Canabalt. If you’ve played for more than ten games in a row, I think it’s intuitively clear that you can warm up and start to do better. Whether or not this holds up to a rigorous analysis is another matter, but I haven’t gotten around to collecting the right data yet.</p>\n<p>The mere fact that you can do better with a little bit of practice suggests that the distribution of scores cannot be entirely attributed to chance variation: there’s at least some level of skill that matters. What’s really interesting, though, is how differently different people can perform, regardless of how long they’ve had to warm up.</p>\n<p>Originally, I thought that it would be useful to see how well a probabilistic model could explain the distribution of scores. I’ll return to my results fitting probabilistic models in the second part of this post, but I now realize that the basic idea behind this approach was deeply flawed. <i>Even if the distribution of scores looks random, it could easily be the case that it’s the distribution of skill levels that’s random, and that, for any given skill level, your score is more or less deterministic.</i> Though that point doesn’t apply here (where we know that there’s variation in the scores that a single player gets), I think the point in general makes clear that the quality of fit of a probabilistic model to empirical data says nothing about the determinism implicit in the process that generates the data. (Which I should have known already, being a fan of Laplace’s demon.)</p>\n<p>Instead, if we want to understand the importance of skill, we should look directly at the difference in average scores between people of seemingly high skill and those of seemingly low skill — that is, we should simply look at the distributions of scores for people with different values of some operationalization of skill level to see how much they differ.</p>\n<p>How do we measure skill level? I tried two naïve approaches, and both gave pretty similar results. The first approach is to consider all of the players who have at least one score in the top 10% of scores. The other approach is to consider all of the players whose median score is in the top 10%. I chose the median rather than the mean, because the mean will be biased by a single very high score, which could make these two measures of skill almost identical.</p>\n<p>Once you have the two groups defined in your data set using these rules, you can simply perform a t-test to compare the performance of these two groups. Both times, I find that the scores are blatantly different across the two groups. The expert users’ mean score is always above 10,000 under both definitions, while the non-expert users’ mean score is around 3,500. <i>In short, I’d say that there’s no way that you can realistically argue that skill doesn’t play a part in determining the score you get.</i></p>\n<p>NB: The R code for doing all of this is at the bottom of this post.</p>\n<h3>Model Fitting</h3>\n<p>During the original conversation with my fellow graduate students about Canabalt that inspired this post and the one before it, I claimed that the distribution of scores looked like a Poisson distribution based on my (now known to be poor) recollection of the shape of the Poisson distribution. So, when I started my analyses, my first idea was to fit a Poisson distribution to my data using a call to the <code>glm</code> function in R:</p>\n<div class=\"wp_codebox\"><table><tr id=\"p3605153\"><td class=\"line_numbers\"><pre>1\n</pre></td><td class=\"code\" id=\"p3605code153\"><pre>glm(scores ~ 1, data = data.set, family = poisson)</pre></td></tr></table></div>\n<p>To convince myself that this was the correct way to fit a constant Poisson model in R, I derived the maximum likelihood estimator for the mean parameter of a Poisson distribution given a set of sample observations by hand. The math deriving the right parameter estimate is outlined at the bottom of this post and can easily be skipped if that sort of thing is opaque or boring to you.</p>\n<p>Once you fit this model, you can simulate data from it and see how much it looks like the original data. To refresh your memory, the original score data looks like this:</p>\n<div style=\"text-align:center;\"><img alt=\"Canabalt Score Distribution.png\" border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i1.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Distribution11.png?w=450&amp;is-pending-load=1\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i1.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Distribution11.png?w=450\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img alt=\"Canabalt Score Distribution.png\" border=\"0\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i1.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Distribution11.png?w=450\"/></noscript></div>\n<p>The results of a Poisson simulation look like this:</p>\n<div style=\"text-align:center;\"><img alt=\"Canabalt Score Poisson Simulation.png\" border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i0.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Poisson-Simulation1.png?w=450&amp;is-pending-load=1\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i0.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Poisson-Simulation1.png?w=450\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img alt=\"Canabalt Score Poisson Simulation.png\" border=\"0\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i0.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Poisson-Simulation1.png?w=450\"/></noscript></div>\n<p>As you can see, this simulation is laughably bad. The Poisson model, lacking a variance parameter, is an absolutely terrible model for the scores from Canabalt. Knowing that I had been basically defeated, I tried a discretized Normal distribution (with a useful independent variance parameter) that was bound to fail because of its symmetry. The results from the Normal distribution look like this:</p>\n<div style=\"text-align:center;\"><img alt=\"Canabalt Score Discretized Normal Simulation.png\" border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Discretized-Normal-Simulation1.png?w=450&amp;is-pending-load=1\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Discretized-Normal-Simulation1.png?w=450\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img alt=\"Canabalt Score Discretized Normal Simulation.png\" border=\"0\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Discretized-Normal-Simulation1.png?w=450\"/></noscript></div>\n<p>This is also pretty terrible: there are actually negative results in the data set,  no skew at all (which is obviously a given with any parameterization of the Normal distribution) and no outliers. The class of other models I could think of trying beyond these two didn’t seem much more promising, so I didn’t continue on. I hope that’s a reflection of my weak knowledge of probability theory more than anything else.</p>\n<h3>A Proper Probabilistic Model?</h3>\n<p>I’d still be interested in seeing whether there is a canonical probability distribution out there that fits the data qualitatively, but I haven’t had any luck searching through lists of distributions online. The desired distribution should:</p>\n<ol>\n<li>Be integer valued.</li>\n<li>Be strictly positive.</li>\n<li>Have heavy tails.</li>\n</ol>\n<p>The third point above makes the Poisson distribution, with its small variance, useless, while the second point makes a discretized Normal, with its problematic symmetry, useless. If you have any solutions, please let me know. (I should note that I’ve considered, but decided against, using an over-dispersed Poisson model because I’m not familiar enough with its analytic properties.)</p>\n<h3>Complete Analysis Code and Results for Across Group Comparisons</h3>\n<p>You can download my data set <a href=\"http://www.johnmyleswhite.com/content/data_sets/canabalt_data.csv\" rel=\"nofollow\" target=\"_blank\">here.</a></p>\n<div class=\"wp_codebox\"><table><tr id=\"p3605154\"><td class=\"line_numbers\"><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n</pre></td><td class=\"code\" id=\"p3605code154\"><pre>data.set &lt;- read.csv('canabalt_data.csv', header = TRUE, sep = ',')\n \ntop.ten.score &lt;- as.numeric(quantile(data.set$score, c(.9)))\n \ntop.users &lt;- unique(data.set$user[data.set$score &gt; top.ten.score])\n \ntop.users.scores &lt;- with(subset(data.set, user %in% top.users), score)\nbottom.users.scores &lt;- with(subset(data.set, !(user %in% top.users)), score)\n \nt.test(bottom.users.scores, top.users.scores)\n \n#\tWelch Two Sample t-test\n#\n#data:  bottom.users.scores and top.users.scores \n#t = -16.7087, df = 196.389, p-value &lt; 2.2e-16\n#alternative hypothesis: true difference in means is not equal to 0 \n#95 percent confidence interval:\n# -7743.866 -6108.843 \n#sample estimates:\n#mean of x mean of y \n# 3443.313 10369.668 \n \nalt.top.users &lt;- c()\n \nfor (user in with(data.set, unique(user)))\n{\n  median.user.score &lt;- median(data.set$score[data.set$user == user])\n  if (median.user.score &gt; top.ten.score)\n  {\n    alt.top.users &lt;- c(alt.top.users, user)\n  }\n}\n \nalt.top.users.scores &lt;- with(subset(data.set, user %in% alt.top.users), score)\nalt.bottom.users.scores &lt;- with(subset(data.set, !(user %in% alt.top.users)), score)\n \nt.test(alt.bottom.users.scores, alt.top.users.scores)\n \n#\tWelch Two Sample t-test\n#\n#data:  alt.bottom.users.scores and alt.top.users.scores \n#t = -16.376, df = 139.206, p-value &lt; 2.2e-16\n#alternative hypothesis: true difference in means is not equal to 0 \n#95 percent confidence interval:\n# -9130.290 -7163.108 \n#sample estimates:\n#mean of x mean of y \n# 3586.801 11733.500</pre></td></tr></table></div>\n<h3>Derivation of the MLE Analytic Formula for the Poisson Distribution</h3>\n<div style=\"text-align:center;\"><img alt=\"Poisson_Small.jpg\" border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Poisson_Small1.jpg?resize=309%2C472&amp;is-pending-load=1\" data-recalc-dims=\"1\" height=\"472\" loading=\"lazy\" src=\"https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Poisson_Small1.jpg?resize=309%2C472\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" width=\"309\"/><noscript><img alt=\"Poisson_Small.jpg\" border=\"0\" data-recalc-dims=\"1\" height=\"472\" loading=\"lazy\" src=\"https://i2.wp.com/www.r-bloggers.com/wp-content/uploads/2009/11/Poisson_Small1.jpg?resize=309%2C472\" width=\"309\"/></noscript></div>\n\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.0-->\n<p class=\"syndicated-attribution\"><div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"http://www.johnmyleswhite.com/notebook/2009/11/15/the-top-scores-for-canabalt-take-2/\"> John Myles White: Die Sudelbücher » Statistics</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div></p> </div>\n<div class=\"post-tags clearfix\"><ul><li class=\"round-corners\"><a href=\"https://www.r-bloggers.com/tag/statistics/\" rel=\"tag\">statistics</a></li></ul></div></article>",
      "main_text": "The Top Scores for Canabalt, Take 2\nPosted on\nNovember 15, 2009\nby\nJohn Myles White\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nJohn Myles White: Die Sudelbücher » Statistics\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nIntroduction\nAs promised on Thursday, here’s my second pass at a statistical analysis of Canabalt scores. There are some useful results I’ll present right at the start, and then there are some results that are more or less worthless, except that working through my own mistakes helped me to think more clearly about statistical modeling in general and about Poisson models for discrete data in particular.\nRethinking the Problem\nFirst, let’s review the reason why I proposed analyzing the scores in the first place. My goal was to determine whether or not it was reasonable to assume that learned skill really influences your score on Canabalt. If you’ve played for more than ten games in a row, I think it’s intuitively clear that you can warm up and start to do better. Whether or not this holds up to a rigorous analysis is another matter, but I haven’t gotten around to collecting the right data yet.\nThe mere fact that you can do better with a little bit of practice suggests that the distribution of scores cannot be entirely attributed to chance variation: there’s at least some level of skill that matters. What’s really interesting, though, is how differently different people can perform, regardless of how long they’ve had to warm up.\nOriginally, I thought that it would be useful to see how well a probabilistic model could explain the distribution of scores. I’ll return to my results fitting probabilistic models in the second part of this post, but I now realize that the basic idea behind this approach was deeply flawed.\nEven if the distribution of scores looks random, it could easily be the case that it’s the distribution of skill levels that’s random, and that, for any given skill level, your score is more or less deterministic.\nThough that point doesn’t apply here (where we know that there’s variation in the scores that a single player gets), I think the point in general makes clear that the quality of fit of a probabilistic model to empirical data says nothing about the determinism implicit in the process that generates the data. (Which I should have known already, being a fan of Laplace’s demon.)\nInstead, if we want to understand the importance of skill, we should look directly at the difference in average scores between people of seemingly high skill and those of seemingly low skill — that is, we should simply look at the distributions of scores for people with different values of some operationalization of skill level to see how much they differ.\nHow do we measure skill level? I tried two naïve approaches, and both gave pretty similar results. The first approach is to consider all of the players who have at least one score in the top 10% of scores. The other approach is to consider all of the players whose median score is in the top 10%. I chose the median rather than the mean, because the mean will be biased by a single very high score, which could make these two measures of skill almost identical.\nOnce you have the two groups defined in your data set using these rules, you can simply perform a t-test to compare the performance of these two groups. Both times, I find that the scores are blatantly different across the two groups. The expert users’ mean score is always above 10,000 under both definitions, while the non-expert users’ mean score is around 3,500.\nIn short, I’d say that there’s no way that you can realistically argue that skill doesn’t play a part in determining the score you get.\nNB: The R code for doing all of this is at the bottom of this post.\nModel Fitting\nDuring the original conversation with my fellow graduate students about Canabalt that inspired this post and the one before it, I claimed that the distribution of scores looked like a Poisson distribution based on my (now known to be poor) recollection of the shape of the Poisson distribution. So, when I started my analyses, my first idea was to fit a Poisson distribution to my data using a call to the\nglm\nfunction in R:\n1\nglm(scores ~ 1, data = data.set, family = poisson)\nTo convince myself that this was the correct way to fit a constant Poisson model in R, I derived the maximum likelihood estimator for the mean parameter of a Poisson distribution given a set of sample observations by hand. The math deriving the right parameter estimate is outlined at the bottom of this post and can easily be skipped if that sort of thing is opaque or boring to you.\nOnce you fit this model, you can simulate data from it and see how much it looks like the original data. To refresh your memory, the original score data looks like this:\nThe results of a Poisson simulation look like this:\nAs you can see, this simulation is laughably bad. The Poisson model, lacking a variance parameter, is an absolutely terrible model for the scores from Canabalt. Knowing that I had been basically defeated, I tried a discretized Normal distribution (with a useful independent variance parameter) that was bound to fail because of its symmetry. The results from the Normal distribution look like this:\nThis is also pretty terrible: there are actually negative results in the data set,  no skew at all (which is obviously a given with any parameterization of the Normal distribution) and no outliers. The class of other models I could think of trying beyond these two didn’t seem much more promising, so I didn’t continue on. I hope that’s a reflection of my weak knowledge of probability theory more than anything else.\nA Proper Probabilistic Model?\nI’d still be interested in seeing whether there is a canonical probability distribution out there that fits the data qualitatively, but I haven’t had any luck searching through lists of distributions online. The desired distribution should:\nBe integer valued.\nBe strictly positive.\nHave heavy tails.\nThe third point above makes the Poisson distribution, with its small variance, useless, while the second point makes a discretized Normal, with its problematic symmetry, useless. If you have any solutions, please let me know. (I should note that I’ve considered, but decided against, using an over-dispersed Poisson model because I’m not familiar enough with its analytic properties.)\nComplete Analysis Code and Results for Across Group Comparisons\nYou can download my data set\nhere.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\ndata.set <- read.csv('canabalt_data.csv', header = TRUE, sep = ',')\n \ntop.ten.score <- as.numeric(quantile(data.set$score, c(.9)))\n \ntop.users <- unique(data.set$user[data.set$score > top.ten.score])\n \ntop.users.scores <- with(subset(data.set, user %in% top.users), score)\nbottom.users.scores <- with(subset(data.set, !(user %in% top.users)), score)\n \nt.test(bottom.users.scores, top.users.scores)\n \n#\tWelch Two Sample t-test\n#\n#data:  bottom.users.scores and top.users.scores \n#t = -16.7087, df = 196.389, p-value < 2.2e-16\n#alternative hypothesis: true difference in means is not equal to 0 \n#95 percent confidence interval:\n# -7743.866 -6108.843 \n#sample estimates:\n#mean of x mean of y \n# 3443.313 10369.668 \n \nalt.top.users <- c()\n \nfor (user in with(data.set, unique(user)))\n{\n  median.user.score <- median(data.set$score[data.set$user == user])\n  if (median.user.score > top.ten.score)\n  {\n    alt.top.users <- c(alt.top.users, user)\n  }\n}\n \nalt.top.users.scores <- with(subset(data.set, user %in% alt.top.users), score)\nalt.bottom.users.scores <- with(subset(data.set, !(user %in% alt.top.users)), score)\n \nt.test(alt.bottom.users.scores, alt.top.users.scores)\n \n#\tWelch Two Sample t-test\n#\n#data:  alt.bottom.users.scores and alt.top.users.scores \n#t = -16.376, df = 139.206, p-value < 2.2e-16\n#alternative hypothesis: true difference in means is not equal to 0 \n#95 percent confidence interval:\n# -9130.290 -7163.108 \n#sample estimates:\n#mean of x mean of y \n# 3586.801 11733.500\nDerivation of the MLE Analytic Formula for the Poisson Distribution\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nJohn Myles White: Die Sudelbücher » Statistics\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nstatistics",
      "meta_description": "Introduction As promised on Thursday, here’s my second pass at a statistical analysis of Canabalt scores. There are some useful results I’ll present right at the start, and then there are some results that are more or less worthless, except that working through my own mistakes helped me to think more clearly about statistical modeling in [...]",
      "meta_keywords": "statistics",
      "og_description": "Introduction As promised on Thursday, here’s my second pass at a statistical analysis of Canabalt scores. There are some useful results I’ll present right at the start, and then there are some results that are more or less worthless, except that working through my own mistakes helped me to think more clearly about statistical modeling in [...]",
      "og_image": "https://www.r-bloggers.com/wp-content/uploads/2009/11/Canabalt-Score-Distribution11.png",
      "og_title": "The Top Scores for Canabalt, Take 2 | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 7.8,
      "sitemap_lastmod": "2011-05-11T21:26:24+00:00",
      "twitter_description": "Introduction As promised on Thursday, here’s my second pass at a statistical analysis of Canabalt scores. There are some useful results I’ll present right at the start, and then there are some results that are more or less worthless, except that working through my own mistakes helped me to think more clearly about statistical modeling in [...]",
      "twitter_title": "The Top Scores for Canabalt, Take 2 | R-bloggers",
      "url": "https://www.r-bloggers.com/2009/11/the-top-scores-for-canabalt-take-2/",
      "word_count": 1561
    }
  }
}