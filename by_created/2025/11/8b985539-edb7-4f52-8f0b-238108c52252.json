{
  "uuid": "8b985539-edb7-4f52-8f0b-238108c52252",
  "created_at": "2025-11-22 19:58:37",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/05/tuning/",
    "crawled_at": "2025-11-22T10:49:02.939815",
    "external_links": [
      {
        "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-tuning/",
        "text": "mlr-org"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://github.com/mlr-org/mlr3website/",
        "text": "GitHub"
      },
      {
        "href": "https://mlr3mbo.mlr-org.com/",
        "text": "mlr3mbo"
      },
      {
        "href": "https://mlr3hyperband.mlr-org.com/",
        "text": "mlr3hyperband"
      },
      {
        "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-tuning/",
        "text": "mlr-org"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Tuning | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAANBAMAAACJLlk1AAAAKlBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADmU0mKAAAADXRSTlMAMrsQRO+Jq81mdlTdgIQlWAAAAAlwSFlzAAAOxAAADsQBlSsOGwAAAEZJREFUCB1jYNQ1YAACVxDBkA4mO8HkRRDJcoFxFgMDm4KgKAMDR6KDNQMDrwpI3FdMAUgmMRxhZGDoZDjIzcBwmGHXBgYAM30KYRgI//EAAAAASUVORK5CYII=",
        "src": "https://latex.codecogs.com/png.latex?k"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/giuseppe-casalicchio/",
        "text": "Giuseppe Casalicchio"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-392907 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Tuning</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 13, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/giuseppe-casalicchio/\">Giuseppe Casalicchio</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-tuning/\"> mlr-org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n<strong>JavaScript is required to unlock solutions.</strong><br/>\n    Please enable JavaScript and reload the page,<br/>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" rel=\"nofollow\" target=\"_blank\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n<section class=\"level1\" id=\"goal\">\n<h1>Goal</h1>\n<p>After this exercise, you should be able to define search spaces for learning algorithms and apply different hyperparameter (HP) optimization (HPO) techniques to search through the search space to find a well-performing hyperparameter configuration (HPC).</p>\n</section>\n<section class=\"level1\" id=\"exercises\">\n<h1>Exercises</h1>\n<p>Again, we are looking at the <code>german_credit</code> data set and corresponding task (you can quickly load the task with <code>tsk(\"german_credit\")</code>). We want to train a k-NN model but ask ourselves what the best choice of <img data-lazy-src=\"https://latex.codecogs.com/png.latex?k\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?k\"/></noscript> might be? Furthermore, we are not sure how to set other HPs of the learner, e.g., if we should scale the data or not. In this exercise, we conduct HPO for k-NN to automatically find a good HPC.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3verse)\ntask = tsk(\"german_credit\")</pre>\n</div>\n<details>\n<summary>\n<strong>Recap: k-NN</strong>\n</summary>\nk-NN is a machine learning method that predicts new data by averaging over the responses of the k nearest neighbors.\n</details>\n<section class=\"level2\" id=\"parameter-spaces\">\n<h2 class=\"anchored\" data-anchor-id=\"parameter-spaces\">Parameter spaces</h2>\n<p>Define a meaningful search space for the HPs <code>k</code> and <code>scale</code>. You can checkout the help page <code>lrn(\"classif.kknn\")$help()</code> for an overview of the k-NN learner.</p>\n<details>\n<summary>\n<strong>Hint 1</strong>\n</summary>\nEach learner has a slot <code>param_set</code> that contains all HPs that can be used for the tuning. In this use case we tune a learner with the key <code>\"classif.kknn\"</code>. The functions to define the search space are <code>ps</code> and <code>p_int</code>, <code>p_dbl</code>, <code>p_fct</code>, or <code>p_lgl</code> for HPs in the search space.\n</details>\n<details>\n<summary>\n<strong>Hint 2</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3tuning)\n\nsearch_space = ps(\n  k = p_int(...),\n  scale = ...\n)</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-1\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-1-contents callout-collapse collapse\" id=\"callout-1\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-e7b694bb0b2ac7db2c0b5f29-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5saWJyYXJ5PC9zcGFuPihtbHIzdHVuaW5nKTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS0yIj48YSBocmVmPSIjY2IxLTIiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT48L3NwYW4+CjxzcGFuIGlkPSJjYjEtMyI+PGEgaHJlZj0iI2NiMS0zIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+c2VhcmNoX3NwYWNlIDxzcGFuIGNsYXNzPSJvdCI+PTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImZ1Ij5wczwvc3Bhbj4oPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTQiPjxhIGhyZWY9IiNjYjEtNCIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPiAgPHNwYW4gY2xhc3M9ImF0Ij5rID08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+cF9pbnQ8L3NwYW4+KDxzcGFuIGNsYXNzPSJkdiI+MTwvc3Bhbj4sIDxzcGFuIGNsYXNzPSJkdiI+MTAwPC9zcGFuPiksPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTUiPjxhIGhyZWY9IiNjYjEtNSIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPiAgPHNwYW4gY2xhc3M9ImF0Ij5zY2FsZSA9PC9zcGFuPiA8c3BhbiBjbGFzcz0iZnUiPnBfbGdsPC9zcGFuPigpPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTYiPjxhIGhyZWY9IiNjYjEtNiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPik8L3NwYW4+PC9jb2RlPjwvcHJlPjwvZGl2Pgo8L2Rpdj4=\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"hyperparameter-optimization\">\n<h2 class=\"anchored\" data-anchor-id=\"hyperparameter-optimization\">Hyperparameter optimization</h2>\n<p>Now, we want to tune the k-NN model with the search space from the previous exercise. As resampling strategy we use a 3 fold cross validation. The tuning strategy should be a random search. As termination criteria we choose 40 evaluations.</p>\n<details>\n<summary>\n<strong>Hint 1</strong>\n</summary>\n<p>The elements required for the tuning are:</p>\n<ul>\n<li>Task: German credit\n<ul>\n<li>Algorithm: k-NN algorithm from <code>lrn()</code></li>\n<li>Resampling: 3-fold cross validation using <code>rsmp()</code></li>\n<li>Terminator: 40 evaluations using <code>trm()</code></li>\n<li>Search space: See previous exercise</li>\n<li>We use the default performance measure (<code>msr(\"classif.ce\")</code> for classification and <code>msr(\"classif.mse\")</code> for regression)</li>\n</ul>\nThe tuning instance is then defined by calling <code>ti()</code>. The random search optimization algorithm is obtained from <code>tnr()</code> with the corresponding key as argument. Furthermore, we allow parallel computations and set the batch size as well as the number of cores to four.</li>\n</ul>\n</details>\n<details>\n<summary>\n<strong>Hint 2</strong>\n</summary>\n<p>The optimization algorithm is obtained from <code>tnr()</code> with the corresponding key as argument. Furthermore we allow parallel computations using four cores:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n\nfuture::plan(\"multicore\", workers = 4L)\n\ntask = tsk(...)\nlrn_knn = lrn(...)\n\nsearch_space = ps(\n  k = p_int(1, 100),\n  scale = p_lgl()\n)\nresampling = rsmp(...)\n\nterminator = trm(..., ... = 40L)\n\ninstance = ti(\n  task = ...,\n  learner = ...,\n  resampling = ...,\n  terminator = ...,\n  search_space = ...\n)\n\noptimizer = tnr(...)\noptimizer$...(...)</pre>\n</div>\nFinally, the optimization is started by passing the tuning instance to the <code>$optimize()</code> method of the tuner.\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-2\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-2-contents callout-collapse collapse\" id=\"callout-2\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-e7b694bb0b2ac7db2c0b5f29-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3learners)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3tuning)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kknn)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>future<span class="sc">::</span><span class="fu">plan</span>(<span class="st">&quot;multicore&quot;</span>, <span class="at">workers =</span> <span class="dv">4</span><span class="dt">L</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">&quot;german_credit&quot;</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>lrn_knn <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;classif.kknn&quot;</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>search_space <span class="ot">=</span> <span class="fu">ps</span>(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">k =</span> <span class="fu">p_int</span>(<span class="dv">1</span>, <span class="dv">100</span>),</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">scale =</span> <span class="fu">p_lgl</span>()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">3</span><span class="dt">L</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>terminator <span class="ot">=</span> <span class="fu">trm</span>(<span class="st">&quot;evals&quot;</span>, <span class="at">n_evals =</span> <span class="dv">40</span><span class="dt">L</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> task,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_knn,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> resampling,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> terminator,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">search_space =</span> search_space</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">&quot;random_search&quot;</span>, <span class="at">batch_size =</span> <span class="dv">4</span><span class="dt">L</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>optimizer<span class="sc">$</span><span class="fu">optimize</span>(instance)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INFO  [08:37:05.480] [bbotk] Starting to optimize 2 parameter(s) with &#39;&lt;OptimizerBatchRandomSearch&gt;&#39; and &#39;&lt;TerminatorEvals&gt; [n_evals=40, k=0]&#39;
INFO  [08:37:05.498] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:05.503] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:05.515] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.537] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.556] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.579] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.592] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.604] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.619] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.633] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.647] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.661] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.676] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.688] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.701] [mlr3] Finished benchmark
INFO  [08:37:05.720] [bbotk] Result of batch 1:
INFO  [08:37:05.722] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:05.722] [bbotk]  15  TRUE  0.2690145        0      0            0.053 194d7f18-d03b-427d-81b1-4871a7341fbf
INFO  [08:37:05.722] [bbotk]  18 FALSE  0.3279927        0      0            0.030 ff9f5509-11e1-4356-8d38-d9ce324e90e0
INFO  [08:37:05.722] [bbotk]  34 FALSE  0.3220106        0      0            0.032 dd397f22-7b1b-4950-8cb6-ab6cb60dc114
INFO  [08:37:05.722] [bbotk]  19 FALSE  0.3279957        0      0            0.029 6e4fc6a9-d4a5-47d2-abc2-28497de394d3
INFO  [08:37:05.725] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:05.727] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:05.729] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.742] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.752] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.762] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.787] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.810] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.836] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.851] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.867] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.885] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.906] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:05.928] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:05.948] [mlr3] Finished benchmark
INFO  [08:37:05.969] [bbotk] Result of batch 2:
INFO  [08:37:05.970] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:05.970] [bbotk]   2 FALSE  0.3789898        0      0            0.021 828e118e-7e36-48e5-86ca-2fbb7b4fdd19
INFO  [08:37:05.970] [bbotk]  62  TRUE  0.2810355        0      0            0.062 b38d9b38-5706-42e5-931e-019221df8cf0
INFO  [08:37:05.970] [bbotk]  47 FALSE  0.3150156        0      0            0.038 85b8ee55-6924-4143-8f93-2d4b9b278147
INFO  [08:37:05.970] [bbotk]  36  TRUE  0.2680315        0      0            0.052 3912efd5-12f4-47d5-9950-ed2910931552
INFO  [08:37:05.973] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:05.975] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:05.977] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:05.994] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.020] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.036] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.055] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.077] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.096] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.121] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.149] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.174] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.193] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.214] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.232] [mlr3] Finished benchmark
INFO  [08:37:06.251] [bbotk] Result of batch 3:
INFO  [08:37:06.252] [bbotk]    k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:06.252] [bbotk]    7  TRUE  0.2930086        0      0            0.049 50887057-53b6-434a-b24e-3da7018935be
INFO  [08:37:06.252] [bbotk]   24  TRUE  0.2710285        0      0            0.050 c78f3a13-b69a-4695-b86f-516b84d1492c
INFO  [08:37:06.252] [bbotk]  100  TRUE  0.2870325        0      0            0.067 7457bb02-5572-4627-a519-80d75583b3ee
INFO  [08:37:06.252] [bbotk]   68 FALSE  0.3030156        0      0            0.048 eaa4d9b7-8b20-4b43-808e-27feebcaeb86
INFO  [08:37:06.254] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:06.259] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:06.262] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.279] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.296] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.316] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.335] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.358] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.377] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.397] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.419] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.438] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.550] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.563] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.576] [mlr3] Finished benchmark
INFO  [08:37:06.595] [bbotk] Result of batch 4:
INFO  [08:37:06.595] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:06.595] [bbotk]   6  TRUE  0.2930086        0      0            0.045 f4124aa0-3f6a-4724-b4ef-23f375cf000d
INFO  [08:37:06.595] [bbotk]  24  TRUE  0.2710285        0      0            0.050 1a988904-b489-4bb1-812d-edbd69943ba6
INFO  [08:37:06.595] [bbotk]  24  TRUE  0.2710285        0      0            0.051 ea40f74b-a2d9-4820-82df-90ca797d1f56
INFO  [08:37:06.595] [bbotk]  24 FALSE  0.3170176        0      0            0.126 de9203d5-6ebc-4dd0-99a2-829427bc0c3b
INFO  [08:37:06.598] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:06.600] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:06.603] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.614] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.625] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.637] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.665] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.690] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.714] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.740] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.764] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.787] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.807] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.830] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.850] [mlr3] Finished benchmark
INFO  [08:37:06.868] [bbotk] Result of batch 5:
INFO  [08:37:06.869] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:06.869] [bbotk]   9 FALSE  0.3429927        0      0            0.025 d7fb035b-f8f5-4c4e-b7a8-899c62dc9b90
INFO  [08:37:06.869] [bbotk]  95  TRUE  0.2860315        0      0            0.067 00d05971-e9af-4879-9083-9c1cf9c9e4dc
INFO  [08:37:06.869] [bbotk]  73  TRUE  0.2800345        0      0            0.063 4bad2fbc-fd82-4578-a1b9-4f79a41c386a
INFO  [08:37:06.869] [bbotk]  93 FALSE  0.3030156        0      0            0.052 94ddd018-e085-499a-8320-b7176d5e2eed
INFO  [08:37:06.872] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:06.874] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:06.876] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.903] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.927] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.950] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:06.967] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:06.982] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:06.997] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.021] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.048] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.072] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.097] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.119] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.142] [mlr3] Finished benchmark
INFO  [08:37:07.162] [bbotk] Result of batch 6:
INFO  [08:37:07.163] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:07.163] [bbotk]  78  TRUE  0.2830375        0      0            0.064 1c7663c2-b733-412a-af72-a75110468a08
INFO  [08:37:07.163] [bbotk]  34 FALSE  0.3220106        0      0            0.037 65e4a779-0084-4a2b-aab3-0827cc106014
INFO  [08:37:07.163] [bbotk]  86  TRUE  0.2840325        0      0            0.063 4ad58e75-20fb-4b16-935e-24978829eb0b
INFO  [08:37:07.163] [bbotk]  59  TRUE  0.2820335        0      0            0.060 6434dafc-a93d-43eb-809f-243644a56dae
INFO  [08:37:07.166] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:07.168] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:07.170] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.190] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.212] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.231] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.246] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.263] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.278] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.305] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.329] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.355] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.375] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.401] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.421] [mlr3] Finished benchmark
INFO  [08:37:07.440] [bbotk] Result of batch 7:
INFO  [08:37:07.440] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:07.440] [bbotk]  93 FALSE  0.3030156        0      0            0.051 bc0fea7b-e390-4242-acef-9694b892f6bb
INFO  [08:37:07.440] [bbotk]  44 FALSE  0.3130136        0      0            0.034 01217a5c-e673-488b-ac21-1fb5f8351450
INFO  [08:37:07.440] [bbotk]  87  TRUE  0.2840325        0      0            0.068 303be45d-8ed5-425c-9bf4-15c47c4a4e37
INFO  [08:37:07.440] [bbotk]  31  TRUE  0.2600295        0      0            0.055 8c9cc31e-3693-4486-a1fc-aa38c5c12a17
INFO  [08:37:07.443] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:07.445] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:07.447] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.470] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.493] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.520] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.541] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.562] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.583] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.598] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.609] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.621] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.638] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.655] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.676] [mlr3] Finished benchmark
INFO  [08:37:07.694] [bbotk] Result of batch 8:
INFO  [08:37:07.695] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:07.695] [bbotk]  66  TRUE  0.2780325        0      0            0.061 6f813c04-dabc-4f7a-886a-6f37a052e156
INFO  [08:37:07.695] [bbotk]  52  TRUE  0.2840295        0      0            0.053 9ee033ae-7f61-458e-a8a1-07c91bc93050
INFO  [08:37:07.695] [bbotk]  15 FALSE  0.3279867        0      0            0.027 aad91229-9a39-4c6a-ae50-7fd3b520215c
INFO  [08:37:07.695] [bbotk]  62 FALSE  0.3010196        0      0            0.046 cf59574e-2ae9-4962-a148-6f270b9bb477
INFO  [08:37:07.697] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:07.699] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:07.701] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.717] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.732] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.751] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.770] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.790] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.812] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.829] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.846] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.863] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.884] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.902] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:07.921] [mlr3] Finished benchmark
INFO  [08:37:07.942] [bbotk] Result of batch 9:
INFO  [08:37:07.943] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:07.943] [bbotk]  51 FALSE  0.3100166        0      0            0.040 1528f3f1-b00a-43ef-8794-47da8b0d2f4a
INFO  [08:37:07.943] [bbotk]  28  TRUE  0.2590285        0      0            0.050 7c4bdf00-533e-46c7-bff2-0a914854c4f3
INFO  [08:37:07.943] [bbotk]  60 FALSE  0.3010196        0      0            0.042 0f6aa967-1452-440f-8f71-06efb9a4728f
INFO  [08:37:07.943] [bbotk]  74 FALSE  0.3030156        0      0            0.049 70cb050f-4fa5-46ec-9510-21055fdaae27
INFO  [08:37:07.945] [bbotk] Evaluating 4 configuration(s)
INFO  [08:37:07.947] [mlr3] Running benchmark with 12 resampling iterations
INFO  [08:37:07.949] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:07.968] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:07.988] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:08.009] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:08.027] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:08.050] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:08.068] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:08.093] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:08.118] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:08.147] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 1/3)
INFO  [08:37:08.166] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 2/3)
INFO  [08:37:08.185] [mlr3] Applying learner &#39;classif.kknn&#39; on task &#39;german_credit&#39; (iter 3/3)
INFO  [08:37:08.203] [mlr3] Finished benchmark
INFO  [08:37:08.222] [bbotk] Result of batch 10:
INFO  [08:37:08.223] [bbotk]   k scale classif.ce warnings errors runtime_learners                                uhash
INFO  [08:37:08.223] [bbotk]  84 FALSE  0.3030156        0      0            0.047 d956ba30-738e-41c9-a459-dbe9325fb781
INFO  [08:37:08.223] [bbotk]  12  TRUE  0.2780145        0      0            0.049 870165ae-bff5-47d1-868a-70545961d868
INFO  [08:37:08.223] [bbotk]  96  TRUE  0.2860315        0      0            0.070 f80ae578-3fae-4470-95ac-71e42d8a05a5
INFO  [08:37:08.223] [bbotk]  75 FALSE  0.3030156        0      0            0.045 b5394957-24b1-40f9-bb63-eda7c105a85b
INFO  [08:37:08.230] [bbotk] Finished optimizing after 40 evaluation(s)
INFO  [08:37:08.230] [bbotk] Result:
INFO  [08:37:08.231] [bbotk]      k  scale learner_param_vals  x_domain classif.ce
INFO  [08:37:08.231] [bbotk]  &lt;int&gt; &lt;lgcl&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;
INFO  [08:37:08.231] [bbotk]     28   TRUE          &lt;list[2]&gt; &lt;list[2]&gt;  0.2590285</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       k  scale learner_param_vals  x_domain classif.ce
   &lt;int&gt; &lt;lgcl&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;
1:    28   TRUE          &lt;list[2]&gt; &lt;list[2]&gt;  0.2590285</code></pre>
</div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result_y</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
 0.2590285 </code></pre>
</div>
<div class="sourceCode" id="cb6"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       k  scale learner_param_vals  x_domain classif.ce
   &lt;int&gt; &lt;lgcl&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;
1:    28   TRUE          &lt;list[2]&gt; &lt;list[2]&gt;  0.2590285</code></pre>
</div>
</div>
<p><strong>Syntactic sugar to define the HP space</strong></p>
<p><code>mlr3</code> provides syntactic sugar to shorten the process of
search space definition. To do so, it is possible to directly specify
the HP range in the learner construction:</p>
<details>
<summary>
<strong>Click me</strong>
</summary>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3learners)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kknn)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">&quot;german_credit&quot;</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>lrn_knn <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;classif.kknn&quot;</span>, <span class="at">k =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">100</span>), <span class="at">scale =</span> <span class="fu">to_tune</span>())</span></code></pre></div>
</div>
This adjust the parameter set (<code>lrn_knn$param_set</code>) attached
to the learner and flags it as “tunable”.
</details>\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"analyzing-the-tuning-archive\">\n<h2 class=\"anchored\" data-anchor-id=\"analyzing-the-tuning-archive\">Analyzing the tuning archive</h2>\n<p>Inspect the archive of hyperparameters evaluated during the tuning process with <code>instance$archive</code>. Create a simple plot with the goal of illustrating the association between the hyperparametere <code>k</code> and the estimated classification error.</p>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-3\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-3-contents callout-collapse collapse\" id=\"callout-3\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-e7b694bb0b2ac7db2c0b5f29-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5wbG90PC9zcGFuPig8c3BhbiBjbGFzcz0iYXQiPnggPTwvc3Bhbj4gaW5zdGFuY2U8c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+YXJjaGl2ZTxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5kYXRhPHNwYW4gY2xhc3M9InNjIj4kPC9zcGFuPmssIDxzcGFuIGNsYXNzPSJhdCI+eSA9PC9zcGFuPiBpbnN0YW5jZTxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5hcmNoaXZlPHNwYW4gY2xhc3M9InNjIj4kPC9zcGFuPmRhdGE8c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+Y2xhc3NpZi5jZSk8L3NwYW4+PC9jb2RlPjwvcHJlPjwvZGl2Pgo8ZGl2IGNsYXNzPSJjZWxsLW91dHB1dC1kaXNwbGF5Ij4KPHA+PGltZyBzcmM9ImluZGV4X2ZpbGVzL2ZpZ3VyZS1odG1sL3VubmFtZWQtY2h1bmstMTAtMS5wbmciCmRhdGEtZmlnLWFsaWduPSJjZW50ZXIiIHdpZHRoPSI2NzIiIC8+PC9wPgo8L2Rpdj4KPC9kaXY+\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"visualizing-hyperparameters\">\n<h2 class=\"anchored\" data-anchor-id=\"visualizing-hyperparameters\">Visualizing hyperparameters</h2>\n<p>To see how effective the tuning was, it is useful to look at the effect of the HPs on the performance. It also helps us to understand how important different HPs are. Therefore, access the archive of the tuning instance and visualize the effect.</p>\n<details>\n<summary>\n<strong>Hint 1</strong>\n</summary>\nAccess the <code>archive</code> of the tuning instance to get all information about the tuning. You can use all known plotting techniques after transforming it to a <code>data.table</code>.\n</details>\n<details>\n<summary>\n<strong>Hint 2</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>arx = as...(instance$...)\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\ngg_k = ggplot(..., aes(...)) + ...()\ngg_scale = ggplot(..., aes(...)) + ...()\n\ngg_k + gg_scale &amp; theme(legend.position = \"bottom\")</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-4\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-4-contents callout-collapse collapse\" id=\"callout-4\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-e7b694bb0b2ac7db2c0b5f29-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"hyperparameter-dependencies\">\n<h2 class=\"anchored\" data-anchor-id=\"hyperparameter-dependencies\">Hyperparameter dependencies</h2>\n<p>When defining a hyperparameter search space via the <code>ps()</code> function, we sometimes encounter nested search spaces, also called hyperparameter dependencies. One example for this are SVMs. Here, the hyperparameter <code>degree</code> is only relevant if the hyperparameter <code>kernel</code> is set to <code>\"polynomial\"</code>. Therefore, we only have to consider different configurations for <code>degree</code> if we evaluate candidate configurations with polynomial kernel. Construct a search space for a SVM with hyperparameters <code>kernel</code> (candidates should be <code>\"polynomial\"</code> and <code>\"radial\"</code>) and <code>degree</code> (integer ranging from 1 to 3, but only for polynomial kernels), and account for the dependency structure.</p>\n<details>\n<summary>\n<strong>Hint 1</strong>\n</summary>\nIn the <code>p_fct</code>, <code>p_dbl</code>,  functions, we specify this using the <code>depends</code> argument, which takes a named argument of the form <code>&lt;param&gt; == value</code> or <code>&lt;param&gt; %in% &lt;vector&gt;</code>.\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-5\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-5-contents callout-collapse collapse\" id=\"callout-5\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-e7b694bb0b2ac7db2c0b5f29-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PHNwYW4gY2xhc3M9ImZ1Ij5wczwvc3Bhbj4oPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTIiPjxhIGhyZWY9IiNjYjEtMiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPiAgPHNwYW4gY2xhc3M9ImF0Ij5rZXJuZWwgPTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImZ1Ij5wX2ZjdDwvc3Bhbj4oPHNwYW4gY2xhc3M9ImZ1Ij5jPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O3BvbHlub21pYWwmcXVvdDs8L3NwYW4+LCA8c3BhbiBjbGFzcz0ic3QiPiZxdW90O3JhZGlhbCZxdW90Ozwvc3Bhbj4pKSw8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMyI+PGEgaHJlZj0iI2NiMS0zIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+ICA8c3BhbiBjbGFzcz0iYXQiPmRlZ3JlZSA9PC9zcGFuPiA8c3BhbiBjbGFzcz0iZnUiPnBfaW50PC9zcGFuPig8c3BhbiBjbGFzcz0iZHYiPjE8L3NwYW4+LCA8c3BhbiBjbGFzcz0iZHYiPjM8L3NwYW4+LCA8c3BhbiBjbGFzcz0iYXQiPmRlcGVuZHMgPTwvc3Bhbj4gKGtlcm5lbCA8c3BhbiBjbGFzcz0ic2MiPj09PC9zcGFuPiA8c3BhbiBjbGFzcz0ic3QiPiZxdW90O3BvbHlub21pYWwmcXVvdDs8L3NwYW4+KSk8L3NwYW4+CjxzcGFuIGlkPSJjYjEtNCI+PGEgaHJlZj0iI2NiMS00IiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+KTwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0IGNlbGwtb3V0cHV0LXN0ZG91dCI+CjxwcmU+PGNvZGU+Jmx0O1BhcmFtU2V0KDIpJmd0OwpLZXk6ICZsdDtpZCZndDsKICAgICAgIGlkICAgIGNsYXNzIGxvd2VyIHVwcGVyIG5sZXZlbHMgICAgICAgIGRlZmF1bHQgcGFyZW50cyAgdmFsdWUKICAgJmx0O2NoYXImZ3Q7ICAgJmx0O2NoYXImZ3Q7ICZsdDtudW0mZ3Q7ICZsdDtudW0mZ3Q7ICAgJmx0O251bSZndDsgICAgICAgICAmbHQ7bGlzdCZndDsgICZsdDtsaXN0Jmd0OyAmbHQ7bGlzdCZndDsKMTogZGVncmVlIFBhcmFtSW50ICAgICAxICAgICAzICAgICAgIDMgJmx0O05vRGVmYXVsdFswXSZndDsgIGtlcm5lbCBbTlVMTF0KMjoga2VybmVsIFBhcmFtRmN0ICAgIE5BICAgIE5BICAgICAgIDIgJmx0O05vRGVmYXVsdFswXSZndDsgIFtOVUxMXSBbTlVMTF08L2NvZGU+PC9wcmU+CjwvZGl2Pgo8L2Rpdj4=\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"hyperparameter-transformations\">\n<h2 class=\"anchored\" data-anchor-id=\"hyperparameter-transformations\">Hyperparameter transformations</h2>\n<p>When tuning non-negative hyperparameters with a broad range, using a logarithmic scale can be more efficient. This approach works especially well if we want to test many small values, but also a few very large ones. By selecting values on a logarithmic scale and then exponentiating them, we ensure a concentrated exploration of smaller values while still considering the possibility of very large values, allowing for a targeted and efficient search in finding optimal hyperparameter configurations.</p>\n<p>A simple way to do this is to pass <code>logscale = TRUE</code> when using <code>to_tune()</code> to define the parameter search space while constructing the learner:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>lrn = lrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE))\nlrn$param_set$search_space()</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>&lt;ParamSet(1)&gt;\n       id    class     lower    upper nlevels        default  value\n   &lt;char&gt;   &lt;char&gt;     &lt;num&gt;    &lt;num&gt;   &lt;num&gt;         &lt;list&gt; &lt;list&gt;\n1:   cost ParamDbl -11.51293 11.51293     Inf &lt;NoDefault[0]&gt; [NULL]\nTrafo is set.</pre>\n</div>\n</div>\n<p>To manually create the same transformation, we can pass the transformation to the more general <code>trafo</code> argument in <code>p_dbl()</code> and related functions and set the bounds using the <code>log()</code> function. For the following search space, implement a logarithmic transformation. the output should look exactly as the search space above.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre># Change this to a log trafo:\nps(cost = p_dbl(1e-5, 1e5))</pre>\n</div>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-6\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-6-contents callout-collapse collapse\" id=\"callout-6\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-e7b694bb0b2ac7db2c0b5f29-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+c2VhcmNoX3NwYWNlIDxzcGFuIGNsYXNzPSJvdCI+PTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImZ1Ij5wczwvc3Bhbj4oPHNwYW4gY2xhc3M9ImF0Ij5jb3N0ID08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+cF9kYmw8L3NwYW4+KDxzcGFuIGNsYXNzPSJmdSI+bG9nPC9zcGFuPig8c3BhbiBjbGFzcz0iZmwiPjFlLTU8L3NwYW4+KSwgPHNwYW4gY2xhc3M9ImZ1Ij5sb2c8L3NwYW4+KDxzcGFuIGNsYXNzPSJmbCI+MWU1PC9zcGFuPiksPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTIiPjxhIGhyZWY9IiNjYjEtMiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICA8c3BhbiBjbGFzcz0iYXQiPnRyYWZvID08L3NwYW4+IDxzcGFuIGNsYXNzPSJjZiI+ZnVuY3Rpb248L3NwYW4+KHgpIDxzcGFuIGNsYXNzPSJmdSI+ZXhwPC9zcGFuPih4KSkpIDxzcGFuIGNsYXNzPSJjbyI+IyBhbHRlcm5hdGl2ZWx5OiAmIzM5O3RyYWZvID0gZXhwJiMzOTs8L3NwYW4+PC9zcGFuPgo8c3BhbiBpZD0iY2IxLTMiPjxhIGhyZWY9IiNjYjEtMyIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPnNlYXJjaF9zcGFjZTwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0IGNlbGwtb3V0cHV0LXN0ZG91dCI+CjxwcmU+PGNvZGU+Jmx0O1BhcmFtU2V0KDEpJmd0OwogICAgICAgaWQgICAgY2xhc3MgICAgIGxvd2VyICAgIHVwcGVyIG5sZXZlbHMgICAgICAgIGRlZmF1bHQgIHZhbHVlCiAgICZsdDtjaGFyJmd0OyAgICZsdDtjaGFyJmd0OyAgICAgJmx0O251bSZndDsgICAgJmx0O251bSZndDsgICAmbHQ7bnVtJmd0OyAgICAgICAgICZsdDtsaXN0Jmd0OyAmbHQ7bGlzdCZndDsKMTogICBjb3N0IFBhcmFtRGJsIC0xMS41MTI5MyAxMS41MTI5MyAgICAgSW5mICZsdDtOb0RlZmF1bHRbMF0mZ3Q7IFtOVUxMXQpUcmFmbyBpcyBzZXQuPC9jb2RlPjwvcHJlPgo8L2Rpdj4KPC9kaXY+\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"summary\">\n<h1>Summary</h1>\n<ul>\n<li>In this use-case we learned how to define search spaces for learner HPs.</li>\n<li>Based on this search space, we defined a tuning strategy to try a number of random configurations.</li>\n<li>We visualized the tested configurations to get an idea how the HP effect the performance of our learner.</li>\n<li>We learned about scale transformations in tuning.</li>\n<li>Finally we added a transformation to favor a certain range in the parameter space.</li>\n</ul>\n</section>\n<section class=\"level1\" id=\"further-information\">\n<h1>Further information</h1>\n<p>Other (more advanced) tuning algorithms:</p>\n<ul>\n<li><code>Simuated annealing</code>: Random HPC are sampled and accepted based on an acceptance probability function which states how likely an improvement in performance is. The method is implemented in <code>tnr(\"gensa\")</code>.</li>\n<li><code>Model-based optimization (MBO)</code>: Guess the most promising HPC by estimating the expected improvement of new points. Available in <a href=\"https://mlr3mbo.mlr-org.com/\" rel=\"nofollow\" target=\"_blank\"><code>mlr3mbo</code></a>.</li>\n<li><code>Multifidelity optimization/Successive halving algorithm</code>: This technique starts with multiple HPC and throws away unpromising candidates. This is repeated several times to efficiently use the tuning budget. The method is implemented in <a href=\"https://mlr3hyperband.mlr-org.com/\" rel=\"nofollow\" target=\"_blank\"><code>mlr3hyperband</code></a>.</li>\n</ul>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-tuning-tuning/\"> mlr-org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Tuning\nPosted on\nMay 13, 2025\nby\nGiuseppe Casalicchio\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nmlr-org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nJavaScript is required to unlock solutions.\nPlease enable JavaScript and reload the page,\nor download the source files from\nGitHub\nand run the code locally.\nGoal\nAfter this exercise, you should be able to define search spaces for learning algorithms and apply different hyperparameter (HP) optimization (HPO) techniques to search through the search space to find a well-performing hyperparameter configuration (HPC).\nExercises\nAgain, we are looking at the\ngerman_credit\ndata set and corresponding task (you can quickly load the task with\ntsk(\"german_credit\")\n). We want to train a k-NN model but ask ourselves what the best choice of\nmight be? Furthermore, we are not sure how to set other HPs of the learner, e.g., if we should scale the data or not. In this exercise, we conduct HPO for k-NN to automatically find a good HPC.\nlibrary(mlr3verse)\ntask = tsk(\"german_credit\")\nRecap: k-NN\nk-NN is a machine learning method that predicts new data by averaging over the responses of the k nearest neighbors.\nParameter spaces\nDefine a meaningful search space for the HPs\nk\nand\nscale\n. You can checkout the help page\nlrn(\"classif.kknn\")$help()\nfor an overview of the k-NN learner.\nHint 1\nEach learner has a slot\nparam_set\nthat contains all HPs that can be used for the tuning. In this use case we tune a learner with the key\n\"classif.kknn\"\n. The functions to define the search space are\nps\nand\np_int\n,\np_dbl\n,\np_fct\n, or\np_lgl\nfor HPs in the search space.\nHint 2\nlibrary(mlr3tuning)\n\nsearch_space = ps(\n  k = p_int(...),\n  scale = ...\n)\nSolution\nUnlock solution\nHyperparameter optimization\nNow, we want to tune the k-NN model with the search space from the previous exercise. As resampling strategy we use a 3 fold cross validation. The tuning strategy should be a random search. As termination criteria we choose 40 evaluations.\nHint 1\nThe elements required for the tuning are:\nTask: German credit\nAlgorithm: k-NN algorithm from\nlrn()\nResampling: 3-fold cross validation using\nrsmp()\nTerminator: 40 evaluations using\ntrm()\nSearch space: See previous exercise\nWe use the default performance measure (\nmsr(\"classif.ce\")\nfor classification and\nmsr(\"classif.mse\")\nfor regression)\nThe tuning instance is then defined by calling\nti()\n. The random search optimization algorithm is obtained from\ntnr()\nwith the corresponding key as argument. Furthermore, we allow parallel computations and set the batch size as well as the number of cores to four.\nHint 2\nThe optimization algorithm is obtained from\ntnr()\nwith the corresponding key as argument. Furthermore we allow parallel computations using four cores:\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n\nfuture::plan(\"multicore\", workers = 4L)\n\ntask = tsk(...)\nlrn_knn = lrn(...)\n\nsearch_space = ps(\n  k = p_int(1, 100),\n  scale = p_lgl()\n)\nresampling = rsmp(...)\n\nterminator = trm(..., ... = 40L)\n\ninstance = ti(\n  task = ...,\n  learner = ...,\n  resampling = ...,\n  terminator = ...,\n  search_space = ...\n)\n\noptimizer = tnr(...)\noptimizer$...(...)\nFinally, the optimization is started by passing the tuning instance to the\n$optimize()\nmethod of the tuner.\nSolution\nUnlock solution\nAnalyzing the tuning archive\nInspect the archive of hyperparameters evaluated during the tuning process with\ninstance$archive\n. Create a simple plot with the goal of illustrating the association between the hyperparametere\nk\nand the estimated classification error.\nSolution\nUnlock solution\nVisualizing hyperparameters\nTo see how effective the tuning was, it is useful to look at the effect of the HPs on the performance. It also helps us to understand how important different HPs are. Therefore, access the archive of the tuning instance and visualize the effect.\nHint 1\nAccess the\narchive\nof the tuning instance to get all information about the tuning. You can use all known plotting techniques after transforming it to a\ndata.table\n.\nHint 2\narx = as...(instance$...)\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\ngg_k = ggplot(..., aes(...)) + ...()\ngg_scale = ggplot(..., aes(...)) + ...()\n\ngg_k + gg_scale & theme(legend.position = \"bottom\")\nSolution\nUnlock solution\nHyperparameter dependencies\nWhen defining a hyperparameter search space via the\nps()\nfunction, we sometimes encounter nested search spaces, also called hyperparameter dependencies. One example for this are SVMs. Here, the hyperparameter\ndegree\nis only relevant if the hyperparameter\nkernel\nis set to\n\"polynomial\"\n. Therefore, we only have to consider different configurations for\ndegree\nif we evaluate candidate configurations with polynomial kernel. Construct a search space for a SVM with hyperparameters\nkernel\n(candidates should be\n\"polynomial\"\nand\n\"radial\"\n) and\ndegree\n(integer ranging from 1 to 3, but only for polynomial kernels), and account for the dependency structure.\nHint 1\nIn the\np_fct\n,\np_dbl\n,  functions, we specify this using the\ndepends\nargument, which takes a named argument of the form\n<param> == value\nor\n<param> %in% <vector>\n.\nSolution\nUnlock solution\nHyperparameter transformations\nWhen tuning non-negative hyperparameters with a broad range, using a logarithmic scale can be more efficient. This approach works especially well if we want to test many small values, but also a few very large ones. By selecting values on a logarithmic scale and then exponentiating them, we ensure a concentrated exploration of smaller values while still considering the possibility of very large values, allowing for a targeted and efficient search in finding optimal hyperparameter configurations.\nA simple way to do this is to pass\nlogscale = TRUE\nwhen using\nto_tune()\nto define the parameter search space while constructing the learner:\nlrn = lrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE))\nlrn$param_set$search_space()\n<ParamSet(1)>\n       id    class     lower    upper nlevels        default  value\n   <char>   <char>     <num>    <num>   <num>         <list> <list>\n1:   cost ParamDbl -11.51293 11.51293     Inf <NoDefault[0]> [NULL]\nTrafo is set.\nTo manually create the same transformation, we can pass the transformation to the more general\ntrafo\nargument in\np_dbl()\nand related functions and set the bounds using the\nlog()\nfunction. For the following search space, implement a logarithmic transformation. the output should look exactly as the search space above.\n# Change this to a log trafo:\nps(cost = p_dbl(1e-5, 1e5))\nSolution\nUnlock solution\nSummary\nIn this use-case we learned how to define search spaces for learner HPs.\nBased on this search space, we defined a tuning strategy to try a number of random configurations.\nWe visualized the tested configurations to get an idea how the HP effect the performance of our learner.\nWe learned about scale transformations in tuning.\nFinally we added a transformation to favor a certain range in the parameter space.\nFurther information\nOther (more advanced) tuning algorithms:\nSimuated annealing\n: Random HPC are sampled and accepted based on an acceptance probability function which states how likely an improvement in performance is. The method is implemented in\ntnr(\"gensa\")\n.\nModel-based optimization (MBO)\n: Guess the most promising HPC by estimating the expected improvement of new points. Available in\nmlr3mbo\n.\nMultifidelity optimization/Successive halving algorithm\n: This technique starts with multiple HPC and throws away unpromising candidates. This is repeated several times to efficiently use the tuning budget. The method is implemented in\nmlr3hyperband\n.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nmlr-org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal After this exercise, you should be able to define search spaces for learning algorithms and apply different hyperparameter (HP) optimization (HPO) techniques to search through the search space to find a well-performing hyperparameter configuration (HPC). Exercises Again, we are looking at the german_credit data set and corresponding task (you can quickly load the task with tsk(\"german_credit\")). We want to train a k-NN model but ask ourselves what the best choice of might be? Furthermore, we are not sure how to set other HPs of the learner, e.g., if we should scale the data or not. In this exercise, we conduct HPO for k-NN to automatically find a good HPC. library(mlr3verse) task = tsk(\"german_credit\") Recap: k-NN k-NN is a machine learning method that predicts new data by averaging over the responses of the k nearest neighbors. Parameter spaces Define a meaningful search space for the HPs k and scale. You can checkout the help page lrn(\"classif.kknn\")$help() for an overview of the k-NN learner. Hint 1 Each learner has a slot param_set that contains all HPs that can be used for the tuning. In this use case we tune a learner with the key \"classif.kknn\". The functions to define the search space are ps and p_int, p_dbl, p_fct, or p_lgl for HPs in the search space. Hint 2 library(mlr3tuning) search_space = ps( k = p_int(...), scale = ... ) Solution Unlock solution Hyperparameter optimization Now, we want to tune the k-NN model with the search space from the previous exercise. As resampling strategy we use a 3 fold cross validation. The tuning strategy should be a random search. As termination criteria we choose 40 evaluations. Hint 1 The elements required for the tuning are: Task: German credit Algorithm: k-NN algorithm from lrn() Resampling: 3-fold cross validation using rsmp() Terminator: 40 evaluations using trm() Search space: See previous exercise We use the default performance measure (msr(\"classif.ce\") for classification and msr(\"classif.mse\") for regression) The tuning instance is then defined by calling ti(). The random search optimization algorithm is obtained from tnr() with the corresponding key as argument. Furthermore, we allow parallel computations and set the batch size as well as the number of cores to four. Hint 2 The optimization algorithm is obtained from tnr() with the corresponding key as argument. Furthermore we allow parallel computations using four cores: library(mlr3) library(mlr3learners) library(mlr3tuning) future::plan(\"multicore\", workers = 4L) task = tsk(...) lrn_knn = lrn(...) search_space = ps( k = p_int(1, 100), scale = p_lgl() ) resampling = rsmp(...) terminator = trm(..., ... = 40L) instance = ti( task = ..., learner = ..., resampling = ..., terminator = ..., search_space = ... ) optimizer = tnr(...) optimizer$...(...) Finally, the optimization is started by passing the tuning instance to the $optimize() method of the tuner. Solution Unlock solution Analyzing the tuning archive Inspect the archive of hyperparameters evaluated during the tuning process with instance$archive. Create a simple plot with the goal of illustrating the association between the hyperparametere k and the estimated classification error. Solution Unlock solution Visualizing hyperparameters To see how effective the tuning was, it is useful to look at the effect of the HPs on the performance. It also helps us to understand how important different HPs are. Therefore, access the archive of the tuning instance and visualize the effect. Hint 1 Access the archive of the tuning instance to get all information about the tuning. You can use all known plotting techniques after transforming it to a data.table. Hint 2 arx = as...(instance$...) library(ggplot2) library(patchwork) gg_k = ggplot(..., aes(...)) + ...() gg_scale = ggplot(..., aes(...)) + ...() gg_k + gg_scale & theme(legend.position = \"bottom\") Solution Unlock solution Hyperparameter dependencies When defining a hyperparameter search space via the ps() function, we sometimes encounter nested search spaces, also called hyperparameter dependencies. One example for this are SVMs. Here, the hyperparameter degree is only relevant if the hyperparameter kernel is set to \"polynomial\". Therefore, we only have to consider different configurations for degree if we evaluate candidate configurations with polynomial kernel. Construct a search space for a SVM with hyperparameters kernel (candidates should be \"polynomial\" and \"radial\") and degree (integer ranging from 1 to 3, but only for polynomial kernels), and account for the dependency structure. Hint 1 In the p_fct, p_dbl,  functions, we specify this using the depends argument, which takes a named argument of the form == value or %in% . Solution Unlock solution Hyperparameter transformations When tuning non-negative hyperparameters with a broad range, using a logarithmic scale can be more efficient. This approach works especially well if we want to test many small values, but also a few very large ones. By selecting values on a logarithmic scale and then exponentiating them, we ensure a concentrated exploration of smaller values while still considering the possibility of very large values, allowing for a targeted and efficient search in finding optimal hyperparameter configurations. A simple way to do this is to pass logscale = TRUE when using to_tune() to define the parameter search space while constructing the learner: lrn = lrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE)) lrn$param_set$search_space() id class lower upper nlevels default value 1: cost ParamDbl -11.51293 11.51293 Inf [NULL] Trafo is set. To manually create the same transformation, we can pass the transformation to the more general trafo argument in p_dbl() and related functions and set the bounds using the log() function. For the following search space, implement a logarithmic transformation. the output should look exactly as the search space above. # Change this to a log trafo: ps(cost = p_dbl(1e-5, 1e5)) Solution Unlock solution Summary In this use-case we learned how to define search spaces for learner HPs. Based on this search space, we defined a tuning strategy to try a number of random configurations. We visualized the tested configurations to get an idea how the HP effect the performance of our learner. We learned about scale transformations in tuning. Finally we added a transformation to favor a certain range in the parameter space. Further information Other (more advanced) tuning algorithms: Simuated annealing: Random HPC are sampled and accepted based on an acceptance probability function which states how likely an improvement in performance is. The method is implemented in tnr(\"gensa\"). Model-based optimization (MBO): Guess the most promising HPC by estimating the expected improvement of new points. Available in mlr3mbo. Multifidelity optimization/Successive halving algorithm: This technique starts with multiple HPC and throws away unpromising candidates. This is repeated several times to efficiently use the tuning budget. The method is implemented in mlr3hyperband.",
    "meta_keywords": null,
    "og_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal After this exercise, you should be able to define search spaces for learning algorithms and apply different hyperparameter (HP) optimization (HPO) techniques to search through the search space to find a well-performing hyperparameter configuration (HPC). Exercises Again, we are looking at the german_credit data set and corresponding task (you can quickly load the task with tsk(\"german_credit\")). We want to train a k-NN model but ask ourselves what the best choice of might be? Furthermore, we are not sure how to set other HPs of the learner, e.g., if we should scale the data or not. In this exercise, we conduct HPO for k-NN to automatically find a good HPC. library(mlr3verse) task = tsk(\"german_credit\") Recap: k-NN k-NN is a machine learning method that predicts new data by averaging over the responses of the k nearest neighbors. Parameter spaces Define a meaningful search space for the HPs k and scale. You can checkout the help page lrn(\"classif.kknn\")$help() for an overview of the k-NN learner. Hint 1 Each learner has a slot param_set that contains all HPs that can be used for the tuning. In this use case we tune a learner with the key \"classif.kknn\". The functions to define the search space are ps and p_int, p_dbl, p_fct, or p_lgl for HPs in the search space. Hint 2 library(mlr3tuning) search_space = ps( k = p_int(...), scale = ... ) Solution Unlock solution Hyperparameter optimization Now, we want to tune the k-NN model with the search space from the previous exercise. As resampling strategy we use a 3 fold cross validation. The tuning strategy should be a random search. As termination criteria we choose 40 evaluations. Hint 1 The elements required for the tuning are: Task: German credit Algorithm: k-NN algorithm from lrn() Resampling: 3-fold cross validation using rsmp() Terminator: 40 evaluations using trm() Search space: See previous exercise We use the default performance measure (msr(\"classif.ce\") for classification and msr(\"classif.mse\") for regression) The tuning instance is then defined by calling ti(). The random search optimization algorithm is obtained from tnr() with the corresponding key as argument. Furthermore, we allow parallel computations and set the batch size as well as the number of cores to four. Hint 2 The optimization algorithm is obtained from tnr() with the corresponding key as argument. Furthermore we allow parallel computations using four cores: library(mlr3) library(mlr3learners) library(mlr3tuning) future::plan(\"multicore\", workers = 4L) task = tsk(...) lrn_knn = lrn(...) search_space = ps( k = p_int(1, 100), scale = p_lgl() ) resampling = rsmp(...) terminator = trm(..., ... = 40L) instance = ti( task = ..., learner = ..., resampling = ..., terminator = ..., search_space = ... ) optimizer = tnr(...) optimizer$...(...) Finally, the optimization is started by passing the tuning instance to the $optimize() method of the tuner. Solution Unlock solution Analyzing the tuning archive Inspect the archive of hyperparameters evaluated during the tuning process with instance$archive. Create a simple plot with the goal of illustrating the association between the hyperparametere k and the estimated classification error. Solution Unlock solution Visualizing hyperparameters To see how effective the tuning was, it is useful to look at the effect of the HPs on the performance. It also helps us to understand how important different HPs are. Therefore, access the archive of the tuning instance and visualize the effect. Hint 1 Access the archive of the tuning instance to get all information about the tuning. You can use all known plotting techniques after transforming it to a data.table. Hint 2 arx = as...(instance$...) library(ggplot2) library(patchwork) gg_k = ggplot(..., aes(...)) + ...() gg_scale = ggplot(..., aes(...)) + ...() gg_k + gg_scale & theme(legend.position = \"bottom\") Solution Unlock solution Hyperparameter dependencies When defining a hyperparameter search space via the ps() function, we sometimes encounter nested search spaces, also called hyperparameter dependencies. One example for this are SVMs. Here, the hyperparameter degree is only relevant if the hyperparameter kernel is set to \"polynomial\". Therefore, we only have to consider different configurations for degree if we evaluate candidate configurations with polynomial kernel. Construct a search space for a SVM with hyperparameters kernel (candidates should be \"polynomial\" and \"radial\") and degree (integer ranging from 1 to 3, but only for polynomial kernels), and account for the dependency structure. Hint 1 In the p_fct, p_dbl,  functions, we specify this using the depends argument, which takes a named argument of the form == value or %in% . Solution Unlock solution Hyperparameter transformations When tuning non-negative hyperparameters with a broad range, using a logarithmic scale can be more efficient. This approach works especially well if we want to test many small values, but also a few very large ones. By selecting values on a logarithmic scale and then exponentiating them, we ensure a concentrated exploration of smaller values while still considering the possibility of very large values, allowing for a targeted and efficient search in finding optimal hyperparameter configurations. A simple way to do this is to pass logscale = TRUE when using to_tune() to define the parameter search space while constructing the learner: lrn = lrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE)) lrn$param_set$search_space() id class lower upper nlevels default value 1: cost ParamDbl -11.51293 11.51293 Inf [NULL] Trafo is set. To manually create the same transformation, we can pass the transformation to the more general trafo argument in p_dbl() and related functions and set the bounds using the log() function. For the following search space, implement a logarithmic transformation. the output should look exactly as the search space above. # Change this to a log trafo: ps(cost = p_dbl(1e-5, 1e5)) Solution Unlock solution Summary In this use-case we learned how to define search spaces for learner HPs. Based on this search space, we defined a tuning strategy to try a number of random configurations. We visualized the tested configurations to get an idea how the HP effect the performance of our learner. We learned about scale transformations in tuning. Finally we added a transformation to favor a certain range in the parameter space. Further information Other (more advanced) tuning algorithms: Simuated annealing: Random HPC are sampled and accepted based on an acceptance probability function which states how likely an improvement in performance is. The method is implemented in tnr(\"gensa\"). Model-based optimization (MBO): Guess the most promising HPC by estimating the expected improvement of new points. Available in mlr3mbo. Multifidelity optimization/Successive halving algorithm: This technique starts with multiple HPC and throws away unpromising candidates. This is repeated several times to efficiently use the tuning budget. The method is implemented in mlr3hyperband.",
    "og_image": "https://latex.codecogs.com/png.latex?k",
    "og_title": "Tuning | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 6.3,
    "sitemap_lastmod": null,
    "twitter_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal After this exercise, you should be able to define search spaces for learning algorithms and apply different hyperparameter (HP) optimization (HPO) techniques to search through the search space to find a well-performing hyperparameter configuration (HPC). Exercises Again, we are looking at the german_credit data set and corresponding task (you can quickly load the task with tsk(\"german_credit\")). We want to train a k-NN model but ask ourselves what the best choice of might be? Furthermore, we are not sure how to set other HPs of the learner, e.g., if we should scale the data or not. In this exercise, we conduct HPO for k-NN to automatically find a good HPC. library(mlr3verse) task = tsk(\"german_credit\") Recap: k-NN k-NN is a machine learning method that predicts new data by averaging over the responses of the k nearest neighbors. Parameter spaces Define a meaningful search space for the HPs k and scale. You can checkout the help page lrn(\"classif.kknn\")$help() for an overview of the k-NN learner. Hint 1 Each learner has a slot param_set that contains all HPs that can be used for the tuning. In this use case we tune a learner with the key \"classif.kknn\". The functions to define the search space are ps and p_int, p_dbl, p_fct, or p_lgl for HPs in the search space. Hint 2 library(mlr3tuning) search_space = ps( k = p_int(...), scale = ... ) Solution Unlock solution Hyperparameter optimization Now, we want to tune the k-NN model with the search space from the previous exercise. As resampling strategy we use a 3 fold cross validation. The tuning strategy should be a random search. As termination criteria we choose 40 evaluations. Hint 1 The elements required for the tuning are: Task: German credit Algorithm: k-NN algorithm from lrn() Resampling: 3-fold cross validation using rsmp() Terminator: 40 evaluations using trm() Search space: See previous exercise We use the default performance measure (msr(\"classif.ce\") for classification and msr(\"classif.mse\") for regression) The tuning instance is then defined by calling ti(). The random search optimization algorithm is obtained from tnr() with the corresponding key as argument. Furthermore, we allow parallel computations and set the batch size as well as the number of cores to four. Hint 2 The optimization algorithm is obtained from tnr() with the corresponding key as argument. Furthermore we allow parallel computations using four cores: library(mlr3) library(mlr3learners) library(mlr3tuning) future::plan(\"multicore\", workers = 4L) task = tsk(...) lrn_knn = lrn(...) search_space = ps( k = p_int(1, 100), scale = p_lgl() ) resampling = rsmp(...) terminator = trm(..., ... = 40L) instance = ti( task = ..., learner = ..., resampling = ..., terminator = ..., search_space = ... ) optimizer = tnr(...) optimizer$...(...) Finally, the optimization is started by passing the tuning instance to the $optimize() method of the tuner. Solution Unlock solution Analyzing the tuning archive Inspect the archive of hyperparameters evaluated during the tuning process with instance$archive. Create a simple plot with the goal of illustrating the association between the hyperparametere k and the estimated classification error. Solution Unlock solution Visualizing hyperparameters To see how effective the tuning was, it is useful to look at the effect of the HPs on the performance. It also helps us to understand how important different HPs are. Therefore, access the archive of the tuning instance and visualize the effect. Hint 1 Access the archive of the tuning instance to get all information about the tuning. You can use all known plotting techniques after transforming it to a data.table. Hint 2 arx = as...(instance$...) library(ggplot2) library(patchwork) gg_k = ggplot(..., aes(...)) + ...() gg_scale = ggplot(..., aes(...)) + ...() gg_k + gg_scale & theme(legend.position = \"bottom\") Solution Unlock solution Hyperparameter dependencies When defining a hyperparameter search space via the ps() function, we sometimes encounter nested search spaces, also called hyperparameter dependencies. One example for this are SVMs. Here, the hyperparameter degree is only relevant if the hyperparameter kernel is set to \"polynomial\". Therefore, we only have to consider different configurations for degree if we evaluate candidate configurations with polynomial kernel. Construct a search space for a SVM with hyperparameters kernel (candidates should be \"polynomial\" and \"radial\") and degree (integer ranging from 1 to 3, but only for polynomial kernels), and account for the dependency structure. Hint 1 In the p_fct, p_dbl,  functions, we specify this using the depends argument, which takes a named argument of the form == value or %in% . Solution Unlock solution Hyperparameter transformations When tuning non-negative hyperparameters with a broad range, using a logarithmic scale can be more efficient. This approach works especially well if we want to test many small values, but also a few very large ones. By selecting values on a logarithmic scale and then exponentiating them, we ensure a concentrated exploration of smaller values while still considering the possibility of very large values, allowing for a targeted and efficient search in finding optimal hyperparameter configurations. A simple way to do this is to pass logscale = TRUE when using to_tune() to define the parameter search space while constructing the learner: lrn = lrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE)) lrn$param_set$search_space() id class lower upper nlevels default value 1: cost ParamDbl -11.51293 11.51293 Inf [NULL] Trafo is set. To manually create the same transformation, we can pass the transformation to the more general trafo argument in p_dbl() and related functions and set the bounds using the log() function. For the following search space, implement a logarithmic transformation. the output should look exactly as the search space above. # Change this to a log trafo: ps(cost = p_dbl(1e-5, 1e5)) Solution Unlock solution Summary In this use-case we learned how to define search spaces for learner HPs. Based on this search space, we defined a tuning strategy to try a number of random configurations. We visualized the tested configurations to get an idea how the HP effect the performance of our learner. We learned about scale transformations in tuning. Finally we added a transformation to favor a certain range in the parameter space. Further information Other (more advanced) tuning algorithms: Simuated annealing: Random HPC are sampled and accepted based on an acceptance probability function which states how likely an improvement in performance is. The method is implemented in tnr(\"gensa\"). Model-based optimization (MBO): Guess the most promising HPC by estimating the expected improvement of new points. Available in mlr3mbo. Multifidelity optimization/Successive halving algorithm: This technique starts with multiple HPC and throws away unpromising candidates. This is repeated several times to efficiently use the tuning budget. The method is implemented in mlr3hyperband.",
    "twitter_title": "Tuning | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/05/tuning/",
    "word_count": 1259
  }
}