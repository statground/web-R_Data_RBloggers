{
  "id": "4668d5637475bae9fbeabc2ca37bc36729fc1c5c",
  "url": "https://www.r-bloggers.com/2023/11/mlsauce-version-0-8-10-statistical-machine-learning-with-python-and-r/",
  "created_at_utc": "2025-11-17T20:39:23Z",
  "data": null,
  "raw_original": {
    "uuid": "19945178-4fcf-41ab-9d86-1254df5c334d",
    "created_at": "2025-11-17 20:39:23",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2023/11/mlsauce-version-0-8-10-statistical-machine-learning-with-python-and-r/",
      "crawled_at": "2025-11-17T10:00:28.228931",
      "external_links": [
        {
          "href": "https://thierrymoudiki.github.io//blog/2023/11/05/python/r/adaopt/lsboost/mlsauce_classification",
          "text": "T. Moudiki's Webpage - R"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://github.com/Techtonique/mlsauce",
          "text": "mlsauce"
        },
        {
          "href": "https://www.researchgate.net/publication/341409169_AdaOpt_Multivariable_optimization_for_classification",
          "text": "AdaOpt"
        },
        {
          "href": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
          "text": "nearest neighbors"
        },
        {
          "href": "https://www.researchgate.net/publication/346059361_LSBoost_gradient_boosted_penalized_nonlinear_least_squares",
          "text": "LSBoost"
        },
        {
          "href": "https://www.researchgate.net/publication/346059361_LSBoost_gradient_boosted_penalized_nonlinear_least_squares",
          "text": "on ResearchGate"
        },
        {
          "href": "https://cython.org/",
          "text": "Cython"
        },
        {
          "href": "https://github.com/Techtonique/mlsauce/blob/master/mlsauce/demo/thierrymoudiki_051123_GPopt_mlsauce_classification.ipynb",
          "text": "https://github.com/Techtonique/mlsauce/blob/master/mlsauce/demo/thierrymoudiki_051123_GPopt_mlsauce_classification.ipynb"
        },
        {
          "href": "https://thierrymoudiki.github.io//blog/2023/11/05/python/r/adaopt/lsboost/mlsauce_classification",
          "text": "T. Moudiki's Webpage - R"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "mlsauce version 0.8.10: Statistical/Machine Learning with Python and R | R-bloggers",
      "images": [
        {
          "alt": "image-title-here",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "image-title-here",
          "base64": null,
          "src": "https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image1.png?w=578&ssl=1"
        },
        {
          "alt": "image-title-here",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "image-title-here",
          "base64": null,
          "src": "https://i1.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image2.png?w=578&ssl=1"
        },
        {
          "alt": "image-title-here",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "image-title-here",
          "base64": null,
          "src": "https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image3.png?w=578&ssl=1"
        },
        {
          "alt": "image-title-here",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "image-title-here",
          "base64": null,
          "src": "https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image4.png?w=578&ssl=1"
        },
        {
          "alt": "image-title-here",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "image-title-here",
          "base64": null,
          "src": "https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image5.png?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/t-moudiki/",
          "text": "T. Moudiki"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-379694 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">mlsauce version 0.8.10: Statistical/Machine Learning with Python and R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 4, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/t-moudiki/\">T. Moudiki</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://thierrymoudiki.github.io//blog/2023/11/05/python/r/adaopt/lsboost/mlsauce_classification\"> T. Moudiki's Webpage - R</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9--><p>This week, among other things, I’ve been working on updating <a href=\"https://github.com/Techtonique/mlsauce\" rel=\"nofollow\" target=\"_blank\">mlsauce</a> for both Python and R (that’s version <code>0.8.10</code> of the package).</p>\n<p><code>mlsauce</code> is a package for Statistical/Machine Learning that contains in particular:</p>\n<ul>\n<li> <a href=\"https://www.researchgate.net/publication/341409169_AdaOpt_Multivariable_optimization_for_classification\" rel=\"nofollow\" target=\"_blank\">AdaOpt</a>, a probabilistic classifier which uses <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" rel=\"nofollow\" target=\"_blank\">nearest neighbors</a> to obtain predictions. Interestingly, with AdaOpt, one neighbor can suffice to obtain a high accuracy. </li>\n<li> <a href=\"https://www.researchgate.net/publication/346059361_LSBoost_gradient_boosted_penalized_nonlinear_least_squares\" rel=\"nofollow\" target=\"_blank\">LSBoost</a>\n  , a gradient boosting algorithm based on randomized nnetworks (similar to XGBoost, LightGBM or Catboost, but not using Gradient Boosted Decision Trees a.k.a GBDT). </li>\n</ul>\n<p>Not a lot of GitHub stars for <code>mlsauce</code>’s repository but someday, to my surprise, I noticed that <code>mlsauce.LSBoost</code>’s 2020 “paper” had more than  2000 reads <a href=\"https://www.researchgate.net/publication/346059361_LSBoost_gradient_boosted_penalized_nonlinear_least_squares\" rel=\"nofollow\" target=\"_blank\">on ResearchGate</a>. Well, people, <em>starring</em> the repository on GitHub is pretty cool too.</p>\n<p>Then, I had a ResearchGate recommendation on that same <code>mlsauce.LSBoost</code>’s “paper”, and I told to myself: ‘I’ve probably been missing something in this work for 3 years’. Yes I know I designed it from beginning to  end, but some people can be using it better than I did so far!</p>\n<p>Indeed, I’ve never obtained great results with <code>mlsauce.LSBoost</code> <strong>IN THE PAST</strong>. Eventually, as of today, my feelings are: <code>mlsauce</code> is <em>fast</em>, thanks to <a href=\"https://cython.org/\" rel=\"nofollow\" target=\"_blank\">Cython</a> (which is not easy to package though, IMHO), and quite competitive when well-tuned; as you’ll see below.</p>\n<p>In this post, I revisit <code>mlsauce</code>, with examples of use of <code>AdaOpt</code> and <code>LSBoostclassifier</code>.<code>AdaOpt</code> is used for digits recognition (and seems to be doing well on this type of tasks, more on this in the future). <code>LSBoostclassifier</code> is used on toy examples from scikit-learn as done in the paper, but with better hyperparameters’ tuning. For both models, <code>AdaOpt</code> and <code>LSBoostclassifier</code>, a <strong>distribution of test set accuracy</strong> is presented.</p>\n<p><strong>Contents</strong></p>\n<ol>\n<li>Install and import Python packages</li>\n<li><code>AdaOpt</code> Python — with test set accuracy’s distribution</li>\n<li><code>LSBoostclassifier</code> Python — with test set accuracy’s distribution</li>\n<li>R example</li>\n</ol>\n<p>A notebook can also be found here: <a href=\"https://github.com/Techtonique/mlsauce/blob/master/mlsauce/demo/thierrymoudiki_051123_GPopt_mlsauce_classification.ipynb\" rel=\"nofollow\" target=\"_blank\">https://github.com/Techtonique/mlsauce/blob/master/mlsauce/demo/thierrymoudiki_051123_GPopt_mlsauce_classification.ipynb</a>.</p>\n<h1 id=\"1---install-and-import-python-packages\">1 – Install and import Python packages</h1>\n<pre>!pip install mlsauce\n\r\n!pip install GPopt # a package that implements Bayesian optimization, used here for hyperparameters' tuning\n\r\nimport GPopt as gp\nimport mlsauce as ms\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer, load_wine, load_digits\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report\nfrom time import time\n</pre>\n<h1 id=\"2---adaopt-python--with-test-set-accuracys-distribution\">2 – <code>AdaOpt</code> Python – with test set accuracy’s distribution</h1>\n<pre>import numpy as np\nfrom sklearn.datasets import load_digits # a dataset for digits recognition\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom time import time\n\n\ndigits = load_digits()\nZ = digits.data\nt = digits.target\nnp.random.seed(13239)\nX_train, X_test, y_train, y_test = train_test_split(Z, t,\n                                                    test_size=0.2)\n\nobj = ms.AdaOpt(n_iterations=50,\n           learning_rate=0.3,\n           reg_lambda=0.1,\n           reg_alpha=0.5,\n           eta=0.01,\n           gamma=0.01,\n           tolerance=1e-4,\n           row_sample=1,\n           k=1,\n           n_jobs=3, type_dist=\"euclidean\", verbose=1)\n\nstart = time()\nobj.fit(X_train, y_train)\nprint(f\"\\n\\n Elapsed train: {time()-start} \\n\")\n\nstart = time()\nprint(f\"\\n\\n Accuracy: {obj.score(X_test, y_test)}\")\nprint(f\"\\n Elapsed predict: {time()-start}\")\n\r\n100%|██████████| 360/360 [00:00&lt;00:00, 1979.13it/s]    \n\n Elapsed train: 0.01917862892150879 \n\n Accuracy: 0.9916666666666667\n\n Elapsed predict: 0.19308829307556152\n</pre>\n<p><strong>Obtaining test set accuracy distribution with the same hyperparameters</strong></p>\n<pre>from collections import namedtuple\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom scipy import stats\n\r\ndef eval_adaopt(k=1, B=250):\n\n  res_metric = []\n  training_times = []\n  testing_times = []\n\n  DescribeResult = namedtuple('DescribeResult', ('accuracy',\n                                                 'training_time',\n                                                 'testing_time'))\n  obj = ms.AdaOpt(n_iterations=50,\n              learning_rate=0.3,\n              reg_lambda=0.1,\n              reg_alpha=0.5,\n              eta=0.01,\n              gamma=0.01,\n              tolerance=1e-4,\n              row_sample=1,\n              k=k,\n              n_jobs=-1, type_dist=\"euclidean\", verbose=0)\n\n  for i in tqdm(range(B)):\n\n    np.random.seed(10*i+100)\n    X_train, X_test, y_train, y_test = train_test_split(Z, t,\n                                                        test_size=0.2)\n\n    start = time()\n    obj.fit(X_train, y_train)\n    training_times.append(time()-start)\n    start = time()\n    res_metric.append(obj.score(X_test, y_test))\n    testing_times.append(time()-start)\n\n  return DescribeResult(res_metric, training_times, testing_times), stats.describe(res_metric), stats.describe(training_times), stats.describe(testing_times)\n\r\nres_k1_B250 = eval_adaopt(k=1, B=250)\nres_k2_B250 = eval_adaopt(k=2, B=250)\nres_k3_B250 = eval_adaopt(k=3, B=250)\nres_k4_B250 = eval_adaopt(k=4, B=250)\nres_k5_B250 = eval_adaopt(k=5, B=250)\n\r\n100%|██████████| 250/250 [00:50&lt;00:00,  4.96it/s]\n100%|██████████| 250/250 [00:50&lt;00:00,  4.94it/s]\n100%|██████████| 250/250 [00:50&lt;00:00,  4.96it/s]\n100%|██████████| 250/250 [00:51&lt;00:00,  4.90it/s]\n100%|██████████| 250/250 [00:51&lt;00:00,  4.90it/s]\n\r\ndisplay(res_k1_B250[1])\ndisplay(res_k2_B250[1])\ndisplay(res_k3_B250[1])\ndisplay(res_k4_B250[1])\ndisplay(res_k5_B250[1])\n\r\nDescribeResult(nobs=250, minmax=(0.9722222222222222, 1.0), mean=0.9872888888888888, variance=2.5628935495066882e-05, skewness=-0.13898324248427138, kurtosis=0.22445816198359791)\n\nDescribeResult(nobs=250, minmax=(0.9666666666666667, 0.9972222222222222), mean=0.9846888888888888, variance=3.354355694382497e-05, skewness=-0.2014633213050366, kurtosis=-0.16851847469456605)\n\nDescribeResult(nobs=250, minmax=(0.9611111111111111, 0.9972222222222222), mean=0.9836666666666666, variance=3.45951708066838e-05, skewness=-0.3714590259216959, kurtosis=0.264762318251484)\n\nDescribeResult(nobs=250, minmax=(0.9555555555555556, 1.0), mean=0.9793777777777778, variance=4.80023798899302e-05, skewness=-0.24910751075977636, kurtosis=0.4395617044106124)\n\nDescribeResult(nobs=250, minmax=(0.9555555555555556, 0.9972222222222222), mean=0.9770444444444444, variance=5.1334225792057076e-05, skewness=-0.12883539300214827, kurtosis=0.1411098033435696)\n</pre>\n<p><strong>Obtaining a distribution of training timings</strong></p>\n<pre>display(res_k1_B250[2])\ndisplay(res_k2_B250[2])\ndisplay(res_k3_B250[2])\ndisplay(res_k4_B250[2])\ndisplay(res_k5_B250[2])\n\r\nDescribeResult(nobs=250, minmax=(0.00498199462890625, 0.021169185638427734), mean=0.007840995788574218, variance=4.368068123193988e-06, skewness=2.175594596266775, kurtosis=7.499194342725625)\n\nDescribeResult(nobs=250, minmax=(0.005329132080078125, 0.016299962997436523), mean=0.007670882225036621, variance=3.612048206608975e-06, skewness=1.7118375802873183, kurtosis=3.358366931595608)\n\nDescribeResult(nobs=250, minmax=(0.0053746700286865234, 0.015506505966186523), mean=0.007794314384460449, variance=2.920214088930605e-06, skewness=1.6360801483869196, kurtosis=3.2315493234819064)\n\nDescribeResult(nobs=250, minmax=(0.005369901657104492, 0.02190709114074707), mean=0.007874348640441894, variance=4.55353231021138e-06, skewness=2.3223174208412916, kurtosis=8.922678944294534)\n\nDescribeResult(nobs=250, minmax=(0.005362033843994141, 0.017331361770629883), mean=0.00786894702911377, variance=4.207144846754069e-06, skewness=1.8494401442014954, kurtosis=3.8446086533270085)\n</pre>\n<p><strong>Obtaining a distribution of testing timings</strong></p>\n<pre>display(res_k1_B250[3])\ndisplay(res_k2_B250[3])\ndisplay(res_k3_B250[3])\ndisplay(res_k4_B250[3])\ndisplay(res_k5_B250[3])\n\r\nDescribeResult(nobs=250, minmax=(0.1675705909729004, 0.3001070022583008), mean=0.19125074195861816, variance=0.0003424395337048105, skewness=2.2500063799757677, kurtosis=5.9722526245151375)\n\nDescribeResult(nobs=250, minmax=(0.16643667221069336, 0.31163525581359863), mean=0.1923248109817505, variance=0.0003310783018211768, skewness=2.476834016032642, kurtosis=8.109087286878708)\n\nDescribeResult(nobs=250, minmax=(0.17519187927246094, 0.37604689598083496), mean=0.1916730365753174, variance=0.0003895799321858523, skewness=4.280046315900402, kurtosis=30.357835694940057)\n\nDescribeResult(nobs=250, minmax=(0.17512750625610352, 0.3540067672729492), mean=0.19378959369659424, variance=0.00035161275596300016, skewness=3.595469226517824, kurtosis=21.271489103625353)\n\nDescribeResult(nobs=250, minmax=(0.17573857307434082, 0.2584831714630127), mean=0.19390375328063963, variance=0.0002475867594812809, skewness=2.0323201018310013, kurtosis=3.343216700759352)\n</pre>\n<p><strong>Graph: distribution of test set accuracy for different numbers of neighbors (1 to 4)</strong></p>\n<pre># library &amp; dataset\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame(np.column_stack((res_k1_B250[0][0],\n                               res_k2_B250[0][0],\n                               res_k3_B250[0][0],\n                               res_k4_B250[0][0])),\n               columns=['k1', 'k2', 'k3', 'k4'])\n\r\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"k1\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k2\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k3\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k4\"], hist=True, kde=True, rug=True)\n</pre>\n<p><img alt=\"image-title-here\" class=\"img-responsive\" data-lazy-src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image1.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"image-title-here\" class=\"img-responsive\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image1.png?w=578&amp;ssl=1\"/></noscript></p>\n<p><strong>Graph: distribution of training timings for different numbers of neighbors (1 to 4)</strong></p>\n<pre>df = pd.DataFrame(np.column_stack((res_k1_B250[0][1],\n                               res_k2_B250[0][1],\n                               res_k3_B250[0][1],\n                               res_k4_B250[0][1])),\n               columns=['k1', 'k2', 'k3', 'k4'])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"k1\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k2\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k3\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k4\"], hist=True, kde=True, rug=True)\n</pre>\n<p><img alt=\"image-title-here\" class=\"img-responsive\" data-lazy-src=\"https://i1.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image2.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"image-title-here\" class=\"img-responsive\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image2.png?w=578&amp;ssl=1\"/></noscript></p>\n<p><strong>Graph: distribution of testing timings for different numbers of neighbors (1 to 4)</strong></p>\n<pre>df = pd.DataFrame(np.column_stack((res_k1_B250[0][2],\n                               res_k2_B250[0][2],\n                               res_k3_B250[0][2],\n                               res_k4_B250[0][2])),\n               columns=['k1', 'k2', 'k3', 'k4'])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"k1\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k2\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k3\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k4\"], hist=True, kde=True, rug=True)\n</pre>\n<p><img alt=\"image-title-here\" class=\"img-responsive\" data-lazy-src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image3.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"image-title-here\" class=\"img-responsive\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image3.png?w=578&amp;ssl=1\"/></noscript></p>\n<h1 id=\"3---lsboostclassifier-python--with-test-set-accuracys-distribution\">3 - <code>LSBoostClassifier</code> Python – with test set accuracy’s distribution</h1>\n<h2 id=\"3---1-classification-of-breast-cancer-dataset\">3 - 1 <strong>Classification of Breast Cancer dataset</strong></h2>\n<pre>data = load_breast_cancer()\nX = data.data\ny = data.target\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2, random_state=13)\n\r\ndef lsboost_cv(X_train, y_train,\n               n_estimators=100,\n               learning_rate=0.1,\n               n_hidden_features=5,\n               reg_lambda=0.1,\n               dropout=0,\n               tolerance=1e-4,\n               seed=123):\n\n  estimator = ms.LSBoostClassifier(n_estimators=int(n_estimators),\n                                   learning_rate=learning_rate,\n                                   n_hidden_features=int(n_hidden_features),\n                                   reg_lambda=reg_lambda,\n                                   dropout=dropout,\n                                   tolerance=tolerance,\n                                   seed=seed, verbose=0)\n\n  return -cross_val_score(estimator, X_train, y_train,\n                          scoring='accuracy', cv=5, n_jobs=-1).mean()\n\n\r\ndef optimize_lsboost(X_train, y_train):\n  \n  # objective function for hyperparams tuning\n  def crossval_objective(x):\n\n    return lsboost_cv(\n      X_train=X_train,\n      y_train=y_train,\n      n_estimators=int(x[0]),\n      learning_rate=x[1],\n      n_hidden_features=int(x[2]),\n      reg_lambda=x[3],\n      dropout=x[4],\n      tolerance=x[5])\n\n  gp_opt = gp.GPOpt(objective_func=crossval_objective,\n                      lower_bound = np.array([10, 0.001, 5, 1e-2, 0, 0]),\n                      upper_bound = np.array([100, 0.4, 250, 1e4, 0.7, 1e-1]),\n                      n_init=10, n_iter=190, seed=123)\n  return {'parameters': gp_opt.optimize(verbose=2, abs_tol=1e-2), 'opt_object':  gp_opt}\n\r\n# hyperparams tuning\nres1 = optimize_lsboost(X_train, y_train)\nprint(res1)\nparameters = res1[\"parameters\"]\nstart = time()\nestimator = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=1).fit(X_train, y_train)\n\r\nprint(f\"\\n\\n Test set accuracy: {estimator.score(X_test, y_test)}\")\nprint(f\"\\n Elapsed: {time() - start}\")\n\r\n Test set accuracy: 0.9912280701754386\n\n Elapsed: 0.11275959014892578\n\r\nfrom collections import namedtuple\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom scipy import stats\n</pre>\n<p><strong>Distribution of test set accuracy of LSBoost on Breast Cancer dataset</strong></p>\n<pre>def eval_lsboost(B=250):\n\n  res_metric = []\n  training_times = []\n  testing_times = []\n\n  DescribeResult = namedtuple('DescribeResult', ('accuracy',\n                                                 'training_time',\n                                                 'testing_time'))\n\n  for i in tqdm(range(B)):\n\n    np.random.seed(10*i+100)\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2)\n\n    #try:\n    start = time()\n    obj = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=0).fit(X_train, y_train)\n    training_times.append(time()-start)\n    start = time()\n    res_metric.append(obj.score(X_test, y_test))\n    testing_times.append(time()-start)\n\n  return DescribeResult(res_metric, training_times, testing_times), stats.describe(res_metric), stats.describe(training_times), stats.describe(testing_times)\n\r\nres_lsboost_B250 = eval_lsboost(B=250)\n\r\n100%|██████████| 250/250 [00:11&lt;00:00, 21.07it/s]\n\r\n# library &amp; dataset\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame(res_lsboost_B250[0][0],\n                  columns=[\"accuracy\"])\n\r\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"accuracy\"], hist=True, kde=True, rug=True)\n</pre>\n<p><img alt=\"image-title-here\" class=\"img-responsive\" data-lazy-src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image4.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"image-title-here\" class=\"img-responsive\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image4.png?w=578&amp;ssl=1\"/></noscript></p>\n<h2 id=\"3---2-classification-of-wine-dataset\">3 - 2 <strong>Classification of Wine dataset</strong></h2>\n<pre>data = load_wine()\nX = data.data\ny = data.target\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2, random_state=13)\n\r\nres2 = optimize_lsboost(X_train, y_train)\nprint(res2)\nparameters = res2[\"parameters\"]\nstart = time()\nestimator = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=1).fit(X_train, y_train)\n\r\nprint(f\"\\n\\n Test set accuracy: {estimator.score(X_test, y_test)}\")\nprint(f\"\\n Elapsed: {time() - start}\")\n\r\n Test set accuracy: 1.0\n\n Elapsed: 0.6752924919128418\n</pre>\n<p><strong>test set accuracy’s distribution</strong></p>\n<pre>def eval_lsboost2(B=250):\n\n  res_metric = []\n  training_times = []\n  testing_times = []\n\n  DescribeResult = namedtuple('DescribeResult', ('accuracy',\n                                                 'training_time',\n                                                 'testing_time'))\n\n  for i in tqdm(range(B)):\n\n    np.random.seed(10*i+100)\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2)\n\n    start = time()\n    obj = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=0).fit(X_train, y_train)\n    training_times.append(time()-start)\n    start = time()\n    res_metric.append(obj.score(X_test, y_test))\n    testing_times.append(time()-start)\n\n  return DescribeResult(res_metric, training_times, testing_times), stats.describe(res_metric), stats.describe(training_times), stats.describe(testing_times)\n\r\nres_lsboost2_B250 = eval_lsboost2(B=250)\n\r\n100%|██████████| 250/250 [01:23&lt;00:00,  3.01it/s]\n\r\n# library &amp; dataset\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame(res_lsboost2_B250[0][0],\n                  columns=[\"accuracy\"])\n\r\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"accuracy\"], hist=True, kde=True, rug=True)\n</pre>\n<p><img alt=\"image-title-here\" class=\"img-responsive\" data-lazy-src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image5.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"image-title-here\" class=\"img-responsive\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image5.png?w=578&amp;ssl=1\"/></noscript></p>\n<h1 id=\"4---r-example\">4 - R example</h1>\n<pre>install.packages(\"remotes\")\n\nremotes::install_github(\"Techtonique/mlsauce/R-package\")\n\nlibrary(datasets)\n\nX &lt;- as.matrix(iris[, 1:4])\ny &lt;- as.integer(iris[, 5]) - 1L\n\nn &lt;- dim(X)[1]\np &lt;- dim(X)[2]\nset.seed(21341)\ntrain_index &lt;- sample(x = 1:n, size = floor(0.8*n), replace = TRUE)\ntest_index &lt;- -train_index\nX_train &lt;- as.matrix(iris[train_index, 1:4])\ny_train &lt;- as.integer(iris[train_index, 5]) - 1L\nX_test &lt;- as.matrix(iris[test_index, 1:4])\ny_test &lt;- as.integer(iris[test_index, 5]) - 1L\n\nobj &lt;- mlsauce::AdaOpt()\n\nprint(obj$get_params())\n\nobj$fit(X_train, y_train)\n\n# Accuracy (\\~ 97\\%)\nprint(obj$score(X_test, y_test))\n</pre>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://thierrymoudiki.github.io//blog/2023/11/05/python/r/adaopt/lsboost/mlsauce_classification\"> T. Moudiki's Webpage - R</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
      "main_text": "mlsauce version 0.8.10: Statistical/Machine Learning with Python and R\nPosted on\nNovember 4, 2023\nby\nT. Moudiki\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nT. Moudiki's Webpage - R\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nThis week, among other things, I’ve been working on updating\nmlsauce\nfor both Python and R (that’s version\n0.8.10\nof the package).\nmlsauce\nis a package for Statistical/Machine Learning that contains in particular:\nAdaOpt\n, a probabilistic classifier which uses\nnearest neighbors\nto obtain predictions. Interestingly, with AdaOpt, one neighbor can suffice to obtain a high accuracy.\nLSBoost\n, a gradient boosting algorithm based on randomized nnetworks (similar to XGBoost, LightGBM or Catboost, but not using Gradient Boosted Decision Trees a.k.a GBDT).\nNot a lot of GitHub stars for\nmlsauce\n’s repository but someday, to my surprise, I noticed that\nmlsauce.LSBoost\n’s 2020 “paper” had more than  2000 reads\non ResearchGate\n. Well, people,\nstarring\nthe repository on GitHub is pretty cool too.\nThen, I had a ResearchGate recommendation on that same\nmlsauce.LSBoost\n’s “paper”, and I told to myself: ‘I’ve probably been missing something in this work for 3 years’. Yes I know I designed it from beginning to  end, but some people can be using it better than I did so far!\nIndeed, I’ve never obtained great results with\nmlsauce.LSBoost\nIN THE PAST\n. Eventually, as of today, my feelings are:\nmlsauce\nis\nfast\n, thanks to\nCython\n(which is not easy to package though, IMHO), and quite competitive when well-tuned; as you’ll see below.\nIn this post, I revisit\nmlsauce\n, with examples of use of\nAdaOpt\nand\nLSBoostclassifier\n.\nAdaOpt\nis used for digits recognition (and seems to be doing well on this type of tasks, more on this in the future).\nLSBoostclassifier\nis used on toy examples from scikit-learn as done in the paper, but with better hyperparameters’ tuning. For both models,\nAdaOpt\nand\nLSBoostclassifier\n, a\ndistribution of test set accuracy\nis presented.\nContents\nInstall and import Python packages\nAdaOpt\nPython — with test set accuracy’s distribution\nLSBoostclassifier\nPython — with test set accuracy’s distribution\nR example\nA notebook can also be found here:\nhttps://github.com/Techtonique/mlsauce/blob/master/mlsauce/demo/thierrymoudiki_051123_GPopt_mlsauce_classification.ipynb\n.\n1 – Install and import Python packages\n!pip install mlsauce\n\n!pip install GPopt # a package that implements Bayesian optimization, used here for hyperparameters' tuning\n\nimport GPopt as gp\nimport mlsauce as ms\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer, load_wine, load_digits\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report\nfrom time import time\n2 –\nAdaOpt\nPython – with test set accuracy’s distribution\nimport numpy as np\nfrom sklearn.datasets import load_digits # a dataset for digits recognition\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom time import time\n\ndigits = load_digits()\nZ = digits.data\nt = digits.target\nnp.random.seed(13239)\nX_train, X_test, y_train, y_test = train_test_split(Z, t,\n                                                    test_size=0.2)\n\nobj = ms.AdaOpt(n_iterations=50,\n           learning_rate=0.3,\n           reg_lambda=0.1,\n           reg_alpha=0.5,\n           eta=0.01,\n           gamma=0.01,\n           tolerance=1e-4,\n           row_sample=1,\n           k=1,\n           n_jobs=3, type_dist=\"euclidean\", verbose=1)\n\nstart = time()\nobj.fit(X_train, y_train)\nprint(f\"\\n\\n Elapsed train: {time()-start} \\n\")\n\nstart = time()\nprint(f\"\\n\\n Accuracy: {obj.score(X_test, y_test)}\")\nprint(f\"\\n Elapsed predict: {time()-start}\")\n\n100%|██████████| 360/360 [00:00<00:00, 1979.13it/s]    \n\n Elapsed train: 0.01917862892150879 \n\n Accuracy: 0.9916666666666667\n\n Elapsed predict: 0.19308829307556152\nObtaining test set accuracy distribution with the same hyperparameters\nfrom collections import namedtuple\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom scipy import stats\n\ndef eval_adaopt(k=1, B=250):\n\n  res_metric = []\n  training_times = []\n  testing_times = []\n\n  DescribeResult = namedtuple('DescribeResult', ('accuracy',\n                                                 'training_time',\n                                                 'testing_time'))\n  obj = ms.AdaOpt(n_iterations=50,\n              learning_rate=0.3,\n              reg_lambda=0.1,\n              reg_alpha=0.5,\n              eta=0.01,\n              gamma=0.01,\n              tolerance=1e-4,\n              row_sample=1,\n              k=k,\n              n_jobs=-1, type_dist=\"euclidean\", verbose=0)\n\n  for i in tqdm(range(B)):\n\n    np.random.seed(10*i+100)\n    X_train, X_test, y_train, y_test = train_test_split(Z, t,\n                                                        test_size=0.2)\n\n    start = time()\n    obj.fit(X_train, y_train)\n    training_times.append(time()-start)\n    start = time()\n    res_metric.append(obj.score(X_test, y_test))\n    testing_times.append(time()-start)\n\n  return DescribeResult(res_metric, training_times, testing_times), stats.describe(res_metric), stats.describe(training_times), stats.describe(testing_times)\n\nres_k1_B250 = eval_adaopt(k=1, B=250)\nres_k2_B250 = eval_adaopt(k=2, B=250)\nres_k3_B250 = eval_adaopt(k=3, B=250)\nres_k4_B250 = eval_adaopt(k=4, B=250)\nres_k5_B250 = eval_adaopt(k=5, B=250)\n\n100%|██████████| 250/250 [00:50<00:00,  4.96it/s]\n100%|██████████| 250/250 [00:50<00:00,  4.94it/s]\n100%|██████████| 250/250 [00:50<00:00,  4.96it/s]\n100%|██████████| 250/250 [00:51<00:00,  4.90it/s]\n100%|██████████| 250/250 [00:51<00:00,  4.90it/s]\n\ndisplay(res_k1_B250[1])\ndisplay(res_k2_B250[1])\ndisplay(res_k3_B250[1])\ndisplay(res_k4_B250[1])\ndisplay(res_k5_B250[1])\n\nDescribeResult(nobs=250, minmax=(0.9722222222222222, 1.0), mean=0.9872888888888888, variance=2.5628935495066882e-05, skewness=-0.13898324248427138, kurtosis=0.22445816198359791)\n\nDescribeResult(nobs=250, minmax=(0.9666666666666667, 0.9972222222222222), mean=0.9846888888888888, variance=3.354355694382497e-05, skewness=-0.2014633213050366, kurtosis=-0.16851847469456605)\n\nDescribeResult(nobs=250, minmax=(0.9611111111111111, 0.9972222222222222), mean=0.9836666666666666, variance=3.45951708066838e-05, skewness=-0.3714590259216959, kurtosis=0.264762318251484)\n\nDescribeResult(nobs=250, minmax=(0.9555555555555556, 1.0), mean=0.9793777777777778, variance=4.80023798899302e-05, skewness=-0.24910751075977636, kurtosis=0.4395617044106124)\n\nDescribeResult(nobs=250, minmax=(0.9555555555555556, 0.9972222222222222), mean=0.9770444444444444, variance=5.1334225792057076e-05, skewness=-0.12883539300214827, kurtosis=0.1411098033435696)\nObtaining a distribution of training timings\ndisplay(res_k1_B250[2])\ndisplay(res_k2_B250[2])\ndisplay(res_k3_B250[2])\ndisplay(res_k4_B250[2])\ndisplay(res_k5_B250[2])\n\nDescribeResult(nobs=250, minmax=(0.00498199462890625, 0.021169185638427734), mean=0.007840995788574218, variance=4.368068123193988e-06, skewness=2.175594596266775, kurtosis=7.499194342725625)\n\nDescribeResult(nobs=250, minmax=(0.005329132080078125, 0.016299962997436523), mean=0.007670882225036621, variance=3.612048206608975e-06, skewness=1.7118375802873183, kurtosis=3.358366931595608)\n\nDescribeResult(nobs=250, minmax=(0.0053746700286865234, 0.015506505966186523), mean=0.007794314384460449, variance=2.920214088930605e-06, skewness=1.6360801483869196, kurtosis=3.2315493234819064)\n\nDescribeResult(nobs=250, minmax=(0.005369901657104492, 0.02190709114074707), mean=0.007874348640441894, variance=4.55353231021138e-06, skewness=2.3223174208412916, kurtosis=8.922678944294534)\n\nDescribeResult(nobs=250, minmax=(0.005362033843994141, 0.017331361770629883), mean=0.00786894702911377, variance=4.207144846754069e-06, skewness=1.8494401442014954, kurtosis=3.8446086533270085)\nObtaining a distribution of testing timings\ndisplay(res_k1_B250[3])\ndisplay(res_k2_B250[3])\ndisplay(res_k3_B250[3])\ndisplay(res_k4_B250[3])\ndisplay(res_k5_B250[3])\n\nDescribeResult(nobs=250, minmax=(0.1675705909729004, 0.3001070022583008), mean=0.19125074195861816, variance=0.0003424395337048105, skewness=2.2500063799757677, kurtosis=5.9722526245151375)\n\nDescribeResult(nobs=250, minmax=(0.16643667221069336, 0.31163525581359863), mean=0.1923248109817505, variance=0.0003310783018211768, skewness=2.476834016032642, kurtosis=8.109087286878708)\n\nDescribeResult(nobs=250, minmax=(0.17519187927246094, 0.37604689598083496), mean=0.1916730365753174, variance=0.0003895799321858523, skewness=4.280046315900402, kurtosis=30.357835694940057)\n\nDescribeResult(nobs=250, minmax=(0.17512750625610352, 0.3540067672729492), mean=0.19378959369659424, variance=0.00035161275596300016, skewness=3.595469226517824, kurtosis=21.271489103625353)\n\nDescribeResult(nobs=250, minmax=(0.17573857307434082, 0.2584831714630127), mean=0.19390375328063963, variance=0.0002475867594812809, skewness=2.0323201018310013, kurtosis=3.343216700759352)\nGraph: distribution of test set accuracy for different numbers of neighbors (1 to 4)\n# library & dataset\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame(np.column_stack((res_k1_B250[0][0],\n                               res_k2_B250[0][0],\n                               res_k3_B250[0][0],\n                               res_k4_B250[0][0])),\n               columns=['k1', 'k2', 'k3', 'k4'])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"k1\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k2\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k3\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k4\"], hist=True, kde=True, rug=True)\nGraph: distribution of training timings for different numbers of neighbors (1 to 4)\ndf = pd.DataFrame(np.column_stack((res_k1_B250[0][1],\n                               res_k2_B250[0][1],\n                               res_k3_B250[0][1],\n                               res_k4_B250[0][1])),\n               columns=['k1', 'k2', 'k3', 'k4'])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"k1\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k2\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k3\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k4\"], hist=True, kde=True, rug=True)\nGraph: distribution of testing timings for different numbers of neighbors (1 to 4)\ndf = pd.DataFrame(np.column_stack((res_k1_B250[0][2],\n                               res_k2_B250[0][2],\n                               res_k3_B250[0][2],\n                               res_k4_B250[0][2])),\n               columns=['k1', 'k2', 'k3', 'k4'])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"k1\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k2\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k3\"], hist=True, kde=True, rug=True)\nsns.distplot(a=df[\"k4\"], hist=True, kde=True, rug=True)\n3 -\nLSBoostClassifier\nPython – with test set accuracy’s distribution\n3 - 1\nClassification of Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2, random_state=13)\n\ndef lsboost_cv(X_train, y_train,\n               n_estimators=100,\n               learning_rate=0.1,\n               n_hidden_features=5,\n               reg_lambda=0.1,\n               dropout=0,\n               tolerance=1e-4,\n               seed=123):\n\n  estimator = ms.LSBoostClassifier(n_estimators=int(n_estimators),\n                                   learning_rate=learning_rate,\n                                   n_hidden_features=int(n_hidden_features),\n                                   reg_lambda=reg_lambda,\n                                   dropout=dropout,\n                                   tolerance=tolerance,\n                                   seed=seed, verbose=0)\n\n  return -cross_val_score(estimator, X_train, y_train,\n                          scoring='accuracy', cv=5, n_jobs=-1).mean()\n\ndef optimize_lsboost(X_train, y_train):\n  \n  # objective function for hyperparams tuning\n  def crossval_objective(x):\n\n    return lsboost_cv(\n      X_train=X_train,\n      y_train=y_train,\n      n_estimators=int(x[0]),\n      learning_rate=x[1],\n      n_hidden_features=int(x[2]),\n      reg_lambda=x[3],\n      dropout=x[4],\n      tolerance=x[5])\n\n  gp_opt = gp.GPOpt(objective_func=crossval_objective,\n                      lower_bound = np.array([10, 0.001, 5, 1e-2, 0, 0]),\n                      upper_bound = np.array([100, 0.4, 250, 1e4, 0.7, 1e-1]),\n                      n_init=10, n_iter=190, seed=123)\n  return {'parameters': gp_opt.optimize(verbose=2, abs_tol=1e-2), 'opt_object':  gp_opt}\n\n# hyperparams tuning\nres1 = optimize_lsboost(X_train, y_train)\nprint(res1)\nparameters = res1[\"parameters\"]\nstart = time()\nestimator = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=1).fit(X_train, y_train)\n\nprint(f\"\\n\\n Test set accuracy: {estimator.score(X_test, y_test)}\")\nprint(f\"\\n Elapsed: {time() - start}\")\n\n Test set accuracy: 0.9912280701754386\n\n Elapsed: 0.11275959014892578\n\nfrom collections import namedtuple\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom scipy import stats\nDistribution of test set accuracy of LSBoost on Breast Cancer dataset\ndef eval_lsboost(B=250):\n\n  res_metric = []\n  training_times = []\n  testing_times = []\n\n  DescribeResult = namedtuple('DescribeResult', ('accuracy',\n                                                 'training_time',\n                                                 'testing_time'))\n\n  for i in tqdm(range(B)):\n\n    np.random.seed(10*i+100)\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2)\n\n    #try:\n    start = time()\n    obj = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=0).fit(X_train, y_train)\n    training_times.append(time()-start)\n    start = time()\n    res_metric.append(obj.score(X_test, y_test))\n    testing_times.append(time()-start)\n\n  return DescribeResult(res_metric, training_times, testing_times), stats.describe(res_metric), stats.describe(training_times), stats.describe(testing_times)\n\nres_lsboost_B250 = eval_lsboost(B=250)\n\n100%|██████████| 250/250 [00:11<00:00, 21.07it/s]\n\n# library & dataset\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame(res_lsboost_B250[0][0],\n                  columns=[\"accuracy\"])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"accuracy\"], hist=True, kde=True, rug=True)\n3 - 2\nClassification of Wine dataset\ndata = load_wine()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2, random_state=13)\n\nres2 = optimize_lsboost(X_train, y_train)\nprint(res2)\nparameters = res2[\"parameters\"]\nstart = time()\nestimator = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=1).fit(X_train, y_train)\n\nprint(f\"\\n\\n Test set accuracy: {estimator.score(X_test, y_test)}\")\nprint(f\"\\n Elapsed: {time() - start}\")\n\n Test set accuracy: 1.0\n\n Elapsed: 0.6752924919128418\ntest set accuracy’s distribution\ndef eval_lsboost2(B=250):\n\n  res_metric = []\n  training_times = []\n  testing_times = []\n\n  DescribeResult = namedtuple('DescribeResult', ('accuracy',\n                                                 'training_time',\n                                                 'testing_time'))\n\n  for i in tqdm(range(B)):\n\n    np.random.seed(10*i+100)\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        test_size=0.2)\n\n    start = time()\n    obj = ms.LSBoostClassifier(n_estimators=int(parameters[0][0]),\n                                   learning_rate=parameters[0][1],\n                                   n_hidden_features=int(parameters[0][2]),\n                                   reg_lambda=parameters[0][3],\n                                   dropout=parameters[0][4],\n                                   tolerance=parameters[0][5],\n                                   seed=123, verbose=0).fit(X_train, y_train)\n    training_times.append(time()-start)\n    start = time()\n    res_metric.append(obj.score(X_test, y_test))\n    testing_times.append(time()-start)\n\n  return DescribeResult(res_metric, training_times, testing_times), stats.describe(res_metric), stats.describe(training_times), stats.describe(testing_times)\n\nres_lsboost2_B250 = eval_lsboost2(B=250)\n\n100%|██████████| 250/250 [01:23<00:00,  3.01it/s]\n\n# library & dataset\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame(res_lsboost2_B250[0][0],\n                  columns=[\"accuracy\"])\n\n# Plot the histogram thanks to the distplot function\nsns.distplot(a=df[\"accuracy\"], hist=True, kde=True, rug=True)\n4 - R example\ninstall.packages(\"remotes\")\n\nremotes::install_github(\"Techtonique/mlsauce/R-package\")\n\nlibrary(datasets)\n\nX <- as.matrix(iris[, 1:4])\ny <- as.integer(iris[, 5]) - 1L\n\nn <- dim(X)[1]\np <- dim(X)[2]\nset.seed(21341)\ntrain_index <- sample(x = 1:n, size = floor(0.8*n), replace = TRUE)\ntest_index <- -train_index\nX_train <- as.matrix(iris[train_index, 1:4])\ny_train <- as.integer(iris[train_index, 5]) - 1L\nX_test <- as.matrix(iris[test_index, 1:4])\ny_test <- as.integer(iris[test_index, 5]) - 1L\n\nobj <- mlsauce::AdaOpt()\n\nprint(obj$get_params())\n\nobj$fit(X_train, y_train)\n\n# Accuracy (\\~ 97\\%)\nprint(obj$score(X_test, y_test))\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nT. Moudiki's Webpage - R\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Statistical/Machine Learning with Python and R, using mlsauce's AdaOpt and LSBoost",
      "meta_keywords": null,
      "og_description": "Statistical/Machine Learning with Python and R, using mlsauce's AdaOpt and LSBoost",
      "og_image": "https://thierrymoudiki.github.io/images/2023-11-05/2023-11-05-image1.png",
      "og_title": "mlsauce version 0.8.10: Statistical/Machine Learning with Python and R | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 11.6,
      "sitemap_lastmod": "2023-11-05T00:00:00+00:00",
      "twitter_description": "Statistical/Machine Learning with Python and R, using mlsauce's AdaOpt and LSBoost",
      "twitter_title": "mlsauce version 0.8.10: Statistical/Machine Learning with Python and R | R-bloggers",
      "url": "https://www.r-bloggers.com/2023/11/mlsauce-version-0-8-10-statistical-machine-learning-with-python-and-r/",
      "word_count": 2323
    }
  }
}