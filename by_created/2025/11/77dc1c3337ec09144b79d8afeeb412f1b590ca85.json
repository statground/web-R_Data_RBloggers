{
  "id": "77dc1c3337ec09144b79d8afeeb412f1b590ca85",
  "url": "https://www.r-bloggers.com/2025/04/explained-vs-predictive-power-r%C2%B2-adjusted-r%C2%B2-and-beyond/",
  "created_at_utc": "2025-11-22T19:58:44Z",
  "data": null,
  "raw_original": {
    "uuid": "63652f6c-1d02-4c3f-b499-417de1ad86a9",
    "created_at": "2025-11-22 19:58:44",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/04/explained-vs-predictive-power-r%c2%b2-adjusted-r%c2%b2-and-beyond/",
      "crawled_at": "2025-11-22T10:49:46.859259",
      "external_links": [
        {
          "href": "https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/",
          "text": "A Statistician's R Notebook"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/",
          "text": "A Statistician's R Notebook"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Explained vs. Predictive Power: R¬≤, Adjusted R¬≤, and Beyond | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/quote_box.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAHQAAAApBAMAAAAWpBA9AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid2ZuxCrzWZUMiJ270Sf5u5eAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAB8UlEQVRIDe1VP0scQRx965173u5ye3iNlfgRAkmTPyQnJBJSyJ5WVxi2CmKRnN9g0kVsvG4JFoOChc3dtSJhU1iICBZ+ALHwQMEYxMYUm52f7K6zeytzdoFM8fb33rw3w87AbwDF0a6troFAMRDbylVsMIJYUi1GHGiMQDUR++wuTEYQS6pF6R3QJVBNxD4jWHRAEEvKxVbwGyBQjsRG4w0DCGJJrTCBQosg4/dqGUkSrHDHKQJJDonp69W0JvESUHEJTtu+981cmYymjdZYPaoHfu0qipxA39uu6w3+MbFpD++qNX6+AIG+pn0+Oz9mIjp+XZt2MOckyzxY6bzgY7T5XpiKbnh4WEr5+zwlRFTnRt3sOfOC2wzlW4t3ozn67na4xBOy8RJtz9o/EEqnC+vmEn4yKyqby3wgOwQWVoMgdUxK0Rk0Z7NrKkWvvWfR6RY+iUH7q0SNP7Baj9vVOsLYVU608lYMFs4G2XF3rUc50awsKeG1jrRG76Qh//UXR6n+Q1pNEPskI6UF7elzGIssLTe+LHNJ+9/CpePIIf9wCy/TfX/P+bEBctLCixT9MMCTIyUtvD8Bb72yzHKMWfleC+eGazxZylrylHstnGs+roaIJkvqvCei4as39ChcbLsV9/XO0MHwrXyF9a/Opo+/ZiSimzZh/ZgAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?R%5E2%20=%201%20-%20%5Cfrac%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Bres%7D%7D%7D%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D%7D"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAPBAMAAAB+RXBeAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVImZZiIy3RDNu6t2RO9ilkPgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAArElEQVQYGWNgEFJ2FYAQDFDAqMCQ7gAmoAIMDOwNDEwOYAIuxB/AwOoAJuBCnCsYGALABFyI7f+WBgYwARdiyP7/iQFCIMTY7B0YGMAEVIyVgYF5ApiAK+IBqigAE9eFDoi4sDrqMnAyMPBuABPcJ9IWcDcl9DLwKzBwJYAJbgGmvbevvXJgYGo6ZAYhuBOYDzBwNC6Bm8nAwJ3AtoA1qKEZSSjdlEFIhOfgUQDFyCo1aOntUgAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?%5Ctext%7BSS%7D_%7B%5Ctext%7Bres%7D%7D"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaBAMAAAC6D0SsAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMA70RmuyKrEJlU3YnNdjLNMJPYAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABv0lEQVQ4EY1UO0gDQRScaC6Xf2IhduYKEawMGEUQTEDFSlARsZFcZWNzYpNGTGFhp2AjVoKFrWAlFgYbS1PGRtIKFioqqMX6Ntm3dxcT4sDdmzczL+znCDKvfRIZIYSF/yD/3kwZE6LQNR+ZKiMosipX+ug6sI5NmBmdO+468IwcMPrNuZgD7HDzt1aldA3ERNlj2h6uqTkvaYke85xe91+ybSJhMfPW8JrsIrTZUJ3IlliUfQNxJr5aNSkLVIBHg2pEEFM4YtKuXqHHSkjj4Efbc5q1IcMoikY06t5xGngqo+V8WQnwjxhigOkpzMNBLMC7NK2kOIX8p6JGBT3JFwxqRxKtBBzWQ3ywyQrdi00jEkOzhBkirET1QPyikaB7OQWiVqLQa8MDVvQesKHdNLDrBKtZeaEarOiBREV7d0AKxTrtxQNWVlg70WvDA9B7eYNYlj1ZWRlTYrLfdVclrSG67Uqu0vgEqV2yXDfo4Bb9CI+7ElgxbCWesRkDjDLssG89ZColrFYevOCBSSIjGKN9+KGUaaXusVt6IyY/3w5YbOrqTM3lnEh3SPrlUfpDUtBr8ydautq+htNitW1/AZXScbxDeoUWAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?%5Csum%20(y_i%20-%20%5Chat%7By%7D_i)%5E2"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAACMAAAAPBAMAAACcmWsnAAAALVBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAOrOgAAAADnRSTlMAVImZZiIy3RDNu6t2REQ0T5UAAAAJcEhZcwAADsQAAA7EAZUrDhsAAACvSURBVBgZY2AQUnYVgBAMUMCowJDuACagAgwM7A0MTA5gAi7EF8DA6gAm4EKcKxgYAsAEXIjt3ZYGBjABF2LIfveIAUIgxNjsHBgYgIQYSIgxgYGBlYGBeQKYWAYS4gIK8QBVFICI2kkOjI4HbqowMHAyMPBuABPbGLIZkrmBqvgUgIrBxDYGHQY+kBBT0yEzCLGNFSwUADQRBmzDshluM19NgPGBdPIBxiMabDYMAGiIJ/d9lwCCAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAFsAAAAaBAMAAAAqHD1jAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMA70RmuyKrEJlU3YnNdjLNMJPYAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABvElEQVQ4EYWUP0vDUBTFT5qkoU1T6iCOLX6BdpAiFLGD4CRUB3FwcHKUuFWngoJuBlzEUYeuBUGELv0GFnFw7AcQrOIfVPD5bvLuSyrYXkjuub9zeHkvgaD4PEVVFEKUMLkWX6OMXRX1CenTG8ASFZVqvo2P2/10B05Rp87Gx/M9sw6Uvzjl+nD6PPzpHs1uR16iFVvpWI4qh0L7vrzdfsbOXSz/qFM57xDbFQ1t9bRKiFq3GyAHeAMKZoQOZSqJ1Kh0A8yiT+zkmx0rfg4j7qktW4gOTVn9PQ0gf4ZMizNhV6TA0BYzSmaBPWNg9Uc2FRJgyHEsviu5CZSeYAy0Q0IRXhJI86t8lO41nsJ0aomqokk7pHTLhUeQYl9eF7hHlaguIoDeO7bZkUd1PrBiZn0mshNJxr0emzkZG9o/kGdIFBHYdSbnei0vAGq1Ia7YCjsRmBXF8tPatLek9Fposxc5ksDyI43VkhKyLSNXMho4aMQoIlhncsnCBTaQxgHPqkfkSE1Wh+15IBWYCwHPqofEHKjpkN3mi1TJXbBB3VaDeovO2pwoJP1/dFn+YFTpXf0TJfxwrMsfE1PWL460dLtfT0fnAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?%5Csum%20(y_i%20-%20%5Cbar%7By%7D)%5E2"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAQ0AAAAsCAMAAABISx0dAAAAM1BMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADxgEwMAAAAEHRSTlMAid2ZuxCrzWZUMiJ270RsGkpY4wAAAAlwSFlzAAAOxAAADsQBlSsOGwAABKhJREFUaAXVWom2oyAMBRHLJuP/f+2wyGIlQa3taz1npj6IucmVhAAS8oOXMLcaradb1X1YmWQ3A9rxZoUfVGcfV8HsQ7UfFT87OvjA2y71WrVRC8AGmUTv6S/tH18wHGSDz/pL3cXNkgPej/aCbBD6m7EyQENDGsGFmKBYCCzBbHC4C+X3bzvlAuBrq2ZHlESnB8TlnxwcDwqwIYnwMWTQEY+wwZeLyRmw5xPNeoGzHfNETakws2O+SmwhbJBHerL4IQW9u7Qpyvt3XXiK5NDZOgCELd+LZBWzU60dPxM0FvvO7CQkgr4TJiTBwxYM8LvSPqPIB/GcQBfGht51KlfmCTQPQTjNdgu71ZRP8ArymS+y+aBvFL5EddFS4mIvumBU+STsLzosjLKRhjxCT7oQVTT/P1ZCb9BJgKeA1RJJdcY7o6gEkyGn0/Kg8GgdUwKON8xbf6LwBcv+lRoa81JPjFToKzwHeKRzk/V7GllKHEsYJNJHDl6+VLhY2R/Fgul9MVKhJ/ipHREjwFJl1vVbsZYyKmZi48ahUQSew56RdnlnI5ATEC5GKvQML9Oo3WgkQ7t5K3T1r7SmM+EtcpdGrAvY/bwL6cfdzFM/LkYKeoEPE8Qedr4vp+2V6zVF+8DlcrREL+46joi7GeZ/D4qLhbQR0Gv49jMxpvaO3NLCV+0Lk4KNYDJ2WGzKV0VW2+RkWi6EcDHSRG8vzt7KBllCVMTANeczFOqmzrMhKramjWf0UifrIU+KWLmR3sELvzEqYuCmlHpCXXGzsSpQebWJiq1p4xl9zENQ5YhzkVSmGh/T91zZ4TkUfet8v53LffFhoJpwVVDczBrLTYuN0lvumuiksFEk3TAqbFTNd91GNmI0Ds79KnX4AojnyfZC3jgaKU301orSrWPey0aIFBtf8WMkuiqIoXKwfhFo2c9LFq3U1o+H+zY6aWbRlPV3Su5p8FmUMsb8AFSTcBWpkczZbql0aY0zbAHXK/tJjPeO2Ba9eNUOwpj1i9S9d/uR52r1mWgXJeEfxkbXkvF4GbfTBVRfJ2qhncpug24Mda4GblxCD3njJTaA6rprlRcAtjeR7Y1Dap+EtuvJNWZrmUm4Rew9bLhBdvkCVm2TG7S3Xc/rSZELpAQh3Jw6838OVLmY4S+NDULX7ZOk/PgvT4vrp0dMrmGeOi7+uclO++0CPklpmJZGGpdTXmSDXCYT2u2xu9d3kYb1sQ0bY8dc3envmWIv5lFwJ5BUia57miSN0cIPdvjasIFPWEqoV7cTTu4SJ7NzVZ4a8m9Jo93TJC5DecKwcK3ZSNsbGWt7oxnNtei25+/+iksqj989TbLE+jSOnqHVbBTVf+feSeT6uLR7mhTO3aYYKo1lpYOu2WiujE6a92nx6rSne5o0+hJ7xhZ6FRvYKd6nnTyMF0Z/kO6fJvkJyAJzdQSs2AgHBofN+BbBvJzrniYpzwb0bdPKRl5P1iH4La4esCMPju5pkmFCGmQe2KwnzYvFxAHL3yJyeDF45pOoX/3S6fhhYN5fOfBSfvYrOHLwC8myE9ln43e/kHSn8Wi1vfpuH8c3Q9SPJo3o6tu+rP4Pm2wkeDGnhogAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?R%5E2_%7B%5Ctext%7Badj%7D%7D%20=%201%20-%20%5Cleft(1%20-%20R%5E2%5Cright)%20%5Ccdot%20%5Cleft(%5Cfrac%7Bn%20-%201%7D%7Bn%20-%20p%20-%201%7D%5Cright)"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAJsAAAApBAMAAADQTUbBAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAid2ZuxCrzWZUMiJ270Sf5u5eAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAC80lEQVRIDa2VS2gTURSG/7SZNPMgj4aWViIVRVBcaKm0VsRWUJGCMq5EQaiLSg3WRnAhrkawYNFFCi5ERIa21GXTVX2h6UZrQezehUGKLgSNxEdfNJ570k4TOhMzsWfx5cy593y5mbm5A7gMaWs23GoKNuuY2NscOdmJocidRB4uZTTdk4GyIqh9AJoAJS6HMGww3NugzQJ9zJ+sQ6hKh8dgVKjr0IV0u9BJGAgmoRqMCnW8OmlZ6DToNceAJKMynbpAP1aaSwndGbp7uV49j0p083dfm9AWHl6n5qZwP3Es92MV7n1aRvTQvasKidV5xJXSYaxCXLkKSyeSJsiQVaA6zrBE8s19Vl46WdctCR3g12hl2xhW5wtcsPLSiaXzr+R1j2qAQDfD6nyMW1ZeOqH/A0V1BuqyJlYn7QmG4DUZBZ3hgrxEKtVmE4AaaUng3uDEx+ZwX8ZzeuogGKKvNhs5okPaWcLhZsjbTQ8GPtOhJ3DDYcChHDQgL2KGnrRdqDOLdmXn2ngS2q9Amp60fbjUvQPOG6/E/8Q+XOqO4+wpe1G+6lKXvb9f36BrvUiR39nudMoStPgGXUHBnY7OBj9v9DVD21qy+sm6uaMUJ6gSzJUI8ohtN1tooAOxKNytjrZdVVwqELCu4nv33URN58tPDdHA7qh82wg0vilQi5QOjvLD09IOpdfwJbyJy+dG0TOJyaJmqf7PlqICX8jhA0/B2DgmKr60v/sS6r9dmRIvk3/GEJRdYNhPFboYDgHl6ej4aQDDQZfwpmMYxfNJDNvPKKrSuf4EjKKydeFrn1Z2JOVGQ377PmFVHZNrOlQw7Kf40vZ1h+r47wGAYT/BpU7pz4XAsNcN99jXHasj8zTEcJxS/kASaAPjgWiSzfJb7WamQBuA0SWGvaZg5REHPkMgcNWQB1NfGipXiU56E3TlEcMYRhxfg2V+S100YoIRQxTB/9Wtf2tMZR09lk2Jw8/G8Ln6q7kpMtp6KXm6TqGzg+Mvis7qp2mmgb8AAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?R%5E2_%7B%5Ctext%7Bpred%7D%7D%20=%201%20-%20%5Cfrac%7B%5Ctext%7BPRESS%7D%7D%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D%7D"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-4-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-5-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-6-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-12-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/m-fatih-tuzen/",
          "text": "M. Fatih T√ºzen"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392184 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Explained vs.¬†Predictive Power: R¬≤, Adjusted R¬≤, and Beyond</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">April 29, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/m-fatih-tuzen/\">M. Fatih T√ºzen</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/\"> A Statistician's R Notebook</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div class=\"quarto-figure quarto-figure-center\">\n<figure class=\"figure\">\n<p><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/quote_box.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" style=\"width:8cm\"/><noscript><img class=\"img-fluid quarto-figure quarto-figure-center figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/quote_box.png?w=578&amp;ssl=1\" style=\"width:8cm\"/></noscript></p>\n</figure>\n</div>\n<section class=\"level1\" data-number=\"1\" id=\"introduction\">\n<h1 data-number=\"1\"><span class=\"header-section-number\">1</span> Introduction</h1>\n<blockquote class=\"blockquote\">\n<p><strong>You trust R¬≤. Should you?</strong><br/>\nYou proudly present a model with R¬≤ = 0.95. Everyone applauds.<br/>\nBut what if your model fails miserably on the next new data?</p>\n</blockquote>\n<p>When building a statistical model, one of the first numbers analysts and data scientists often cite is the <strong>R¬≤</strong>, or coefficient of determination. It‚Äôs widely reported in research, academic theses, and industry reports ‚Äî and yet, frequently misunderstood or misused.</p>\n<p>Does a high R¬≤ mean your model is good? Is it enough to evaluate model performance? What about its adjusted or predictive counterparts?</p>\n<p>This article will explore in depth: ‚Äì What R¬≤, Adjusted R¬≤, and Predicted R¬≤ actually mean ‚Äì Why relying solely on R¬≤ can mislead you ‚Äì How to evaluate models using <strong>both explanatory and predictive power</strong> ‚Äì Real-life implementation using the <strong>{tidymodels}</strong> framework in R</p>\n<p>We‚Äôll also discuss best practices and common pitfalls, and equip you with a mindset to look beyond surface-level model summaries.</p>\n</section>\n<section class=\"level1\" data-number=\"2\" id=\"theoretical-background\">\n<h1 data-number=\"2\"><span class=\"header-section-number\">2</span> Theoretical Background</h1>\n<section class=\"level2\" data-number=\"2.1\" id=\"what-is-r¬≤\">\n<h2 class=\"anchored\" data-anchor-id=\"what-is-r¬≤\" data-number=\"2.1\"><span class=\"header-section-number\">2.1</span> What is R¬≤?</h2>\n<p>The <strong>coefficient of determination</strong>, R¬≤, is defined as:</p>\n<p><img data-lazy-src=\"https://latex.codecogs.com/png.latex?R%5E2%20=%201%20-%20%5Cfrac%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Bres%7D%7D%7D%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D%7D\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?R%5E2%20=%201%20-%20%5Cfrac%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Bres%7D%7D%7D%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D%7D\"/></noscript></p>\n<p>Where:</p>\n<ul>\n<li><p><img data-lazy-src=\"https://latex.codecogs.com/png.latex?%5Ctext%7BSS%7D_%7B%5Ctext%7Bres%7D%7D\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%5Ctext%7BSS%7D_%7B%5Ctext%7Bres%7D%7D\"/></noscript> = Sum of squares of residuals = <img data-lazy-src=\"https://latex.codecogs.com/png.latex?%5Csum%20(y_i%20-%20%5Chat%7By%7D_i)%5E2\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%5Csum%20(y_i%20-%20%5Chat%7By%7D_i)%5E2\"/></noscript></p></li>\n<li><p><img data-lazy-src=\"https://latex.codecogs.com/png.latex?%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D\"/></noscript> = Total sum of squares = <img data-lazy-src=\"https://latex.codecogs.com/png.latex?%5Csum%20(y_i%20-%20%5Cbar%7By%7D)%5E2\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%5Csum%20(y_i%20-%20%5Cbar%7By%7D)%5E2\"/></noscript></p></li>\n</ul>\n<p>It tells us the <strong>proportion of variance explained by the model</strong>. An R¬≤ of 0.80 implies that 80% of the variability in the dependent variable is explained by the model.</p>\n<p>But beware ‚Äî it only measures <strong>fit to training data</strong>, not the model‚Äôs ability to <strong>generalize</strong>.</p>\n</section>\n<section class=\"level2\" data-number=\"2.2\" id=\"adjusted-r¬≤\">\n<h2 class=\"anchored\" data-anchor-id=\"adjusted-r¬≤\" data-number=\"2.2\"><span class=\"header-section-number\">2.2</span> Adjusted R¬≤</h2>\n<p>When we add predictors to a regression model, R¬≤ will never decrease ‚Äî even if the added variables are irrelevant.</p>\n<p><strong>Adjusted R¬≤</strong> corrects this by penalizing the number of predictors: <img data-lazy-src=\"https://latex.codecogs.com/png.latex?R%5E2_%7B%5Ctext%7Badj%7D%7D%20=%201%20-%20%5Cleft(1%20-%20R%5E2%5Cright)%20%5Ccdot%20%5Cleft(%5Cfrac%7Bn%20-%201%7D%7Bn%20-%20p%20-%201%7D%5Cright)\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?R%5E2_%7B%5Ctext%7Badj%7D%7D%20=%201%20-%20%5Cleft(1%20-%20R%5E2%5Cright)%20%5Ccdot%20%5Cleft(%5Cfrac%7Bn%20-%201%7D%7Bn%20-%20p%20-%201%7D%5Cright)\"/></noscript></p>\n<p>Where:</p>\n<ul>\n<li><p>n : number of observations</p></li>\n<li><p>p : number of predictors</p></li>\n</ul>\n<p>Thus, Adjusted R¬≤ will <strong>only increase</strong> if the new predictor improves the model more than expected by chance.</p>\n</section>\n<section class=\"level2\" data-number=\"2.3\" id=\"predicted-r¬≤\">\n<h2 class=\"anchored\" data-anchor-id=\"predicted-r¬≤\" data-number=\"2.3\"><span class=\"header-section-number\">2.3</span> Predicted R¬≤</h2>\n<p><strong>Predicted R¬≤</strong> (or cross-validated R¬≤) is the most honest estimate of model utility. It answers the question:</p>\n<blockquote class=\"blockquote\">\n<p><em>How well will this model predict new, unseen data?</em></p>\n</blockquote>\n<p>This is typically calculated using cross-validation, and unlike regular R¬≤, it reflects <strong>out-of-sample performance</strong>.</p>\n<p>You can also view it as:</p>\n<p><img data-lazy-src=\"https://latex.codecogs.com/png.latex?R%5E2_%7B%5Ctext%7Bpred%7D%7D%20=%201%20-%20%5Cfrac%7B%5Ctext%7BPRESS%7D%7D%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D%7D\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?R%5E2_%7B%5Ctext%7Bpred%7D%7D%20=%201%20-%20%5Cfrac%7B%5Ctext%7BPRESS%7D%7D%7B%5Ctext%7BSS%7D_%7B%5Ctext%7Btot%7D%7D%7D\"/></noscript></p>\n<p>Where PRESS is the <strong>Prediction Error Sum of Squares</strong> based on cross-validation.</p>\n</section>\n</section>\n<section class=\"level1\" data-number=\"3\" id=\"dataset-overview\">\n<h1 data-number=\"3\"><span class=\"header-section-number\">3</span> Dataset Overview</h1>\n<p>We‚Äôll use the classic <strong>Boston Housing Dataset</strong> to demonstrate. It includes:</p>\n<ul>\n<li><p>Socio-economic and housing variables for 506 Boston suburbs</p></li>\n<li><p>Target: <code>medv</code> (median value of owner-occupied homes in $1000s)</p></li>\n</ul>\n<p>Below are the key variables:</p>\n<ul>\n<li><strong>crim</strong>: per capita crime rate by town</li>\n<li><strong>zn</strong>: proportion of residential land zoned for large lots</li>\n<li><strong>indus</strong>: proportion of non-retail business acres</li>\n<li><strong>chas</strong>: Charles River dummy variable (1 = tract bounds river; 0 = otherwise)</li>\n<li><strong>nox</strong>: nitric oxides concentration (parts per 10 million)</li>\n<li><strong>rm</strong>: average number of rooms per dwelling</li>\n<li><strong>age</strong>: proportion of owner-occupied units built before 1940</li>\n<li><strong>dis</strong>: weighted distance to employment centers</li>\n<li><strong>rad</strong>: index of accessibility to radial highways</li>\n<li><strong>tax</strong>: property-tax rate per $10,000</li>\n<li><strong>ptratio</strong>: pupil-teacher ratio by town</li>\n<li><strong>black</strong>: 1000(Bk ‚Äì 0.63)^2 where Bk is the proportion of Black residents</li>\n<li><strong>lstat</strong>: percentage of lower status of the population</li>\n<li><strong>medv</strong>: <strong>target</strong> ‚Äî median value of owner-occupied homes (in $1000s)</li>\n</ul>\n<p>This regression problem mimics common real estate or socio-economic modeling use cases. Let‚Äôs first examine the dataset‚Äôs summary statistics.</p>\n<div class=\"cell\">\n<pre>library(tidymodels)\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(patchwork)\n\n\nboston &lt;- MASS::Boston\nskim(boston)</pre>\n<div class=\"cell-output-display\">\n<table class=\"caption-top table table-sm table-striped small\">\n<caption>Data summary</caption>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Name</td>\n<td style=\"text-align: left;\">boston</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Number of rows</td>\n<td style=\"text-align: left;\">506</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Number of columns</td>\n<td style=\"text-align: left;\">14</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">_______________________</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Column type frequency:</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">numeric</td>\n<td style=\"text-align: left;\">14</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">________________________</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Group variables</td>\n<td style=\"text-align: left;\">None</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Variable type: numeric</strong></p>\n<table class=\"caption-top table table-sm table-striped small\">\n<colgroup>\n<col style=\"width: 15%\"/>\n<col style=\"width: 10%\"/>\n<col style=\"width: 15%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 6%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">skim_variable</th>\n<th style=\"text-align: right;\">n_missing</th>\n<th style=\"text-align: right;\">complete_rate</th>\n<th style=\"text-align: right;\">mean</th>\n<th style=\"text-align: right;\">sd</th>\n<th style=\"text-align: right;\">p0</th>\n<th style=\"text-align: right;\">p25</th>\n<th style=\"text-align: right;\">p50</th>\n<th style=\"text-align: right;\">p75</th>\n<th style=\"text-align: right;\">p100</th>\n<th style=\"text-align: left;\">hist</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">crim</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">3.61</td>\n<td style=\"text-align: right;\">8.60</td>\n<td style=\"text-align: right;\">0.01</td>\n<td style=\"text-align: right;\">0.08</td>\n<td style=\"text-align: right;\">0.26</td>\n<td style=\"text-align: right;\">3.68</td>\n<td style=\"text-align: right;\">88.98</td>\n<td style=\"text-align: left;\">‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">zn</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">11.36</td>\n<td style=\"text-align: right;\">23.32</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">12.50</td>\n<td style=\"text-align: right;\">100.00</td>\n<td style=\"text-align: left;\">‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">indus</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">11.14</td>\n<td style=\"text-align: right;\">6.86</td>\n<td style=\"text-align: right;\">0.46</td>\n<td style=\"text-align: right;\">5.19</td>\n<td style=\"text-align: right;\">9.69</td>\n<td style=\"text-align: right;\">18.10</td>\n<td style=\"text-align: right;\">27.74</td>\n<td style=\"text-align: left;\">‚ñá‚ñÜ‚ñÅ‚ñá‚ñÅ</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">chas</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">0.07</td>\n<td style=\"text-align: right;\">0.25</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">0.00</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: left;\">‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">nox</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">0.55</td>\n<td style=\"text-align: right;\">0.12</td>\n<td style=\"text-align: right;\">0.38</td>\n<td style=\"text-align: right;\">0.45</td>\n<td style=\"text-align: right;\">0.54</td>\n<td style=\"text-align: right;\">0.62</td>\n<td style=\"text-align: right;\">0.87</td>\n<td style=\"text-align: left;\">‚ñá‚ñá‚ñÜ‚ñÖ‚ñÅ</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">rm</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">6.28</td>\n<td style=\"text-align: right;\">0.70</td>\n<td style=\"text-align: right;\">3.56</td>\n<td style=\"text-align: right;\">5.89</td>\n<td style=\"text-align: right;\">6.21</td>\n<td style=\"text-align: right;\">6.62</td>\n<td style=\"text-align: right;\">8.78</td>\n<td style=\"text-align: left;\">‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÅ</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">age</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">68.57</td>\n<td style=\"text-align: right;\">28.15</td>\n<td style=\"text-align: right;\">2.90</td>\n<td style=\"text-align: right;\">45.02</td>\n<td style=\"text-align: right;\">77.50</td>\n<td style=\"text-align: right;\">94.07</td>\n<td style=\"text-align: right;\">100.00</td>\n<td style=\"text-align: left;\">‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">dis</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">3.80</td>\n<td style=\"text-align: right;\">2.11</td>\n<td style=\"text-align: right;\">1.13</td>\n<td style=\"text-align: right;\">2.10</td>\n<td style=\"text-align: right;\">3.21</td>\n<td style=\"text-align: right;\">5.19</td>\n<td style=\"text-align: right;\">12.13</td>\n<td style=\"text-align: left;\">‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">rad</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">9.55</td>\n<td style=\"text-align: right;\">8.71</td>\n<td style=\"text-align: right;\">1.00</td>\n<td style=\"text-align: right;\">4.00</td>\n<td style=\"text-align: right;\">5.00</td>\n<td style=\"text-align: right;\">24.00</td>\n<td style=\"text-align: right;\">24.00</td>\n<td style=\"text-align: left;\">‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÉ</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">tax</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">408.24</td>\n<td style=\"text-align: right;\">168.54</td>\n<td style=\"text-align: right;\">187.00</td>\n<td style=\"text-align: right;\">279.00</td>\n<td style=\"text-align: right;\">330.00</td>\n<td style=\"text-align: right;\">666.00</td>\n<td style=\"text-align: right;\">711.00</td>\n<td style=\"text-align: left;\">‚ñá‚ñá‚ñÉ‚ñÅ‚ñá</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">ptratio</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">18.46</td>\n<td style=\"text-align: right;\">2.16</td>\n<td style=\"text-align: right;\">12.60</td>\n<td style=\"text-align: right;\">17.40</td>\n<td style=\"text-align: right;\">19.05</td>\n<td style=\"text-align: right;\">20.20</td>\n<td style=\"text-align: right;\">22.00</td>\n<td style=\"text-align: left;\">‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">black</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">356.67</td>\n<td style=\"text-align: right;\">91.29</td>\n<td style=\"text-align: right;\">0.32</td>\n<td style=\"text-align: right;\">375.38</td>\n<td style=\"text-align: right;\">391.44</td>\n<td style=\"text-align: right;\">396.22</td>\n<td style=\"text-align: right;\">396.90</td>\n<td style=\"text-align: left;\">‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">lstat</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">12.65</td>\n<td style=\"text-align: right;\">7.14</td>\n<td style=\"text-align: right;\">1.73</td>\n<td style=\"text-align: right;\">6.95</td>\n<td style=\"text-align: right;\">11.36</td>\n<td style=\"text-align: right;\">16.96</td>\n<td style=\"text-align: right;\">37.97</td>\n<td style=\"text-align: left;\">‚ñá‚ñá‚ñÖ‚ñÇ‚ñÅ</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">medv</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: right;\">22.53</td>\n<td style=\"text-align: right;\">9.20</td>\n<td style=\"text-align: right;\">5.00</td>\n<td style=\"text-align: right;\">17.02</td>\n<td style=\"text-align: right;\">21.20</td>\n<td style=\"text-align: right;\">25.00</td>\n<td style=\"text-align: right;\">50.00</td>\n<td style=\"text-align: left;\">‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÅ</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<p><strong>Commentary:</strong></p>\n<ul>\n<li><p>Variables like <code>crim</code>, <code>tax</code>, and <code>lstat</code> exhibit high variability and potential skewness.</p></li>\n<li><p><code>chas</code> is binary and acts like a categorical indicator.</p></li>\n<li><p>The target variable <code>medv</code> ranges from $5,000 to $50,000 (capped).</p></li>\n<li><p><code>rm</code> (average number of rooms) and <code>lstat</code> (lower status population) show notable spread and will likely play strong roles in the model.</p></li>\n</ul>\n<p>Next, we examine correlations with <code>medv</code>:</p>\n<div class=\"cell\">\n<pre>boston %&gt;% correlate() %&gt;% corrr::focus(medv) %&gt;% arrange(desc(medv))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 13 √ó 2\n   term      medv\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 rm       0.695\n 2 zn       0.360\n 3 black    0.333\n 4 dis      0.250\n 5 chas     0.175\n 6 age     -0.377\n 7 rad     -0.382\n 8 crim    -0.388\n 9 nox     -0.427\n10 tax     -0.469\n11 indus   -0.484\n12 ptratio -0.508\n13 lstat   -0.738</pre>\n</div>\n</div>\n<p><strong>Interpretation of Correlations:</strong></p>\n<ul>\n<li><p><code>rm</code> shows a <strong>strong positive</strong> correlation with <code>medv</code> ‚Äî more rooms generally imply higher value.</p></li>\n<li><p><code>lstat</code> and <code>crim</code> have <strong>strong negative</strong> correlations ‚Äî as lower status or crime increases, housing values drop.</p></li>\n<li><p><code>nox</code>, <code>age</code>, and <code>ptratio</code> also show negative correlations with price, hinting at socio-environmental effects.</p></li>\n</ul>\n<p>These insights will guide us in building and evaluating our model.</p>\n</section>\n<section class=\"level1\" data-number=\"4\" id=\"exploratory-data-analysis\">\n<h1 data-number=\"4\"><span class=\"header-section-number\">4</span> Exploratory Data Analysis</h1>\n<p>Let‚Äôs visualize some of the most influential variables in relation to <code>medv</code>, our target variable. These exploratory graphs help reveal potential linear or nonlinear relationships, outliers, or the need for transformation.</p>\n<div class=\"cell\">\n<pre># Define individual plots with improved formatting for Quarto rendering\np1 &lt;- ggplot(boston, aes(rm, medv)) +\n  geom_point(alpha = 0.5, color = \"#2c7fb8\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Rooms\\nvs. Median Value\",\n    x = \"Average Number of Rooms (rm)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np2 &lt;- ggplot(boston, aes(lstat, medv)) +\n  geom_point(alpha = 0.5, color = \"#de2d26\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Lower Status %\\nvs. Median Value\",\n    x = \"% Lower Status Population (lstat)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np3 &lt;- ggplot(boston, aes(nox, medv)) +\n  geom_point(alpha = 0.5, color = \"#31a354\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"NOx Concentration\\nvs. Median Value\",\n    x = \"NOx concentration (ppm)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np4 &lt;- ggplot(boston, aes(age, medv)) +\n  geom_point(alpha = 0.5, color = \"#ff7f00\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Old Homes %\\nvs. Median Value\",\n    x = \"% Homes Built Before 1940 (age)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np5 &lt;- ggplot(boston, aes(tax, medv)) +\n  geom_point(alpha = 0.5, color = \"#6a3d9a\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Tax Rate\\nvs. Median Value\",\n    x = \"Tax Rate (per $10,000)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np6 &lt;- ggplot(boston, aes(dis, medv)) +\n  geom_point(alpha = 0.5, color = \"#1f78b4\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Distance to Jobs\\nvs. Median Value\",\n    x = \"Weighted Distance to Employment Centers (dis)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))</pre>\n</div>\n<div class=\"cell\">\n<pre>(p1 | p2) + plot_layout(guides = 'collect')</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-4-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-4-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<ul>\n<li><strong>Rooms (<code>rm</code>)</strong>: Strong positive linear relationship with <code>medv</code>. More rooms correlate with higher home values.</li>\n<li><strong>Lower Status Population (<code>lstat</code>)</strong>: Strong nonlinear inverse relation. Poorer areas tend to have significantly lower housing values.</li>\n</ul>\n<div class=\"cell\">\n<pre>(p3 | p4) + plot_layout(guides = 'collect')</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-5-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-5-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<ul>\n<li><strong>Nitric Oxide (<code>nox</code>)</strong>: Moderate negative relationship ‚Äî environmental factors like pollution impact price.</li>\n<li><strong>Old Homes (<code>age</code>)</strong>: Slight negative trend ‚Äî older areas may have reduced appeal or value.</li>\n</ul>\n<div class=\"cell\">\n<pre>(p5 | p6) + plot_layout(guides = 'collect')</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<ul>\n<li><strong>Tax Rate (<code>tax</code>)</strong>: Higher taxes often relate to lower housing value, possibly due to location or socio-economic constraints.</li>\n<li><strong>Distance to Employment Centers (<code>dis</code>)</strong>: Weak to moderate positive correlation. Suburban or well-connected areas might command higher value.</li>\n</ul>\n<p>These six plots combine both socioeconomic and environmental dimensions of housing value ‚Äî providing both intuition and modeling direction.</p>\n</section>\n<section class=\"level1\" data-number=\"5\" id=\"modeling-with-tidymodels\">\n<h1 data-number=\"5\"><span class=\"header-section-number\">5</span> Modeling with Tidymodels</h1>\n<p>Now that we‚Äôve explored the data, it‚Äôs time to fit a model using the <strong>tidymodels</strong> framework. We‚Äôll use a simple linear regression to predict <code>medv</code>, the median home value.</p>\n<section class=\"level2\" data-number=\"5.1\" id=\"data-splitting-and-preprocessing\">\n<h2 class=\"anchored\" data-anchor-id=\"data-splitting-and-preprocessing\" data-number=\"5.1\"><span class=\"header-section-number\">5.1</span> Data Splitting and Preprocessing</h2>\n<p>We begin by splitting the dataset into training and testing sets. The training set will be used to fit the model, and the test set will evaluate its generalization performance.</p>\n<div class=\"cell\">\n<pre>set.seed(42)\nsplit &lt;- initial_split(boston, prop = 0.8)\ntrain &lt;- training(split)\ntest &lt;- testing(split)\n\nrec &lt;- recipe(medv ~ ., data = train)\nmodel &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nworkflow &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% add_model(model)</pre>\n</div>\n</section>\n<section class=\"level2\" data-number=\"5.2\" id=\"model-fitting\">\n<h2 class=\"anchored\" data-anchor-id=\"model-fitting\" data-number=\"5.2\"><span class=\"header-section-number\">5.2</span> Model Fitting</h2>\n<p>We now fit the model to the training data:</p>\n<div class=\"cell\">\n<pre>fit &lt;- fit(workflow, data = train)</pre>\n</div>\n</section>\n<section class=\"level2\" data-number=\"5.3\" id=\"evaluating-the-model-on-the-training-set\">\n<h2 class=\"anchored\" data-anchor-id=\"evaluating-the-model-on-the-training-set\" data-number=\"5.3\"><span class=\"header-section-number\">5.3</span> Evaluating the Model on the Training Set</h2>\n<p>Let‚Äôs extract the R¬≤ and Adjusted R¬≤ values from the fitted model:</p>\n<div class=\"cell\">\n<pre>training_summary &lt;- glance(extract_fit_parsnip(fit))\ntraining_summary %&gt;% dplyr::select(r.squared, adj.r.squared)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.726         0.717</pre>\n</div>\n</div>\n<p><strong>üîç Interpretation:</strong></p>\n<ul>\n<li><strong>R¬≤</strong> measures the proportion of variance in <code>medv</code> explained by the predictors in the training set.</li>\n<li><strong>Adjusted R¬≤</strong> adjusts this value by penalizing for the number of predictors, making it more reliable in multi-variable contexts.</li>\n</ul>\n<p>If R¬≤ and Adjusted R¬≤ differ significantly, it indicates that some predictors may not be contributing meaningfully to the model.</p>\n<blockquote class=\"blockquote\">\n<p>Example: A model with 12 predictors might show R¬≤ = 0.76, but Adjusted R¬≤ = 0.72 ‚Äî suggesting some predictors are adding complexity without real explanatory power.</p>\n</blockquote>\n</section>\n<section class=\"level2\" data-number=\"5.4\" id=\"test-set-performance\">\n<h2 class=\"anchored\" data-anchor-id=\"test-set-performance\" data-number=\"5.4\"><span class=\"header-section-number\">5.4</span> Test Set Performance</h2>\n<p>Now we assess the model on the unseen test data:</p>\n<div class=\"cell\">\n<pre>preds &lt;- predict(fit, test) %&gt;% bind_cols(test)\nmetrics(preds, truth = medv, estimate = .pred)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       4.79 \n2 rsq     standard       0.784\n3 mae     standard       3.32 </pre>\n</div>\n</div>\n<p><strong>üìâ Interpretation:</strong></p>\n<ul>\n<li>If <strong>test R¬≤</strong> is <strong>much lower</strong> than training R¬≤, overfitting may be present.</li>\n<li>If <strong>test RMSE</strong> is high, the model‚Äôs absolute prediction error is large ‚Äî another sign of poor generalization.</li>\n</ul>\n</section>\n<section class=\"level2\" data-number=\"5.5\" id=\"cross-validation-for-predicted-r¬≤\">\n<h2 class=\"anchored\" data-anchor-id=\"cross-validation-for-predicted-r¬≤\" data-number=\"5.5\"><span class=\"header-section-number\">5.5</span> Cross-Validation for Predicted R¬≤</h2>\n<p>To get a more robust performance estimate, we use 10-fold cross-validation:</p>\n<div class=\"cell\">\n<pre>set.seed(42)\ncv &lt;- vfold_cv(train, v = 10)\nresample &lt;- fit_resamples(\n  workflow,\n  resamples = cv,\n  metrics = metric_set(rsq, rmse),\n  control = control_resamples(save_pred = TRUE)\n)\ncollect_metrics(resample)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.79     10  0.384  Preprocessor1_Model1\n2 rsq     standard   0.712    10  0.0341 Preprocessor1_Model1</pre>\n</div>\n</div>\n<p><strong>‚úÖ Interpretation:</strong></p>\n<ul>\n<li><strong>Predicted R¬≤ (via CV)</strong> tells us how well the model would perform on unseen data across multiple resamples.</li>\n<li>It typically lies between training R¬≤ and test R¬≤.</li>\n<li>Consistency between cross-validated and test R¬≤ implies a stable model.</li>\n</ul>\n<div class=\"callout callout-style-default callout-tip callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nTip\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>Use cross-validation as a standard evaluation tool, especially when data is limited.</p>\n</div>\n</div>\n<p><strong>üí¨ Summary of Findings:</strong></p>\n<ul>\n<li>Our linear model explains a good portion of the variance, but some predictors might be irrelevant or redundant.</li>\n<li>Cross-validation confirms the model is relatively stable but leaves room for refinement ‚Äî possibly through feature selection or nonlinear modeling.</li>\n</ul>\n<p>In the next step, we can analyze residuals or explore model improvements such as polynomial terms or regularization.</p>\n</section>\n<section class=\"level2\" data-number=\"5.6\" id=\"residual-diagnostics\">\n<h2 class=\"anchored\" data-anchor-id=\"residual-diagnostics\" data-number=\"5.6\"><span class=\"header-section-number\">5.6</span> Residual Diagnostics</h2>\n<p>Let‚Äôs now check if our linear model satisfies basic regression assumptions. We‚Äôll plot residuals and assess patterns, non-linearity, and potential heteroskedasticity.</p>\n<div class=\"cell\">\n<pre>library(broom)\nlibrary(ggthemes)\n\naug &lt;- augment(fit$fit$fit$fit)\n\nggplot(aug, aes(.fitted, .resid)) +\n  geom_point(alpha = 0.5, color = \"#2c7fb8\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/index_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p><strong>üìå Interpretation:</strong></p>\n<ul>\n<li>We want residuals to be randomly scattered around zero.</li>\n<li>If there‚Äôs a pattern or funnel shape, that may indicate <strong>non-linearity</strong> or <strong>heteroskedasticity</strong>.</li>\n</ul>\n</section>\n<section class=\"level2\" data-number=\"5.7\" id=\"improving-the-model-transforming-lstat\">\n<h2 class=\"anchored\" data-anchor-id=\"improving-the-model-transforming-lstat\" data-number=\"5.7\"><span class=\"header-section-number\">5.7</span> Improving the Model: Transforming <code>lstat</code></h2>\n<p>From our earlier EDA, we saw a strong <strong>nonlinear relationship</strong> between <code>lstat</code> (lower status %) and <code>medv</code>. Let‚Äôs try <strong>log-transforming</strong> <code>lstat</code> to capture that curvature.</p>\n<section class=\"level3\" data-number=\"5.7.1\" id=\"updated-recipe-with-transformation\">\n<h3 class=\"anchored\" data-anchor-id=\"updated-recipe-with-transformation\" data-number=\"5.7.1\"><span class=\"header-section-number\">5.7.1</span> Updated Recipe with Transformation</h3>\n<div class=\"cell\">\n<pre>rec_log &lt;- recipe(medv ~ ., data = train) %&gt;%\n  step_log(lstat)\n\nworkflow_log &lt;- workflow() %&gt;%\n  add_model(model) %&gt;%\n  add_recipe(rec_log)\n\nfit_log &lt;- fit(workflow_log, data = train)</pre>\n</div>\n</section>\n<section class=\"level3\" data-number=\"5.7.2\" id=\"evaluation-of-transformed-model\">\n<h3 class=\"anchored\" data-anchor-id=\"evaluation-of-transformed-model\" data-number=\"5.7.2\"><span class=\"header-section-number\">5.7.2</span> Evaluation of Transformed Model</h3>\n<div class=\"cell\">\n<pre>preds_log &lt;- predict(fit_log, test) %&gt;% bind_cols(test)\nmetrics(preds_log, truth = medv, estimate = .pred)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       4.43 \n2 rsq     standard       0.815\n3 mae     standard       3.16 </pre>\n</div>\n</div>\n<div class=\"cell\">\n<pre>glance(fit_log)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.785         0.778  4.21      110. 2.64e-121    13 -1147. 2324. 2384.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</pre>\n</div>\n</div>\n<p><strong>üß† Interpretation:</strong></p>\n<ul>\n<li>Compare RMSE and R¬≤ from the transformed model to the original.</li>\n<li>If we see improvement, the transformation helped capture underlying nonlinearity.</li>\n<li><strong>Adjusted R¬≤</strong> is especially helpful here to assess whether the transformation truly improved fit ‚Äî not just overfit.</li>\n</ul>\n<div class=\"callout callout-style-default callout-tip callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nTip\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>Transformations, polynomial terms, and splines are all valid strategies to improve linear models without abandoning interpretability.</p>\n</div>\n</div>\n<p>With residuals checked and a transformation tested, our next step could be to explore <strong>regularized models</strong> like ridge or lasso regression, or even move beyond linearity with <strong>tree-based</strong> models.</p>\n</section>\n</section>\n</section>\n<section class=\"level1\" data-number=\"6\" id=\"common-pitfalls-and-misconceptions\">\n<h1 data-number=\"6\"><span class=\"header-section-number\">6</span> Common Pitfalls and Misconceptions</h1>\n<p>Even though R¬≤ is widely reported and intuitively appealing, its interpretation is often flawed ‚Äî even by experienced analysts. Here, we‚Äôll go beyond textbook definitions and highlight real-world traps and misunderstandings related to R¬≤ and its variants.</p>\n<p><strong>üö´ Misconception 1: High R¬≤ means the model is good</strong></p>\n<ul>\n<li>A model with R¬≤ = 0.95 may <strong>look impressive</strong>, but that doesn‚Äôt guarantee predictive power.</li>\n<li>High R¬≤ can result from overfitting, especially when the model is complex or contains many predictors.</li>\n<li><strong>Adjusted R¬≤ and Predicted R¬≤</strong> must be considered to evaluate true usefulness.</li>\n</ul>\n<p><strong>‚ö†Ô∏è Misconception 2: Adding predictors always improves the model</strong></p>\n<ul>\n<li>While R¬≤ never decreases with more variables, <strong>Adjusted R¬≤ can</strong> ‚Äî and should ‚Äî if the new variable doesn‚Äôt add real value.</li>\n<li>Including irrelevant predictors increases complexity without improving explanatory power.</li>\n<li>This is a form of <strong>dimensional overfitting</strong>.</li>\n</ul>\n<p><strong>‚ùå Misconception 3: R¬≤ indicates causality</strong></p>\n<ul>\n<li>R¬≤ quantifies correlation, <strong>not causation</strong>.</li>\n<li>A high R¬≤ can arise from spurious relationships or confounding variables.</li>\n<li>Always supplement with <strong>domain knowledge</strong> and causal reasoning.</li>\n</ul>\n<p><strong>üìâ Misconception 4: R¬≤ is a universal performance metric</strong></p>\n<ul>\n<li>R¬≤ only applies to <strong>regression tasks</strong>. Using it for classification models is inappropriate and meaningless.</li>\n<li>For binary classification, use metrics like <strong>AUC</strong>, <strong>accuracy</strong>, <strong>precision</strong>, and <strong>recall</strong>.</li>\n</ul>\n<p><strong>üîç Misconception 5: Residual plots don‚Äôt matter if R¬≤ is high</strong></p>\n<ul>\n<li>A good R¬≤ doesn‚Äôt guarantee that model assumptions are met.</li>\n<li>Residual patterns may still reveal <strong>non-linearity</strong>, <strong>heteroskedasticity</strong>, or <strong>influential outliers</strong>.</li>\n<li>Always inspect residual diagnostics.</li>\n</ul>\n<p><strong>üí° Misconception 6: Predicted R¬≤ isn‚Äôt necessary</strong></p>\n<ul>\n<li>Many practitioners report R¬≤ and Adjusted R¬≤, but <strong>omit cross-validation entirely</strong>.</li>\n<li><strong>Predicted R¬≤</strong> (e.g., via 10-fold CV) is the <strong>most honest measure</strong> of model generalizability.</li>\n</ul>\n<p><strong>üî¨ Misconception 7: R¬≤ has a fixed interpretation</strong></p>\n<ul>\n<li>R¬≤ values <strong>depend on the context</strong>. In social sciences, an R¬≤ of 0.3 can be meaningful, while in physics we expect 0.99+.</li>\n<li>A ‚Äúlow‚Äù R¬≤ doesn‚Äôt mean the model is useless ‚Äî it may reflect inherent variability in human behavior or macroeconomic data.</li>\n</ul>\n<hr/>\n<blockquote class=\"blockquote\">\n<p><strong>Insight:</strong> Always use R¬≤ in context ‚Äî alongside other metrics, validation strategies, and graphical checks.</p>\n</blockquote>\n<p>For a deeper dive into R¬≤ misconceptions and proper regression diagnostics, see:</p>\n<ul>\n<li><p>Harrell, F. (2015). <em>Regression Modeling Strategies</em>. Springer.</p></li>\n<li><p>Gelman &amp; Hill (2006). <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>.</p></li>\n<li><p>Burnham &amp; Anderson (2002). <em>Model Selection and Multimodel Inference</em>.</p></li>\n<li><p>Kutner et al.¬†(2004). <em>Applied Linear Regression Models</em>.</p></li>\n</ul>\n<p>Together, these references build the foundation for <strong>responsible model interpretation</strong>.</p>\n</section>\n<section class=\"level1\" data-number=\"7\" id=\"conclusion-recommendations\">\n<h1 data-number=\"7\"><span class=\"header-section-number\">7</span> Conclusion &amp; Recommendations</h1>\n<section class=\"level2\" data-number=\"7.1\" id=\"summary\">\n<h2 class=\"anchored\" data-anchor-id=\"summary\" data-number=\"7.1\"><span class=\"header-section-number\">7.1</span> üìå Summary</h2>\n<p>In this post, we explored <strong>R¬≤</strong>, <strong>Adjusted R¬≤</strong>, and <strong>Predicted R¬≤</strong> in depth ‚Äî not just as mathematical constructs, but as tools for critical thinking in modeling. We walked through theory, practical application in R with tidymodels, residual diagnostics, and even model improvement through transformation.</p>\n<p>Let‚Äôs recap: - <strong>R¬≤</strong> tells us how well our model fits the training data, but can be misleading on its own. - <strong>Adjusted R¬≤</strong> improves upon R¬≤ by accounting for model complexity. - <strong>Predicted R¬≤</strong>, evaluated via cross-validation, provides the most trustworthy estimate of real-world performance.</p>\n<p>High R¬≤ values can be seductive. But as we saw, <strong>they don‚Äôt guarantee causality, generalizability, or correctness</strong>. Only by combining R¬≤ with residual diagnostics, domain knowledge, and out-of-sample validation can we judge a model responsibly.</p>\n</section>\n<section class=\"level2\" data-number=\"7.2\" id=\"recommendations-for-practitioners\">\n<h2 class=\"anchored\" data-anchor-id=\"recommendations-for-practitioners\" data-number=\"7.2\"><span class=\"header-section-number\">7.2</span> üí° Recommendations for Practitioners</h2>\n<ol type=\"1\">\n<li><strong>Always accompany R¬≤ with Adjusted and Predicted R¬≤</strong> ‚Äî never rely on one metric alone.</li>\n<li><strong>Perform residual diagnostics</strong> to check linearity, variance assumptions, and outlier influence.</li>\n<li><strong>Use cross-validation (e.g., 10-fold)</strong> as a default evaluation strategy, especially when the dataset is not large.</li>\n<li><strong>Transform nonlinear predictors</strong> (as we did with <code>lstat</code>) or use flexible models (e.g., splines, GAMs) when needed.</li>\n<li><strong>Avoid including irrelevant predictors</strong> ‚Äî they inflate R¬≤ without improving generalization.</li>\n<li><strong>Contextualize your R¬≤</strong> ‚Äî in some fields, a lower R¬≤ is still useful; in others, it may signal inadequacy.</li>\n<li><strong>Complement numerical metrics with visual tools</strong> ‚Äî scatterplots, predicted vs.¬†actual plots, and residuals reveal insights numbers alone may miss.</li>\n</ol>\n</section>\n<section class=\"level2\" data-number=\"7.3\" id=\"looking-ahead\">\n<h2 class=\"anchored\" data-anchor-id=\"looking-ahead\" data-number=\"7.3\"><span class=\"header-section-number\">7.3</span> üöÄ Looking Ahead</h2>\n<p>If you want to take your modeling further: - Try <strong>ridge or lasso regression</strong> to handle multicollinearity. - Explore <strong>tree-based models</strong> (e.g., random forests) when relationships are complex and nonlinear. - Use tools like <strong><code>yardstick</code></strong> and <strong><code>modeltime</code></strong> to automate robust validation and reporting.</p>\n<blockquote class=\"blockquote\">\n<p>In the end, modeling isn‚Äôt just about maximizing R¬≤ ‚Äî it‚Äôs about <strong>understanding your data, validating your decisions</strong>, and making <strong>informed predictions</strong>.</p>\n</blockquote>\n<p>Thanks for reading!</p>\n<p>Feel free to share, fork, or reuse this analysis. Questions and comments are welcome.</p>\n<!-- -->\n</section>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/\"> A Statistician's R Notebook</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Explained vs.¬†Predictive Power: R¬≤, Adjusted R¬≤, and Beyond\nPosted on\nApril 29, 2025\nby\nM. Fatih T√ºzen\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nA Statistician's R Notebook\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\n1\nIntroduction\nYou trust R¬≤. Should you?\nYou proudly present a model with R¬≤ = 0.95. Everyone applauds.\nBut what if your model fails miserably on the next new data?\nWhen building a statistical model, one of the first numbers analysts and data scientists often cite is the\nR¬≤\n, or coefficient of determination. It‚Äôs widely reported in research, academic theses, and industry reports ‚Äî and yet, frequently misunderstood or misused.\nDoes a high R¬≤ mean your model is good? Is it enough to evaluate model performance? What about its adjusted or predictive counterparts?\nThis article will explore in depth: ‚Äì What R¬≤, Adjusted R¬≤, and Predicted R¬≤ actually mean ‚Äì Why relying solely on R¬≤ can mislead you ‚Äì How to evaluate models using\nboth explanatory and predictive power\n‚Äì Real-life implementation using the\n{tidymodels}\nframework in R\nWe‚Äôll also discuss best practices and common pitfalls, and equip you with a mindset to look beyond surface-level model summaries.\n2\nTheoretical Background\n2.1\nWhat is R¬≤?\nThe\ncoefficient of determination\n, R¬≤, is defined as:\nWhere:\n= Sum of squares of residuals =\n= Total sum of squares =\nIt tells us the\nproportion of variance explained by the model\n. An R¬≤ of 0.80 implies that 80% of the variability in the dependent variable is explained by the model.\nBut beware ‚Äî it only measures\nfit to training data\n, not the model‚Äôs ability to\ngeneralize\n.\n2.2\nAdjusted R¬≤\nWhen we add predictors to a regression model, R¬≤ will never decrease ‚Äî even if the added variables are irrelevant.\nAdjusted R¬≤\ncorrects this by penalizing the number of predictors:\nWhere:\nn : number of observations\np : number of predictors\nThus, Adjusted R¬≤ will\nonly increase\nif the new predictor improves the model more than expected by chance.\n2.3\nPredicted R¬≤\nPredicted R¬≤\n(or cross-validated R¬≤) is the most honest estimate of model utility. It answers the question:\nHow well will this model predict new, unseen data?\nThis is typically calculated using cross-validation, and unlike regular R¬≤, it reflects\nout-of-sample performance\n.\nYou can also view it as:\nWhere PRESS is the\nPrediction Error Sum of Squares\nbased on cross-validation.\n3\nDataset Overview\nWe‚Äôll use the classic\nBoston Housing Dataset\nto demonstrate. It includes:\nSocio-economic and housing variables for 506 Boston suburbs\nTarget:\nmedv\n(median value of owner-occupied homes in $1000s)\nBelow are the key variables:\ncrim\n: per capita crime rate by town\nzn\n: proportion of residential land zoned for large lots\nindus\n: proportion of non-retail business acres\nchas\n: Charles River dummy variable (1 = tract bounds river; 0 = otherwise)\nnox\n: nitric oxides concentration (parts per 10 million)\nrm\n: average number of rooms per dwelling\nage\n: proportion of owner-occupied units built before 1940\ndis\n: weighted distance to employment centers\nrad\n: index of accessibility to radial highways\ntax\n: property-tax rate per $10,000\nptratio\n: pupil-teacher ratio by town\nblack\n: 1000(Bk ‚Äì 0.63)^2 where Bk is the proportion of Black residents\nlstat\n: percentage of lower status of the population\nmedv\n:\ntarget\n‚Äî median value of owner-occupied homes (in $1000s)\nThis regression problem mimics common real estate or socio-economic modeling use cases. Let‚Äôs first examine the dataset‚Äôs summary statistics.\nlibrary(tidymodels)\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(patchwork)\n\nboston <- MASS::Boston\nskim(boston)\nData summary\nName\nboston\nNumber of rows\n506\nNumber of columns\n14\n_______________________\nColumn type frequency:\nnumeric\n14\n________________________\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ncrim\n0\n1\n3.61\n8.60\n0.01\n0.08\n0.26\n3.68\n88.98\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nzn\n0\n1\n11.36\n23.32\n0.00\n0.00\n0.00\n12.50\n100.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nindus\n0\n1\n11.14\n6.86\n0.46\n5.19\n9.69\n18.10\n27.74\n‚ñá‚ñÜ‚ñÅ‚ñá‚ñÅ\nchas\n0\n1\n0.07\n0.25\n0.00\n0.00\n0.00\n0.00\n1.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\nnox\n0\n1\n0.55\n0.12\n0.38\n0.45\n0.54\n0.62\n0.87\n‚ñá‚ñá‚ñÜ‚ñÖ‚ñÅ\nrm\n0\n1\n6.28\n0.70\n3.56\n5.89\n6.21\n6.62\n8.78\n‚ñÅ‚ñÇ‚ñá‚ñÇ‚ñÅ\nage\n0\n1\n68.57\n28.15\n2.90\n45.02\n77.50\n94.07\n100.00\n‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá\ndis\n0\n1\n3.80\n2.11\n1.13\n2.10\n3.21\n5.19\n12.13\n‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ\nrad\n0\n1\n9.55\n8.71\n1.00\n4.00\n5.00\n24.00\n24.00\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÉ\ntax\n0\n1\n408.24\n168.54\n187.00\n279.00\n330.00\n666.00\n711.00\n‚ñá‚ñá‚ñÉ‚ñÅ‚ñá\nptratio\n0\n1\n18.46\n2.16\n12.60\n17.40\n19.05\n20.20\n22.00\n‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá\nblack\n0\n1\n356.67\n91.29\n0.32\n375.38\n391.44\n396.22\n396.90\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\nlstat\n0\n1\n12.65\n7.14\n1.73\n6.95\n11.36\n16.96\n37.97\n‚ñá‚ñá‚ñÖ‚ñÇ‚ñÅ\nmedv\n0\n1\n22.53\n9.20\n5.00\n17.02\n21.20\n25.00\n50.00\n‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÅ\nCommentary:\nVariables like\ncrim\n,\ntax\n, and\nlstat\nexhibit high variability and potential skewness.\nchas\nis binary and acts like a categorical indicator.\nThe target variable\nmedv\nranges from $5,000 to $50,000 (capped).\nrm\n(average number of rooms) and\nlstat\n(lower status population) show notable spread and will likely play strong roles in the model.\nNext, we examine correlations with\nmedv\n:\nboston %>% correlate() %>% corrr::focus(medv) %>% arrange(desc(medv))\n# A tibble: 13 √ó 2\n   term      medv\n   <chr>    <dbl>\n 1 rm       0.695\n 2 zn       0.360\n 3 black    0.333\n 4 dis      0.250\n 5 chas     0.175\n 6 age     -0.377\n 7 rad     -0.382\n 8 crim    -0.388\n 9 nox     -0.427\n10 tax     -0.469\n11 indus   -0.484\n12 ptratio -0.508\n13 lstat   -0.738\nInterpretation of Correlations:\nrm\nshows a\nstrong positive\ncorrelation with\nmedv\n‚Äî more rooms generally imply higher value.\nlstat\nand\ncrim\nhave\nstrong negative\ncorrelations ‚Äî as lower status or crime increases, housing values drop.\nnox\n,\nage\n, and\nptratio\nalso show negative correlations with price, hinting at socio-environmental effects.\nThese insights will guide us in building and evaluating our model.\n4\nExploratory Data Analysis\nLet‚Äôs visualize some of the most influential variables in relation to\nmedv\n, our target variable. These exploratory graphs help reveal potential linear or nonlinear relationships, outliers, or the need for transformation.\n# Define individual plots with improved formatting for Quarto rendering\np1 <- ggplot(boston, aes(rm, medv)) +\n  geom_point(alpha = 0.5, color = \"#2c7fb8\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Rooms\\nvs. Median Value\",\n    x = \"Average Number of Rooms (rm)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np2 <- ggplot(boston, aes(lstat, medv)) +\n  geom_point(alpha = 0.5, color = \"#de2d26\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Lower Status %\\nvs. Median Value\",\n    x = \"% Lower Status Population (lstat)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np3 <- ggplot(boston, aes(nox, medv)) +\n  geom_point(alpha = 0.5, color = \"#31a354\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"NOx Concentration\\nvs. Median Value\",\n    x = \"NOx concentration (ppm)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np4 <- ggplot(boston, aes(age, medv)) +\n  geom_point(alpha = 0.5, color = \"#ff7f00\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Old Homes %\\nvs. Median Value\",\n    x = \"% Homes Built Before 1940 (age)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np5 <- ggplot(boston, aes(tax, medv)) +\n  geom_point(alpha = 0.5, color = \"#6a3d9a\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Tax Rate\\nvs. Median Value\",\n    x = \"Tax Rate (per $10,000)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n\np6 <- ggplot(boston, aes(dis, medv)) +\n  geom_point(alpha = 0.5, color = \"#1f78b4\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Distance to Jobs\\nvs. Median Value\",\n    x = \"Weighted Distance to Employment Centers (dis)\",\n    y = \"Median Value of Homes ($1000s)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 11, lineheight = 1.1))\n(p1 | p2) + plot_layout(guides = 'collect')\nRooms (\nrm\n)\n: Strong positive linear relationship with\nmedv\n. More rooms correlate with higher home values.\nLower Status Population (\nlstat\n)\n: Strong nonlinear inverse relation. Poorer areas tend to have significantly lower housing values.\n(p3 | p4) + plot_layout(guides = 'collect')\nNitric Oxide (\nnox\n)\n: Moderate negative relationship ‚Äî environmental factors like pollution impact price.\nOld Homes (\nage\n)\n: Slight negative trend ‚Äî older areas may have reduced appeal or value.\n(p5 | p6) + plot_layout(guides = 'collect')\nTax Rate (\ntax\n)\n: Higher taxes often relate to lower housing value, possibly due to location or socio-economic constraints.\nDistance to Employment Centers (\ndis\n)\n: Weak to moderate positive correlation. Suburban or well-connected areas might command higher value.\nThese six plots combine both socioeconomic and environmental dimensions of housing value ‚Äî providing both intuition and modeling direction.\n5\nModeling with Tidymodels\nNow that we‚Äôve explored the data, it‚Äôs time to fit a model using the\ntidymodels\nframework. We‚Äôll use a simple linear regression to predict\nmedv\n, the median home value.\n5.1\nData Splitting and Preprocessing\nWe begin by splitting the dataset into training and testing sets. The training set will be used to fit the model, and the test set will evaluate its generalization performance.\nset.seed(42)\nsplit <- initial_split(boston, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\n\nrec <- recipe(medv ~ ., data = train)\nmodel <- linear_reg() %>% set_engine(\"lm\")\nworkflow <- workflow() %>% add_recipe(rec) %>% add_model(model)\n5.2\nModel Fitting\nWe now fit the model to the training data:\nfit <- fit(workflow, data = train)\n5.3\nEvaluating the Model on the Training Set\nLet‚Äôs extract the R¬≤ and Adjusted R¬≤ values from the fitted model:\ntraining_summary <- glance(extract_fit_parsnip(fit))\ntraining_summary %>% dplyr::select(r.squared, adj.r.squared)\n# A tibble: 1 √ó 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.726         0.717\nüîç Interpretation:\nR¬≤\nmeasures the proportion of variance in\nmedv\nexplained by the predictors in the training set.\nAdjusted R¬≤\nadjusts this value by penalizing for the number of predictors, making it more reliable in multi-variable contexts.\nIf R¬≤ and Adjusted R¬≤ differ significantly, it indicates that some predictors may not be contributing meaningfully to the model.\nExample: A model with 12 predictors might show R¬≤ = 0.76, but Adjusted R¬≤ = 0.72 ‚Äî suggesting some predictors are adding complexity without real explanatory power.\n5.4\nTest Set Performance\nNow we assess the model on the unseen test data:\npreds <- predict(fit, test) %>% bind_cols(test)\nmetrics(preds, truth = medv, estimate = .pred)\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.79 \n2 rsq     standard       0.784\n3 mae     standard       3.32\nüìâ Interpretation:\nIf\ntest R¬≤\nis\nmuch lower\nthan training R¬≤, overfitting may be present.\nIf\ntest RMSE\nis high, the model‚Äôs absolute prediction error is large ‚Äî another sign of poor generalization.\n5.5\nCross-Validation for Predicted R¬≤\nTo get a more robust performance estimate, we use 10-fold cross-validation:\nset.seed(42)\ncv <- vfold_cv(train, v = 10)\nresample <- fit_resamples(\n  workflow,\n  resamples = cv,\n  metrics = metric_set(rsq, rmse),\n  control = control_resamples(save_pred = TRUE)\n)\ncollect_metrics(resample)\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   4.79     10  0.384  Preprocessor1_Model1\n2 rsq     standard   0.712    10  0.0341 Preprocessor1_Model1\n‚úÖ Interpretation:\nPredicted R¬≤ (via CV)\ntells us how well the model would perform on unseen data across multiple resamples.\nIt typically lies between training R¬≤ and test R¬≤.\nConsistency between cross-validated and test R¬≤ implies a stable model.\nTip\nUse cross-validation as a standard evaluation tool, especially when data is limited.\nüí¨ Summary of Findings:\nOur linear model explains a good portion of the variance, but some predictors might be irrelevant or redundant.\nCross-validation confirms the model is relatively stable but leaves room for refinement ‚Äî possibly through feature selection or nonlinear modeling.\nIn the next step, we can analyze residuals or explore model improvements such as polynomial terms or regularization.\n5.6\nResidual Diagnostics\nLet‚Äôs now check if our linear model satisfies basic regression assumptions. We‚Äôll plot residuals and assess patterns, non-linearity, and potential heteroskedasticity.\nlibrary(broom)\nlibrary(ggthemes)\n\naug <- augment(fit$fit$fit$fit)\n\nggplot(aug, aes(.fitted, .resid)) +\n  geom_point(alpha = 0.5, color = \"#2c7fb8\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()\nüìå Interpretation:\nWe want residuals to be randomly scattered around zero.\nIf there‚Äôs a pattern or funnel shape, that may indicate\nnon-linearity\nor\nheteroskedasticity\n.\n5.7\nImproving the Model: Transforming\nlstat\nFrom our earlier EDA, we saw a strong\nnonlinear relationship\nbetween\nlstat\n(lower status %) and\nmedv\n. Let‚Äôs try\nlog-transforming\nlstat\nto capture that curvature.\n5.7.1\nUpdated Recipe with Transformation\nrec_log <- recipe(medv ~ ., data = train) %>%\n  step_log(lstat)\n\nworkflow_log <- workflow() %>%\n  add_model(model) %>%\n  add_recipe(rec_log)\n\nfit_log <- fit(workflow_log, data = train)\n5.7.2\nEvaluation of Transformed Model\npreds_log <- predict(fit_log, test) %>% bind_cols(test)\nmetrics(preds_log, truth = medv, estimate = .pred)\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.43 \n2 rsq     standard       0.815\n3 mae     standard       3.16\nglance(fit_log)\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.785         0.778  4.21      110. 2.64e-121    13 -1147. 2324. 2384.\n# ‚Ñπ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nüß† Interpretation:\nCompare RMSE and R¬≤ from the transformed model to the original.\nIf we see improvement, the transformation helped capture underlying nonlinearity.\nAdjusted R¬≤\nis especially helpful here to assess whether the transformation truly improved fit ‚Äî not just overfit.\nTip\nTransformations, polynomial terms, and splines are all valid strategies to improve linear models without abandoning interpretability.\nWith residuals checked and a transformation tested, our next step could be to explore\nregularized models\nlike ridge or lasso regression, or even move beyond linearity with\ntree-based\nmodels.\n6\nCommon Pitfalls and Misconceptions\nEven though R¬≤ is widely reported and intuitively appealing, its interpretation is often flawed ‚Äî even by experienced analysts. Here, we‚Äôll go beyond textbook definitions and highlight real-world traps and misunderstandings related to R¬≤ and its variants.\nüö´ Misconception 1: High R¬≤ means the model is good\nA model with R¬≤ = 0.95 may\nlook impressive\n, but that doesn‚Äôt guarantee predictive power.\nHigh R¬≤ can result from overfitting, especially when the model is complex or contains many predictors.\nAdjusted R¬≤ and Predicted R¬≤\nmust be considered to evaluate true usefulness.\n‚ö†Ô∏è Misconception 2: Adding predictors always improves the model\nWhile R¬≤ never decreases with more variables,\nAdjusted R¬≤ can\n‚Äî and should ‚Äî if the new variable doesn‚Äôt add real value.\nIncluding irrelevant predictors increases complexity without improving explanatory power.\nThis is a form of\ndimensional overfitting\n.\n‚ùå Misconception 3: R¬≤ indicates causality\nR¬≤ quantifies correlation,\nnot causation\n.\nA high R¬≤ can arise from spurious relationships or confounding variables.\nAlways supplement with\ndomain knowledge\nand causal reasoning.\nüìâ Misconception 4: R¬≤ is a universal performance metric\nR¬≤ only applies to\nregression tasks\n. Using it for classification models is inappropriate and meaningless.\nFor binary classification, use metrics like\nAUC\n,\naccuracy\n,\nprecision\n, and\nrecall\n.\nüîç Misconception 5: Residual plots don‚Äôt matter if R¬≤ is high\nA good R¬≤ doesn‚Äôt guarantee that model assumptions are met.\nResidual patterns may still reveal\nnon-linearity\n,\nheteroskedasticity\n, or\ninfluential outliers\n.\nAlways inspect residual diagnostics.\nüí° Misconception 6: Predicted R¬≤ isn‚Äôt necessary\nMany practitioners report R¬≤ and Adjusted R¬≤, but\nomit cross-validation entirely\n.\nPredicted R¬≤\n(e.g., via 10-fold CV) is the\nmost honest measure\nof model generalizability.\nüî¨ Misconception 7: R¬≤ has a fixed interpretation\nR¬≤ values\ndepend on the context\n. In social sciences, an R¬≤ of 0.3 can be meaningful, while in physics we expect 0.99+.\nA ‚Äúlow‚Äù R¬≤ doesn‚Äôt mean the model is useless ‚Äî it may reflect inherent variability in human behavior or macroeconomic data.\nInsight:\nAlways use R¬≤ in context ‚Äî alongside other metrics, validation strategies, and graphical checks.\nFor a deeper dive into R¬≤ misconceptions and proper regression diagnostics, see:\nHarrell, F. (2015).\nRegression Modeling Strategies\n. Springer.\nGelman & Hill (2006).\nData Analysis Using Regression and Multilevel/Hierarchical Models\n.\nBurnham & Anderson (2002).\nModel Selection and Multimodel Inference\n.\nKutner et al.¬†(2004).\nApplied Linear Regression Models\n.\nTogether, these references build the foundation for\nresponsible model interpretation\n.\n7\nConclusion & Recommendations\n7.1\nüìå Summary\nIn this post, we explored\nR¬≤\n,\nAdjusted R¬≤\n, and\nPredicted R¬≤\nin depth ‚Äî not just as mathematical constructs, but as tools for critical thinking in modeling. We walked through theory, practical application in R with tidymodels, residual diagnostics, and even model improvement through transformation.\nLet‚Äôs recap: -\nR¬≤\ntells us how well our model fits the training data, but can be misleading on its own. -\nAdjusted R¬≤\nimproves upon R¬≤ by accounting for model complexity. -\nPredicted R¬≤\n, evaluated via cross-validation, provides the most trustworthy estimate of real-world performance.\nHigh R¬≤ values can be seductive. But as we saw,\nthey don‚Äôt guarantee causality, generalizability, or correctness\n. Only by combining R¬≤ with residual diagnostics, domain knowledge, and out-of-sample validation can we judge a model responsibly.\n7.2\nüí° Recommendations for Practitioners\nAlways accompany R¬≤ with Adjusted and Predicted R¬≤\n‚Äî never rely on one metric alone.\nPerform residual diagnostics\nto check linearity, variance assumptions, and outlier influence.\nUse cross-validation (e.g., 10-fold)\nas a default evaluation strategy, especially when the dataset is not large.\nTransform nonlinear predictors\n(as we did with\nlstat\n) or use flexible models (e.g., splines, GAMs) when needed.\nAvoid including irrelevant predictors\n‚Äî they inflate R¬≤ without improving generalization.\nContextualize your R¬≤\n‚Äî in some fields, a lower R¬≤ is still useful; in others, it may signal inadequacy.\nComplement numerical metrics with visual tools\n‚Äî scatterplots, predicted vs.¬†actual plots, and residuals reveal insights numbers alone may miss.\n7.3\nüöÄ Looking Ahead\nIf you want to take your modeling further: - Try\nridge or lasso regression\nto handle multicollinearity. - Explore\ntree-based models\n(e.g., random forests) when relationships are complex and nonlinear. - Use tools like\nyardstick\nand\nmodeltime\nto automate robust validation and reporting.\nIn the end, modeling isn‚Äôt just about maximizing R¬≤ ‚Äî it‚Äôs about\nunderstanding your data, validating your decisions\n, and making\ninformed predictions\n.\nThanks for reading!\nFeel free to share, fork, or reuse this analysis. Questions and comments are welcome.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nA Statistician's R Notebook\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "1 Introduction You trust R¬≤. Should you? You proudly present a model with R¬≤ = 0.95. Everyone applauds. But what if your model fails miserably on the next new data? When building a statistical model, one of the first numbers analysts and...",
      "meta_keywords": null,
      "og_description": "1 Introduction You trust R¬≤. Should you? You proudly present a model with R¬≤ = 0.95. Everyone applauds. But what if your model fails miserably on the next new data? When building a statistical model, one of the first numbers analysts and...",
      "og_image": "https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/quote_box.png",
      "og_title": "Explained vs. Predictive Power: R¬≤, Adjusted R¬≤, and Beyond | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 16.1,
      "sitemap_lastmod": null,
      "twitter_description": "1 Introduction You trust R¬≤. Should you? You proudly present a model with R¬≤ = 0.95. Everyone applauds. But what if your model fails miserably on the next new data? When building a statistical model, one of the first numbers analysts and...",
      "twitter_title": "Explained vs. Predictive Power: R¬≤, Adjusted R¬≤, and Beyond | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/04/explained-vs-predictive-power-r%C2%B2-adjusted-r%C2%B2-and-beyond/",
      "word_count": 3215
    }
  }
}