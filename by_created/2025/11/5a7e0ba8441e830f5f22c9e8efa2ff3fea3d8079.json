{
  "id": "5a7e0ba8441e830f5f22c9e8efa2ff3fea3d8079",
  "url": "https://www.r-bloggers.com/2025/05/xgboost/",
  "created_at_utc": "2025-11-22T19:58:32Z",
  "data": null,
  "raw_original": {
    "uuid": "87194e5f-5284-4240-a36e-e6c269492642",
    "created_at": "2025-11-22 19:58:32",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/05/xgboost/",
      "crawled_at": "2025-11-22T10:48:30.408226",
      "external_links": [
        {
          "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-xgboost/",
          "text": "mlr-org"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://github.com/mlr-org/mlr3website/",
          "text": "GitHub"
        },
        {
          "href": "https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29",
          "text": "Statlog (German Credit Data) Data Set"
        },
        {
          "href": "https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-xgboost/",
          "text": "mlr-org"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Xgboost | R-bloggers",
      "images": [],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/giuseppe-casalicchio/",
          "text": "Giuseppe Casalicchio"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-392884 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Xgboost</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 22, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/giuseppe-casalicchio/\">Giuseppe Casalicchio</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-xgboost/\"> mlr-org</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n<strong>JavaScript is required to unlock solutions.</strong><br/>\n    Please enable JavaScript and reload the page,<br/>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" rel=\"nofollow\" target=\"_blank\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n<section class=\"level1\" id=\"goal\">\n<h1>Goal</h1>\n<p>Our goal for this exercise sheet is to understand how to apply and work with XGBoost. The XGBoost algorithm has a large range of hyperparameters. We learn specifically how to tune these hyperparameters to optimize our XGBoost model for the task at hand.</p>\n</section>\n<section class=\"level1\" id=\"german-credit-dataset\">\n<h1>German Credit Dataset</h1>\n<p>As in previous exercises, we use the German credit dataset of Prof. Dr. Hans Hoffman of the University of Hamburg in 1994. By using XGBoost, we want to classify people as a good or bad credit risk based on 20 personal, demographic and financial features. The dataset is available at the UCI repository as <a href=\"https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29\" rel=\"nofollow\" target=\"_blank\">Statlog (German Credit Data) Data Set</a>.</p>\n<section class=\"level2\" id=\"preprocessing\">\n<h2 class=\"anchored\" data-anchor-id=\"preprocessing\">Preprocessing</h2>\n<p>To apply the XGBoost algorithm to the <code>credit</code> dataset, categorical features need to be converted into numeric features e.g. using one-hot-encoding. We use a factor encoding <code>PipeOp</code> from <code>mlr3pipelines</code> to do so.</p>\n<p>First, we setup a classification task:</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(mlr3verse)\ntask = tsk(\"german_credit\")\ntask$positive = \"good\"</pre>\n</div>\n<p>Next, we can initialize a factor encoding and apply it to the task at hand.</p>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>poe = po(\"encode\")\ntask = poe$train(list(task))[[1]]</pre>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"xgboost-learner\">\n<h1>1 XGBoost Learner</h1>\n<section class=\"level2\" id=\"initialize-an-xgboost-learner\">\n<h2 class=\"anchored\" data-anchor-id=\"initialize-an-xgboost-learner\">1.1 Initialize an XGBoost Learner</h2>\n<p>Initialize a XGBoost <code>mlr3</code> learner with 100 iterations. Make sure that that you have installed the <code>xgboost</code> R package.</p>\n<details>\n<summary>\n<strong>Details on iterations:</strong>\n</summary>\n<p>The number of iterations must always be chosen by the user, since the hyperparameter has no proper default value in <code>mlr3</code>.</p>\n<p>“No proper default value” means that <code>mlr3</code> has an adjusted default of 1 iteration to avoid errors when constructing the learner. One single iteration is, in general, not a good default, since we only conduct a single boosting step.</p>\n<p>There is a trade-off between underfitting (not enough iterations) and overfitting (too many iterations). Therefore, it is always better to tune such a hyperparameter. In this exercise, we chose 100 iterations because we believe it is an upper bound for the number of iterations. We will later conduct early stopping to avoid overfitting.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>The number of iterations can be specified via the <code>nrounds</code> hyperparameter of the <code>classif.xgboost</code> learner, set this hyperparameter to <code>100</code>.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>xgboost_lrn = lrn(..., nrounds = ...)</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-1\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-1-contents callout-collapse collapse\" id=\"callout-1\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+eGdib29zdF9scm4gPHNwYW4gY2xhc3M9Im90Ij49PC9zcGFuPiA8c3BhbiBjbGFzcz0iZnUiPmxybjwvc3Bhbj4oPHNwYW4gY2xhc3M9InN0Ij4mcXVvdDtjbGFzc2lmLnhnYm9vc3QmcXVvdDs8L3NwYW4+KTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS0yIj48YSBocmVmPSIjY2IxLTIiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT54Z2Jvb3N0X2xybjxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5wYXJhbV9zZXQ8c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+PHNwYW4gY2xhc3M9ImZ1Ij5zZXRfdmFsdWVzPC9zcGFuPig8c3BhbiBjbGFzcz0iYXQiPm5yb3VuZHMgPTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImR2Ij4xMDA8L3NwYW4+PHNwYW4gY2xhc3M9ImR0Ij5MPC9zcGFuPik8L3NwYW4+PC9jb2RlPjwvcHJlPjwvZGl2Pgo8L2Rpdj4=\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"performance-assessment-using-cross-validation\">\n<h2 class=\"anchored\" data-anchor-id=\"performance-assessment-using-cross-validation\">1.2 Performance Assessment using Cross-validation</h2>\n<p>Use 5-fold cross-validation to estimate the generalization error of the XGBoost learner with 100 boosting iterations on the one-hot-encoded credit task. Measure the performance of the learner using the classification error. Set up a seed to make your results reproducible (e.g., <code>set.seed(8002L)</code>).</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Specifically, you need to conduct three steps:</p>\n<ol type=\"1\">\n<li>Specify a <code>Resampling</code> object using <code>rsmp()</code>.</li>\n<li>Use this object together with the task and learner specified above as an input to the <code>resample()</code> method.</li>\n<li>Measure the performance with the <code>$aggregate()</code> method of the resulting <code>ResampleResult</code> object.</li>\n</ol>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>set.seed(8002L)\nresampling = rsmp(\"cv\", ...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$aggregate()</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-2\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-2-contents callout-collapse collapse\" id=\"callout-2\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"hyperparameters\">\n<h1>2 Hyperparameters</h1>\n<section class=\"level2\" id=\"overview-of-hyperparameters\">\n<h2 class=\"anchored\" data-anchor-id=\"overview-of-hyperparameters\">2.1 Overview of Hyperparameters</h2>\n<p>Apart from the number of iterations (<code>nrounds</code>), the XGBoost learner has a several other hyperparameters which were kept to their default values in the previous exercise. Extract an overview of all hyperparameters from the initalized XGBoost learner (previous exercise) as well as its default values.</p>\n<p>Given the extracted hyperparameter list above and the help page of <code>xgboost</code> (<code>?xgboost</code>), answer the following questions:</p>\n<ul>\n<li>Does the learner rely on a tree or a linear booster by default?</li>\n<li>Do more hyperparameters exist for the tree or the linear booster?</li>\n<li>What do <code>max_depth</code> and <code>eta</code> mean and what are their default values?</li>\n<li>Does a larger value for <code>eta</code> imply a larger value for <code>nrounds</code>?</li>\n</ul>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>The hyperparameters and their default values could be extracted by the <code>$param_set</code> field of the XGBoost learner. Alternatively, you could call the help page of <code>LearnerClassifXgboost</code>.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<p>You can answer all questions concerning defaults with the output of the <code>$param_set</code>. A description of the hyperparameters could be found on the <code>xgboost</code> help page (<code>?xgboost</code>). The help page also offers an answer to the last question concerning the connection between <code>eta</code> and <code>nrounds</code>.</p>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-3\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-3-contents callout-collapse collapse\" id=\"callout-3\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"<p>The <code>param_set</code> field gives an overview of the
hyperparameters and their default values:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre
class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>xgboost_lrn<span class="sc">$</span>param_set</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ParamSet(61)&gt;
Key: &lt;id&gt;
                             id    class lower upper nlevels         default                  parents  value
                         &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;          &lt;list&gt;                   &lt;list&gt; &lt;list&gt;
 1:                       alpha ParamDbl     0   Inf     Inf               0                   [NULL] [NULL]
 2:               approxcontrib ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
 3:                  base_score ParamDbl  -Inf   Inf     Inf             0.5                   [NULL] [NULL]
 4:                     booster ParamFct    NA    NA       3          gbtree                   [NULL] [NULL]
 5:                   callbacks ParamUty    NA    NA     Inf       &lt;list[0]&gt;                   [NULL] [NULL]
 6:           colsample_bylevel ParamDbl     0     1     Inf               1                   [NULL] [NULL]
 7:            colsample_bynode ParamDbl     0     1     Inf               1                   [NULL] [NULL]
 8:            colsample_bytree ParamDbl     0     1     Inf               1                   [NULL] [NULL]
 9:                      device ParamUty    NA    NA     Inf             cpu                   [NULL] [NULL]
10: disable_default_eval_metric ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
11:       early_stopping_rounds ParamInt     1   Inf     Inf          [NULL]                   [NULL] [NULL]
12:                         eta ParamDbl     0     1     Inf             0.3                   [NULL] [NULL]
13:                 eval_metric ParamUty    NA    NA     Inf  &lt;NoDefault[0]&gt;                   [NULL] [NULL]
14:            feature_selector ParamFct    NA    NA       5          cyclic                  booster [NULL]
15:                       gamma ParamDbl     0   Inf     Inf               0                   [NULL] [NULL]
16:                 grow_policy ParamFct    NA    NA       2       depthwise              tree_method [NULL]
17:     interaction_constraints ParamUty    NA    NA     Inf  &lt;NoDefault[0]&gt;                   [NULL] [NULL]
18:              iterationrange ParamUty    NA    NA     Inf  &lt;NoDefault[0]&gt;                   [NULL] [NULL]
19:                      lambda ParamDbl     0   Inf     Inf               1                   [NULL] [NULL]
20:                 lambda_bias ParamDbl     0   Inf     Inf               0                  booster [NULL]
21:                     max_bin ParamInt     2   Inf     Inf             256              tree_method [NULL]
22:              max_delta_step ParamDbl     0   Inf     Inf               0                   [NULL] [NULL]
23:                   max_depth ParamInt     0   Inf     Inf               6                   [NULL] [NULL]
24:                  max_leaves ParamInt     0   Inf     Inf               0              grow_policy [NULL]
25:                    maximize ParamLgl    NA    NA       2          [NULL]                   [NULL] [NULL]
26:            min_child_weight ParamDbl     0   Inf     Inf               1                   [NULL] [NULL]
27:                     missing ParamDbl  -Inf   Inf     Inf              NA                   [NULL] [NULL]
28:        monotone_constraints ParamUty    NA    NA     Inf               0                   [NULL] [NULL]
29:              normalize_type ParamFct    NA    NA       2            tree                  booster [NULL]
30:                     nrounds ParamInt     1   Inf     Inf  &lt;NoDefault[0]&gt;                   [NULL]    100
31:                     nthread ParamInt     1   Inf     Inf               1                   [NULL]      1
32:                  ntreelimit ParamInt     1   Inf     Inf          [NULL]                   [NULL] [NULL]
33:           num_parallel_tree ParamInt     1   Inf     Inf               1                   [NULL] [NULL]
34:                   objective ParamUty    NA    NA     Inf binary:logistic                   [NULL] [NULL]
35:                    one_drop ParamLgl    NA    NA       2           FALSE                  booster [NULL]
36:                outputmargin ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
37:                 predcontrib ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
38:             predinteraction ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
39:                    predleaf ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
40:               print_every_n ParamInt     1   Inf     Inf               1                  verbose [NULL]
41:                process_type ParamFct    NA    NA       2         default                   [NULL] [NULL]
42:                   rate_drop ParamDbl     0     1     Inf               0                  booster [NULL]
43:                refresh_leaf ParamLgl    NA    NA       2            TRUE                   [NULL] [NULL]
44:                     reshape ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
45:                 sample_type ParamFct    NA    NA       2         uniform                  booster [NULL]
46:             sampling_method ParamFct    NA    NA       2         uniform                  booster [NULL]
47:                   save_name ParamUty    NA    NA     Inf          [NULL]                   [NULL] [NULL]
48:                 save_period ParamInt     0   Inf     Inf          [NULL]                   [NULL] [NULL]
49:            scale_pos_weight ParamDbl  -Inf   Inf     Inf               1                   [NULL] [NULL]
50:          seed_per_iteration ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
51:                   skip_drop ParamDbl     0     1     Inf               0                  booster [NULL]
52:                strict_shape ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
53:                   subsample ParamDbl     0     1     Inf               1                   [NULL] [NULL]
54:                       top_k ParamInt     0   Inf     Inf               0 feature_selector,booster [NULL]
55:                    training ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]
56:                 tree_method ParamFct    NA    NA       5            auto                  booster [NULL]
57:      tweedie_variance_power ParamDbl     1     2     Inf             1.5                objective [NULL]
58:                     updater ParamUty    NA    NA     Inf  &lt;NoDefault[0]&gt;                   [NULL] [NULL]
59:                     verbose ParamInt     0     2       3               1                   [NULL]      0
60:                   watchlist ParamUty    NA    NA     Inf          [NULL]                   [NULL] [NULL]
61:                   xgb_model ParamUty    NA    NA     Inf          [NULL]                   [NULL] [NULL]
                             id    class lower upper nlevels         default                  parents  value</code></pre>
</div>
</div>
<p>Together with the help page of <code>xgboost</code> the answers to
the above questions are:</p>
<ul>
<li>Does the learner rely on a tree or a linear booster per
default?</li>
</ul>
<p>The <code>booster</code> hyperparameter reveals that it relies on a
<code>gbtree</code> and, therefore, a tree booster per default.</p>
<ul>
<li>Do more hyperparameters exist for the tree or the linear
booster?</li>
</ul>
<p>According to the help page of <code>xgboost</code>, tree boosters
have more hyperparameters than the linear booster (only three are
mentioned for the latter: <code>lambda</code>, <code>lambda_bias</code>
and <code>alpha</code> for regularization of the linear booster).</p>
<ul>
<li>What do <code>max_depth</code>, <code>eta</code> and
<code>nrounds</code> mean and what are their default values?</li>
</ul>
<p><code>max_depth</code> and <code>eta</code> affect the tree booster:
<code>max_depth</code> gives the depth of the tree with a default of 6
and <code>eta</code> specifies the learning rate, i.e., how each tree
contributes to the overall model, the default is <code>0.3</code>.</p>
<ul>
<li>Does a larger value for <code>eta</code> imply a larger value for
<code>nrounds</code>?</li>
</ul>
<p>A larger value of <code>eta</code> implies a lower value of
<code>nrounds</code> according to the help page. Since each tree
contributes more to the overall model due to a larger <code>eta</code>,
the boosting model also starts to overfit faster which necessitates a
lower value for <code>nrounds</code>.</p>\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"tune-hyperparameters\">\n<h2 class=\"anchored\" data-anchor-id=\"tune-hyperparameters\">2.2 Tune Hyperparameters</h2>\n<p>Tune the the depth of tree of the <code>xgboost</code> learner on the German credit data using random search</p>\n<ul>\n<li>with the search space for <code>max_depth</code> between 1 and 8 and for <code>eta</code> between 0.2 and 0.4</li>\n<li>with 20 evaluations as termination criterion</li>\n<li>the classification error <code>msr(\"classif.ce\")</code> as performance measure</li>\n<li>3-fold CV as resampling strategy.</li>\n</ul>\n<p>Set a seed for reproducibility (e.g., <code>set.seed(8002L)</code>).</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Specifically, you should conduct the following steps:</p>\n<ol type=\"1\">\n<li>Setup a search space with <code>ps()</code> consisting of a <code>p_int()</code> for <code>max_depth</code> and <code>p_dbl()</code> for <code>eta</code>.</li>\n<li>Setup the classification error as a tuning measure with <code>msr()</code>.</li>\n<li>Initialize cross-validation as the resampling strategy using <code>rsmp()</code>.</li>\n<li>Setup 10 evaluations as the termination criterion using <code>trm()</code>.</li>\n<li>Initialize a <code>TuningInstanceSingleCrit</code> object using <code>ti()</code> and the objects produced in steps 1.-4. as well as the task and learner as an input.</li>\n<li>Define random search as the tuner object using <code>tnr()</code>.</li>\n<li>Call the <code>$optimize()</code> method of the tuner object with the initialized <code>TuningInstanceSingleCrit</code> as an input.</li>\n</ol>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>set.seed(8002L)\n\nsearch_space = ps(\n  max_depth = ...(1L, 8L),\n  eta = ...(0.2, 0.4)\n)\nmeasure = msr(\"classif....\")\nresampling = rsmp(\"cv\", folds = ...)\nterminator = trm(\"evals\", n_evals = ...)\n\ninstance_random = ti(\n  task = ..., \n  learner = ..., \n  measure = ..., \n  resampling = ..., \n  terminator = ..., \n  search_space = ...\n)\n \ntuner_random = tnr(...)\ntuner_random$optimize(...)</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-4\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-4-contents callout-collapse collapse\" id=\"callout-4\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PHA+RGVmaW5lIGFsbCB0dW5pbmcgcmVsYXRlZCBvYmplY3RzPC9wPgo8ZGl2IGNsYXNzPSJjZWxsIiBkYXRhLWxheW91dC1hbGlnbj0iY2VudGVyIj4KPGRpdiBjbGFzcz0ic291cmNlQ29kZSIgaWQ9ImNiMSI+PHByZQpjbGFzcz0ic291cmNlQ29kZSByIGNlbGwtY29kZSI+PGNvZGUgY2xhc3M9InNvdXJjZUNvZGUgciI+PHNwYW4gaWQ9ImNiMS0xIj48YSBocmVmPSIjY2IxLTEiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT48c3BhbiBjbGFzcz0iZnUiPnNldC5zZWVkPC9zcGFuPig8c3BhbiBjbGFzcz0iZHYiPjgwMDI8L3NwYW4+PHNwYW4gY2xhc3M9ImR0Ij5MPC9zcGFuPik8L3NwYW4+CjxzcGFuIGlkPSJjYjEtMiI+PGEgaHJlZj0iI2NiMS0yIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+PC9zcGFuPgo8c3BhbiBpZD0iY2IxLTMiPjxhIGhyZWY9IiNjYjEtMyIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPnNlYXJjaF9zcGFjZSA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+cHM8L3NwYW4+KDwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS00Ij48YSBocmVmPSIjY2IxLTQiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT4gIDxzcGFuIGNsYXNzPSJhdCI+bWF4X2RlcHRoID08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+cF9pbnQ8L3NwYW4+KDxzcGFuIGNsYXNzPSJkdiI+MTwvc3Bhbj48c3BhbiBjbGFzcz0iZHQiPkw8L3NwYW4+LCA8c3BhbiBjbGFzcz0iZHYiPjg8L3NwYW4+PHNwYW4gY2xhc3M9ImR0Ij5MPC9zcGFuPiksPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTUiPjxhIGhyZWY9IiNjYjEtNSIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPiAgPHNwYW4gY2xhc3M9ImF0Ij5ldGEgPTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImZ1Ij5wX2RibDwvc3Bhbj4oPHNwYW4gY2xhc3M9ImZsIj4wLjI8L3NwYW4+LCA8c3BhbiBjbGFzcz0iZmwiPjAuNDwvc3Bhbj4pPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTYiPjxhIGhyZWY9IiNjYjEtNiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPik8L3NwYW4+CjxzcGFuIGlkPSJjYjEtNyI+PGEgaHJlZj0iI2NiMS03IiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+bWVhc3VyZSA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+bXNyPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O2NsYXNzaWYuY2UmcXVvdDs8L3NwYW4+KTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMS04Ij48YSBocmVmPSIjY2IxLTgiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT5yZXNhbXBsaW5nIDxzcGFuIGNsYXNzPSJvdCI+PTwvc3Bhbj4gPHNwYW4gY2xhc3M9ImZ1Ij5yc21wPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O2N2JnF1b3Q7PC9zcGFuPiwgPHNwYW4gY2xhc3M9ImF0Ij5mb2xkcyA9PC9zcGFuPiA8c3BhbiBjbGFzcz0iZHYiPjM8L3NwYW4+PHNwYW4gY2xhc3M9ImR0Ij5MPC9zcGFuPik8L3NwYW4+CjxzcGFuIGlkPSJjYjEtOSI+PGEgaHJlZj0iI2NiMS05IiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+dGVybWluYXRvciA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+dHJtPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O2V2YWxzJnF1b3Q7PC9zcGFuPiwgPHNwYW4gY2xhc3M9ImF0Ij5uX2V2YWxzID08L3NwYW4+IDxzcGFuIGNsYXNzPSJkdiI+MjA8L3NwYW4+PHNwYW4gY2xhc3M9ImR0Ij5MPC9zcGFuPik8L3NwYW4+PC9jb2RlPjwvcHJlPjwvZGl2Pgo8L2Rpdj4KPHA+SW5zdGFudGlhdGUgYSB0dW5pbmcgaW5zdGFuY2U8L3A+CjxkaXYgY2xhc3M9ImNlbGwiIGRhdGEtbGF5b3V0LWFsaWduPSJjZW50ZXIiPgo8ZGl2IGNsYXNzPSJzb3VyY2VDb2RlIiBpZD0iY2IyIj48cHJlCmNsYXNzPSJzb3VyY2VDb2RlIHIgY2VsbC1jb2RlIj48Y29kZSBjbGFzcz0ic291cmNlQ29kZSByIj48c3BhbiBpZD0iY2IyLTEiPjxhIGhyZWY9IiNjYjItMSIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPmluc3RhbmNlX3JhbmRvbSA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+dGk8L3NwYW4+KDwvc3Bhbj4KPHNwYW4gaWQ9ImNiMi0yIj48YSBocmVmPSIjY2IyLTIiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT4gIDxzcGFuIGNsYXNzPSJhdCI+dGFzayA9PC9zcGFuPiB0YXNrLCA8L3NwYW4+CjxzcGFuIGlkPSJjYjItMyI+PGEgaHJlZj0iI2NiMi0zIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+ICA8c3BhbiBjbGFzcz0iYXQiPmxlYXJuZXIgPTwvc3Bhbj4geGdib29zdF9scm4sIDwvc3Bhbj4KPHNwYW4gaWQ9ImNiMi00Ij48YSBocmVmPSIjY2IyLTQiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT4gIDxzcGFuIGNsYXNzPSJhdCI+bWVhc3VyZSA9PC9zcGFuPiBtZWFzdXJlLCA8L3NwYW4+CjxzcGFuIGlkPSJjYjItNSI+PGEgaHJlZj0iI2NiMi01IiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+ICA8c3BhbiBjbGFzcz0iYXQiPnJlc2FtcGxpbmcgPTwvc3Bhbj4gcmVzYW1wbGluZywgPC9zcGFuPgo8c3BhbiBpZD0iY2IyLTYiPjxhIGhyZWY9IiNjYjItNiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPiAgPHNwYW4gY2xhc3M9ImF0Ij50ZXJtaW5hdG9yID08L3NwYW4+IHRlcm1pbmF0b3IsIDwvc3Bhbj4KPHNwYW4gaWQ9ImNiMi03Ij48YSBocmVmPSIjY2IyLTciIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT4gIDxzcGFuIGNsYXNzPSJhdCI+c2VhcmNoX3NwYWNlID08L3NwYW4+IHNlYXJjaF9zcGFjZTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMi04Ij48YSBocmVmPSIjY2IyLTgiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT4pPC9zcGFuPjwvY29kZT48L3ByZT48L2Rpdj4KPC9kaXY+CjxwPlVzZSByYW5kb20gc2VhcmNoIGFzIHRoZSB0dW5pbmcgYXBwcm9hY2g8L3A+CjxkaXYgY2xhc3M9ImNlbGwiIGRhdGEtbGF5b3V0LWFsaWduPSJjZW50ZXIiPgo8ZGl2IGNsYXNzPSJzb3VyY2VDb2RlIiBpZD0iY2IzIj48cHJlCmNsYXNzPSJzb3VyY2VDb2RlIHIgY2VsbC1jb2RlIj48Y29kZSBjbGFzcz0ic291cmNlQ29kZSByIj48c3BhbiBpZD0iY2IzLTEiPjxhIGhyZWY9IiNjYjMtMSIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPnR1bmVyX3JhbmRvbSA8c3BhbiBjbGFzcz0ib3QiPj08L3NwYW4+IDxzcGFuIGNsYXNzPSJmdSI+dG5yPC9zcGFuPig8c3BhbiBjbGFzcz0ic3QiPiZxdW90O3JhbmRvbV9zZWFyY2gmcXVvdDs8L3NwYW4+KTwvc3Bhbj4KPHNwYW4gaWQ9ImNiMy0yIj48YSBocmVmPSIjY2IzLTIiIGFyaWEtaGlkZGVuPSJ0cnVlIiB0YWJpbmRleD0iLTEiPjwvYT50dW5lcl9yYW5kb208c3BhbiBjbGFzcz0ic2MiPiQ8L3NwYW4+PHNwYW4gY2xhc3M9ImZ1Ij5vcHRpbWl6ZTwvc3Bhbj4oaW5zdGFuY2VfcmFuZG9tKTwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjwvZGl2Pg==\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"inspect-the-the-best-performing-setup\">\n<h2 class=\"anchored\" data-anchor-id=\"inspect-the-the-best-performing-setup\">2.3 Inspect the the Best Performing Setup</h2>\n<p>Which tree depth was the best performing one?</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Inspect the tuned instance (of class <code>TuningInstanceSingleCrit</code>, it was the input to <code>$optimize()</code>). Look, for example, at the <code>$result</code> field.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>instance_random$result</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-5\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-5-contents callout-collapse collapse\" id=\"callout-5\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+aW5zdGFuY2VfcmFuZG9tPHNwYW4gY2xhc3M9InNjIj4kPC9zcGFuPnJlc3VsdDwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0IGNlbGwtb3V0cHV0LXN0ZG91dCI+CjxwcmU+PGNvZGU+ICAgbWF4X2RlcHRoICAgICAgIGV0YSBsZWFybmVyX3BhcmFtX3ZhbHMgIHhfZG9tYWluIGNsYXNzaWYuY2UKICAgICAgICZsdDtpbnQmZ3Q7ICAgICAmbHQ7bnVtJmd0OyAgICAgICAgICAgICAmbHQ7bGlzdCZndDsgICAgJmx0O2xpc3QmZ3Q7ICAgICAgJmx0O251bSZndDsKMTogICAgICAgICAxIDAuMjUxNzY0OCAgICAgICAgICAmbHQ7bGlzdFs1XSZndDsgJmx0O2xpc3RbMl0mZ3Q7ICAwLjI1MDk4NDU8L2NvZGU+PC9wcmU+CjwvZGl2Pgo8L2Rpdj4KPHA+VGhlIGJlc3QgcGVyZm9ybWluZyBpbnN0YW5jZSBoYWQgPGNvZGU+ZXRhPC9jb2RlPiBzZXQgdG8KMC4yNTE3NjQ4PGJyIC8+CmFuZCA8Y29kZT5tYXhfZGVwdGg8L2NvZGU+IHNldCB0byAxLjwvcD4=\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"early-stopping\">\n<h1>3 Early Stopping</h1>\n<section class=\"level2\" id=\"set-up-an-xgboost-learner-with-early-stopping\">\n<h2 class=\"anchored\" data-anchor-id=\"set-up-an-xgboost-learner-with-early-stopping\">3.1 Set up an XGBoost Learner with Early Stopping</h2>\n<p>Now that we derived the best hyperparameter for the maximum depth and eta, we could train our final model. To avoid overfitting we conduct early stopping – meaning that the algorithm stops as soon as a the performance does not improve for a given number of rounds to avoid overfitting. The performance for stopping should be assessed via a validation data set.</p>\n<p>Set up an XGBoost learner with the following hyperparameters:</p>\n<ul>\n<li><code>max_depth</code> and <code>eta</code> set to the best configurations according to the previous tuning task.</li>\n<li><code>nrounds</code> set to 100L.</li>\n<li>The number of early stopping rounds set to 5 (this could be tuned, as well, but we simplify things) in order to stop earlier if there was no improvement in the previous 5 iterations.</li>\n</ul>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>library(\"xgboost\")\nset.seed(2001L)</pre>\n</div>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Specify the hyperparameters within <code>lrn()</code>. The number of rounds could be specified with <code>early_stopping_rounds</code>.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>lrn(\"...\", nrounds = ..., \n  max_depth = instance_random$result$..., \n  eta = instance_random$result$..., \n  early_stopping_rounds = ....\n) </pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-6\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-6-contents callout-collapse collapse\" id=\"callout-6\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level2\" id=\"training-on-credit-data\">\n<h2 class=\"anchored\" data-anchor-id=\"training-on-credit-data\">3.2 Training on Credit Data</h2>\n<p>Train the XGBoost learner from the previous exercise on the credit data set How many iterations were conducted before the boosting algorithm stopped?</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>By calling <code>$train()</code> a model is trained which can be accessed via <code>$model</code>. This model has a field <code>$niter</code> – the number of conducted iterations.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>xgboost$train(...)\nxgboost$...$niter</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-7\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-7-contents callout-collapse collapse\" id=\"callout-7\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"PGRpdiBjbGFzcz0iY2VsbCIgZGF0YS1sYXlvdXQtYWxpZ249ImNlbnRlciI+CjxkaXYgY2xhc3M9InNvdXJjZUNvZGUiIGlkPSJjYjEiPjxwcmUKY2xhc3M9InNvdXJjZUNvZGUgciBjZWxsLWNvZGUiPjxjb2RlIGNsYXNzPSJzb3VyY2VDb2RlIHIiPjxzcGFuIGlkPSJjYjEtMSI+PGEgaHJlZj0iI2NiMS0xIiBhcmlhLWhpZGRlbj0idHJ1ZSIgdGFiaW5kZXg9Ii0xIj48L2E+eGdib29zdF9scm4yPHNwYW4gY2xhc3M9InNjIj4kPC9zcGFuPjxzcGFuIGNsYXNzPSJmdSI+dHJhaW48L3NwYW4+KHRhc2spPC9zcGFuPgo8c3BhbiBpZD0iY2IxLTIiPjxhIGhyZWY9IiNjYjEtMiIgYXJpYS1oaWRkZW49InRydWUiIHRhYmluZGV4PSItMSI+PC9hPnhnYm9vc3RfbHJuMjxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5tb2RlbDxzcGFuIGNsYXNzPSJzYyI+JDwvc3Bhbj5uaXRlcjwvc3Bhbj48L2NvZGU+PC9wcmU+PC9kaXY+CjxkaXYgY2xhc3M9ImNlbGwtb3V0cHV0IGNlbGwtb3V0cHV0LXN0ZG91dCI+CjxwcmU+PGNvZGU+WzFdIDE3PC9jb2RlPjwvcHJlPgo8L2Rpdj4KPC9kaXY+\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level1\" id=\"extra-nested-resampling\">\n<h1>4 Extra: Nested Resampling</h1>\n<p>To receive an unbiased performance estimate when tuning hyperparameters, conduct nested resampling with</p>\n<ul>\n<li>3-fold cross-validation for the outer and inner resampling loop.</li>\n<li>a search space for <code>max_depth</code> between 1 and 8 and <code>eta</code> between 0.2 and 0.4.</li>\n<li>random search with 20 evaluations</li>\n<li>the classification error <code>msr(\"classif.ce\")</code> as performance measure.</li>\n</ul>\n<p>Extract the performance estimate on the outer resampling folds.</p>\n<details>\n<summary>\n<strong>Hint 1:</strong>\n</summary>\n<p>Specifically, you need to conduct the following steps:</p>\n<ol type=\"1\">\n<li>Set up an XGBoost learner.</li>\n<li>Initialize a search space for <code>max_depth</code> and <code>eta</code> using <code>ps()</code>.</li>\n<li>Initialize an <code>AutoTuner</code> with the xgboost model from the previous exercise an an input. The <code>AutoTuner</code> reflects the inner resampling loop. It should be initialized for 3-fold CV, random search with 20 evaluations and the classification error as performance measure.</li>\n<li>Specify a <code>Resampling</code> object using <code>rsmp()</code>.</li>\n<li>Use this object with the credit task and <code>AutoTuner</code> as an input to <code>resample()</code>.</li>\n<li>Extract the results via <code>$aggregate()</code>.</li>\n</ol>\n<p>Important: Early stopping requires a validation set. But <code>AutoTuner</code> uses internal resampling instead of splitting the data manually, and does not provide a “validate” set to the learner by default. That is why we should not use early stopping here.</p>\n</details>\n<details>\n<summary>\n<strong>Hint 2:</strong>\n</summary>\n<div class=\"cell\" data-layout-align=\"center\">\n<pre>xgboost_lrn3 = lrn(...)\n\ntune_ps = ps(\n  max_depth = p_int(..., ...),\n  eta = p_dbl(..., ...)\n)\n\nat = auto_tuner(xgboost_lrn2, \n  resampling = rsmp(\"cv\", folds = ...),\n  search_space = ...,\n  measure = msr(\"...\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"...\", resolution = 5L))\n\nresampling = rsmp(\"...\", folds = ...)\n\nset.seed(8002L)\nnestrr = resample(task = ..., learner = ..., resampling = resampling)\n\nnestrr$aggregate()</pre>\n</div>\n</details>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div aria-controls=\"callout-8\" aria-expanded=\"false\" aria-label=\"Toggle callout\" class=\"callout-header d-flex align-content-center\" data-bs-=\"\" data-bs-toggle=\"collapse\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nSolution\n</div>\n<div class=\"callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end\"><i class=\"callout-toggle\"></i></div>\n</div>\n<div class=\"callout-8-contents callout-collapse collapse\" id=\"callout-8\">\n<div class=\"callout-body-container callout-body\">\n<div>\n<div class=\"b64-wrapper\"><button class=\"unlock-btn\" data-cf-modified-c1b428b29e1b68426ffc89f4-=\"\" onclick=\"if (!window.__cfRLUnblockHandlers) return false; unlockOne(this)\">Unlock solution</button><div class=\"hidden-solution\" data-encoded=\"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\" style=\"display:none\"></div></div>\n</div>\n</div>\n</div>\n</div>\n</section>\n<section class=\"level1\" id=\"summary\">\n<h1>Summary</h1>\n<p>In this exercise sheet, we learned how to apply a XGBoost learner to the credit data set By using resampling, we estimated the performance. XGBoost has a lot of hyperparameters and we only had a closer look on two of them. We also saw how early stopping could be facilitated which should help to avoid overfitting of the XGBoost model.</p>\n<p>Interestingly, we obtained best results, when we used 100 iterations, without tuning or early stopping. However, performance differences were quite small – if we set a different seed, we might see a different ranking. Furthermore, we could extend our tuning search space such that more hyperparameters are considered to increase overall performance of the learner for the task at hand. Of course, this also requires more budget for the tuning (e.g., more evaluations of random search).</p>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mlr-org.com/gallery/appliedml/2025-04-28-feature-preproc-xgboost/\"> mlr-org</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Xgboost\nPosted on\nMay 22, 2025\nby\nGiuseppe Casalicchio\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nmlr-org\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nJavaScript is required to unlock solutions.\nPlease enable JavaScript and reload the page,\nor download the source files from\nGitHub\nand run the code locally.\nGoal\nOur goal for this exercise sheet is to understand how to apply and work with XGBoost. The XGBoost algorithm has a large range of hyperparameters. We learn specifically how to tune these hyperparameters to optimize our XGBoost model for the task at hand.\nGerman Credit Dataset\nAs in previous exercises, we use the German credit dataset of Prof. Dr. Hans Hoffman of the University of Hamburg in 1994. By using XGBoost, we want to classify people as a good or bad credit risk based on 20 personal, demographic and financial features. The dataset is available at the UCI repository as\nStatlog (German Credit Data) Data Set\n.\nPreprocessing\nTo apply the XGBoost algorithm to the\ncredit\ndataset, categorical features need to be converted into numeric features e.g. using one-hot-encoding. We use a factor encoding\nPipeOp\nfrom\nmlr3pipelines\nto do so.\nFirst, we setup a classification task:\nlibrary(mlr3verse)\ntask = tsk(\"german_credit\")\ntask$positive = \"good\"\nNext, we can initialize a factor encoding and apply it to the task at hand.\npoe = po(\"encode\")\ntask = poe$train(list(task))[[1]]\n1 XGBoost Learner\n1.1 Initialize an XGBoost Learner\nInitialize a XGBoost\nmlr3\nlearner with 100 iterations. Make sure that that you have installed the\nxgboost\nR package.\nDetails on iterations:\nThe number of iterations must always be chosen by the user, since the hyperparameter has no proper default value in\nmlr3\n.\n“No proper default value” means that\nmlr3\nhas an adjusted default of 1 iteration to avoid errors when constructing the learner. One single iteration is, in general, not a good default, since we only conduct a single boosting step.\nThere is a trade-off between underfitting (not enough iterations) and overfitting (too many iterations). Therefore, it is always better to tune such a hyperparameter. In this exercise, we chose 100 iterations because we believe it is an upper bound for the number of iterations. We will later conduct early stopping to avoid overfitting.\nHint 1:\nThe number of iterations can be specified via the\nnrounds\nhyperparameter of the\nclassif.xgboost\nlearner, set this hyperparameter to\n100\n.\nHint 2:\nxgboost_lrn = lrn(..., nrounds = ...)\nSolution\nUnlock solution\n1.2 Performance Assessment using Cross-validation\nUse 5-fold cross-validation to estimate the generalization error of the XGBoost learner with 100 boosting iterations on the one-hot-encoded credit task. Measure the performance of the learner using the classification error. Set up a seed to make your results reproducible (e.g.,\nset.seed(8002L)\n).\nHint 1:\nSpecifically, you need to conduct three steps:\nSpecify a\nResampling\nobject using\nrsmp()\n.\nUse this object together with the task and learner specified above as an input to the\nresample()\nmethod.\nMeasure the performance with the\n$aggregate()\nmethod of the resulting\nResampleResult\nobject.\nHint 2:\nset.seed(8002L)\nresampling = rsmp(\"cv\", ...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$aggregate()\nSolution\nUnlock solution\n2 Hyperparameters\n2.1 Overview of Hyperparameters\nApart from the number of iterations (\nnrounds\n), the XGBoost learner has a several other hyperparameters which were kept to their default values in the previous exercise. Extract an overview of all hyperparameters from the initalized XGBoost learner (previous exercise) as well as its default values.\nGiven the extracted hyperparameter list above and the help page of\nxgboost\n(\n?xgboost\n), answer the following questions:\nDoes the learner rely on a tree or a linear booster by default?\nDo more hyperparameters exist for the tree or the linear booster?\nWhat do\nmax_depth\nand\neta\nmean and what are their default values?\nDoes a larger value for\neta\nimply a larger value for\nnrounds\n?\nHint 1:\nThe hyperparameters and their default values could be extracted by the\n$param_set\nfield of the XGBoost learner. Alternatively, you could call the help page of\nLearnerClassifXgboost\n.\nHint 2:\nYou can answer all questions concerning defaults with the output of the\n$param_set\n. A description of the hyperparameters could be found on the\nxgboost\nhelp page (\n?xgboost\n). The help page also offers an answer to the last question concerning the connection between\neta\nand\nnrounds\n.\nSolution\nUnlock solution\n2.2 Tune Hyperparameters\nTune the the depth of tree of the\nxgboost\nlearner on the German credit data using random search\nwith the search space for\nmax_depth\nbetween 1 and 8 and for\neta\nbetween 0.2 and 0.4\nwith 20 evaluations as termination criterion\nthe classification error\nmsr(\"classif.ce\")\nas performance measure\n3-fold CV as resampling strategy.\nSet a seed for reproducibility (e.g.,\nset.seed(8002L)\n).\nHint 1:\nSpecifically, you should conduct the following steps:\nSetup a search space with\nps()\nconsisting of a\np_int()\nfor\nmax_depth\nand\np_dbl()\nfor\neta\n.\nSetup the classification error as a tuning measure with\nmsr()\n.\nInitialize cross-validation as the resampling strategy using\nrsmp()\n.\nSetup 10 evaluations as the termination criterion using\ntrm()\n.\nInitialize a\nTuningInstanceSingleCrit\nobject using\nti()\nand the objects produced in steps 1.-4. as well as the task and learner as an input.\nDefine random search as the tuner object using\ntnr()\n.\nCall the\n$optimize()\nmethod of the tuner object with the initialized\nTuningInstanceSingleCrit\nas an input.\nHint 2:\nset.seed(8002L)\n\nsearch_space = ps(\n  max_depth = ...(1L, 8L),\n  eta = ...(0.2, 0.4)\n)\nmeasure = msr(\"classif....\")\nresampling = rsmp(\"cv\", folds = ...)\nterminator = trm(\"evals\", n_evals = ...)\n\ninstance_random = ti(\n  task = ..., \n  learner = ..., \n  measure = ..., \n  resampling = ..., \n  terminator = ..., \n  search_space = ...\n)\n \ntuner_random = tnr(...)\ntuner_random$optimize(...)\nSolution\nUnlock solution\n2.3 Inspect the the Best Performing Setup\nWhich tree depth was the best performing one?\nHint 1:\nInspect the tuned instance (of class\nTuningInstanceSingleCrit\n, it was the input to\n$optimize()\n). Look, for example, at the\n$result\nfield.\nHint 2:\ninstance_random$result\nSolution\nUnlock solution\n3 Early Stopping\n3.1 Set up an XGBoost Learner with Early Stopping\nNow that we derived the best hyperparameter for the maximum depth and eta, we could train our final model. To avoid overfitting we conduct early stopping – meaning that the algorithm stops as soon as a the performance does not improve for a given number of rounds to avoid overfitting. The performance for stopping should be assessed via a validation data set.\nSet up an XGBoost learner with the following hyperparameters:\nmax_depth\nand\neta\nset to the best configurations according to the previous tuning task.\nnrounds\nset to 100L.\nThe number of early stopping rounds set to 5 (this could be tuned, as well, but we simplify things) in order to stop earlier if there was no improvement in the previous 5 iterations.\nlibrary(\"xgboost\")\nset.seed(2001L)\nHint 1:\nSpecify the hyperparameters within\nlrn()\n. The number of rounds could be specified with\nearly_stopping_rounds\n.\nHint 2:\nlrn(\"...\", nrounds = ..., \n  max_depth = instance_random$result$..., \n  eta = instance_random$result$..., \n  early_stopping_rounds = ....\n)\nSolution\nUnlock solution\n3.2 Training on Credit Data\nTrain the XGBoost learner from the previous exercise on the credit data set How many iterations were conducted before the boosting algorithm stopped?\nHint 1:\nBy calling\n$train()\na model is trained which can be accessed via\n$model\n. This model has a field\n$niter\n– the number of conducted iterations.\nHint 2:\nxgboost$train(...)\nxgboost$...$niter\nSolution\nUnlock solution\n4 Extra: Nested Resampling\nTo receive an unbiased performance estimate when tuning hyperparameters, conduct nested resampling with\n3-fold cross-validation for the outer and inner resampling loop.\na search space for\nmax_depth\nbetween 1 and 8 and\neta\nbetween 0.2 and 0.4.\nrandom search with 20 evaluations\nthe classification error\nmsr(\"classif.ce\")\nas performance measure.\nExtract the performance estimate on the outer resampling folds.\nHint 1:\nSpecifically, you need to conduct the following steps:\nSet up an XGBoost learner.\nInitialize a search space for\nmax_depth\nand\neta\nusing\nps()\n.\nInitialize an\nAutoTuner\nwith the xgboost model from the previous exercise an an input. The\nAutoTuner\nreflects the inner resampling loop. It should be initialized for 3-fold CV, random search with 20 evaluations and the classification error as performance measure.\nSpecify a\nResampling\nobject using\nrsmp()\n.\nUse this object with the credit task and\nAutoTuner\nas an input to\nresample()\n.\nExtract the results via\n$aggregate()\n.\nImportant: Early stopping requires a validation set. But\nAutoTuner\nuses internal resampling instead of splitting the data manually, and does not provide a “validate” set to the learner by default. That is why we should not use early stopping here.\nHint 2:\nxgboost_lrn3 = lrn(...)\n\ntune_ps = ps(\n  max_depth = p_int(..., ...),\n  eta = p_dbl(..., ...)\n)\n\nat = auto_tuner(xgboost_lrn2, \n  resampling = rsmp(\"cv\", folds = ...),\n  search_space = ...,\n  measure = msr(\"...\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"...\", resolution = 5L))\n\nresampling = rsmp(\"...\", folds = ...)\n\nset.seed(8002L)\nnestrr = resample(task = ..., learner = ..., resampling = resampling)\n\nnestrr$aggregate()\nSolution\nUnlock solution\nSummary\nIn this exercise sheet, we learned how to apply a XGBoost learner to the credit data set By using resampling, we estimated the performance. XGBoost has a lot of hyperparameters and we only had a closer look on two of them. We also saw how early stopping could be facilitated which should help to avoid overfitting of the XGBoost model.\nInterestingly, we obtained best results, when we used 100 iterations, without tuning or early stopping. However, performance differences were quite small – if we set a different seed, we might see a different ranking. Furthermore, we could extend our tuning search space such that more hyperparameters are considered to increase overall performance of the learner for the task at hand. Of course, this also requires more budget for the tuning (e.g., more evaluations of random search).\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nmlr-org\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Our goal for this exercise sheet is to understand how to apply and work with XGBoost. The XGBoost algorithm has a large range of hyperparameters. We learn specifically how to tune these hyperparameters to optimize our XGBoost model for the task at hand. German Credit Dataset As in previous exercises, we use the German credit dataset of Prof. Dr. Hans Hoffman of the University of Hamburg in 1994. By using XGBoost, we want to classify people as a good or bad credit risk based on 20 personal, demographic and financial features. The dataset is available at the UCI repository as Statlog (German Credit Data) Data Set. Preprocessing To apply the XGBoost algorithm to the credit dataset, categorical features need to be converted into numeric features e.g. using one-hot-encoding. We use a factor encoding PipeOp from mlr3pipelines to do so. First, we setup a classification task: library(mlr3verse) task = tsk(\"german_credit\") task$positive = \"good\" Next, we can initialize a factor encoding and apply it to the task at hand. poe = po(\"encode\") task = poe$train(list(task))[[1]] 1 XGBoost Learner 1.1 Initialize an XGBoost Learner Initialize a XGBoost mlr3 learner with 100 iterations. Make sure that that you have installed the xgboost R package. Details on iterations: The number of iterations must always be chosen by the user, since the hyperparameter has no proper default value in mlr3. “No proper default value” means that mlr3 has an adjusted default of 1 iteration to avoid errors when constructing the learner. One single iteration is, in general, not a good default, since we only conduct a single boosting step. There is a trade-off between underfitting (not enough iterations) and overfitting (too many iterations). Therefore, it is always better to tune such a hyperparameter. In this exercise, we chose 100 iterations because we believe it is an upper bound for the number of iterations. We will later conduct early stopping to avoid overfitting. Hint 1: The number of iterations can be specified via the nrounds hyperparameter of the classif.xgboost learner, set this hyperparameter to 100. Hint 2: xgboost_lrn = lrn(..., nrounds = ...) Solution Unlock solution 1.2 Performance Assessment using Cross-validation Use 5-fold cross-validation to estimate the generalization error of the XGBoost learner with 100 boosting iterations on the one-hot-encoded credit task. Measure the performance of the learner using the classification error. Set up a seed to make your results reproducible (e.g., set.seed(8002L)). Hint 1: Specifically, you need to conduct three steps: Specify a Resampling object using rsmp(). Use this object together with the task and learner specified above as an input to the resample() method. Measure the performance with the $aggregate() method of the resulting ResampleResult object. Hint 2: set.seed(8002L) resampling = rsmp(\"cv\", ...) rr = resample(task = ..., learner = ..., resampling = ...) rr$aggregate() Solution Unlock solution 2 Hyperparameters 2.1 Overview of Hyperparameters Apart from the number of iterations (nrounds), the XGBoost learner has a several other hyperparameters which were kept to their default values in the previous exercise. Extract an overview of all hyperparameters from the initalized XGBoost learner (previous exercise) as well as its default values. Given the extracted hyperparameter list above and the help page of xgboost (?xgboost), answer the following questions: Does the learner rely on a tree or a linear booster by default? Do more hyperparameters exist for the tree or the linear booster? What do max_depth and eta mean and what are their default values? Does a larger value for eta imply a larger value for nrounds? Hint 1: The hyperparameters and their default values could be extracted by the $param_set field of the XGBoost learner. Alternatively, you could call the help page of LearnerClassifXgboost. Hint 2: You can answer all questions concerning defaults with the output of the $param_set. A description of the hyperparameters could be found on the xgboost help page (?xgboost). The help page also offers an answer to the last question concerning the connection between eta and nrounds. Solution Unlock solution 2.2 Tune Hyperparameters Tune the the depth of tree of the xgboost learner on the German credit data using random search with the search space for max_depth between 1 and 8 and for eta between 0.2 and 0.4 with 20 evaluations as termination criterion the classification error msr(\"classif.ce\") as performance measure 3-fold CV as resampling strategy. Set a seed for reproducibility (e.g., set.seed(8002L)). Hint 1: Specifically, you should conduct the following steps: Setup a search space with ps() consisting of a p_int() for max_depth and p_dbl() for eta. Setup the classification error as a tuning measure with msr(). Initialize cross-validation as the resampling strategy using rsmp(). Setup 10 evaluations as the termination criterion using trm(). Initialize a TuningInstanceSingleCrit object using ti() and the objects produced in steps 1.-4. as well as the task and learner as an input. Define random search as the tuner object using tnr(). Call the $optimize() method of the tuner object with the initialized TuningInstanceSingleCrit as an input. Hint 2: set.seed(8002L) search_space = ps( max_depth = ...(1L, 8L), eta = ...(0.2, 0.4) ) measure = msr(\"classif....\") resampling = rsmp(\"cv\", folds = ...) terminator = trm(\"evals\", n_evals = ...) instance_random = ti( task = ..., learner = ..., measure = ..., resampling = ..., terminator = ..., search_space = ... ) tuner_random = tnr(...) tuner_random$optimize(...) Solution Unlock solution 2.3 Inspect the the Best Performing Setup Which tree depth was the best performing one? Hint 1: Inspect the tuned instance (of class TuningInstanceSingleCrit, it was the input to $optimize()). Look, for example, at the $result field. Hint 2: instance_random$result Solution Unlock solution 3 Early Stopping 3.1 Set up an XGBoost Learner with Early Stopping Now that we derived the best hyperparameter for the maximum depth and eta, we could train our final model. To avoid overfitting we conduct early stopping - meaning that the algorithm stops as soon as a the performance does not improve for a given number of rounds to avoid overfitting. The performance for stopping should be assessed via a validation data set. Set up an XGBoost learner with the following hyperparameters: max_depth and eta set to the best configurations according to the previous tuning task. nrounds set to 100L. The number of early stopping rounds set to 5 (this could be tuned, as well, but we simplify things) in order to stop earlier if there was no improvement in the previous 5 iterations. library(\"xgboost\") set.seed(2001L) Hint 1: Specify the hyperparameters within lrn(). The number of rounds could be specified with early_stopping_rounds. Hint 2: lrn(\"...\", nrounds = ..., max_depth = instance_random$result$..., eta = instance_random$result$..., early_stopping_rounds = .... ) Solution Unlock solution 3.2 Training on Credit Data Train the XGBoost learner from the previous exercise on the credit data set How many iterations were conducted before the boosting algorithm stopped? Hint 1: By calling $train() a model is trained which can be accessed via $model. This model has a field $niter - the number of conducted iterations. Hint 2: xgboost$train(...) xgboost$...$niter Solution Unlock solution 4 Extra: Nested Resampling To receive an unbiased performance estimate when tuning hyperparameters, conduct nested resampling with 3-fold cross-validation for the outer and inner resampling loop. a search space for max_depth between 1 and 8 and eta between 0.2 and 0.4. random search with 20 evaluations the classification error msr(\"classif.ce\") as performance measure. Extract the performance estimate on the outer resampling folds. Hint 1: Specifically, you need to conduct the following steps: Set up an XGBoost learner. Initialize a search space for max_depth and eta using ps(). Initialize an AutoTuner with the xgboost model from the previous exercise an an input. The AutoTuner reflects the inner resampling loop. It should be initialized for 3-fold CV, random search with 20 evaluations and the classification error as performance measure. Specify a Resampling object using rsmp(). Use this object with the credit task and AutoTuner as an input to resample(). Extract the results via $aggregate(). Important: Early stopping requires a validation set. But AutoTuner uses internal resampling instead of splitting the data manually, and does not provide a “validate” set to the learner by default. That is why we should not use early stopping here. Hint 2: xgboost_lrn3 = lrn(...) tune_ps = ps( max_depth = p_int(..., ...), eta = p_dbl(..., ...) ) at = auto_tuner(xgboost_lrn2, resampling = rsmp(\"cv\", folds = ...), search_space = ..., measure = msr(\"...\"), terminator = trm(\"none\"), tuner = tnr(\"...\", resolution = 5L)) resampling = rsmp(\"...\", folds = ...) set.seed(8002L) nestrr = resample(task = ..., learner = ..., resampling = resampling) nestrr$aggregate() Solution Unlock solution Summary In this exercise sheet, we learned how to apply a XGBoost learner to the credit data set By using resampling, we estimated the performance. XGBoost has a lot of hyperparameters and we only had a closer look on two of them. We also saw how early stopping could be facilitated which should help to avoid overfitting of the XGBoost model. Interestingly, we obtained best results, when we used 100 iterations, without tuning or early stopping. However, performance differences were quite small - if we set a different seed, we might see a different ranking. Furthermore, we could extend our tuning search space such that more hyperparameters are considered to increase overall performance of the learner for the task at hand. Of course, this also requires more budget for the tuning (e.g., more evaluations of random search).",
      "meta_keywords": null,
      "og_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Our goal for this exercise sheet is to understand how to apply and work with XGBoost. The XGBoost algorithm has a large range of hyperparameters. We learn specifically how to tune these hyperparameters to optimize our XGBoost model for the task at hand. German Credit Dataset As in previous exercises, we use the German credit dataset of Prof. Dr. Hans Hoffman of the University of Hamburg in 1994. By using XGBoost, we want to classify people as a good or bad credit risk based on 20 personal, demographic and financial features. The dataset is available at the UCI repository as Statlog (German Credit Data) Data Set. Preprocessing To apply the XGBoost algorithm to the credit dataset, categorical features need to be converted into numeric features e.g. using one-hot-encoding. We use a factor encoding PipeOp from mlr3pipelines to do so. First, we setup a classification task: library(mlr3verse) task = tsk(\"german_credit\") task$positive = \"good\" Next, we can initialize a factor encoding and apply it to the task at hand. poe = po(\"encode\") task = poe$train(list(task))[[1]] 1 XGBoost Learner 1.1 Initialize an XGBoost Learner Initialize a XGBoost mlr3 learner with 100 iterations. Make sure that that you have installed the xgboost R package. Details on iterations: The number of iterations must always be chosen by the user, since the hyperparameter has no proper default value in mlr3. “No proper default value” means that mlr3 has an adjusted default of 1 iteration to avoid errors when constructing the learner. One single iteration is, in general, not a good default, since we only conduct a single boosting step. There is a trade-off between underfitting (not enough iterations) and overfitting (too many iterations). Therefore, it is always better to tune such a hyperparameter. In this exercise, we chose 100 iterations because we believe it is an upper bound for the number of iterations. We will later conduct early stopping to avoid overfitting. Hint 1: The number of iterations can be specified via the nrounds hyperparameter of the classif.xgboost learner, set this hyperparameter to 100. Hint 2: xgboost_lrn = lrn(..., nrounds = ...) Solution Unlock solution 1.2 Performance Assessment using Cross-validation Use 5-fold cross-validation to estimate the generalization error of the XGBoost learner with 100 boosting iterations on the one-hot-encoded credit task. Measure the performance of the learner using the classification error. Set up a seed to make your results reproducible (e.g., set.seed(8002L)). Hint 1: Specifically, you need to conduct three steps: Specify a Resampling object using rsmp(). Use this object together with the task and learner specified above as an input to the resample() method. Measure the performance with the $aggregate() method of the resulting ResampleResult object. Hint 2: set.seed(8002L) resampling = rsmp(\"cv\", ...) rr = resample(task = ..., learner = ..., resampling = ...) rr$aggregate() Solution Unlock solution 2 Hyperparameters 2.1 Overview of Hyperparameters Apart from the number of iterations (nrounds), the XGBoost learner has a several other hyperparameters which were kept to their default values in the previous exercise. Extract an overview of all hyperparameters from the initalized XGBoost learner (previous exercise) as well as its default values. Given the extracted hyperparameter list above and the help page of xgboost (?xgboost), answer the following questions: Does the learner rely on a tree or a linear booster by default? Do more hyperparameters exist for the tree or the linear booster? What do max_depth and eta mean and what are their default values? Does a larger value for eta imply a larger value for nrounds? Hint 1: The hyperparameters and their default values could be extracted by the $param_set field of the XGBoost learner. Alternatively, you could call the help page of LearnerClassifXgboost. Hint 2: You can answer all questions concerning defaults with the output of the $param_set. A description of the hyperparameters could be found on the xgboost help page (?xgboost). The help page also offers an answer to the last question concerning the connection between eta and nrounds. Solution Unlock solution 2.2 Tune Hyperparameters Tune the the depth of tree of the xgboost learner on the German credit data using random search with the search space for max_depth between 1 and 8 and for eta between 0.2 and 0.4 with 20 evaluations as termination criterion the classification error msr(\"classif.ce\") as performance measure 3-fold CV as resampling strategy. Set a seed for reproducibility (e.g., set.seed(8002L)). Hint 1: Specifically, you should conduct the following steps: Setup a search space with ps() consisting of a p_int() for max_depth and p_dbl() for eta. Setup the classification error as a tuning measure with msr(). Initialize cross-validation as the resampling strategy using rsmp(). Setup 10 evaluations as the termination criterion using trm(). Initialize a TuningInstanceSingleCrit object using ti() and the objects produced in steps 1.-4. as well as the task and learner as an input. Define random search as the tuner object using tnr(). Call the $optimize() method of the tuner object with the initialized TuningInstanceSingleCrit as an input. Hint 2: set.seed(8002L) search_space = ps( max_depth = ...(1L, 8L), eta = ...(0.2, 0.4) ) measure = msr(\"classif....\") resampling = rsmp(\"cv\", folds = ...) terminator = trm(\"evals\", n_evals = ...) instance_random = ti( task = ..., learner = ..., measure = ..., resampling = ..., terminator = ..., search_space = ... ) tuner_random = tnr(...) tuner_random$optimize(...) Solution Unlock solution 2.3 Inspect the the Best Performing Setup Which tree depth was the best performing one? Hint 1: Inspect the tuned instance (of class TuningInstanceSingleCrit, it was the input to $optimize()). Look, for example, at the $result field. Hint 2: instance_random$result Solution Unlock solution 3 Early Stopping 3.1 Set up an XGBoost Learner with Early Stopping Now that we derived the best hyperparameter for the maximum depth and eta, we could train our final model. To avoid overfitting we conduct early stopping - meaning that the algorithm stops as soon as a the performance does not improve for a given number of rounds to avoid overfitting. The performance for stopping should be assessed via a validation data set. Set up an XGBoost learner with the following hyperparameters: max_depth and eta set to the best configurations according to the previous tuning task. nrounds set to 100L. The number of early stopping rounds set to 5 (this could be tuned, as well, but we simplify things) in order to stop earlier if there was no improvement in the previous 5 iterations. library(\"xgboost\") set.seed(2001L) Hint 1: Specify the hyperparameters within lrn(). The number of rounds could be specified with early_stopping_rounds. Hint 2: lrn(\"...\", nrounds = ..., max_depth = instance_random$result$..., eta = instance_random$result$..., early_stopping_rounds = .... ) Solution Unlock solution 3.2 Training on Credit Data Train the XGBoost learner from the previous exercise on the credit data set How many iterations were conducted before the boosting algorithm stopped? Hint 1: By calling $train() a model is trained which can be accessed via $model. This model has a field $niter - the number of conducted iterations. Hint 2: xgboost$train(...) xgboost$...$niter Solution Unlock solution 4 Extra: Nested Resampling To receive an unbiased performance estimate when tuning hyperparameters, conduct nested resampling with 3-fold cross-validation for the outer and inner resampling loop. a search space for max_depth between 1 and 8 and eta between 0.2 and 0.4. random search with 20 evaluations the classification error msr(\"classif.ce\") as performance measure. Extract the performance estimate on the outer resampling folds. Hint 1: Specifically, you need to conduct the following steps: Set up an XGBoost learner. Initialize a search space for max_depth and eta using ps(). Initialize an AutoTuner with the xgboost model from the previous exercise an an input. The AutoTuner reflects the inner resampling loop. It should be initialized for 3-fold CV, random search with 20 evaluations and the classification error as performance measure. Specify a Resampling object using rsmp(). Use this object with the credit task and AutoTuner as an input to resample(). Extract the results via $aggregate(). Important: Early stopping requires a validation set. But AutoTuner uses internal resampling instead of splitting the data manually, and does not provide a “validate” set to the learner by default. That is why we should not use early stopping here. Hint 2: xgboost_lrn3 = lrn(...) tune_ps = ps( max_depth = p_int(..., ...), eta = p_dbl(..., ...) ) at = auto_tuner(xgboost_lrn2, resampling = rsmp(\"cv\", folds = ...), search_space = ..., measure = msr(\"...\"), terminator = trm(\"none\"), tuner = tnr(\"...\", resolution = 5L)) resampling = rsmp(\"...\", folds = ...) set.seed(8002L) nestrr = resample(task = ..., learner = ..., resampling = resampling) nestrr$aggregate() Solution Unlock solution Summary In this exercise sheet, we learned how to apply a XGBoost learner to the credit data set By using resampling, we estimated the performance. XGBoost has a lot of hyperparameters and we only had a closer look on two of them. We also saw how early stopping could be facilitated which should help to avoid overfitting of the XGBoost model. Interestingly, we obtained best results, when we used 100 iterations, without tuning or early stopping. However, performance differences were quite small - if we set a different seed, we might see a different ranking. Furthermore, we could extend our tuning search space such that more hyperparameters are considered to increase overall performance of the learner for the task at hand. Of course, this also requires more budget for the tuning (e.g., more evaluations of random search).",
      "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
      "og_title": "Xgboost | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 8.5,
      "sitemap_lastmod": null,
      "twitter_description": "JavaScript is required to unlock solutions. Please enable JavaScript and reload the page, or download the source files from GitHub and run the code locally. Goal Our goal for this exercise sheet is to understand how to apply and work with XGBoost. The XGBoost algorithm has a large range of hyperparameters. We learn specifically how to tune these hyperparameters to optimize our XGBoost model for the task at hand. German Credit Dataset As in previous exercises, we use the German credit dataset of Prof. Dr. Hans Hoffman of the University of Hamburg in 1994. By using XGBoost, we want to classify people as a good or bad credit risk based on 20 personal, demographic and financial features. The dataset is available at the UCI repository as Statlog (German Credit Data) Data Set. Preprocessing To apply the XGBoost algorithm to the credit dataset, categorical features need to be converted into numeric features e.g. using one-hot-encoding. We use a factor encoding PipeOp from mlr3pipelines to do so. First, we setup a classification task: library(mlr3verse) task = tsk(\"german_credit\") task$positive = \"good\" Next, we can initialize a factor encoding and apply it to the task at hand. poe = po(\"encode\") task = poe$train(list(task))[[1]] 1 XGBoost Learner 1.1 Initialize an XGBoost Learner Initialize a XGBoost mlr3 learner with 100 iterations. Make sure that that you have installed the xgboost R package. Details on iterations: The number of iterations must always be chosen by the user, since the hyperparameter has no proper default value in mlr3. “No proper default value” means that mlr3 has an adjusted default of 1 iteration to avoid errors when constructing the learner. One single iteration is, in general, not a good default, since we only conduct a single boosting step. There is a trade-off between underfitting (not enough iterations) and overfitting (too many iterations). Therefore, it is always better to tune such a hyperparameter. In this exercise, we chose 100 iterations because we believe it is an upper bound for the number of iterations. We will later conduct early stopping to avoid overfitting. Hint 1: The number of iterations can be specified via the nrounds hyperparameter of the classif.xgboost learner, set this hyperparameter to 100. Hint 2: xgboost_lrn = lrn(..., nrounds = ...) Solution Unlock solution 1.2 Performance Assessment using Cross-validation Use 5-fold cross-validation to estimate the generalization error of the XGBoost learner with 100 boosting iterations on the one-hot-encoded credit task. Measure the performance of the learner using the classification error. Set up a seed to make your results reproducible (e.g., set.seed(8002L)). Hint 1: Specifically, you need to conduct three steps: Specify a Resampling object using rsmp(). Use this object together with the task and learner specified above as an input to the resample() method. Measure the performance with the $aggregate() method of the resulting ResampleResult object. Hint 2: set.seed(8002L) resampling = rsmp(\"cv\", ...) rr = resample(task = ..., learner = ..., resampling = ...) rr$aggregate() Solution Unlock solution 2 Hyperparameters 2.1 Overview of Hyperparameters Apart from the number of iterations (nrounds), the XGBoost learner has a several other hyperparameters which were kept to their default values in the previous exercise. Extract an overview of all hyperparameters from the initalized XGBoost learner (previous exercise) as well as its default values. Given the extracted hyperparameter list above and the help page of xgboost (?xgboost), answer the following questions: Does the learner rely on a tree or a linear booster by default? Do more hyperparameters exist for the tree or the linear booster? What do max_depth and eta mean and what are their default values? Does a larger value for eta imply a larger value for nrounds? Hint 1: The hyperparameters and their default values could be extracted by the $param_set field of the XGBoost learner. Alternatively, you could call the help page of LearnerClassifXgboost. Hint 2: You can answer all questions concerning defaults with the output of the $param_set. A description of the hyperparameters could be found on the xgboost help page (?xgboost). The help page also offers an answer to the last question concerning the connection between eta and nrounds. Solution Unlock solution 2.2 Tune Hyperparameters Tune the the depth of tree of the xgboost learner on the German credit data using random search with the search space for max_depth between 1 and 8 and for eta between 0.2 and 0.4 with 20 evaluations as termination criterion the classification error msr(\"classif.ce\") as performance measure 3-fold CV as resampling strategy. Set a seed for reproducibility (e.g., set.seed(8002L)). Hint 1: Specifically, you should conduct the following steps: Setup a search space with ps() consisting of a p_int() for max_depth and p_dbl() for eta. Setup the classification error as a tuning measure with msr(). Initialize cross-validation as the resampling strategy using rsmp(). Setup 10 evaluations as the termination criterion using trm(). Initialize a TuningInstanceSingleCrit object using ti() and the objects produced in steps 1.-4. as well as the task and learner as an input. Define random search as the tuner object using tnr(). Call the $optimize() method of the tuner object with the initialized TuningInstanceSingleCrit as an input. Hint 2: set.seed(8002L) search_space = ps( max_depth = ...(1L, 8L), eta = ...(0.2, 0.4) ) measure = msr(\"classif....\") resampling = rsmp(\"cv\", folds = ...) terminator = trm(\"evals\", n_evals = ...) instance_random = ti( task = ..., learner = ..., measure = ..., resampling = ..., terminator = ..., search_space = ... ) tuner_random = tnr(...) tuner_random$optimize(...) Solution Unlock solution 2.3 Inspect the the Best Performing Setup Which tree depth was the best performing one? Hint 1: Inspect the tuned instance (of class TuningInstanceSingleCrit, it was the input to $optimize()). Look, for example, at the $result field. Hint 2: instance_random$result Solution Unlock solution 3 Early Stopping 3.1 Set up an XGBoost Learner with Early Stopping Now that we derived the best hyperparameter for the maximum depth and eta, we could train our final model. To avoid overfitting we conduct early stopping - meaning that the algorithm stops as soon as a the performance does not improve for a given number of rounds to avoid overfitting. The performance for stopping should be assessed via a validation data set. Set up an XGBoost learner with the following hyperparameters: max_depth and eta set to the best configurations according to the previous tuning task. nrounds set to 100L. The number of early stopping rounds set to 5 (this could be tuned, as well, but we simplify things) in order to stop earlier if there was no improvement in the previous 5 iterations. library(\"xgboost\") set.seed(2001L) Hint 1: Specify the hyperparameters within lrn(). The number of rounds could be specified with early_stopping_rounds. Hint 2: lrn(\"...\", nrounds = ..., max_depth = instance_random$result$..., eta = instance_random$result$..., early_stopping_rounds = .... ) Solution Unlock solution 3.2 Training on Credit Data Train the XGBoost learner from the previous exercise on the credit data set How many iterations were conducted before the boosting algorithm stopped? Hint 1: By calling $train() a model is trained which can be accessed via $model. This model has a field $niter - the number of conducted iterations. Hint 2: xgboost$train(...) xgboost$...$niter Solution Unlock solution 4 Extra: Nested Resampling To receive an unbiased performance estimate when tuning hyperparameters, conduct nested resampling with 3-fold cross-validation for the outer and inner resampling loop. a search space for max_depth between 1 and 8 and eta between 0.2 and 0.4. random search with 20 evaluations the classification error msr(\"classif.ce\") as performance measure. Extract the performance estimate on the outer resampling folds. Hint 1: Specifically, you need to conduct the following steps: Set up an XGBoost learner. Initialize a search space for max_depth and eta using ps(). Initialize an AutoTuner with the xgboost model from the previous exercise an an input. The AutoTuner reflects the inner resampling loop. It should be initialized for 3-fold CV, random search with 20 evaluations and the classification error as performance measure. Specify a Resampling object using rsmp(). Use this object with the credit task and AutoTuner as an input to resample(). Extract the results via $aggregate(). Important: Early stopping requires a validation set. But AutoTuner uses internal resampling instead of splitting the data manually, and does not provide a “validate” set to the learner by default. That is why we should not use early stopping here. Hint 2: xgboost_lrn3 = lrn(...) tune_ps = ps( max_depth = p_int(..., ...), eta = p_dbl(..., ...) ) at = auto_tuner(xgboost_lrn2, resampling = rsmp(\"cv\", folds = ...), search_space = ..., measure = msr(\"...\"), terminator = trm(\"none\"), tuner = tnr(\"...\", resolution = 5L)) resampling = rsmp(\"...\", folds = ...) set.seed(8002L) nestrr = resample(task = ..., learner = ..., resampling = resampling) nestrr$aggregate() Solution Unlock solution Summary In this exercise sheet, we learned how to apply a XGBoost learner to the credit data set By using resampling, we estimated the performance. XGBoost has a lot of hyperparameters and we only had a closer look on two of them. We also saw how early stopping could be facilitated which should help to avoid overfitting of the XGBoost model. Interestingly, we obtained best results, when we used 100 iterations, without tuning or early stopping. However, performance differences were quite small - if we set a different seed, we might see a different ranking. Furthermore, we could extend our tuning search space such that more hyperparameters are considered to increase overall performance of the learner for the task at hand. Of course, this also requires more budget for the tuning (e.g., more evaluations of random search).",
      "twitter_title": "Xgboost | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/05/xgboost/",
      "word_count": 1704
    }
  }
}