{
  "uuid": "c8bf65e9-0a29-420e-a41e-f279abfb7df5",
  "created_at": "2025-11-17 20:39:10",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2023/11/qeml-example-issues-of-overfitting-dimension-reduction-etc/",
    "crawled_at": "2025-11-17T09:53:46.945903",
    "external_links": [
      {
        "href": "https://matloff.wordpress.com/2023/11/21/qeml-example-issues-of-overfitting-dimension-reduction-etc/",
        "text": "Mad (Data) Scientist"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://t.co/E6ztc0cVbs",
        "text": "https://tinyurl.com/4hwr2vf"
      },
      {
        "href": "https://matloff.wordpress.com/2023/11/21/qeml-example-issues-of-overfitting-dimension-reduction-etc/",
        "text": "Mad (Data) Scientist"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "qeML Example: Issues of Overfitting, Dimension Reduction Etc. | R-bloggers",
    "images": [],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/matloff/",
        "text": "matloff"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-380206 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">qeML Example: Issues of Overfitting, Dimension Reduction Etc.</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 21, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/matloff/\">matloff</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://matloff.wordpress.com/2023/11/21/qeml-example-issues-of-overfitting-dimension-reduction-etc/\"> Mad (Data) Scientist</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<p>What about variable selection? Which predictor variables/features should we use? No matter what anyone tells you, <em>this is an unsolved problem.</em> But there are lots of useful methods. See the <strong>qeML</strong> vignettes on feature selection and overfitting for detailed background on the issues involved.</p>\n<p>We note at the outset what our concluding statement will be: <em>Even a very simple, very clean-looking dataset like this one may be much more nuanced than it looks.</em> Real life is not like those simplistic textbooks, eh?</p>\n<p>Here I’ll discuss <strong>qeML::qeLeaveOut1Var</strong>. (I usually omit parentheses in referring to function names; see <a href=\"https://t.co/E6ztc0cVbs\" rel=\"nofollow\" target=\"_blank\">https://tinyurl.com/4hwr2vf</a>.) The idea is simple: For each variable, find prediction accuracy with and without that variable.</p>\n<p>Let’s try it on the famous NYC taxi trip data, included (with modification) in <strong>qeML</strong>. First, note that <strong>qeML</strong> prediction calls automatically split the data into training and test sets, and compute test accuracy (mean absolute prediction error or overall misclassification error) on the latter.</p>\n<p>The call <strong>qeLeaveOut1Var(nyctaxi,’tripTime’,’qeLin’,10) </strong>predicts trip time using <strong>qeML</strong>‘s linear model. (The latter wraps <strong>lm</strong>, but adds some things and sets the standard <strong>qeML</strong> call form..) Since the test set is random (as is our data), we’ll do 10 repetitions and average the results. Instead of <strong>qeLin</strong>, we could have used any other <strong>qeML</strong> prediction function, e.g. <strong>qeKNN</strong> for k-Nearest Neighbors.</p>\n<pre>&gt; qeLeaveOut1Var(nyctaxi,'tripTime','qeLin',10)\n         full trip_distance  PULocationID  DOLocationID     DayOfWeek\n     238.4611      353.2409      253.2761      246.3186      239.2277\nThere were 50 or more warnings (use warnings() to see the first 50)</pre>\n<p>We’ll discuss the warnings shortly, but not surprisingly, trip distance is the most important variable. The pickup and dropoff locations also seem to have predictive value, though day of the week may not.</p>\n<p>But let’s take a closer look. There were 224 pickup locations. (run <strong>levels(nyctaxi$PULocationID)</strong> to see this). That’s 223 dummy (“one-hot”) variables; are some more predictive than others? To explore that in <strong>qeLeaveOut1Var</strong>, we could make the dummies explicit, so each dummy is removed one at a time:</p>\n<pre>nyct &lt;- factorsToDummies(nyctaxi,omitLast=TRUE)</pre>\n<p>This function is actually from the <strong>regtools</strong> package, included in <strong>qeML</strong>. Then we could try, say,</p>\n<pre>nyct &lt;- as.data.frame(nyct)\nqeLeaveOut1Var(nyct,'tripTime','qeLin',10)</pre>\n<p>But with so many dummies, this would take a long time to run. We could directly look at mean trip times for each pickup location to get at least some idea of their individual predictive power,</p>\n<pre>tapply(nyctaxi$tripTime,nyctaxi$PULocationID,mean)\ntapply(nyctaxi$tripTime,nyctaxi$PULocationID,length)</pre>\n<p>Many locations have very little data, so we’d have to deal with that. Note too the possibility of overfitting.</p>\n<pre>&gt; dim(nyct)\n[1] 10000  479\n</pre>\n<p>An old rule of thumb is to use under sqrt(n) variables, 100 here. Just a guide, but much less than 479. (Note: Even our analysis using the original factors still converts to dummies internally; <strong>nyctaxi </strong>has 4 columns, but <strong>lm</strong> will expand them as in <strong>nyct</strong>.)</p>\n<p>We may wish to delete pickup location entirely. Or, possibly use PCA for dimension reduction,</p>\n<pre>z &lt;- qePCA(nyctaxi,'tripTime','qeLin',pcaProp=0.75)\n</pre>\n<p>This <strong>qeML</strong> call says, “Compute PCA on the predictors, retaining enough of them for 0.75 of the total variance, and then run <strong>qeLin </strong>on the resulting PCs.” </p>\n<p>But…remember those warning messages? Running <strong>warnings()</strong> we see messages like “6 rows removed from test set, due to new factor levels.” The problem is that, in dividing the data into training and test sets, some pickup or dropoff locations appeared only in the latter, thus impossible to predict.  So, many of the columns in the training set are all 0s, thus 0 variance, thus problems with PCA. We then might run <strong>qeML::constCols</strong> to find out which columns have 0 variance, then delete those, and try <strong>qePCA </strong>again.</p>\n<p>And we haven’t even mentioned using, say, <strong>qeLASSO</strong> or <strong>qeXGBoost</strong> instead of <strong>qeLin</strong>, etc. But the point is clear: <em>Even a very simple, very clean-looking application like this one may be much more nuanced than it looks.</em></p>\n<blockquote class=\"wp-block-quote\">\n<p></p>\n</blockquote>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://matloff.wordpress.com/2023/11/21/qeml-example-issues-of-overfitting-dimension-reduction-etc/\"> Mad (Data) Scientist</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div> </div>\n</article>",
    "main_text": "qeML Example: Issues of Overfitting, Dimension Reduction Etc.\nPosted on\nNovember 21, 2023\nby\nmatloff\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nMad (Data) Scientist\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nWhat about variable selection? Which predictor variables/features should we use? No matter what anyone tells you,\nthis is an unsolved problem.\nBut there are lots of useful methods. See the\nqeML\nvignettes on feature selection and overfitting for detailed background on the issues involved.\nWe note at the outset what our concluding statement will be:\nEven a very simple, very clean-looking dataset like this one may be much more nuanced than it looks.\nReal life is not like those simplistic textbooks, eh?\nHere I’ll discuss\nqeML::qeLeaveOut1Var\n. (I usually omit parentheses in referring to function names; see\nhttps://tinyurl.com/4hwr2vf\n.) The idea is simple: For each variable, find prediction accuracy with and without that variable.\nLet’s try it on the famous NYC taxi trip data, included (with modification) in\nqeML\n. First, note that\nqeML\nprediction calls automatically split the data into training and test sets, and compute test accuracy (mean absolute prediction error or overall misclassification error) on the latter.\nThe call\nqeLeaveOut1Var(nyctaxi,’tripTime’,’qeLin’,10)\npredicts trip time using\nqeML\n‘s linear model. (The latter wraps\nlm\n, but adds some things and sets the standard\nqeML\ncall form..) Since the test set is random (as is our data), we’ll do 10 repetitions and average the results. Instead of\nqeLin\n, we could have used any other\nqeML\nprediction function, e.g.\nqeKNN\nfor k-Nearest Neighbors.\n> qeLeaveOut1Var(nyctaxi,'tripTime','qeLin',10)\n         full trip_distance  PULocationID  DOLocationID     DayOfWeek\n     238.4611      353.2409      253.2761      246.3186      239.2277\nThere were 50 or more warnings (use warnings() to see the first 50)\nWe’ll discuss the warnings shortly, but not surprisingly, trip distance is the most important variable. The pickup and dropoff locations also seem to have predictive value, though day of the week may not.\nBut let’s take a closer look. There were 224 pickup locations. (run\nlevels(nyctaxi$PULocationID)\nto see this). That’s 223 dummy (“one-hot”) variables; are some more predictive than others? To explore that in\nqeLeaveOut1Var\n, we could make the dummies explicit, so each dummy is removed one at a time:\nnyct <- factorsToDummies(nyctaxi,omitLast=TRUE)\nThis function is actually from the\nregtools\npackage, included in\nqeML\n. Then we could try, say,\nnyct <- as.data.frame(nyct)\nqeLeaveOut1Var(nyct,'tripTime','qeLin',10)\nBut with so many dummies, this would take a long time to run. We could directly look at mean trip times for each pickup location to get at least some idea of their individual predictive power,\ntapply(nyctaxi$tripTime,nyctaxi$PULocationID,mean)\ntapply(nyctaxi$tripTime,nyctaxi$PULocationID,length)\nMany locations have very little data, so we’d have to deal with that. Note too the possibility of overfitting.\n> dim(nyct)\n[1] 10000  479\nAn old rule of thumb is to use under sqrt(n) variables, 100 here. Just a guide, but much less than 479. (Note: Even our analysis using the original factors still converts to dummies internally;\nnyctaxi\nhas 4 columns, but\nlm\nwill expand them as in\nnyct\n.)\nWe may wish to delete pickup location entirely. Or, possibly use PCA for dimension reduction,\nz <- qePCA(nyctaxi,'tripTime','qeLin',pcaProp=0.75)\nThis\nqeML\ncall says, “Compute PCA on the predictors, retaining enough of them for 0.75 of the total variance, and then run\nqeLin\non the resulting PCs.”\nBut…remember those warning messages? Running\nwarnings()\nwe see messages like “6 rows removed from test set, due to new factor levels.” The problem is that, in dividing the data into training and test sets, some pickup or dropoff locations appeared only in the latter, thus impossible to predict.  So, many of the columns in the training set are all 0s, thus 0 variance, thus problems with PCA. We then might run\nqeML::constCols\nto find out which columns have 0 variance, then delete those, and try\nqePCA\nagain.\nAnd we haven’t even mentioned using, say,\nqeLASSO\nor\nqeXGBoost\ninstead of\nqeLin\n, etc. But the point is clear:\nEven a very simple, very clean-looking application like this one may be much more nuanced than it looks.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nMad (Data) Scientist\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "What about variable selection? Which predictor variables/features should we use? No matter what anyone tells you, this is an unsolved problem. But there are lots of useful methods. See the qeML vignettes on feature selection and overfitting for detailed background on the issues involved. We note at the outset what our concluding statement will be: … Continue reading qeML Example: Issues of Overfitting, Dimension Reduction Etc. →",
    "meta_keywords": null,
    "og_description": "What about variable selection? Which predictor variables/features should we use? No matter what anyone tells you, this is an unsolved problem. But there are lots of useful methods. See the qeML vignettes on feature selection and overfitting for detailed background on the issues involved. We note at the outset what our concluding statement will be: … Continue reading qeML Example: Issues of Overfitting, Dimension Reduction Etc. →",
    "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
    "og_title": "qeML Example: Issues of Overfitting, Dimension Reduction Etc. | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 4.1,
    "sitemap_lastmod": "2023-11-21T19:27:54+00:00",
    "twitter_description": "What about variable selection? Which predictor variables/features should we use? No matter what anyone tells you, this is an unsolved problem. But there are lots of useful methods. See the qeML vignettes on feature selection and overfitting for detailed background on the issues involved. We note at the outset what our concluding statement will be: … Continue reading qeML Example: Issues of Overfitting, Dimension Reduction Etc. →",
    "twitter_title": "qeML Example: Issues of Overfitting, Dimension Reduction Etc. | R-bloggers",
    "url": "https://www.r-bloggers.com/2023/11/qeml-example-issues-of-overfitting-dimension-reduction-etc/",
    "word_count": 825
  }
}