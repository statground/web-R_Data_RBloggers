{
  "id": "ee0486312e3841bff1ddc337522db246fd54456f",
  "url": "https://www.r-bloggers.com/2023/11/designed-experiments-with-replicates-principal-components-or-canonical-variates/",
  "created_at_utc": "2025-11-17T20:39:25Z",
  "data": null,
  "raw_original": {
    "uuid": "d0ee9945-4e18-4b96-bd42-84b1800d08f0",
    "created_at": "2025-11-17 20:39:25",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2023/11/designed-experiments-with-replicates-principal-components-or-canonical-variates/",
      "crawled_at": "2025-11-17T10:01:13.238517",
      "external_links": [
        {
          "href": "https://www.statforbiology.com/2023/stat_multivar_cva/",
          "text": "R on The broken bridge between biologists and statisticians"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://capitaloneshopping.com/p/apple-2022-mac-book-pro-laptop-w/MGSN7BZ7SG",
          "text": "Apple Mac-Book PRO"
        },
        {
          "href": "https://www.statforbiology.com/2021/stat_multivar_svd_biplots/",
          "text": "post here"
        },
        {
          "href": "https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw",
          "text": "Follow@onofriandreapg"
        },
        {
          "href": "http://www.itl.nist.gov/div898/handbook/",
          "text": "http://www.itl.nist.gov/div898/handbook/"
        },
        {
          "href": "https://www.statforbiology.com/2023/stat_multivar_cva/",
          "text": "R on The broken bridge between biologists and statisticians"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Designed experiments with replicates: Principal components or Canonical Variates? | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-2-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-3-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-6-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-7-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/r-on-the-broken-bridge-between-biologists-and-stat/",
          "text": "R on The broken bridge between biologists and statisticians"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/cdn-cgi/l/email-protection#bfded1dbcddade91d0d1d0d9cdd6ffcad1d6cfd891d6cb",
          "text": "[email protected]"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-379630 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Designed experiments with replicates: Principal components or Canonical Variates?</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">November 1, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-the-broken-bridge-between-biologists-and-stat/\">R on The broken bridge between biologists and statisticians</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://www.statforbiology.com/2023/stat_multivar_cva/\"> R on The broken bridge between biologists and statisticians</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<p>A few days ago, a colleague of mine wanted to hear my opinion about what multivariate method would be the best for a randomised field experiment with replicates. We had a nice discussion and I thought that such a case-study might be generally interesting for the agricultural sciences; thus, I decided to take my <a href=\"https://capitaloneshopping.com/p/apple-2022-mac-book-pro-laptop-w/MGSN7BZ7SG\" rel=\"nofollow\" target=\"_blank\">Apple Mac-Book PRO</a>, sit down, relax and write a new post on this matter.</p>\n<p>My colleague’s research study was similar to this one: a randomised block field experiment (three replicates) with 16 durum wheat genotypes, which was repeated in four years. The quality of grain yield was assessed by recording the following four traits:</p>\n<ol style=\"list-style-type: decimal\">\n<li>kernel weight per hectoliter (WPH)</li>\n<li>percentage of yellow berries (YB)</li>\n<li>kernel weight (grams per 1000 kernels; TKW)</li>\n<li>protein content (% d.m.; PC)</li>\n</ol>\n<p>My colleague had averaged the three replicates for each genotype in each year, so that the final dataset consisted of a matrix of 64 rows (i.e. 16 varieties x 4 years) and 4 columns (the 4 response variables). Taking the year effect as random, we have <strong>four random replicates for each genotype, across the experimental seasons.</strong></p>\n<p>You can have a look at this dataset by loading the ‘WheatQuality4years.csv’ file, that is online available, as shown in the following box.</p>\n<pre>rm(list=ls())\nfileName &lt;- \"https://www.casaonofri.it/_datasets/WheatQuality4years.csv\"\ndataset &lt;- read.csv(fileName)\ndataset$Year &lt;- factor(dataset$Year)\nhead(dataset)\n##     Genotype Year   WPH    YB   TKW    PC\n## 1 ARCOBALENO 1999 81.67 46.67 44.67 12.71\n## 2 ARCOBALENO 2000 82.83 19.67 43.32 11.90\n## 3 ARCOBALENO 2001 83.50 38.67 46.78 13.00\n## 4 ARCOBALENO 2002 78.60 82.67 43.03 12.40\n## 5       BAIO 1999 80.30 41.00 51.83 13.91\n## 6       BAIO 2000 81.40 20.00 41.43 12.80</pre>\n<div class=\"section level1\" id=\"my-colleagues-question\">\n<h1>My colleague’s question</h1>\n<p>My colleague’s question was: “<em>can I use a PCA biplot, to have a clear graphical understanding about (i) which qualitative trait gave the best contribution to the discrimination of genotypes and (ii) which genotypes were high/low in which qualitative traits?</em>”.</p>\n<p>I think that the above question may be translated into the more general question: “<em>can we use PCA with data from designed experiments with replicates?</em>”. For this general question my general answer has to be NO; it very much depends on the situation and aims. In this post I would like to show my point of view, although I am open to discussion, as usual.</p>\n</div>\n<div class=\"section level1\" id=\"independent-subjects-or-not\">\n<h1>Independent subjects or not?</h1>\n<p>I must admit that I appreciated that my colleague wanted to use a multivariate method; indeed, the quality of winter wheat is a ‘multivariable’ problem and the four recorded traits are very likely correlated to each other. Univariate analyses, such as a set of four separate ANOVAs (one per trait) would lead us to neglect all the reciprocal relationships between the variables, which is not an efficient way to go.</p>\n<p>PCA is a very widespread multivariate method and it is useful whenever the data matrix is composed by a set of independent subjects, for which we have recorded a number of variables and we are only interested in the differences between those independent subjects. In contrast to this, data from experiments with replicates are not composed by independent subjects, but there are groups of subjects treated alike. For example, in our case study we have four replicates per genotype and these replicates are not independent, because they share the same genotype. Our primary interest is not to study the differences between replicates, but, rather, the differences between genotypes, that are groups of replicates.</p>\n<p>What happens if we submit our matrix of raw data to PCA? The subjects are regarded as totally independent from one another and no effort is made to keep them together, depending on the group (genotype) they belong to. Consequently, a PCA biplot (left side, below) offers little insight: when we isolate, e.g., the genotypes ARCOBALENO and COLORADO, we see that the four replicates are spread around the space spanned by the PC axes, so that we have no idea about whether and how these two groups are discriminated (right side, below).</p>\n<pre># PCA with raw data\npar(mfrow = c(1,2))\npcdata &lt;- dataset[,3:6]\nrow.names(pcdata) &lt;- paste(abbreviate(dataset$Genotype, 3),\n                           dataset$Year, sep = \"-\")\npcaobj &lt;- vegan::rda(pcdata, scale = T)\nbiplot(summary(pcaobj)$sites,\n       summary(pcaobj)$species, \n       cex = 0.5, xlim = c(-1,1), ylim =c(-2,2),\n       expand = 0.5)\nbiplot(summary(pcaobj)$sites[c(1:4, 13:16),],\n       summary(pcaobj)$species, \n       cex = 0.5, xlim = c(-1,1), ylim =c(-2,2),\n       expand = 3)</pre>\n<p><img data-lazy-src=\"https://i1.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-2-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-2-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>After seeing this, my colleague came out with his next question: “<em>what if we work on the genotype means?</em>”. Well, when we make a PCA on genotype means, the resulting biplot appears to be clearer (see below), but such clarity is not to be trusted. Indeed, the year-to-year variability of genotypes has been totally ‘erased’ and played no role in the construction of such biplot. Therefore, there is no guarantee that, for each genotype, all the replicates can be found in the close vicinity of the genotype mark. For example, in the biplot below we see that COLORADO and ARCOBALENO are very distant, although we have previously seen that the replicates were not very well discriminated.</p>\n<pre># PCA with genotype means\npar(mfrow = c(1,1))\navgs &lt;- aggregate(dataset[,3:6], list(dataset$Genotype),\n                  mean)\nrownames(avgs) &lt;- avgs[,1]\navgs &lt;- avgs[,-1]\npcaobj2 &lt;- vegan::rda(avgs, scale = T)\nbiplot(pcaobj2, scaling = 2)</pre>\n<p><img data-lazy-src=\"https://i2.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-3-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-3-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>In simple words, PCA is not the right tool, because it looks at the distance between individuals, but we are more concerned about the distance between groups of individuals, which is a totally different concept.</p>\n<p>Obviously, the next question is: “<em>if PCA is not the right tool, what is the right tool, then?</em>”. My proposal is that Canonical Variate Analysis (CVA) is much better suited to the purpose of group discrimination.</p>\n</div>\n<div class=\"section level1\" id=\"what-is-cva\">\n<h1>What is CVA?</h1>\n<p>Canonical variates (CVs) are similar to principal components, in the sense that they are obtained by using a linear transformation of the original variables (<span class=\"math inline\">\\(Y\\)</span>), such as:</p>\n<p><span class=\"math display\">\\[CV = Y \\times V\\]</span></p>\n<p>where <span class=\"math inline\">\\(V\\)</span> is the matrix of transformation coefficients. Unlike PCA, the matrix <span class=\"math inline\">\\(V\\)</span> is selected in a way that, in the resulting variables, the subjects belonging to the same group are kept close together and, thus, the discrimination of groups is ‘enhanced’.</p>\n<p>This is clearly visible if we compare the previous PCA biplot with a CVA biplot. Therefore, let’s skip the detail (so far) and perform a CVA, by using the <code>CVA()</code> function in the <code>aomisc</code> package, that is the companion package for this blog. Please, note that, dealing with variables in different scales and measurement units, I decided to perform a preliminary standardisation process, by using the <code>scale()</code> function.</p>\n<pre># Loads the packages\nlibrary(aomisc)\n\n# Standardise the data\ngroups &lt;- dataset$Genotype\nZ &lt;- apply(dataset[,3:6], 2, scale, center=T, scale=T)\nhead(Z)\n##             WPH           YB         TKW         PC\n## [1,]  0.3375814 -0.003873758 -0.37675661 -0.5193726\n## [2,]  0.7681267 -1.154020460 -0.59544503 -1.5621410\n## [3,]  1.0168038 -0.344657966 -0.03495471 -0.1460358\n## [4,] -0.8018791  1.529655178 -0.64242254 -0.9184568\n## [5,] -0.1709075 -0.245404565  0.78310197  1.0254693\n## [6,]  0.2373683 -1.139963112 -0.90160881 -0.4035095\n# Perform a CVA with the aomisc package\ncvaobj &lt;- CVA(Z, groups)</pre>\n</div>\n<div class=\"section level1\" id=\"the-cva-biplot\">\n<h1>The CVA biplot</h1>\n<p>The main results of a CVA consist of a matrix of <strong>canonical coefficients</strong> and a matrix of <strong>canonical scores</strong>. Both these entities are available as the output of the <code>CVA()</code> function.</p>\n<pre>Vst &lt;- cvaobj$coef # canonical coefficients\nCVst &lt;- cvaobj$scores # canonical scores</pre>\n<p>These two entities resemble, respectively, the rotation matrix and principal component scores from PCA and, although they have different properties, they can be used to draw a CVA biplot.</p>\n<pre># biplot code\npar(mfrow = c(1, 2))\nrow.names(CVst) &lt;- paste(abbreviate(dataset$Genotype, 3),\n                           dataset$Year, sep = \"-\")\nbiplot(CVst[,1:2], Vst[,1:2], cex = 0.5,\n       xlim = c(-3,4), ylim = c(-3, 4))\nabline(h=0, lty = 2)\nabline(v=0, lty = 2)\nbiplot(CVst[c(1:4, 13:16),1:2], Vst[,1:2], cex = 0.5,\n       xlim = c(-3,4), ylim = c(-3, 4),\n       expand = 24)</pre>\n<p><img data-lazy-src=\"https://i1.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-6-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>We see that, in contrast to the PCA biplot, the four replicates of each variable are ‘kept’ relatively close together, so that the groups are well discriminated. For example, we see that the genotype COLORADO is mainly found on the second quadrant and it is pretty well discriminated by the genotype ARCOBALENO, which is mainly found on the third quadrant.</p>\n<p>Furthermore, we can also plot the <strong>scores of centroids</strong> for all groups, that are available as the output of the <code>CVA()</code> function.</p>\n<pre>cscores &lt;- cvaobj$centroids\n\n# biplot code\npar(mfrow = c(1,1))\nbiplot(cscores[,1:2], Vst[,1:2], cex = 0.5,\n       xlim = c(-3,3.5), ylim = c(-3, 3.5))\nabline(h=0, lty = 2)\nabline(v=0, lty = 2)</pre>\n<p><img data-lazy-src=\"https://i2.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-7-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-7-1.png?w=450&amp;ssl=1\"/></noscript></p>\n<p>Due to the fact that the groups are mostly kept together in a CVA biplot, we can expect that subjects belonging to a certain group, with highest probability, are found in the close proximity of the respective centroid (which is not true for a PCA biplot, obtained from group means). As the reverse, we can say that the group centroid is a good representative of the whole group and the distances between the centroids will reflect how well the respective groups are discriminated.</p>\n<p>Having said so, we can read the biplot by using the usual ‘inner product rule’ (see this <a href=\"https://www.statforbiology.com/2021/stat_multivar_svd_biplots/\" rel=\"nofollow\" target=\"_blank\">post here</a>): the average value of one genotype in one specific variable can be approximated by considering how long are the respective trait-arrow and the projection of the group-marker on the trait-arrow.</p>\n<p>We can see that COLORADO, BAIO and SANCARLO are mainly discriminated by high protein content (PC) and low number of yellow berries (YB). On the other hand, CLAUDIO and COLOSSEO are discriminated by their low PC and high number of YB.</p>\n<p>GRAZIA showed high weight per hectoliter (WPH), together with high PC and low Thousand Kernel Weight (TKW). ARCOBALENO and IRIDE were discriminated by high WPH, high number of YB, low PC and low TKW.</p>\n<p>Other genotypes were very close to the origin of axes, and thus they were very little discriminated, showing average values for most of the qualitative traits.</p>\n</div>\n<div class=\"section level1\" id=\"nasty-detail-about-cva\">\n<h1>Nasty detail about CVA</h1>\n<p>With this swift example I hope that I have managed to convince my colleague (and you) that, while a PCA biplot is more suited to focus on the differences between subjects, a CVA biplot is more suited to focus on the differences between groups and, therefore, it is preferable for designed experiments with replicates.</p>\n<p>In the next part I would like to give you some ‘nasty’ detail about how the <code>CVA()</code> function works; if you are not interested in such detail, you can safely skip this and I thank you anyway for having followed me up to this point!</p>\n<p>Performing a CVA is a four step process:</p>\n<ol style=\"list-style-type: decimal\">\n<li>data standardisation</li>\n<li>ANOVA/MANOVA</li>\n<li>eigenvalue decomposition</li>\n<li>linear transformation</li>\n</ol>\n<div class=\"section level2\" id=\"step-1-standardisation\">\n<h2>Step 1: standardisation</h2>\n<p>As we said, standardisation is often made as the preliminary step, by taking the values in each column, subtracting the respective column mean and dividing by the respective column standard deviation. Although this is the most widespread method, it is also possible to standardise by using the within group standard deviation (Root Mean Squared Error from one-way ANOVA), as done, for example, in SPSS. In this post we stick to the usual technique, but, please, take this difference in mind if you intend to compare the results obtained with R with those obtained with other statistical packages.</p>\n</div>\n<div class=\"section level2\" id=\"step-2-anovamanova\">\n<h2>Step 2: ANOVA/MANOVA</h2>\n<p>The central point to CVA is to define the discriminating ability of the original variables. In the univariate realm, we use one-way ANOVA to split the total sum of squares into two components, the between-groups sum of squares (<span class=\"math inline\">\\(SS_b\\)</span>; roughly speaking, the amount of variability between group means) and the within-groups sum of squares (<span class=\"math inline\">\\(SS_w\\)</span>; roughly speaking, the amount of variability within each treatment group). We know that the total sum of squares <span class=\"math inline\">\\(SS_T\\)</span> is equal to the sum <span class=\"math inline\">\\(SS_b + SS_w\\)</span> and, therefore, we could use the ratio <span class=\"math inline\">\\(SS_w/SS_b\\)</span> as a measure of the discriminating ability of each variable.</p>\n<p>The multivariate analogous to ANOVA is MANOVA, where we should also consider the relationships (codeviances) between all pairs af variables. In particular, with four variables, we have a <span class=\"math inline\">\\(4 \\times 4\\)</span> matrix of total deviances-codeviances (<span class=\"math inline\">\\(T\\)</span>), that needs to be split into the sum of two components, i.e. the matrix of between-groups deviances-codeviances (<span class=\"math inline\">\\(B\\)</span>) and the matrix of within-groups deviances-codeviances (<span class=\"math inline\">\\(W\\)</span>), so that:</p>\n<p><span class=\"math display\">\\[ T = B + W \\]</span></p>\n<p>These three matrices (<span class=\"math inline\">\\(T\\)</span>, <span class=\"math inline\">\\(B\\)</span> and <span class=\"math inline\">\\(W\\)</span>) can be obtained by matrix multiplication, starting from, respectively, (i) the <span class=\"math inline\">\\(Z\\)</span> matrix of standardised data, (ii) the <span class=\"math inline\">\\(Z\\)</span> matrix where each value has been replaced by the mean of the corresponding variable and genotype and (iii) the matrix of residuals from the group means. More easily, we can derive these matrices from the output of the <code>CVA()</code> function.</p>\n<pre># Solution with 'CVA()' function in 'aomisc' package\nTOT &lt;- cvaobj$TOT\nB &lt;- cvaobj$B\nW &lt;- cvaobj$W\n\nprint(TOT, digits = 4)\n##         WPH      YB    TKW     PC\n## WPH  63.000 -26.212 34.639 -1.293\n## YB  -26.212  63.000 -3.271 -7.053\n## TKW  34.639  -3.271 63.000 30.501\n## PC   -1.293  -7.053 30.501 63.000\nprint(B, digits = 4)\n##         WPH     YB    TKW      PC\n## WPH 20.7760 -2.707  7.986 -0.7946\n## YB  -2.7071 12.009  2.640 -8.5455\n## TKW  7.9862  2.640 27.191 11.6353\n## PC  -0.7946 -8.545 11.635 21.0150\nprint(W, digits = 4)\n##          WPH      YB    TKW      PC\n## WPH  42.2240 -23.505 26.652 -0.4986\n## YB  -23.5053  50.991 -5.911  1.4928\n## TKW  26.6524  -5.911 35.809 18.8661\n## PC   -0.4986   1.493 18.866 41.9850</pre>\n<p>Analogously to one-way ANOVA, we can calculate the ratio <span class=\"math inline\">\\(WB = W^{-1} B\\)</span>:</p>\n<pre>WB &lt;- solve(W) %*% B \nprint(WB, digits = 4)\n##         WPH      YB     TKW      PC\n## WPH  1.2427 -0.4432 -1.0966 -0.5767\n## YB   0.4149  0.1283 -0.2258 -0.3764\n## TKW -0.8169  0.7038  1.8276  0.5567\n## PC   0.3481 -0.5296 -0.5491  0.2569</pre>\n<p>What do the previous matrices tell us? First of all, there are notable total, between-groups and within-groups codeviances between the four quality traits which suggests that these traits are correlated and the contributions they give to the discrimination of genotypes are, partly, overlapping and, thus, redundant.</p>\n<p>The diagonal elements in <span class=\"math inline\">\\(WB\\)</span> can be regarded as measures of the ‘discriminating power’ for each of the four variables: the higher the value the higher the differences between the behaviour of genotypes across years. The total ‘discriminating power’ of the four variables is, respectively, <span class=\"math inline\">\\(1.243 + 0.128 + 1.828 + 0.257 = 3.456\\)</span>.</p>\n</div>\n<div class=\"section level2\" id=\"step-3-eigenvalue-decomposition\">\n<h2>Step 3: eigenvalue decomposition</h2>\n<p>While total deviances-codeviances are central to PCA, the <span class=\"math inline\">\\(WB\\)</span> matrix is central to CVA, because it contains relevant information for group discrimination. Therefore, we submit this matrix to eigenvalue decomposition and calculate its scaled eigenvectors (see code below), to obtain the so-called <em>canonical coefficients</em>.</p>\n<pre># Eigenvalue decomposition\nV1 &lt;- eigen(WB)$vectors\n\n# get the centered canonical variates and their RMSEs\nVCC &lt;- Z %*% V1\naovList &lt;- apply(VCC, 2, function(col) lm(col ~ groups))\nRMSE &lt;- lapply(aovList, function(mod) summary(mod)$sigma)\n\n# Scaling process\nscaling &lt;- diag(1/unlist(RMSE))\nVst &lt;- V1 %*% scaling\nVst\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -1.9722706 -0.5941660 0.73150227  0.2436692\n## [2,] -0.4953078 -0.7217420 0.06457254  0.8815688\n## [3,]  2.3158763 -0.3791882 0.13441587 -0.4566838\n## [4,] -0.8919442  0.7719516 0.66575788  0.6881214</pre>\n</div>\n<div class=\"section level2\" id=\"step-4-linear-transformation\">\n<h2>Step 4: linear transformation</h2>\n<p>The <em>canonical coefficients</em> can be used to transform the original variables into a set of new variables, the so-called <strong>canonical variates</strong> or <strong>canonical scores</strong>:</p>\n<pre>CVst &lt;- Z %*% Vst\ncolnames(CVst) &lt;- c(\"CV1\", \"CV2\", \"CV3\", \"CV4\")\nhead(CVst)\n##             CV1        CV2        CV3         CV4\n## [1,] -1.0731535 -0.4558524 -0.1497271 -0.10648959\n## [2,] -0.9289930 -0.6036012 -0.6326765 -1.63319215\n## [3,] -1.7853954 -0.4548743  0.6196159 -0.14060305\n## [4,]  0.1553135 -1.0929723 -1.1856243  0.81447716\n## [5,]  1.3575325  0.7733359  0.6471100  0.09003153\n## [6,] -1.6316285  0.7121127 -0.2898050 -0.81303000</pre>\n<p>Now, we have four new canonical variates in place of the original quality traits. What did we gain? If we calculate the matrices of total, between-groups and within-groups deviances-codeviances for the CVs, we see that the off-diagonal elements are all 0 which implies that canonical variates are uncorrelated.</p>\n<pre># Deviances-codeviances for the canonical variates\n# $Total\n#               CV1           CV2          CV3           CV4\n# CV1  1.515993e+02 -5.329071e-15 4.152234e-14  4.884981e-15\n# CV2 -5.329071e-15  8.418391e+01 4.019007e-14 -1.287859e-14\n# CV3  4.152234e-14  4.019007e-14 7.090742e+01  2.398082e-14\n# CV4  4.884981e-15 -1.287859e-14 2.398082e-14  5.117518e+01\n# \n# $Between-groups\n#              CV1           CV2          CV3           CV4\n# CV1 1.035993e+02  2.886580e-15 2.797762e-14  1.976197e-14\n# CV2 2.886580e-15  3.618391e+01 3.330669e-15 -3.774758e-15\n# CV3 2.797762e-14  3.330669e-15 2.290742e+01  8.326673e-15\n# CV4 1.976197e-14 -3.774758e-15 8.326673e-15  3.175176e+00\n# \n# $Within-groups\n#               CV1           CV2          CV3           CV4\n# CV1  4.800000e+01 -4.329870e-15 6.217249e-15 -1.260103e-14\n# CV2 -4.329870e-15  4.800000e+01 3.674838e-14 -1.443290e-14\n# CV3  6.217249e-15  3.674838e-14 4.800000e+01  1.776357e-14\n# CV4 -1.260103e-14 -1.443290e-14 1.776357e-14  4.800000e+01\n# \n# $`B/W`\n#              CV1           CV2           CV3           CV4\n# CV1 2.158318e+00  1.281369e-16  5.210524e-16  4.290734e-16\n# CV2 2.548295e-16  7.538314e-01 -2.959802e-16 -5.875061e-17\n# CV3 3.033087e-16 -5.077378e-16  4.772379e-01  1.489921e-16\n# CV4 9.783126e-16  1.480253e-16 -3.141157e-18  6.614950e-02</pre>\n<p>Furthermore, the <span class=\"math inline\">\\(BW\\)</span> matrix above shows that the ratios of ‘between-groups/within-groups’ deviances are in decreasing order and their sum is equal to the sum of the diagonal elements of the <span class=\"math inline\">\\(BW\\)</span> matrix for the original variables.</p>\n<p>In simpler words, the total ‘discriminating power’ of the CVs is the same as that of the original variables, but the first CV, on itself, has a very high ‘discriminating power’, that is equal to 62.5% of the ‘discriminating power’ of the original variables (<span class=\"math inline\">\\(2.155/3.540 \\cdot 100\\)</span>). If we add a second CV, the ‘discriminating power’ raises to the 85% of the original variables. It means that, if we use two CVs in place of the four original variables, the discrimination of genotypes across years is almost as good as that of the original four variables. Therefore, we can conclude that the biplot above is relevant.</p>\n<p>Please, note that the output of the <code>CVA()</code> function also contains the proportion of total discriminating ability that is contributed by each canonical variate (see box below).</p>\n<pre>cvaobj$proportion\n## [1] 0.62459704 0.21815174 0.13810817 0.01914304</pre>\n<p>As the final remark, the <em>canonical coefficients</em> can be used to calculate the <strong>canonical scores for centroids</strong>, which we used for the biplot above:</p>\n<pre>avg &lt;- aggregate(Z, list(groups), mean)\nrow.names(avg) &lt;- avg[,1]\navg &lt;- as.matrix(avg[,-1])\n\nhead(avg %*% Vst)\n##                  [,1]       [,2]       [,3]        [,4]\n## ARCOBALENO -0.9080571 -0.6518251 -0.3371030 -0.26645191\n## BAIO       -0.7823965  0.8928011  0.5747970  0.32086167\n## CLAUDIO    -0.5496314 -1.3845288  0.3590569  0.16423169\n## COLORADO   -1.1481765  1.4231696 -0.5535939 -0.13201115\n## COLOSSEO    1.0654126 -1.3108147 -0.1201515 -0.05709275\n## CRESO       0.3070820 -0.3947049  0.6469757 -0.09565658</pre>\n<p>In conclusion, canonical variate analysis is the best way to represent the multivariate data in reduced rank space, preserving the discrimination between groups. Therefore, it may be much more suitable than PCA with designed experiments with replicates.</p>\n<p>Thanks for reading!</p>\n<p>Prof. Andrea Onofri<br/>\nDepartment of Agricultural, Food and Environmental Sciences<br/>\nUniversity of Perugia (Italy)<br/>\nSend comments to: <a href=\"/cdn-cgi/l/email-protection#bfded1dbcddade91d0d1d0d9cdd6ffcad1d6cfd891d6cb\" rel=\"nofollow\" target=\"_blank\"><span class=\"__cf_email__\" data-cfemail=\"b3d2ddd7c1d6d29ddcdddcd5c1daf3c6dddac3d49ddac7\">[email protected]</span></a></p>\n<a class=\"twitter-follow-button\" data-show-count=\"false\" href=\"https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw\" rel=\"nofollow\" target=\"_blank\">Follow <span class=\"citation\">@onofriandreapg</span></a>\n\n<hr>\n</hr></div>\n</div>\n<div class=\"section level1\" id=\"further-readings\">\n<h1>Further readings</h1>\n<ol style=\"list-style-type: decimal\">\n<li>Crossa, J., 1990. Advances in Agronomy 44, 55-85.</li>\n<li>NIST/SEMATECH, 2004. In “e-Handbook of Statistical Methods”. NIST/SEMATECH, <a class=\"uri\" href=\"http://www.itl.nist.gov/div898/handbook/\" rel=\"nofollow\" target=\"_blank\">http://www.itl.nist.gov/div898/handbook/</a>.</li>\n<li>Manly F.J., 1986. Multivariate statistical methods: a primer. Chapman &amp; Hall, London, pp. 159.</li>\n<li>Adugna W. e Labuschagne M. T., 2003. Cluster and canonical variate analyses in multilocation trials of linseed. Journal of Agricultural Science (140), 297-304.</li>\n<li>Barberi P., Silvestri N. e Bonari E., 1997. Weed communities of winter wheat as influenced by input level and rotation. Weed Research 37, 301-313.</li>\n<li>Casini P. e Proietti C., 2002. Morphological characterisation and production of Quinoa genotypes (Chenopodium quinoa Willd.) in the Mediterranean environment. Agricoltura Mediterranea 132, 15-26.</li>\n<li>Onofri A. e Ciriciofolo E., 2004. Characterisation of yield quality in durum wheat by canonical variate anaysis. Proceedings VIII ESA Congress “European Agriculture in a global context”, Copenhagen, 11-15 July 2004, 541-542.</li>\n<li>Shresta A., Knezevic S. Z., Roy R. C., Ball-Cohelo B. R. e Swanton C. J., 2002. Effect of tillage, cover crop and crop rotation on the composition of weed flora in a sandy soil. Weed Research 42 (1), 76-87.</li>\n<li>Streit B., Rieger S. B., Stamp P. e Richner W., 2003. Weed population in winter wheat as affected by crop sequence, intensity of tillage and time of herbicide application in a cool and humid climate. Weed Research 43, 20-32.</li>\n</ol>\n</div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.statforbiology.com/2023/stat_multivar_cva/\"> R on The broken bridge between biologists and statisticians</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
      "main_text": "Designed experiments with replicates: Principal components or Canonical Variates?\nPosted on\nNovember 1, 2023\nby\nR on The broken bridge between biologists and statisticians\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR on The broken bridge between biologists and statisticians\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nA few days ago, a colleague of mine wanted to hear my opinion about what multivariate method would be the best for a randomised field experiment with replicates. We had a nice discussion and I thought that such a case-study might be generally interesting for the agricultural sciences; thus, I decided to take my\nApple Mac-Book PRO\n, sit down, relax and write a new post on this matter.\nMy colleague’s research study was similar to this one: a randomised block field experiment (three replicates) with 16 durum wheat genotypes, which was repeated in four years. The quality of grain yield was assessed by recording the following four traits:\nkernel weight per hectoliter (WPH)\npercentage of yellow berries (YB)\nkernel weight (grams per 1000 kernels; TKW)\nprotein content (% d.m.; PC)\nMy colleague had averaged the three replicates for each genotype in each year, so that the final dataset consisted of a matrix of 64 rows (i.e. 16 varieties x 4 years) and 4 columns (the 4 response variables). Taking the year effect as random, we have\nfour random replicates for each genotype, across the experimental seasons.\nYou can have a look at this dataset by loading the ‘WheatQuality4years.csv’ file, that is online available, as shown in the following box.\nrm(list=ls())\nfileName <- \"https://www.casaonofri.it/_datasets/WheatQuality4years.csv\"\ndataset <- read.csv(fileName)\ndataset$Year <- factor(dataset$Year)\nhead(dataset)\n##     Genotype Year   WPH    YB   TKW    PC\n## 1 ARCOBALENO 1999 81.67 46.67 44.67 12.71\n## 2 ARCOBALENO 2000 82.83 19.67 43.32 11.90\n## 3 ARCOBALENO 2001 83.50 38.67 46.78 13.00\n## 4 ARCOBALENO 2002 78.60 82.67 43.03 12.40\n## 5       BAIO 1999 80.30 41.00 51.83 13.91\n## 6       BAIO 2000 81.40 20.00 41.43 12.80\nMy colleague’s question\nMy colleague’s question was: “\ncan I use a PCA biplot, to have a clear graphical understanding about (i) which qualitative trait gave the best contribution to the discrimination of genotypes and (ii) which genotypes were high/low in which qualitative traits?\n”.\nI think that the above question may be translated into the more general question: “\ncan we use PCA with data from designed experiments with replicates?\n”. For this general question my general answer has to be NO; it very much depends on the situation and aims. In this post I would like to show my point of view, although I am open to discussion, as usual.\nIndependent subjects or not?\nI must admit that I appreciated that my colleague wanted to use a multivariate method; indeed, the quality of winter wheat is a ‘multivariable’ problem and the four recorded traits are very likely correlated to each other. Univariate analyses, such as a set of four separate ANOVAs (one per trait) would lead us to neglect all the reciprocal relationships between the variables, which is not an efficient way to go.\nPCA is a very widespread multivariate method and it is useful whenever the data matrix is composed by a set of independent subjects, for which we have recorded a number of variables and we are only interested in the differences between those independent subjects. In contrast to this, data from experiments with replicates are not composed by independent subjects, but there are groups of subjects treated alike. For example, in our case study we have four replicates per genotype and these replicates are not independent, because they share the same genotype. Our primary interest is not to study the differences between replicates, but, rather, the differences between genotypes, that are groups of replicates.\nWhat happens if we submit our matrix of raw data to PCA? The subjects are regarded as totally independent from one another and no effort is made to keep them together, depending on the group (genotype) they belong to. Consequently, a PCA biplot (left side, below) offers little insight: when we isolate, e.g., the genotypes ARCOBALENO and COLORADO, we see that the four replicates are spread around the space spanned by the PC axes, so that we have no idea about whether and how these two groups are discriminated (right side, below).\n# PCA with raw data\npar(mfrow = c(1,2))\npcdata <- dataset[,3:6]\nrow.names(pcdata) <- paste(abbreviate(dataset$Genotype, 3),\n                           dataset$Year, sep = \"-\")\npcaobj <- vegan::rda(pcdata, scale = T)\nbiplot(summary(pcaobj)$sites,\n       summary(pcaobj)$species, \n       cex = 0.5, xlim = c(-1,1), ylim =c(-2,2),\n       expand = 0.5)\nbiplot(summary(pcaobj)$sites[c(1:4, 13:16),],\n       summary(pcaobj)$species, \n       cex = 0.5, xlim = c(-1,1), ylim =c(-2,2),\n       expand = 3)\nAfter seeing this, my colleague came out with his next question: “\nwhat if we work on the genotype means?\n”. Well, when we make a PCA on genotype means, the resulting biplot appears to be clearer (see below), but such clarity is not to be trusted. Indeed, the year-to-year variability of genotypes has been totally ‘erased’ and played no role in the construction of such biplot. Therefore, there is no guarantee that, for each genotype, all the replicates can be found in the close vicinity of the genotype mark. For example, in the biplot below we see that COLORADO and ARCOBALENO are very distant, although we have previously seen that the replicates were not very well discriminated.\n# PCA with genotype means\npar(mfrow = c(1,1))\navgs <- aggregate(dataset[,3:6], list(dataset$Genotype),\n                  mean)\nrownames(avgs) <- avgs[,1]\navgs <- avgs[,-1]\npcaobj2 <- vegan::rda(avgs, scale = T)\nbiplot(pcaobj2, scaling = 2)\nIn simple words, PCA is not the right tool, because it looks at the distance between individuals, but we are more concerned about the distance between groups of individuals, which is a totally different concept.\nObviously, the next question is: “\nif PCA is not the right tool, what is the right tool, then?\n”. My proposal is that Canonical Variate Analysis (CVA) is much better suited to the purpose of group discrimination.\nWhat is CVA?\nCanonical variates (CVs) are similar to principal components, in the sense that they are obtained by using a linear transformation of the original variables (\n\\(Y\\)\n), such as:\n\\[CV = Y \\times V\\]\nwhere\n\\(V\\)\nis the matrix of transformation coefficients. Unlike PCA, the matrix\n\\(V\\)\nis selected in a way that, in the resulting variables, the subjects belonging to the same group are kept close together and, thus, the discrimination of groups is ‘enhanced’.\nThis is clearly visible if we compare the previous PCA biplot with a CVA biplot. Therefore, let’s skip the detail (so far) and perform a CVA, by using the\nCVA()\nfunction in the\naomisc\npackage, that is the companion package for this blog. Please, note that, dealing with variables in different scales and measurement units, I decided to perform a preliminary standardisation process, by using the\nscale()\nfunction.\n# Loads the packages\nlibrary(aomisc)\n\n# Standardise the data\ngroups <- dataset$Genotype\nZ <- apply(dataset[,3:6], 2, scale, center=T, scale=T)\nhead(Z)\n##             WPH           YB         TKW         PC\n## [1,]  0.3375814 -0.003873758 -0.37675661 -0.5193726\n## [2,]  0.7681267 -1.154020460 -0.59544503 -1.5621410\n## [3,]  1.0168038 -0.344657966 -0.03495471 -0.1460358\n## [4,] -0.8018791  1.529655178 -0.64242254 -0.9184568\n## [5,] -0.1709075 -0.245404565  0.78310197  1.0254693\n## [6,]  0.2373683 -1.139963112 -0.90160881 -0.4035095\n# Perform a CVA with the aomisc package\ncvaobj <- CVA(Z, groups)\nThe CVA biplot\nThe main results of a CVA consist of a matrix of\ncanonical coefficients\nand a matrix of\ncanonical scores\n. Both these entities are available as the output of the\nCVA()\nfunction.\nVst <- cvaobj$coef # canonical coefficients\nCVst <- cvaobj$scores # canonical scores\nThese two entities resemble, respectively, the rotation matrix and principal component scores from PCA and, although they have different properties, they can be used to draw a CVA biplot.\n# biplot code\npar(mfrow = c(1, 2))\nrow.names(CVst) <- paste(abbreviate(dataset$Genotype, 3),\n                           dataset$Year, sep = \"-\")\nbiplot(CVst[,1:2], Vst[,1:2], cex = 0.5,\n       xlim = c(-3,4), ylim = c(-3, 4))\nabline(h=0, lty = 2)\nabline(v=0, lty = 2)\nbiplot(CVst[c(1:4, 13:16),1:2], Vst[,1:2], cex = 0.5,\n       xlim = c(-3,4), ylim = c(-3, 4),\n       expand = 24)\nWe see that, in contrast to the PCA biplot, the four replicates of each variable are ‘kept’ relatively close together, so that the groups are well discriminated. For example, we see that the genotype COLORADO is mainly found on the second quadrant and it is pretty well discriminated by the genotype ARCOBALENO, which is mainly found on the third quadrant.\nFurthermore, we can also plot the\nscores of centroids\nfor all groups, that are available as the output of the\nCVA()\nfunction.\ncscores <- cvaobj$centroids\n\n# biplot code\npar(mfrow = c(1,1))\nbiplot(cscores[,1:2], Vst[,1:2], cex = 0.5,\n       xlim = c(-3,3.5), ylim = c(-3, 3.5))\nabline(h=0, lty = 2)\nabline(v=0, lty = 2)\nDue to the fact that the groups are mostly kept together in a CVA biplot, we can expect that subjects belonging to a certain group, with highest probability, are found in the close proximity of the respective centroid (which is not true for a PCA biplot, obtained from group means). As the reverse, we can say that the group centroid is a good representative of the whole group and the distances between the centroids will reflect how well the respective groups are discriminated.\nHaving said so, we can read the biplot by using the usual ‘inner product rule’ (see this\npost here\n): the average value of one genotype in one specific variable can be approximated by considering how long are the respective trait-arrow and the projection of the group-marker on the trait-arrow.\nWe can see that COLORADO, BAIO and SANCARLO are mainly discriminated by high protein content (PC) and low number of yellow berries (YB). On the other hand, CLAUDIO and COLOSSEO are discriminated by their low PC and high number of YB.\nGRAZIA showed high weight per hectoliter (WPH), together with high PC and low Thousand Kernel Weight (TKW). ARCOBALENO and IRIDE were discriminated by high WPH, high number of YB, low PC and low TKW.\nOther genotypes were very close to the origin of axes, and thus they were very little discriminated, showing average values for most of the qualitative traits.\nNasty detail about CVA\nWith this swift example I hope that I have managed to convince my colleague (and you) that, while a PCA biplot is more suited to focus on the differences between subjects, a CVA biplot is more suited to focus on the differences between groups and, therefore, it is preferable for designed experiments with replicates.\nIn the next part I would like to give you some ‘nasty’ detail about how the\nCVA()\nfunction works; if you are not interested in such detail, you can safely skip this and I thank you anyway for having followed me up to this point!\nPerforming a CVA is a four step process:\ndata standardisation\nANOVA/MANOVA\neigenvalue decomposition\nlinear transformation\nStep 1: standardisation\nAs we said, standardisation is often made as the preliminary step, by taking the values in each column, subtracting the respective column mean and dividing by the respective column standard deviation. Although this is the most widespread method, it is also possible to standardise by using the within group standard deviation (Root Mean Squared Error from one-way ANOVA), as done, for example, in SPSS. In this post we stick to the usual technique, but, please, take this difference in mind if you intend to compare the results obtained with R with those obtained with other statistical packages.\nStep 2: ANOVA/MANOVA\nThe central point to CVA is to define the discriminating ability of the original variables. In the univariate realm, we use one-way ANOVA to split the total sum of squares into two components, the between-groups sum of squares (\n\\(SS_b\\)\n; roughly speaking, the amount of variability between group means) and the within-groups sum of squares (\n\\(SS_w\\)\n; roughly speaking, the amount of variability within each treatment group). We know that the total sum of squares\n\\(SS_T\\)\nis equal to the sum\n\\(SS_b + SS_w\\)\nand, therefore, we could use the ratio\n\\(SS_w/SS_b\\)\nas a measure of the discriminating ability of each variable.\nThe multivariate analogous to ANOVA is MANOVA, where we should also consider the relationships (codeviances) between all pairs af variables. In particular, with four variables, we have a\n\\(4 \\times 4\\)\nmatrix of total deviances-codeviances (\n\\(T\\)\n), that needs to be split into the sum of two components, i.e. the matrix of between-groups deviances-codeviances (\n\\(B\\)\n) and the matrix of within-groups deviances-codeviances (\n\\(W\\)\n), so that:\n\\[ T = B + W \\]\nThese three matrices (\n\\(T\\)\n,\n\\(B\\)\nand\n\\(W\\)\n) can be obtained by matrix multiplication, starting from, respectively, (i) the\n\\(Z\\)\nmatrix of standardised data, (ii) the\n\\(Z\\)\nmatrix where each value has been replaced by the mean of the corresponding variable and genotype and (iii) the matrix of residuals from the group means. More easily, we can derive these matrices from the output of the\nCVA()\nfunction.\n# Solution with 'CVA()' function in 'aomisc' package\nTOT <- cvaobj$TOT\nB <- cvaobj$B\nW <- cvaobj$W\n\nprint(TOT, digits = 4)\n##         WPH      YB    TKW     PC\n## WPH  63.000 -26.212 34.639 -1.293\n## YB  -26.212  63.000 -3.271 -7.053\n## TKW  34.639  -3.271 63.000 30.501\n## PC   -1.293  -7.053 30.501 63.000\nprint(B, digits = 4)\n##         WPH     YB    TKW      PC\n## WPH 20.7760 -2.707  7.986 -0.7946\n## YB  -2.7071 12.009  2.640 -8.5455\n## TKW  7.9862  2.640 27.191 11.6353\n## PC  -0.7946 -8.545 11.635 21.0150\nprint(W, digits = 4)\n##          WPH      YB    TKW      PC\n## WPH  42.2240 -23.505 26.652 -0.4986\n## YB  -23.5053  50.991 -5.911  1.4928\n## TKW  26.6524  -5.911 35.809 18.8661\n## PC   -0.4986   1.493 18.866 41.9850\nAnalogously to one-way ANOVA, we can calculate the ratio\n\\(WB = W^{-1} B\\)\n:\nWB <- solve(W) %*% B \nprint(WB, digits = 4)\n##         WPH      YB     TKW      PC\n## WPH  1.2427 -0.4432 -1.0966 -0.5767\n## YB   0.4149  0.1283 -0.2258 -0.3764\n## TKW -0.8169  0.7038  1.8276  0.5567\n## PC   0.3481 -0.5296 -0.5491  0.2569\nWhat do the previous matrices tell us? First of all, there are notable total, between-groups and within-groups codeviances between the four quality traits which suggests that these traits are correlated and the contributions they give to the discrimination of genotypes are, partly, overlapping and, thus, redundant.\nThe diagonal elements in\n\\(WB\\)\ncan be regarded as measures of the ‘discriminating power’ for each of the four variables: the higher the value the higher the differences between the behaviour of genotypes across years. The total ‘discriminating power’ of the four variables is, respectively,\n\\(1.243 + 0.128 + 1.828 + 0.257 = 3.456\\)\n.\nStep 3: eigenvalue decomposition\nWhile total deviances-codeviances are central to PCA, the\n\\(WB\\)\nmatrix is central to CVA, because it contains relevant information for group discrimination. Therefore, we submit this matrix to eigenvalue decomposition and calculate its scaled eigenvectors (see code below), to obtain the so-called\ncanonical coefficients\n.\n# Eigenvalue decomposition\nV1 <- eigen(WB)$vectors\n\n# get the centered canonical variates and their RMSEs\nVCC <- Z %*% V1\naovList <- apply(VCC, 2, function(col) lm(col ~ groups))\nRMSE <- lapply(aovList, function(mod) summary(mod)$sigma)\n\n# Scaling process\nscaling <- diag(1/unlist(RMSE))\nVst <- V1 %*% scaling\nVst\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -1.9722706 -0.5941660 0.73150227  0.2436692\n## [2,] -0.4953078 -0.7217420 0.06457254  0.8815688\n## [3,]  2.3158763 -0.3791882 0.13441587 -0.4566838\n## [4,] -0.8919442  0.7719516 0.66575788  0.6881214\nStep 4: linear transformation\nThe\ncanonical coefficients\ncan be used to transform the original variables into a set of new variables, the so-called\ncanonical variates\nor\ncanonical scores\n:\nCVst <- Z %*% Vst\ncolnames(CVst) <- c(\"CV1\", \"CV2\", \"CV3\", \"CV4\")\nhead(CVst)\n##             CV1        CV2        CV3         CV4\n## [1,] -1.0731535 -0.4558524 -0.1497271 -0.10648959\n## [2,] -0.9289930 -0.6036012 -0.6326765 -1.63319215\n## [3,] -1.7853954 -0.4548743  0.6196159 -0.14060305\n## [4,]  0.1553135 -1.0929723 -1.1856243  0.81447716\n## [5,]  1.3575325  0.7733359  0.6471100  0.09003153\n## [6,] -1.6316285  0.7121127 -0.2898050 -0.81303000\nNow, we have four new canonical variates in place of the original quality traits. What did we gain? If we calculate the matrices of total, between-groups and within-groups deviances-codeviances for the CVs, we see that the off-diagonal elements are all 0 which implies that canonical variates are uncorrelated.\n# Deviances-codeviances for the canonical variates\n# $Total\n#               CV1           CV2          CV3           CV4\n# CV1  1.515993e+02 -5.329071e-15 4.152234e-14  4.884981e-15\n# CV2 -5.329071e-15  8.418391e+01 4.019007e-14 -1.287859e-14\n# CV3  4.152234e-14  4.019007e-14 7.090742e+01  2.398082e-14\n# CV4  4.884981e-15 -1.287859e-14 2.398082e-14  5.117518e+01\n# \n# $Between-groups\n#              CV1           CV2          CV3           CV4\n# CV1 1.035993e+02  2.886580e-15 2.797762e-14  1.976197e-14\n# CV2 2.886580e-15  3.618391e+01 3.330669e-15 -3.774758e-15\n# CV3 2.797762e-14  3.330669e-15 2.290742e+01  8.326673e-15\n# CV4 1.976197e-14 -3.774758e-15 8.326673e-15  3.175176e+00\n# \n# $Within-groups\n#               CV1           CV2          CV3           CV4\n# CV1  4.800000e+01 -4.329870e-15 6.217249e-15 -1.260103e-14\n# CV2 -4.329870e-15  4.800000e+01 3.674838e-14 -1.443290e-14\n# CV3  6.217249e-15  3.674838e-14 4.800000e+01  1.776357e-14\n# CV4 -1.260103e-14 -1.443290e-14 1.776357e-14  4.800000e+01\n# \n# $`B/W`\n#              CV1           CV2           CV3           CV4\n# CV1 2.158318e+00  1.281369e-16  5.210524e-16  4.290734e-16\n# CV2 2.548295e-16  7.538314e-01 -2.959802e-16 -5.875061e-17\n# CV3 3.033087e-16 -5.077378e-16  4.772379e-01  1.489921e-16\n# CV4 9.783126e-16  1.480253e-16 -3.141157e-18  6.614950e-02\nFurthermore, the\n\\(BW\\)\nmatrix above shows that the ratios of ‘between-groups/within-groups’ deviances are in decreasing order and their sum is equal to the sum of the diagonal elements of the\n\\(BW\\)\nmatrix for the original variables.\nIn simpler words, the total ‘discriminating power’ of the CVs is the same as that of the original variables, but the first CV, on itself, has a very high ‘discriminating power’, that is equal to 62.5% of the ‘discriminating power’ of the original variables (\n\\(2.155/3.540 \\cdot 100\\)\n). If we add a second CV, the ‘discriminating power’ raises to the 85% of the original variables. It means that, if we use two CVs in place of the four original variables, the discrimination of genotypes across years is almost as good as that of the original four variables. Therefore, we can conclude that the biplot above is relevant.\nPlease, note that the output of the\nCVA()\nfunction also contains the proportion of total discriminating ability that is contributed by each canonical variate (see box below).\ncvaobj$proportion\n## [1] 0.62459704 0.21815174 0.13810817 0.01914304\nAs the final remark, the\ncanonical coefficients\ncan be used to calculate the\ncanonical scores for centroids\n, which we used for the biplot above:\navg <- aggregate(Z, list(groups), mean)\nrow.names(avg) <- avg[,1]\navg <- as.matrix(avg[,-1])\n\nhead(avg %*% Vst)\n##                  [,1]       [,2]       [,3]        [,4]\n## ARCOBALENO -0.9080571 -0.6518251 -0.3371030 -0.26645191\n## BAIO       -0.7823965  0.8928011  0.5747970  0.32086167\n## CLAUDIO    -0.5496314 -1.3845288  0.3590569  0.16423169\n## COLORADO   -1.1481765  1.4231696 -0.5535939 -0.13201115\n## COLOSSEO    1.0654126 -1.3108147 -0.1201515 -0.05709275\n## CRESO       0.3070820 -0.3947049  0.6469757 -0.09565658\nIn conclusion, canonical variate analysis is the best way to represent the multivariate data in reduced rank space, preserving the discrimination between groups. Therefore, it may be much more suitable than PCA with designed experiments with replicates.\nThanks for reading!\nProf. Andrea Onofri\nDepartment of Agricultural, Food and Environmental Sciences\nUniversity of Perugia (Italy)\nSend comments to:\n[email protected]\nFollow\n@onofriandreapg\nFurther readings\nCrossa, J., 1990. Advances in Agronomy 44, 55-85.\nNIST/SEMATECH, 2004. In “e-Handbook of Statistical Methods”. NIST/SEMATECH,\nhttp://www.itl.nist.gov/div898/handbook/\n.\nManly F.J., 1986. Multivariate statistical methods: a primer. Chapman & Hall, London, pp. 159.\nAdugna W. e Labuschagne M. T., 2003. Cluster and canonical variate analyses in multilocation trials of linseed. Journal of Agricultural Science (140), 297-304.\nBarberi P., Silvestri N. e Bonari E., 1997. Weed communities of winter wheat as influenced by input level and rotation. Weed Research 37, 301-313.\nCasini P. e Proietti C., 2002. Morphological characterisation and production of Quinoa genotypes (Chenopodium quinoa Willd.) in the Mediterranean environment. Agricoltura Mediterranea 132, 15-26.\nOnofri A. e Ciriciofolo E., 2004. Characterisation of yield quality in durum wheat by canonical variate anaysis. Proceedings VIII ESA Congress “European Agriculture in a global context”, Copenhagen, 11-15 July 2004, 541-542.\nShresta A., Knezevic S. Z., Roy R. C., Ball-Cohelo B. R. e Swanton C. J., 2002. Effect of tillage, cover crop and crop rotation on the composition of weed flora in a sandy soil. Weed Research 42 (1), 76-87.\nStreit B., Rieger S. B., Stamp P. e Richner W., 2003. Weed population in winter wheat as affected by crop sequence, intensity of tillage and time of herbicide application in a cool and humid climate. Weed Research 43, 20-32.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR on The broken bridge between biologists and statisticians\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "A few days ago, a colleague of mine wanted to hear my opinion about what multivariate method would be the best for a randomised field experiment with replicates. We had a nice discussion and I thought that such a case-study might be generally intere...",
      "meta_keywords": null,
      "og_description": "A few days ago, a colleague of mine wanted to hear my opinion about what multivariate method would be the best for a randomised field experiment with replicates. We had a nice discussion and I thought that such a case-study might be generally intere...",
      "og_image": "https://www.statforbiology.com/post/Stat_multivar_CVA_files/figure-html/unnamed-chunk-2-1.png",
      "og_title": "Designed experiments with replicates: Principal components or Canonical Variates? | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 18.9,
      "sitemap_lastmod": "2023-11-02T00:00:00+00:00",
      "twitter_description": "A few days ago, a colleague of mine wanted to hear my opinion about what multivariate method would be the best for a randomised field experiment with replicates. We had a nice discussion and I thought that such a case-study might be generally intere...",
      "twitter_title": "Designed experiments with replicates: Principal components or Canonical Variates? | R-bloggers",
      "url": "https://www.r-bloggers.com/2023/11/designed-experiments-with-replicates-principal-components-or-canonical-variates/",
      "word_count": 3785
    }
  }
}