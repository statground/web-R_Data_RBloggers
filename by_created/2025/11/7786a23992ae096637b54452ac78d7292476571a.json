{
  "id": "7786a23992ae096637b54452ac78d7292476571a",
  "url": "https://www.r-bloggers.com/2025/05/vers-lagi-a-pas-lents-et-lucides/",
  "created_at_utc": "2025-11-22T19:58:37Z",
  "data": null,
  "raw_original": {
    "uuid": "c0b14500-5b14-4c3e-94fb-6e07d2693684",
    "created_at": "2025-11-22 19:58:37",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/05/vers-lagi-a-pas-lents-et-lucides/",
      "crawled_at": "2025-11-22T10:49:05.149246",
      "external_links": [
        {
          "href": "https://blog.bguarisma.com/vers-lagi-a-pas-lents-et-lucides",
          "text": "Foundations"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.youtube.com/@SabineHossenfelder",
          "text": "Sabine Hossenfelder"
        },
        {
          "href": "https://blog.bguarisma.com/vers-lagi-a-pas-lents-et-lucides",
          "text": "Foundations"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Vers l’AGI, à pas lents et lucides | R-bloggers",
      "images": [],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/boris-guarisma/",
          "text": "Boris Guarisma"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-395836 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Vers l’AGI, à pas lents et lucides</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 13, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/boris-guarisma/\">Boris Guarisma</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://blog.bguarisma.com/vers-lagi-a-pas-lents-et-lucides\"> Foundations</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><!--[CDATA[<p-->Cher journal,<p>Aujourdhui nous sommes le 13 mai 2025, et une question me trotte dans la tte :<br/><strong>Et si lAGI narrivait jamais comme on limagine ?</strong></p><p>Je sais, cest une question un peu provocante, presque sacrilge dans un monde o chaque semaine apporte son lot de promesses, de modles rvolutionnaires et dannonces tambour battant. Mais plus je creuse, plus un malaise sinstalle.</p><p>Cela fait quelques mois que je veille intensment le sujet de lAGI  cette fameuse Intelligence Artificielle Gnrale qui serait cense rivaliser avec lintelligence humaine dans sa polyvalence et son adaptabilit. Mon point de dpart ? Une srie de vidos de <a href=\"https://www.youtube.com/@SabineHossenfelder\" rel=\"nofollow\" target=\"_blank\">Sabine Hossenfelder</a>, physicienne lucide  lhumour tranchant, et surtout dnue dillusions marketing.</p><h3 id=\"heading-premiere-claque-le-modele-o3\">Premire claque : le modle o3</h3><p>Dbut janvier, OpenAI lance o3. Buzz immdiat. Les chiffres sont impressionnants :</p><ul><li><p>87 % au test ARC-AGI, conu pour valuer des capacits abstraites proches de lhumain.</p></li><li><p>Performances hallucinantes en gnration de code.</p></li><li><p>Raisonnement en chane de pense (Chain of Thought).</p></li></ul><p>Sur le papier, tout y est. Pourtant,  y regarder de plus prs tout y est <em>justement</em> trop. Trop beau, trop calibr, trop cher (jusqu 3 000 $ pour une tche). Et surtout : trop semblable aux versions prcdentes.</p><p>GPT-4.5, Claude 3.7, o3 : tous progressent, mais  la marge. Le consensus merge doucement : <strong>on atteint les limites du passage  lchelle</strong>. Les LLMs ne scalent plus de manire significative. Ce nest plus une ligne droite vers lAGI, cest un plateau.</p><h3 id=\"heading-deuxieme-claque-la-definition-dagi-elle-meme\">Deuxime claque : la dfinition dAGI elle-mme</h3><p>Sam Altman le reconnat : AGI est devenu un mot-valise. Chacun y met ce quil veut. Un assistant trs comptent ? Un surhomme numrique ? Une IA qui fait de la physique quantique en coutant Bach ? La confusion rgne, et elle arrange bien les discours marketing.</p><p>Sabine pointe un lment crucial : russir un test (mme impressionnant comme ARC-AGI) ne signifie pas comprendre. o3 peut battre des records en reconnaissance de motifs sans rien saisir du sens profond. Cest lillusion dintelligence. Une simulation de rflexion  pas la rflexion elle-mme.</p><h3 id=\"heading-troisieme-claque-les-limites-des-llms\">Troisime claque : les limites des LLMs</h3><p>Les LLMs ne comprennent pas. Ils imitent. Et leur imitation a des angles morts criants :</p><ul><li><p>Ils napprennent pas aprs leur entranement.</p></li><li><p>Ils chouent sur des tches logiques lmentaires (essayez de leur faire compter les r dans strawberry).</p></li><li><p>Ils nont aucune ide de ce quest le monde rel.</p></li></ul><p>Yann LeCun le dit sans dtour : Ils ne savent pas dbarrasser une table comme un enfant de 10 ans. On y est.</p><h3 id=\"heading-alors-on-fait-quoi\">Alors, on fait quoi ?</h3><p>Cest l que les choses deviennent fascinantes. Loin des projecteurs, une rvolution silencieuse sopre. Deux pistes mergent avec srieux :</p><ol><li><p><strong>Le raisonnement symbolique</strong> : injecter de la logique, des structures formelles, une mmoire organise. Cest le retour du noyau pensant. DeepMind exprimente dj avec AlphaProof et le neurosymbolic AI.</p></li><li><p><strong>Les world models</strong> : des modles qui ne prdisent pas juste le prochain mot, mais simulent la dynamique du monde rel. Genie 2 chez DeepMind, Cosmos chez NVIDIA : on parle ici dintelligence incarne, dapprentissage dans des mondes simuls, de cognition situe.</p></li></ol><p>Et l, enfin, il y a de la substance. Il ne sagit plus dagrandir une bote noire, mais de la <em>repenser</em>.</p><h3 id=\"heading-mon-point-de-vue-un-tournant-epistemologique\">Mon point de vue ? Un tournant pistmologique</h3><p>Ce que je sens, cest quon quitte une phase quantitative pour entrer dans une phase qualitative.<br/>Fini les records creux, place aux explorations profondes. Les annes 2010 ont t celles du passage  lchelle. Les annes 20252030 pourraient bien tre celles de la redcouverte des fondements de lintelligence.</p><p>LAGI narrivera pas par surprise, dans une update mineure dun LLM. Elle mergera peut-tre, un jour, dun agencement fin entre perception, action, mmoire, logique et modestie algorithmique.</p><p>Je termine ce billet avec cette intuition :<br/><strong>LAGI ne sera pas une explosion. Ce sera une cristallisation.</strong><br/>Silencieuse, patiente, mthodique. Comme la recherche, en somme.<br/>Et cest peut-tre pour le mieux.</p>]]&gt;\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://blog.bguarisma.com/vers-lagi-a-pas-lents-et-lucides\"> Foundations</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Vers l’AGI, à pas lents et lucides\nPosted on\nMay 13, 2025\nby\nBoris Guarisma\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nFoundations\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nCher journal,\nAujourdhui nous sommes le 13 mai 2025, et une question me trotte dans la tte :\nEt si lAGI narrivait jamais comme on limagine ?\nJe sais, cest une question un peu provocante, presque sacrilge dans un monde o chaque semaine apporte son lot de promesses, de modles rvolutionnaires et dannonces tambour battant. Mais plus je creuse, plus un malaise sinstalle.\nCela fait quelques mois que je veille intensment le sujet de lAGI  cette fameuse Intelligence Artificielle Gnrale qui serait cense rivaliser avec lintelligence humaine dans sa polyvalence et son adaptabilit. Mon point de dpart ? Une srie de vidos de\nSabine Hossenfelder\n, physicienne lucide  lhumour tranchant, et surtout dnue dillusions marketing.\nPremire claque : le modle o3\nDbut janvier, OpenAI lance o3. Buzz immdiat. Les chiffres sont impressionnants :\n87 % au test ARC-AGI, conu pour valuer des capacits abstraites proches de lhumain.\nPerformances hallucinantes en gnration de code.\nRaisonnement en chane de pense (Chain of Thought).\nSur le papier, tout y est. Pourtant,  y regarder de plus prs tout y est\njustement\ntrop. Trop beau, trop calibr, trop cher (jusqu 3 000 $ pour une tche). Et surtout : trop semblable aux versions prcdentes.\nGPT-4.5, Claude 3.7, o3 : tous progressent, mais  la marge. Le consensus merge doucement :\non atteint les limites du passage  lchelle\n. Les LLMs ne scalent plus de manire significative. Ce nest plus une ligne droite vers lAGI, cest un plateau.\nDeuxime claque : la dfinition dAGI elle-mme\nSam Altman le reconnat : AGI est devenu un mot-valise. Chacun y met ce quil veut. Un assistant trs comptent ? Un surhomme numrique ? Une IA qui fait de la physique quantique en coutant Bach ? La confusion rgne, et elle arrange bien les discours marketing.\nSabine pointe un lment crucial : russir un test (mme impressionnant comme ARC-AGI) ne signifie pas comprendre. o3 peut battre des records en reconnaissance de motifs sans rien saisir du sens profond. Cest lillusion dintelligence. Une simulation de rflexion  pas la rflexion elle-mme.\nTroisime claque : les limites des LLMs\nLes LLMs ne comprennent pas. Ils imitent. Et leur imitation a des angles morts criants :\nIls napprennent pas aprs leur entranement.\nIls chouent sur des tches logiques lmentaires (essayez de leur faire compter les r dans strawberry).\nIls nont aucune ide de ce quest le monde rel.\nYann LeCun le dit sans dtour : Ils ne savent pas dbarrasser une table comme un enfant de 10 ans. On y est.\nAlors, on fait quoi ?\nCest l que les choses deviennent fascinantes. Loin des projecteurs, une rvolution silencieuse sopre. Deux pistes mergent avec srieux :\nLe raisonnement symbolique\n: injecter de la logique, des structures formelles, une mmoire organise. Cest le retour du noyau pensant. DeepMind exprimente dj avec AlphaProof et le neurosymbolic AI.\nLes world models\n: des modles qui ne prdisent pas juste le prochain mot, mais simulent la dynamique du monde rel. Genie 2 chez DeepMind, Cosmos chez NVIDIA : on parle ici dintelligence incarne, dapprentissage dans des mondes simuls, de cognition situe.\nEt l, enfin, il y a de la substance. Il ne sagit plus dagrandir une bote noire, mais de la\nrepenser\n.\nMon point de vue ? Un tournant pistmologique\nCe que je sens, cest quon quitte une phase quantitative pour entrer dans une phase qualitative.\nFini les records creux, place aux explorations profondes. Les annes 2010 ont t celles du passage  lchelle. Les annes 20252030 pourraient bien tre celles de la redcouverte des fondements de lintelligence.\nLAGI narrivera pas par surprise, dans une update mineure dun LLM. Elle mergera peut-tre, un jour, dun agencement fin entre perception, action, mmoire, logique et modestie algorithmique.\nJe termine ce billet avec cette intuition :\nLAGI ne sera pas une explosion. Ce sera une cristallisation.\nSilencieuse, patiente, mthodique. Comme la recherche, en somme.\nEt cest peut-tre pour le mieux.\n]]>\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nFoundations\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Cher journal, Aujourd’hui nous sommes le 13 mai 2025, et une question me trotte dans la tête :“Et si l’AGI n’arrivait jamais comme on l’imagine ?” Je sais, c’est une question un peu provocante, presque sacrilège dans un monde où chaque semaine apport...",
      "meta_keywords": null,
      "og_description": "Cher journal, Aujourd’hui nous sommes le 13 mai 2025, et une question me trotte dans la tête :“Et si l’AGI n’arrivait jamais comme on l’imagine ?” Je sais, c’est une question un peu provocante, presque sacrilège dans un monde où chaque semaine apport...",
      "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
      "og_title": "Vers l’AGI, à pas lents et lucides | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 3.8,
      "sitemap_lastmod": null,
      "twitter_description": "Cher journal, Aujourd’hui nous sommes le 13 mai 2025, et une question me trotte dans la tête :“Et si l’AGI n’arrivait jamais comme on l’imagine ?” Je sais, c’est une question un peu provocante, presque sacrilège dans un monde où chaque semaine apport...",
      "twitter_title": "Vers l’AGI, à pas lents et lucides | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/05/vers-lagi-a-pas-lents-et-lucides/",
      "word_count": 769
    }
  }
}