{
  "uuid": "4b0b4962-9c7e-4014-9af2-244827693c45",
  "created_at": "2025-11-22 19:58:40",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/05/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/",
    "crawled_at": "2025-11-22T10:49:20.597948",
    "external_links": [
      {
        "href": "https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/",
        "text": "R Code – Geekcologist"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://en.wikipedia.org/wiki/Bibliometrics",
        "text": "bibliometric analysis"
      },
      {
        "href": "https://www.bibliometrix.org/",
        "text": "Bibliometrix"
      },
      {
        "href": "https://en.wikipedia.org/wiki/Louvain_method",
        "text": "Louvain method"
      },
      {
        "href": "https://link.springer.com/article/10.1007/BF02019280",
        "text": "a thematic map, using"
      },
      {
        "href": "https://link.springer.com/article/10.1007/BF02019280",
        "text": "Callon’s centrality and density metrics"
      },
      {
        "href": "https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/",
        "text": "R Code – Geekcologist"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Mapping research landscapes and dynamics: Some basic bibliometric analyses with R | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=450&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/vinicius-bastazini/",
        "text": "Vinicius Bastazini"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-392289 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Mapping research landscapes and dynamics: Some basic bibliometric analyses with R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">May 6, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/vinicius-bastazini/\">Vinicius Bastazini</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/\"> R Code – Geekcologist</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>Understanding how scientific knowledge develops requires more than merely counting papers and citations. It requires a careful evaluation of how research topics and themes interconnect and transform over time. This is where <a href=\"https://en.wikipedia.org/wiki/Bibliometrics\" rel=\"nofollow\" target=\"_blank\">bibliometric analysis</a> becomes essential. As the volume of scientific journals and papers continues to grow exponentially, bibliometric analyses become indispensable for mapping and synthesizing an increasingly complex information landscape.</p>\n<p>Through the analysis of publication information and citation patterns, bibliometric analyses allow us not only to assess scholarly productivity and impact, but more importantly, to quantify the scientific communication processes and to analyze and create indicators that reveal the dynamics and evolution of scientific information within specific disciplines and research programs, organizations, research teams or geographical regions. These kinds of tools are especially valuable for gaining a clearer understanding of research dynamics, which is essential when conducting literature reviews or shaping research strategies.</p>\n<p>In this post, I’ll share some R code I developed for a recent bibliometric analysis project I have been involved with. While it’s not as comprehensive or user-friendly as established R packages such as <strong><a href=\"https://www.bibliometrix.org/\" rel=\"nofollow\" target=\"_blank\">Bibliometrix</a></strong>, which offers a rich suite of tools and a easy to use interface, this custom approach gave me the flexibility and control I needed for more tailored data handling and visualization. In this post, we’ll walk through a handful of simple bibliometric analysis and visualization techniques using R to reveal key patterns in research data, focusing on keywords and publication trends. Of course, this approach can also be extended to other important data fields, such as words in titles or abstracts. More specifically we will look at: </p>\n<p><strong>1. Word Cloud</strong></p>\n<p>We’ll start by building a basic word cloud that highlights the most frequently used keywords across the bibliographic dataset. Here, word size will be proportional to its frequency, offering a fast, intuitive snapshot of the field’s dominant terminology.</p>\n<p><strong>2. Keyword Co-Occurrence Network</strong></p>\n<p>Next, we’ll construct a keyword co-occurrence network, a visual map that shows how often keywords appear together in academic papers. Each keyword is represented as a node in the network, and edges (or links) are drawn between them when they co-occur in the same study. The size of a node reflects how frequently a keyword appears, while the thickness of an edge indicates how strongly two keywords are associated. We’ll also apply a community detection algorithm, the <a href=\"https://en.wikipedia.org/wiki/Louvain_method\" rel=\"nofollow\" target=\"_blank\">Louvain method</a>, to identify clusters within the network— that is, groups of keywords that frequently appear together across documents. These clusters represent thematic groupings or potential research subfields, revealing the underlying conceptual structure of the literature and highlighting how different topics are connected. This approach might help to reveal the structure of a research field, showing which themes are more central, which are less developed, and how different areas of research are interconnected.</p>\n<p><strong>3. Thematic Map</strong></p>\n<p>Based on the co-occurrence network, we’ll generate <a href=\"https://link.springer.com/article/10.1007/BF02019280\" rel=\"nofollow\" target=\"_blank\">a thematic map, using </a><a href=\"https://link.springer.com/article/10.1007/BF02019280\" rel=\"nofollow\" target=\"_blank\">Callon’s centrality and density metrics</a>. In this map, each cluster, named after the most common word in a cluster, from the co-occurrence network is represented as a bubble, and its size is determined by the frequency of words in the cluster. The X-axis represents the centrality of the cluster in the network, that is, the degree of interaction with other clusters in the graph, measuring the importance of a research topic. The Y-axis represents density, a metric of the internal strength of a cluster’s network and the growth of the topic. When mapping the themes in this plot, we can identify:</p>\n<p><strong>Motor themes </strong>(top right corner): Themes in this quadrant have high centrality and density, indicating that the themes are well-developed and crucial for structuring the research field.<br/><strong>Niche themes </strong>(top left corner): Themes that are highly specialized and well-developed in terms of internal research but have more limited interaction with other themes.<br/><strong>Peripheral them </strong>(bottom left corner): Themes with low centrality and low density, suggesting that they are underdeveloped and marginal, representing themes that either emerging or in decline in the literature.<br/><strong>Basic themes </strong>(bottom right corner): These themes have high centrality and low density. They are often essential for transdisciplinary research, meaning they may serve as foundational topics that cross the boundaries of multiple themes, but despite their central role in the network, these themes have low density of connections.</p>\n<p><strong>4. Yearly Keyword Trends</strong></p>\n<p>To understand the temporal dynamics of research fields, we’ll build a yearly keyword trend diagram using a Sankey plot. This diagram maps the flow of the most frequent keywords (here, I will be using a cut off of the ten most common key words, but this could be done for as many keywords as necessary), revealing how interest in specific topics rises or fades over time.</p>\n<p><strong>5. Decade-Based Keyword Evolution</strong></p>\n<p>At last, we’ll take a look at how the research field is changing through time, by aggregating keyword data by decade ( or whatever time frame one might want to look at). This decade-based evolution<strong> </strong>diagram shows the progression of top keywords (once again, I will use a cut off of the ten most common key words per decade) from one decade to the next, capturing long-term shifts and the persistence or disappearance of major research themes.</p>\n<p>To start off, we’ll simulate a simple bibliographic dataset to work with. This will consist of a data frame containing 500 publications, each tagged with a publication year ranging from 2000 to 2025, along with a set of keywords. For the purpose of this basic tutorial, I’ve created a list of keywords that might typically appear in an evolutionary ecology or eco-evolutionary research paper, so let’s pretend that we are conducting a review on something like “coevolutionary dynamics in ecological communities”. Of course, in your real-world application, you’ll be working with your own bibliographic data, which will likely include additional fields such as authors, titles, abstracts, journals, citation counts, etc. That’s perfectly fine—the analyses here will use columns in this simulated data frame named <code>\"Year\" </code>and <code>\"Author_Keywords\"</code>, but you can easily adapt the code by modifying the column header to match the structure of your dataset. And as I mention before, some of these analyses can be used for other bibliographic information, besides keywords. And it goes without saying: in your real-world application, you’ll be working with messy, inconsistent data, so you’ll likely need to do a lot of data cleaning/handling, such as combining similar keywords, handling typos and linguistic variations, and so on — so keep that in mind as you build your solution.</p>\n<p>So, let’s start by loading the necessary packages, “creating” and organizing our dataset, and defining parameters for the analysis:</p>\n<pre>#### 1. Load packages\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(igraph)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(RColorBrewer) \nlibrary(wordcloud)  \nlibrary(networkD3)\nlibrary(htmlwidgets)\n\n### 2.Parameters\ncat(\"--- 2. Setting Parameters ---\\n\")\n\n# Simulation Parameters\nn_studies = 500             # Number of simulated studies\nstart_year = 2000           # Start year for publications\nend_year = 2025             # End year for publications\nkeywords_per_study = 5      # Number of keywords per study\n\n# Keyword Pool\nkeyword_pool &lt;- c(\n  \"coevolution\", \"arms race\", \"mutualism\", \"antagonism\", \"host-parasite\",\n  \"plant-pollinator\", \"Red Queen hypothesis\", \"selection pressure\", \"adaptation\",\n  \"phylogeny\", \"gene flow\", \"speciation\", \"ecological interaction\", \"community structure\",\"community assembly\",\"community disassembly\",\n  \"predator-prey\", \"ecophylogenetics\", \"co-speciation\", \"evolutionary dynamics\", \"co-phylogenetic analysis\",\n  \"adaptive dynamics\", \"local adaptation\", \"trait evolution\", \"phylogenetic signal\",\n  \"functional traits\", \"ecological networks\", \"niche differentiation\", \"coevolutionary networks\"\n)\n\n# Analysis Parameters\nkeyword_column_sim = \"Author_Keywords\" # Name for the keyword column in simulated data\nyear_column_sim = \"Year\"               # Name for the year column in simulated data\nmin_cooccurrence = 3                 # Min times two keywords must appear together\nmin_keyword_freq_network = 3           # Min total frequency for a keyword to be in the network plot\nmax_words_cloud = 75                   # Max words to display in the word cloud\n\n# Sankey Parameters\nnum_top_keywords_yearly = 10           # Number of top keywords for the YEARLY Sankey diagram\nnum_top_keywords_per_decade = 10       # Number of top keywords PER DECADE for the DECADE Sankey diagram\n\n### 3. Simulate Bibliographic Data\ncat(\"--- 3. Simulating Study Data ---\\n\")\n\nsimulated_studies &lt;- tibble(\n  paper_id = 1:n_studies,\n  Year = sample(start_year:end_year, n_studies, replace = TRUE)\n)\n\n# Generate keywords for each study\nkeywords_list &lt;- lapply(1:n_studies, function(i) {\n  sample(keyword_pool, keywords_per_study, replace = FALSE)\n})\n\n# Combine into a long format data frame (one row per keyword per paper)\nkeywords_long_sim &lt;- simulated_studies %&gt;%\n  mutate(keywords = keywords_list) %&gt;%\n  unnest(keywords) %&gt;%\n  rename(!!sym(keyword_column_sim) := keywords,\n         !!sym(year_column_sim) := Year) %&gt;%\n  select(paper_id, !!sym(year_column_sim), !!sym(keyword_column_sim)) %&gt;%\n  mutate(keyword = str_trim(tolower(!!sym(keyword_column_sim)))) %&gt;%\n  select(paper_id, year = !!sym(year_column_sim), keyword) %&gt;%\n  distinct(paper_id, year, keyword) # Ensure unique keyword per paper/year instance\n\ncat(\"Generated\", n_studies, \"studies (\", start_year, \"-\", end_year, \") with\",\n    nrow(keywords_long_sim), \"unique keyword instances per paper/year.\\n\")\ncat(\"Example simulated data:\\n\")\nprint(head(keywords_long_sim))\n\n# Use this simulated data for the rest of the analysis\nkeywords_long &lt;- keywords_long_sim\n\n#### 4. Calculate Overall Keyword Frequencies\ncat(\"\\n--- 4. Calculating Overall Keyword Frequencies ---\\n\")\nkeyword_total_freq &lt;- keywords_long %&gt;%\n  # Count unique keywords per paper first, then sum across papers\n  distinct(paper_id, keyword) %&gt;%\n  count(keyword, name = \"total_freq\", sort = TRUE)\n\ncat(\"Top overall keywords (based on number of papers):\\n\")\nprint(head(keyword_total_freq))</pre>\n<p>Now that we have our “dataset”, let’s create a visual representation of the word data, building a simple word cloud:</p>\n<pre>### 5. Word Cloud \ncat(\"\\n--- 5. Generating Word Cloud ---\\n\")\n\nif (exists(\"keyword_total_freq\") &amp;&amp; nrow(keyword_total_freq) &gt; 0) {\n  cat(\"   - Creating Word Cloud (Check RStudio Plots Pane)...\\n\")\n  tryCatch({\n    \n    wordcloud(words = keyword_total_freq$keyword,\n              freq = keyword_total_freq$total_freq,\n              min.freq = 2, # Show words appearing in at least in 2 papers\n              max.words = max_words_cloud,\n              random.order = FALSE, # Plot most frequent words first\n              rot.per = 0.30,      # Percentage of words rotated\n              colors = brewer.pal(8, \"Dark2\")) # Color palette\n    title(main = \"\", line = -1) # If you want, add a title near the top\n  }, error = function(e) {\n    cat(\"     &gt; Error generating word cloud:\", conditionMessage(e), \"\\n\")\n  })\n  \n} else {\n  cat(\"   - Skipping word cloud (no keyword frequency data available).\\n\")\n}\n</pre>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2480\" data-attachment-id=\"2480\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-title=\"word cloud\" data-large-file=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&amp;ssl=1\" data-lazy-sizes=\"(max-width: 640px) 100vw, 640px\" data-lazy-src=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&amp;ssl=1\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png\" data-orig-size=\"855,756\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/word-cloud/\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&amp;ssl=1 640w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=768 768w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png 855w\"/><noscript><img alt=\"\" aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_len=\"\" class=\"wp-image-2480\" data-attachment-id=\"2480\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-meta=\"{\" data-image-title=\"word cloud\" data-large-file=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&amp;ssl=1\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png\" data-orig-size=\"855,756\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/word-cloud/\" data-recalc-dims=\"1\" loading=\"lazy\" sizes=\"(max-width: 640px) 100vw, 640px\" src=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&amp;ssl=1\" srcset_temp=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=450&amp;ssl=1 640w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=768 768w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png 855w\"/></noscript></figure>\n<p>Now, let’s create the co-ocurrence network, and identify clusters:</p>\n<pre>### 6. Keyword Co-occurrence Network Analysis\ncat(\"\\n--- 6. Building Keyword Co-occurrence Network ---\\n\")\n# (Uses keywords_long which contains year info, but pairs are per paper regardless of year)\n\n# 6a. Generate keyword pairs within each paper\ncat(\"   - Generating keyword pairs...\\n\")\nkeyword_pairs_unnested &lt;- keywords_long %&gt;%\n  group_by(paper_id) %&gt;%\n  filter(n() &gt;= 2) %&gt;%\n  summarise(pairs = list(combn(keyword, 2, simplify = FALSE)), .groups = 'drop') %&gt;%\n  unnest(pairs) %&gt;%\n  mutate(\n    keyword1 = sapply(pairs, `[`, 1),\n    keyword2 = sapply(pairs, `[`, 2)\n  ) %&gt;%\n  select(keyword1, keyword2)\n\n# 6b. Standardize &amp; Count Pairs\ncat(\"   - Counting co-occurrences (min =\", min_cooccurrence, \")...\\n\")\nkeyword_pair_counts &lt;- keyword_pairs_unnested %&gt;%\n  mutate(\n    temp_kw1 = pmin(keyword1, keyword2),\n    temp_kw2 = pmax(keyword1, keyword2)\n  ) %&gt;%\n  select(keyword1 = temp_kw1, keyword2 = temp_kw2) %&gt;%\n  count(keyword1, keyword2, name = \"weight\") %&gt;%\n  filter(weight &gt;= min_cooccurrence)\n\n# 6c. Create and Filter Graph\ngraph_plot_obj &lt;- NULL\ncommunities &lt;- NULL\n\nif(nrow(keyword_pair_counts) &gt; 0) {\n  cat(\"   - Creating graph object...\\n\")\n  graph_obj &lt;- graph_from_data_frame(keyword_pair_counts, directed = FALSE)\n  \n  cat(\"   - Filtering graph (min degree =\", min_keyword_freq_network, \") &amp; detecting communities...\\n\")\n  node_degrees &lt;- degree(graph_obj, mode = \"all\")\n  nodes_to_keep &lt;- names(node_degrees[node_degrees &gt;= min_keyword_freq_network])\n  \n  if(length(nodes_to_keep) &gt; 0){\n    graph_filtered &lt;- induced_subgraph(graph_obj, V(graph_obj)$name %in% nodes_to_keep)\n    graph_filtered &lt;- delete.vertices(graph_filtered, degree(graph_filtered) == 0)\n    \n    if (vcount(graph_filtered) &gt; 0 &amp;&amp; ecount(graph_filtered) &gt; 0) {\n      communities &lt;- cluster_louvain(graph_filtered)\n      num_communities &lt;- length(unique(membership(communities)))\n      cat(\"     &gt; Detected\", num_communities, \"communities (Louvain).\\n\")\n      \n      node_data &lt;- tibble(name = V(graph_filtered)$name) %&gt;%\n        left_join(keyword_total_freq, by = c(\"name\" = \"keyword\")) %&gt;%\n        mutate(total_freq = ifelse(is.na(total_freq), 1, total_freq))\n      \n      V(graph_filtered)$size &lt;- log1p(node_data$total_freq) * 2.5\n      V(graph_filtered)$label &lt;- V(graph_filtered)$name\n      V(graph_filtered)$community &lt;- membership(communities)\n      V(graph_filtered)$total_freq &lt;- node_data$total_freq\n      \n      # Assign colors based on community\n      if (num_communities &gt; 0) {\n        num_colors_needed = length(unique(V(graph_filtered)$community))\n        if (num_colors_needed &gt; 8) {\n          community_colors &lt;- colorRampPalette(brewer.pal(8, \"Set2\"))(num_colors_needed)\n        } else if (num_colors_needed &gt; 2) {\n          community_colors &lt;- brewer.pal(max(3, num_colors_needed), \"Set2\")[1:num_colors_needed]\n        } else if (num_colors_needed == 2) {\n          community_colors &lt;- brewer.pal(3, \"Set2\")[1:2]\n        } else { # num_colors_needed == 1\n          community_colors &lt;- brewer.pal(3, \"Set2\")[1]\n        }\n        community_map &lt;- setNames(community_colors, sort(unique(V(graph_filtered)$community)))\n        V(graph_filtered)$color &lt;- community_map[as.character(V(graph_filtered)$community)]\n      } else {\n        V(graph_filtered)$color &lt;- \"grey\"\n        community_map &lt;- NULL\n      }\n      \n      graph_plot_obj &lt;- graph_filtered\n      cat(\"     &gt; Filtered graph ready:\", vcount(graph_plot_obj), \"nodes,\", ecount(graph_plot_obj), \"edges.\\n\")\n      \n    } else {\n      cat(\"     &gt; Warning: Graph empty after filtering.\\n\")\n      graph_plot_obj &lt;- NULL\n      communities &lt;- NULL\n    }\n  } else {\n    cat(\"     &gt; Warning: No nodes met minimum degree requirement.\\n\")\n    graph_plot_obj &lt;- NULL\n    communities &lt;- NULL\n  }\n} else {\n  cat(\"   - Warning: No keyword pairs met the minimum co-occurrence threshold.\\n\")\n  graph_plot_obj &lt;- NULL\n  communities &lt;- NULL\n}\n\n# 6d. Visualize Network\nif (!is.null(graph_plot_obj)) {\n  cat(\"   - Plotting Co-occurrence Network (Check RStudio Plots Pane)...\\n\")\n  tryCatch({\n    par(mar=c(1, 1, 3, 1))\n    plot(graph_plot_obj,\n         layout = layout_nicely(graph_plot_obj),\n         vertex.frame.color = \"grey40\", vertex.label.color = \"black\",\n         vertex.label.cex = 0.7, vertex.label.dist = 0.4,\n         edge.color = rgb(0.5, 0.5, 0.5, alpha = 0.4), edge.curved = 0.1,\n         edge.width = scales::rescale(E(graph_plot_obj)$weight, to = c(0.3, 3.0)),\n         main = \"Keyword Co-occurrence Network (Simulated Data)\",\n         sub = paste(\"Nodes sized by log(# Papers), Min Degree &gt;=\", min_keyword_freq_network)\n    )\n    if (!is.null(community_map) &amp;&amp; length(community_map) &lt;= 12 &amp;&amp; length(community_map) &gt; 1) {\n      legend(\"bottomleft\", legend = paste(\"Cluster\", names(community_map)),\n             fill = community_map, bty = \"n\", cex = 0.7, title=\"Communities\")\n    }\n    par(mar=c(5.1, 4.1, 4.1, 2.1)) # Reset margins\n  }, error = function(e){\n    cat(\"     &gt; Error plotting network:\", conditionMessage(e), \"\\n\")\n    par(mar=c(5.1, 4.1, 4.1, 2.1)) # Reset margins on error\n  })\n} else {\n  cat(\"   - Skipping network plot (no valid graph).\\n\")\n}\n</pre>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2482\" data-attachment-id=\"2482\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-title=\"rede\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=640\" data-lazy-sizes=\"(max-width: 855px) 100vw, 855px\" data-lazy-src=\"https://i1.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=450&amp;ssl=1\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png\" data-orig-size=\"855,531\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/rede/\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png 855w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=768 768w\"/><noscript><img alt=\"\" aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_len=\"\" class=\"wp-image-2482\" data-attachment-id=\"2482\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-meta=\"{\" data-image-title=\"rede\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=640\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png\" data-orig-size=\"855,531\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/rede/\" data-recalc-dims=\"1\" loading=\"lazy\" sizes=\"(max-width: 855px) 100vw, 855px\" src=\"https://i1.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=450&amp;ssl=1\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png 855w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/rede.png?w=768 768w\"/></noscript></figure>\n<p>In this example, the clustering algorithm identified three distinct clusters—groups of words that frequently co-occur across the papers. Based on these clusters, we will create a thematic map, where each cluster is represented as a bubble, visually illustrating the relationships and centrality of research themes within the broader network of keywords. This map will help us to better understand the underlying structure of the field and how different research topics are interconnected.</p>\n<pre>### 7. Thematic Map Analysis\ncat(\"\\n--- 7. Generating Thematic Map (Callon's Metrics) ---\\n\")\n\n# Helper function\ncalculate_callon_metrics &lt;- function(graph, communities_object, cluster_id) {\n  if (is.null(graph) || !is.igraph(graph) || is.null(communities_object)) {\n    return(list(centrality = 0, density = 0, n_keywords = 0))\n  }\n  cluster_nodes_indices &lt;- which(membership(communities_object) == cluster_id)\n  if (length(cluster_nodes_indices) == 0) {\n    return(list(centrality = 0, density = 0, n_keywords = 0))\n  }\n  n_nodes_in_cluster &lt;- length(cluster_nodes_indices)\n  subgraph &lt;- induced_subgraph(graph, cluster_nodes_indices)\n  internal_weight_sum &lt;- if (ecount(subgraph) &gt; 0) sum(E(subgraph)$weight, na.rm = TRUE) else 0\n  density &lt;- internal_weight_sum\n  external_weight_sum &lt;- 0\n  all_incident_edges_indices &lt;- E(graph)[.inc(cluster_nodes_indices)]\n  if (length(all_incident_edges_indices) &gt; 0) {\n    all_incident_edges &lt;- E(graph)[all_incident_edges_indices]\n    ends_matrix &lt;- ends(graph, all_incident_edges, names = FALSE)\n    mem &lt;- membership(communities_object)\n    is_external &lt;- (mem[ends_matrix[,1]] != cluster_id) | (mem[ends_matrix[,2]] != cluster_id)\n    external_edges &lt;- all_incident_edges[is_external]\n    if (length(external_edges) &gt; 0) {\n      external_weight_sum &lt;- sum(E(graph)$weight[external_edges], na.rm = TRUE)\n    }\n  }\n  centrality &lt;- external_weight_sum\n  return(list(centrality = centrality, density = density, n_keywords = n_nodes_in_cluster))\n}\n\n\nthematic_plot_obj &lt;- NULL\n\nif (!is.null(graph_plot_obj) &amp;&amp; !is.null(communities) &amp;&amp; length(unique(membership(communities))) &gt; 0) {\n  cat(\"   - Calculating Centrality and Density for communities...\\n\")\n  \n  community_ids &lt;- unique(membership(communities))\n  thematic_metrics &lt;- lapply(community_ids, function(comm_id) {\n    metrics &lt;- calculate_callon_metrics(graph_plot_obj, communities, comm_id)\n    nodes_in_comm_indices &lt;- which(membership(communities) == comm_id)\n    community_node_names &lt;- V(graph_plot_obj)$name[nodes_in_comm_indices]\n    community_node_freqs &lt;- V(graph_plot_obj)$total_freq[nodes_in_comm_indices]\n    \n    if(length(community_node_names) &gt; 0 &amp;&amp; length(community_node_freqs) &gt; 0 &amp;&amp; !all(is.na(community_node_freqs))){\n      most_frequent_keyword &lt;- community_node_names[which.max(community_node_freqs)]\n      community_label &lt;- str_trunc(most_frequent_keyword, 30)\n    } else {\n      community_label &lt;- paste(\"Cluster\", comm_id)\n    }\n    \n    return(tibble(\n      community_id = comm_id, label = community_label,\n      Centrality = metrics$centrality, Density = metrics$density,\n      n_keywords = metrics$n_keywords\n    ))\n  })\n  \n  thematic_data &lt;- bind_rows(thematic_metrics) %&gt;%\n    mutate(Centrality = as.numeric(Centrality), Density = as.numeric(Density)) %&gt;%\n    filter(!is.na(community_id), n_keywords &gt; 0, is.finite(Centrality), is.finite(Density))\n  \n  if(nrow(thematic_data) &gt; 0) {\n    cat(\"   - Creating Thematic Map plot object...\\n\")\n    median_centrality &lt;- median(thematic_data$Centrality, na.rm = TRUE)\n    median_density &lt;- median(thematic_data$Density, na.rm = TRUE)\n    median_centrality &lt;- ifelse(is.finite(median_centrality), median_centrality, 0)\n    median_density &lt;- ifelse(is.finite(median_density), median_density, 0)\n    cat(\"     &gt; Quadrant thresholds (Medians): Centrality=\", round(median_centrality,2), \", Density=\", round(median_density,2), \"\\n\")\n    \n    thematic_plot_obj &lt;- ggplot(thematic_data, aes(x = Centrality, y = Density)) +\n      geom_hline(yintercept = median_density, linetype = \"dashed\", color = \"grey50\") +\n      geom_vline(xintercept = median_centrality, linetype = \"dashed\", color = \"grey50\") +\n      geom_point(aes(size = n_keywords), alpha = 0.7, color = \"steelblue\") +\n      geom_text_repel(aes(label = label), size = 3.0, max.overlaps = 15,\n                      box.padding = 0.4, point.padding = 0.6) +\n      scale_size_continuous(range = c(4, 12), name = \"# Keywords\\nin Theme\") +\n      ggplot2::annotate(\"text\", x = median_centrality, y = Inf, label = \"Motor Themes\", hjust = 0.5, vjust = 1.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      ggplot2::annotate(\"text\", x = -Inf, y = Inf, label = \"Niche Themes\", hjust = -0.1, vjust = 1.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      ggplot2::annotate(\"text\", x = -Inf, y = -Inf, label = \"Emerging/\\nDeclining\", hjust = -0.1, vjust = -0.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      ggplot2::annotate(\"text\", x = median_centrality, y = -Inf, label = \"Basic Themes\", hjust = 0.5, vjust = -0.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      labs(\n        title = \"Thematic Map (Callon's Centrality &amp; Density)\",\n        subtitle = \"Keyword Clusters from Co-occurrence Network (Simulated Data)\",\n        x = \"Centrality (Links to other themes)\",\n        y = \"Density (Internal theme links)\"\n      ) +\n      theme_minimal(base_size = 12) +\n      theme(\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5),\n        plot.margin = margin(20, 20, 20, 20)\n      )\n    \n    # Visualize Thematic Map (Print to RStudio Plots Pane)\n    cat(\"   - Plotting Thematic Map (Check RStudio Plots Pane)...\\n\")\n    print(thematic_plot_obj)\n    \n  } else {\n    cat(\"   - Warning: No valid thematic data to plot.\\n\")\n  }\n} else {\n  cat(\"   - Skipping Thematic Map (network or communities missing).\\n\")\n}\n</pre>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2485\" data-attachment-id=\"2485\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-title=\"thematic map\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=640\" data-lazy-sizes=\"(max-width: 855px) 100vw, 855px\" data-lazy-src=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=450&amp;ssl=1\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png\" data-orig-size=\"855,531\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/thematic-map/\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png 855w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=768 768w\"/><noscript><img alt=\"\" aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_len=\"\" class=\"wp-image-2485\" data-attachment-id=\"2485\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-meta=\"{\" data-image-title=\"thematic map\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=640\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png\" data-orig-size=\"855,531\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/thematic-map/\" data-recalc-dims=\"1\" loading=\"lazy\" sizes=\"(max-width: 855px) 100vw, 855px\" src=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=450&amp;ssl=1\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png 855w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/thematic-map.png?w=768 768w\"/></noscript></figure>\n<p>In this simulated example, “ecological networks” was positioned at the center of the plot, indicating its central role within the research landscape, while “ecophylogenetics” was classified as a motor theme, reflecting its importance and well-developed nature in the field. On the other hand, “evolutionary dynamics” appeared as a peripheral theme, suggesting that it is underdeveloped or marginal in the current body of research.</p>\n<p>Now, let’s see the evolution of the research field, by first building a Sankey diagram, with the association between years, and the “most”used keywords in our simulated example:</p>\n<pre>### 8. Yearly Keyword Trend Analysis\ncat(\"\\n--- 8. Generating Yearly Keyword Trend Sankey Diagram ---\\n\")\n\nsankey_plot_obj_yearly &lt;- NULL\n\n# 8a. Count Keywords per Year\ncat(\"   - Counting keyword frequency per year (using unique paper/keyword counts)...\\n\")\nkeyword_yearly_counts &lt;- keywords_long %&gt;%\n  distinct(paper_id, year, keyword) %&gt;% # Count keyword once per paper per year\n  count(year, keyword, name = \"yearly_count\") %&gt;%\n  filter(yearly_count &gt; 0)\n\n# 8b. Identify Top Keywords Overall (Using paper frequency calculated earlier)\ncat(\"   - Identifying top\", num_top_keywords_yearly, \"keywords overall for yearly Sankey...\\n\")\nif(exists(\"keyword_total_freq\") &amp;&amp; inherits(keyword_total_freq, \"data.frame\") &amp;&amp; nrow(keyword_total_freq) &gt; 0) {\n  top_keywords_df_yearly &lt;- keyword_total_freq\n} else {\n  cat(\"     &gt; Warning: 'keyword_total_freq' not found. Recalculating based on yearly counts (less accurate representation of 'overall').\\n\")\n  top_keywords_df_yearly &lt;- keyword_yearly_counts %&gt;% group_by(keyword) %&gt;% summarise(total_freq = sum(yearly_count)) %&gt;% arrange(desc(total_freq))\n}\n\ntop_keywords_yearly &lt;- top_keywords_df_yearly %&gt;%\n  slice_head(n = num_top_keywords_yearly) %&gt;%\n  pull(keyword)\n\nif(length(top_keywords_yearly) &gt; 0){\n  cat(\"     &gt; Top keywords for Yearly Sankey:\", paste(top_keywords_yearly, collapse = \", \"), \"\\n\")\n  \n  # 8c. Prepare Data for Yearly Sankey\n  sankey_data_yearly &lt;- keyword_yearly_counts %&gt;%\n    filter(keyword %in% top_keywords_yearly)\n  \n  if(nrow(sankey_data_yearly) == 0) {\n    cat(\"   - Warning: No yearly counts found for the top keywords. Skipping Yearly Sankey.\\n\")\n  } else {\n    cat(\"   - Preparing data for Yearly Sankey diagram...\\n\")\n    year_nodes_chr_yr &lt;- as.character(sort(unique(sankey_data_yearly$year)))\n    keyword_nodes_sankey_yr &lt;- unique(sankey_data_yearly$keyword)\n    # Prefix years to distinguish from keywords if necessary\n    all_node_names_yr &lt;- c(paste0(\"Y:\", year_nodes_chr_yr), keyword_nodes_sankey_yr)\n    \n    nodes_df_yr &lt;- data.frame(name = all_node_names_yr, stringsAsFactors = FALSE) %&gt;%\n      mutate(id = row_number() - 1)\n    \n    links_df_yr &lt;- sankey_data_yearly %&gt;%\n      mutate(\n        source_name = paste0(\"Y:\", as.character(year)),\n        target_name = keyword\n      ) %&gt;%\n      left_join(nodes_df_yr %&gt;% select(name, source_id = id), by = c(\"source_name\" = \"name\")) %&gt;%\n      left_join(nodes_df_yr %&gt;% select(name, target_id = id), by = c(\"target_name\" = \"name\")) %&gt;%\n      filter(!is.na(source_id), !is.na(target_id)) %&gt;%\n      transmute(\n        source = source_id, target = target_id,\n        value = yearly_count, group = target_name # Color links by target keyword\n      ) %&gt;%\n      filter(value &gt; 0)\n    \n    if(nrow(links_df_yr) == 0) {\n      cat(\"   - Warning: Failed to create valid links for Yearly Sankey diagram. Skipping.\\n\")\n    } else {\n      # 8d. Generate Yearly Sankey Diagram Object\n      cat(\"   - Creating Yearly Sankey plot object...\\n\")\n      sankey_plot_obj_yearly &lt;- sankeyNetwork(\n        Links = links_df_yr, Nodes = nodes_df_yr, Source = \"source\",\n        Target = \"target\", Value = \"value\", NodeID = \"name\",\n        NodeGroup = NULL, LinkGroup = \"group\", units = \"Papers\",\n        fontSize = 11, nodeWidth = 30, nodePadding = 15, sinksRight = TRUE,\n        margin = list(top=5, bottom=5, left=5, right=5)\n      )\n      \n      # 8e. Visualize Yearly Sankey (Print to RStudio Viewer Pane)\n      if (!is.null(sankey_plot_obj_yearly)) {\n        cat(\"   - Plotting Yearly Sankey Diagram (Check RStudio Viewer Pane)...\\n\")\n        sankey_title_yr &lt;- paste0(\"Flow of Top \", num_top_keywords_yearly, \" Keywords Over Time (Yearly, Simulated)\")\n        sankey_plot_obj_yr_title &lt;- htmlwidgets::prependContent(sankey_plot_obj_yearly,\n                                                                htmltools::h3(sankey_title_yr, style = \"text-align:center;\"))\n        print(sankey_plot_obj_yr_title)\n      } else {\n        cat (\"   - Warning: Yearly Sankey plot object could not be created.\\n\")\n      }\n    }\n  }\n} else {\n  cat(\"   - Warning: No top keywords identified for yearly Sankey. Skipping.\\n\")\n}\n</pre>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2488\" data-attachment-id=\"2488\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-title=\"anual\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=640\" data-lazy-sizes=\"(max-width: 853px) 100vw, 853px\" data-lazy-src=\"https://i1.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=450&amp;ssl=1\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png\" data-orig-size=\"853,529\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/anual/\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png 853w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=768 768w\"/><noscript><img alt=\"\" aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_len=\"\" class=\"wp-image-2488\" data-attachment-id=\"2488\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-meta=\"{\" data-image-title=\"anual\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=640\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png\" data-orig-size=\"853,529\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/anual/\" data-recalc-dims=\"1\" loading=\"lazy\" sizes=\"(max-width: 853px) 100vw, 853px\" src=\"https://i1.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=450&amp;ssl=1\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png 853w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/anual.png?w=768 768w\"/></noscript></figure>\n<p>At last, let’s see how the importance of keywords has shifted over the past decades by visualizing the changing importance of the most frequently used keywords in our simulated example. To do this, we’ll build another Sankey diagram that illustrates the flow and evolution, and the importance of these keywords across time. Please have in mind , that although I will set the number of top keywords per decade to 10, the Sankey diagram may display more than 10 keywords overall because it includes the top 10 from each decade,  and if different keywords are dominant in different decades, this can lead to a larger combined set of unique keywords across the entire timeline.</p>\n<pre>### 9. Decade-Based Keyword Evolution\ncat(\"\\n--- 9. Generating Decade-Based Keyword Evolution Sankey ---\\n\")\n\nsankey_plot_obj_decades &lt;- NULL\n\n# 9a. Aggregate by Decade and Calculate Frequencies\ncat(\"   - Aggregating by Decade and Calculating Frequencies (using unique paper/keyword counts per decade)...\\n\")\nkeywords_decades &lt;- keywords_long %&gt;%\n  filter(!is.na(year)) %&gt;%\n  mutate(decade = floor(year / 10) * 10) %&gt;% # Calculate decade\n  select(paper_id, decade, keyword) %&gt;%\n  distinct() # Count each keyword only once per paper within a decade\n\nkeyword_decade_counts &lt;- keywords_decades %&gt;%\n  count(decade, keyword, name = \"count\") %&gt;%\n  arrange(decade, desc(count))\n\nif(nrow(keyword_decade_counts) == 0){\n  cat(\"   - Warning: No keyword counts per decade found. Skipping Decade Sankey.\\n\")\n} else {\n  cat(\"   - Decade counts calculated.\\n\")\n  \n  # 9b. Identify Top Keywords for Each Decade\n  cat(\"   - Identifying top\", num_top_keywords_per_decade, \"keywords per decade...\\n\")\n  top_keywords_per_decade &lt;- keyword_decade_counts %&gt;%\n    group_by(decade) %&gt;%\n    slice_max(order_by = count, n = num_top_keywords_per_decade, with_ties = FALSE) %&gt;%\n    ungroup()\n  \n  keywords_to_track &lt;- unique(top_keywords_per_decade$keyword)\n  \n  if(length(keywords_to_track) == 0) {\n    cat(\"   - Warning: No top keywords identified across decades to track. Skipping Decade Sankey.\\n\")\n  } else {\n    cat(\"     &gt; Total unique keywords to track (top\", num_top_keywords_per_decade, \"in any decade):\", length(keywords_to_track), \"\\n\")\n    \n    # Filter the counts to only include these keywords\n    sankey_base_data_dec &lt;- keyword_decade_counts %&gt;%\n      filter(keyword %in% keywords_to_track)\n    \n    if(nrow(sankey_base_data_dec) == 0){\n      cat(\"   - Warning: No counts found for selected keywords to track. Skipping Decade Sankey.\\n\")\n    } else {\n      \n      # 9c. Prepare Nodes and Links for Decade Sankey\n      cat(\"   - Preparing nodes and links for Decade Sankey...\\n\")\n      nodes_df_dec &lt;- sankey_base_data_dec %&gt;%\n        mutate(name = paste0(decade, \"s: \", keyword)) %&gt;% # Node label: \"2000s: coevolution\"\n        select(name) %&gt;%\n        distinct() %&gt;%\n        mutate(id = row_number() - 1)\n      \n      decade_list &lt;- sort(unique(sankey_base_data_dec$decade))\n      links_list_dec &lt;- list()\n      \n      if (length(decade_list) &gt; 1) {\n        for (i in 1:(length(decade_list) - 1)) {\n          current_decade &lt;- decade_list[i]\n          next_decade &lt;- decade_list[i+1]\n          \n          current_decade_data &lt;- sankey_base_data_dec %&gt;% filter(decade == current_decade)\n          next_decade_data &lt;- sankey_base_data_dec %&gt;% filter(decade == next_decade)\n          common_keywords &lt;- intersect(current_decade_data$keyword, next_decade_data$keyword)\n          \n          if (length(common_keywords) &gt; 0) {\n            temp_links_dec &lt;- tibble(keyword = common_keywords) %&gt;%\n              mutate(source_name = paste0(current_decade, \"s: \", keyword)) %&gt;%\n              left_join(nodes_df_dec %&gt;% select(name, source_id = id), by = c(\"source_name\" = \"name\")) %&gt;%\n              mutate(target_name = paste0(next_decade, \"s: \", keyword)) %&gt;%\n              left_join(nodes_df_dec %&gt;% select(name, target_id = id), by = c(\"target_name\" = \"name\")) %&gt;%\n              # Link value is the count in the *target* decade (flow into that decade)\n              left_join(next_decade_data %&gt;% select(keyword, value = count), by = \"keyword\") %&gt;%\n              filter(!is.na(source_id), !is.na(target_id), !is.na(value), value &gt; 0) %&gt;%\n              select(source = source_id, target = target_id, value = value, group = keyword)\n            \n            if(nrow(temp_links_dec) &gt; 0){\n              links_list_dec[[as.character(current_decade)]] &lt;- temp_links_dec\n            }\n          }\n        } # End for loop\n      } # End if length(decade_list) &gt; 1\n      \n      if (length(links_list_dec) &gt; 0) {\n        links_df_dec &lt;- bind_rows(links_list_dec)\n      } else {\n        links_df_dec &lt;- tibble(source = integer(), target = integer(), value = numeric(), group = character()) # Empty tibble\n      }\n      \n      if (nrow(nodes_df_dec) == 0 || nrow(links_df_dec) == 0) {\n        cat(\"   - Warning: Could not create valid nodes or links for Decade Sankey. Skipping.\\n\")\n      } else {\n        cat(\"     &gt; Decade Nodes:\", nrow(nodes_df_dec), \"; Decade Links:\", nrow(links_df_dec), \"created.\\n\")\n        \n        # 9d. Generate Decade Sankey Diagram\n        cat(\"   - Creating Decade Sankey plot object...\\n\")\n        num_groups_dec &lt;- length(unique(links_df_dec$group))\n        if (num_groups_dec &lt;= 12 &amp;&amp; num_groups_dec &gt; 0) {\n          color_palette_dec &lt;- RColorBrewer::brewer.pal(max(3, num_groups_dec), \"Paired\")[1:num_groups_dec]\n          color_scale_js_dec &lt;- paste0('d3.scaleOrdinal([\"', paste(color_palette_dec, collapse = '\",\"'), '\"]);')\n        } else if (num_groups_dec &gt; 12) {\n          color_scale_js_dec &lt;- 'd3.scaleOrdinal(d3.schemeCategory10);'\n          cat(\"     &gt; Warning: &gt;12 keyword groups for Decade Sankey, colors may repeat.\\n\")\n        } else {\n          color_scale_js_dec &lt;- 'd3.scaleOrdinal([\"#cccccc\"]);' # Default grey\n        }\n        \n        \n        sankey_plot_obj_decades &lt;- sankeyNetwork(\n          Links = links_df_dec, Nodes = nodes_df_dec, Source = \"source\",\n          Target = \"target\", Value = \"value\", NodeID = \"name\",\n          LinkGroup = \"group\", NodeGroup = NULL, units = \"Papers\",\n          fontSize = 10, nodeWidth = 35, nodePadding = 10, # Adjusted node width/padding\n          sinksRight = FALSE, # Keep temporal flow L-&gt;R\n          colourScale = JS(color_scale_js_dec),\n          margin = list(top=5, bottom=5, left=5, right=5)\n        )\n        \n        # 9e. Visualize Decade Sankey (Print to RStudio Viewer Pane)\n        if(!is.null(sankey_plot_obj_decades)){\n          cat(\"   - Plotting Decade Sankey Diagram (Check RStudio Viewer Pane)...\\n\")\n          sankey_title_dec &lt;- paste0(\"Evolution of Top \", num_top_keywords_per_decade, \" Keywords by Decade (Simulated)\")\n          sankey_plot_obj_dec_title &lt;- htmlwidgets::prependContent(sankey_plot_obj_decades,\n                                                                   htmltools::h3(sankey_title_dec, style = \"text-align:center;\"))\n          print(sankey_plot_obj_dec_title)\n        } else {\n          cat(\"     &gt; Warning: Decade Sankey plot object is NULL.\\n\")\n        }\n      } \n    } \n  } \n} </pre>\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2491\" data-attachment-id=\"2491\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-title=\"decadas\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=640\" data-lazy-sizes=\"(max-width: 853px) 100vw, 853px\" data-lazy-src=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=450&amp;ssl=1\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png\" data-orig-size=\"853,529\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/decadas/\" data-recalc-dims=\"1\" loading=\"lazy\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png 853w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=768 768w\"/><noscript><img alt=\"\" aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_len=\"\" class=\"wp-image-2491\" data-attachment-id=\"2491\" data-comments-opened=\"1\" data-image-caption=\"\" data-image-description=\"\" data-image-meta=\"{\" data-image-title=\"decadas\" data-large-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=640\" data-medium-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=300\" data-orig-file=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png\" data-orig-size=\"853,529\" data-permalink=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/decadas/\" data-recalc-dims=\"1\" loading=\"lazy\" sizes=\"(max-width: 853px) 100vw, 853px\" src=\"https://i0.wp.com/geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=450&amp;ssl=1\" srcset_temp=\"https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png 853w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=150 150w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=300 300w, https://geekcologist.wordpress.com/wp-content/uploads/2025/05/decadas.png?w=768 768w\"/></noscript></figure>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://geekcologist.wordpress.com/2025/05/06/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/\"> R Code – Geekcologist</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Mapping research landscapes and dynamics: Some basic bibliometric analyses with R\nPosted on\nMay 6, 2025\nby\nVinicius Bastazini\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR Code – Geekcologist\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nUnderstanding how scientific knowledge develops requires more than merely counting papers and citations. It requires a careful evaluation of how research topics and themes interconnect and transform over time. This is where\nbibliometric analysis\nbecomes essential. As the volume of scientific journals and papers continues to grow exponentially, bibliometric analyses become indispensable for mapping and synthesizing an increasingly complex information landscape.\nThrough the analysis of publication information and citation patterns, bibliometric analyses allow us not only to assess scholarly productivity and impact, but more importantly, to quantify the scientific communication processes and to analyze and create indicators that reveal the dynamics and evolution of scientific information within specific disciplines and research programs, organizations, research teams or geographical regions. These kinds of tools are especially valuable for gaining a clearer understanding of research dynamics, which is essential when conducting literature reviews or shaping research strategies.\nIn this post, I’ll share some R code I developed for a recent bibliometric analysis project I have been involved with. While it’s not as comprehensive or user-friendly as established R packages such as\nBibliometrix\n, which offers a rich suite of tools and a easy to use interface, this custom approach gave me the flexibility and control I needed for more tailored data handling and visualization. In this post, we’ll walk through a handful of simple bibliometric analysis and visualization techniques using R to reveal key patterns in research data, focusing on keywords and publication trends. Of course, this approach can also be extended to other important data fields, such as words in titles or abstracts. More specifically we will look at:\n1. Word Cloud\nWe’ll start by building a basic word cloud that highlights the most frequently used keywords across the bibliographic dataset. Here, word size will be proportional to its frequency, offering a fast, intuitive snapshot of the field’s dominant terminology.\n2. Keyword Co-Occurrence Network\nNext, we’ll construct a keyword co-occurrence network, a visual map that shows how often keywords appear together in academic papers. Each keyword is represented as a node in the network, and edges (or links) are drawn between them when they co-occur in the same study. The size of a node reflects how frequently a keyword appears, while the thickness of an edge indicates how strongly two keywords are associated. We’ll also apply a community detection algorithm, the\nLouvain method\n, to identify clusters within the network— that is, groups of keywords that frequently appear together across documents. These clusters represent thematic groupings or potential research subfields, revealing the underlying conceptual structure of the literature and highlighting how different topics are connected. This approach might help to reveal the structure of a research field, showing which themes are more central, which are less developed, and how different areas of research are interconnected.\n3. Thematic Map\nBased on the co-occurrence network, we’ll generate\na thematic map, using\nCallon’s centrality and density metrics\n. In this map, each cluster, named after the most common word in a cluster, from the co-occurrence network is represented as a bubble, and its size is determined by the frequency of words in the cluster. The X-axis represents the centrality of the cluster in the network, that is, the degree of interaction with other clusters in the graph, measuring the importance of a research topic. The Y-axis represents density, a metric of the internal strength of a cluster’s network and the growth of the topic. When mapping the themes in this plot, we can identify:\nMotor themes\n(top right corner): Themes in this quadrant have high centrality and density, indicating that the themes are well-developed and crucial for structuring the research field.\nNiche themes\n(top left corner): Themes that are highly specialized and well-developed in terms of internal research but have more limited interaction with other themes.\nPeripheral them\n(bottom left corner): Themes with low centrality and low density, suggesting that they are underdeveloped and marginal, representing themes that either emerging or in decline in the literature.\nBasic themes\n(bottom right corner): These themes have high centrality and low density. They are often essential for transdisciplinary research, meaning they may serve as foundational topics that cross the boundaries of multiple themes, but despite their central role in the network, these themes have low density of connections.\n4. Yearly Keyword Trends\nTo understand the temporal dynamics of research fields, we’ll build a yearly keyword trend diagram using a Sankey plot. This diagram maps the flow of the most frequent keywords (here, I will be using a cut off of the ten most common key words, but this could be done for as many keywords as necessary), revealing how interest in specific topics rises or fades over time.\n5. Decade-Based Keyword Evolution\nAt last, we’ll take a look at how the research field is changing through time, by aggregating keyword data by decade ( or whatever time frame one might want to look at). This decade-based evolution\ndiagram shows the progression of top keywords (once again, I will use a cut off of the ten most common key words per decade) from one decade to the next, capturing long-term shifts and the persistence or disappearance of major research themes.\nTo start off, we’ll simulate a simple bibliographic dataset to work with. This will consist of a data frame containing 500 publications, each tagged with a publication year ranging from 2000 to 2025, along with a set of keywords. For the purpose of this basic tutorial, I’ve created a list of keywords that might typically appear in an evolutionary ecology or eco-evolutionary research paper, so let’s pretend that we are conducting a review on something like “coevolutionary dynamics in ecological communities”. Of course, in your real-world application, you’ll be working with your own bibliographic data, which will likely include additional fields such as authors, titles, abstracts, journals, citation counts, etc. That’s perfectly fine—the analyses here will use columns in this simulated data frame named\n\"Year\"\nand\n\"Author_Keywords\"\n, but you can easily adapt the code by modifying the column header to match the structure of your dataset. And as I mention before, some of these analyses can be used for other bibliographic information, besides keywords. And it goes without saying: in your real-world application, you’ll be working with messy, inconsistent data, so you’ll likely need to do a lot of data cleaning/handling, such as combining similar keywords, handling typos and linguistic variations, and so on — so keep that in mind as you build your solution.\nSo, let’s start by loading the necessary packages, “creating” and organizing our dataset, and defining parameters for the analysis:\n#### 1. Load packages\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(igraph)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(RColorBrewer) \nlibrary(wordcloud)  \nlibrary(networkD3)\nlibrary(htmlwidgets)\n\n### 2.Parameters\ncat(\"--- 2. Setting Parameters ---\\n\")\n\n# Simulation Parameters\nn_studies = 500             # Number of simulated studies\nstart_year = 2000           # Start year for publications\nend_year = 2025             # End year for publications\nkeywords_per_study = 5      # Number of keywords per study\n\n# Keyword Pool\nkeyword_pool <- c(\n  \"coevolution\", \"arms race\", \"mutualism\", \"antagonism\", \"host-parasite\",\n  \"plant-pollinator\", \"Red Queen hypothesis\", \"selection pressure\", \"adaptation\",\n  \"phylogeny\", \"gene flow\", \"speciation\", \"ecological interaction\", \"community structure\",\"community assembly\",\"community disassembly\",\n  \"predator-prey\", \"ecophylogenetics\", \"co-speciation\", \"evolutionary dynamics\", \"co-phylogenetic analysis\",\n  \"adaptive dynamics\", \"local adaptation\", \"trait evolution\", \"phylogenetic signal\",\n  \"functional traits\", \"ecological networks\", \"niche differentiation\", \"coevolutionary networks\"\n)\n\n# Analysis Parameters\nkeyword_column_sim = \"Author_Keywords\" # Name for the keyword column in simulated data\nyear_column_sim = \"Year\"               # Name for the year column in simulated data\nmin_cooccurrence = 3                 # Min times two keywords must appear together\nmin_keyword_freq_network = 3           # Min total frequency for a keyword to be in the network plot\nmax_words_cloud = 75                   # Max words to display in the word cloud\n\n# Sankey Parameters\nnum_top_keywords_yearly = 10           # Number of top keywords for the YEARLY Sankey diagram\nnum_top_keywords_per_decade = 10       # Number of top keywords PER DECADE for the DECADE Sankey diagram\n\n### 3. Simulate Bibliographic Data\ncat(\"--- 3. Simulating Study Data ---\\n\")\n\nsimulated_studies <- tibble(\n  paper_id = 1:n_studies,\n  Year = sample(start_year:end_year, n_studies, replace = TRUE)\n)\n\n# Generate keywords for each study\nkeywords_list <- lapply(1:n_studies, function(i) {\n  sample(keyword_pool, keywords_per_study, replace = FALSE)\n})\n\n# Combine into a long format data frame (one row per keyword per paper)\nkeywords_long_sim <- simulated_studies %>%\n  mutate(keywords = keywords_list) %>%\n  unnest(keywords) %>%\n  rename(!!sym(keyword_column_sim) := keywords,\n         !!sym(year_column_sim) := Year) %>%\n  select(paper_id, !!sym(year_column_sim), !!sym(keyword_column_sim)) %>%\n  mutate(keyword = str_trim(tolower(!!sym(keyword_column_sim)))) %>%\n  select(paper_id, year = !!sym(year_column_sim), keyword) %>%\n  distinct(paper_id, year, keyword) # Ensure unique keyword per paper/year instance\n\ncat(\"Generated\", n_studies, \"studies (\", start_year, \"-\", end_year, \") with\",\n    nrow(keywords_long_sim), \"unique keyword instances per paper/year.\\n\")\ncat(\"Example simulated data:\\n\")\nprint(head(keywords_long_sim))\n\n# Use this simulated data for the rest of the analysis\nkeywords_long <- keywords_long_sim\n\n#### 4. Calculate Overall Keyword Frequencies\ncat(\"\\n--- 4. Calculating Overall Keyword Frequencies ---\\n\")\nkeyword_total_freq <- keywords_long %>%\n  # Count unique keywords per paper first, then sum across papers\n  distinct(paper_id, keyword) %>%\n  count(keyword, name = \"total_freq\", sort = TRUE)\n\ncat(\"Top overall keywords (based on number of papers):\\n\")\nprint(head(keyword_total_freq))\nNow that we have our “dataset”, let’s create a visual representation of the word data, building a simple word cloud:\n### 5. Word Cloud \ncat(\"\\n--- 5. Generating Word Cloud ---\\n\")\n\nif (exists(\"keyword_total_freq\") && nrow(keyword_total_freq) > 0) {\n  cat(\"   - Creating Word Cloud (Check RStudio Plots Pane)...\\n\")\n  tryCatch({\n    \n    wordcloud(words = keyword_total_freq$keyword,\n              freq = keyword_total_freq$total_freq,\n              min.freq = 2, # Show words appearing in at least in 2 papers\n              max.words = max_words_cloud,\n              random.order = FALSE, # Plot most frequent words first\n              rot.per = 0.30,      # Percentage of words rotated\n              colors = brewer.pal(8, \"Dark2\")) # Color palette\n    title(main = \"\", line = -1) # If you want, add a title near the top\n  }, error = function(e) {\n    cat(\"     > Error generating word cloud:\", conditionMessage(e), \"\\n\")\n  })\n  \n} else {\n  cat(\"   - Skipping word cloud (no keyword frequency data available).\\n\")\n}\nNow, let’s create the co-ocurrence network, and identify clusters:\n### 6. Keyword Co-occurrence Network Analysis\ncat(\"\\n--- 6. Building Keyword Co-occurrence Network ---\\n\")\n# (Uses keywords_long which contains year info, but pairs are per paper regardless of year)\n\n# 6a. Generate keyword pairs within each paper\ncat(\"   - Generating keyword pairs...\\n\")\nkeyword_pairs_unnested <- keywords_long %>%\n  group_by(paper_id) %>%\n  filter(n() >= 2) %>%\n  summarise(pairs = list(combn(keyword, 2, simplify = FALSE)), .groups = 'drop') %>%\n  unnest(pairs) %>%\n  mutate(\n    keyword1 = sapply(pairs, `[`, 1),\n    keyword2 = sapply(pairs, `[`, 2)\n  ) %>%\n  select(keyword1, keyword2)\n\n# 6b. Standardize & Count Pairs\ncat(\"   - Counting co-occurrences (min =\", min_cooccurrence, \")...\\n\")\nkeyword_pair_counts <- keyword_pairs_unnested %>%\n  mutate(\n    temp_kw1 = pmin(keyword1, keyword2),\n    temp_kw2 = pmax(keyword1, keyword2)\n  ) %>%\n  select(keyword1 = temp_kw1, keyword2 = temp_kw2) %>%\n  count(keyword1, keyword2, name = \"weight\") %>%\n  filter(weight >= min_cooccurrence)\n\n# 6c. Create and Filter Graph\ngraph_plot_obj <- NULL\ncommunities <- NULL\n\nif(nrow(keyword_pair_counts) > 0) {\n  cat(\"   - Creating graph object...\\n\")\n  graph_obj <- graph_from_data_frame(keyword_pair_counts, directed = FALSE)\n  \n  cat(\"   - Filtering graph (min degree =\", min_keyword_freq_network, \") & detecting communities...\\n\")\n  node_degrees <- degree(graph_obj, mode = \"all\")\n  nodes_to_keep <- names(node_degrees[node_degrees >= min_keyword_freq_network])\n  \n  if(length(nodes_to_keep) > 0){\n    graph_filtered <- induced_subgraph(graph_obj, V(graph_obj)$name %in% nodes_to_keep)\n    graph_filtered <- delete.vertices(graph_filtered, degree(graph_filtered) == 0)\n    \n    if (vcount(graph_filtered) > 0 && ecount(graph_filtered) > 0) {\n      communities <- cluster_louvain(graph_filtered)\n      num_communities <- length(unique(membership(communities)))\n      cat(\"     > Detected\", num_communities, \"communities (Louvain).\\n\")\n      \n      node_data <- tibble(name = V(graph_filtered)$name) %>%\n        left_join(keyword_total_freq, by = c(\"name\" = \"keyword\")) %>%\n        mutate(total_freq = ifelse(is.na(total_freq), 1, total_freq))\n      \n      V(graph_filtered)$size <- log1p(node_data$total_freq) * 2.5\n      V(graph_filtered)$label <- V(graph_filtered)$name\n      V(graph_filtered)$community <- membership(communities)\n      V(graph_filtered)$total_freq <- node_data$total_freq\n      \n      # Assign colors based on community\n      if (num_communities > 0) {\n        num_colors_needed = length(unique(V(graph_filtered)$community))\n        if (num_colors_needed > 8) {\n          community_colors <- colorRampPalette(brewer.pal(8, \"Set2\"))(num_colors_needed)\n        } else if (num_colors_needed > 2) {\n          community_colors <- brewer.pal(max(3, num_colors_needed), \"Set2\")[1:num_colors_needed]\n        } else if (num_colors_needed == 2) {\n          community_colors <- brewer.pal(3, \"Set2\")[1:2]\n        } else { # num_colors_needed == 1\n          community_colors <- brewer.pal(3, \"Set2\")[1]\n        }\n        community_map <- setNames(community_colors, sort(unique(V(graph_filtered)$community)))\n        V(graph_filtered)$color <- community_map[as.character(V(graph_filtered)$community)]\n      } else {\n        V(graph_filtered)$color <- \"grey\"\n        community_map <- NULL\n      }\n      \n      graph_plot_obj <- graph_filtered\n      cat(\"     > Filtered graph ready:\", vcount(graph_plot_obj), \"nodes,\", ecount(graph_plot_obj), \"edges.\\n\")\n      \n    } else {\n      cat(\"     > Warning: Graph empty after filtering.\\n\")\n      graph_plot_obj <- NULL\n      communities <- NULL\n    }\n  } else {\n    cat(\"     > Warning: No nodes met minimum degree requirement.\\n\")\n    graph_plot_obj <- NULL\n    communities <- NULL\n  }\n} else {\n  cat(\"   - Warning: No keyword pairs met the minimum co-occurrence threshold.\\n\")\n  graph_plot_obj <- NULL\n  communities <- NULL\n}\n\n# 6d. Visualize Network\nif (!is.null(graph_plot_obj)) {\n  cat(\"   - Plotting Co-occurrence Network (Check RStudio Plots Pane)...\\n\")\n  tryCatch({\n    par(mar=c(1, 1, 3, 1))\n    plot(graph_plot_obj,\n         layout = layout_nicely(graph_plot_obj),\n         vertex.frame.color = \"grey40\", vertex.label.color = \"black\",\n         vertex.label.cex = 0.7, vertex.label.dist = 0.4,\n         edge.color = rgb(0.5, 0.5, 0.5, alpha = 0.4), edge.curved = 0.1,\n         edge.width = scales::rescale(E(graph_plot_obj)$weight, to = c(0.3, 3.0)),\n         main = \"Keyword Co-occurrence Network (Simulated Data)\",\n         sub = paste(\"Nodes sized by log(# Papers), Min Degree >=\", min_keyword_freq_network)\n    )\n    if (!is.null(community_map) && length(community_map) <= 12 && length(community_map) > 1) {\n      legend(\"bottomleft\", legend = paste(\"Cluster\", names(community_map)),\n             fill = community_map, bty = \"n\", cex = 0.7, title=\"Communities\")\n    }\n    par(mar=c(5.1, 4.1, 4.1, 2.1)) # Reset margins\n  }, error = function(e){\n    cat(\"     > Error plotting network:\", conditionMessage(e), \"\\n\")\n    par(mar=c(5.1, 4.1, 4.1, 2.1)) # Reset margins on error\n  })\n} else {\n  cat(\"   - Skipping network plot (no valid graph).\\n\")\n}\nIn this example, the clustering algorithm identified three distinct clusters—groups of words that frequently co-occur across the papers. Based on these clusters, we will create a thematic map, where each cluster is represented as a bubble, visually illustrating the relationships and centrality of research themes within the broader network of keywords. This map will help us to better understand the underlying structure of the field and how different research topics are interconnected.\n### 7. Thematic Map Analysis\ncat(\"\\n--- 7. Generating Thematic Map (Callon's Metrics) ---\\n\")\n\n# Helper function\ncalculate_callon_metrics <- function(graph, communities_object, cluster_id) {\n  if (is.null(graph) || !is.igraph(graph) || is.null(communities_object)) {\n    return(list(centrality = 0, density = 0, n_keywords = 0))\n  }\n  cluster_nodes_indices <- which(membership(communities_object) == cluster_id)\n  if (length(cluster_nodes_indices) == 0) {\n    return(list(centrality = 0, density = 0, n_keywords = 0))\n  }\n  n_nodes_in_cluster <- length(cluster_nodes_indices)\n  subgraph <- induced_subgraph(graph, cluster_nodes_indices)\n  internal_weight_sum <- if (ecount(subgraph) > 0) sum(E(subgraph)$weight, na.rm = TRUE) else 0\n  density <- internal_weight_sum\n  external_weight_sum <- 0\n  all_incident_edges_indices <- E(graph)[.inc(cluster_nodes_indices)]\n  if (length(all_incident_edges_indices) > 0) {\n    all_incident_edges <- E(graph)[all_incident_edges_indices]\n    ends_matrix <- ends(graph, all_incident_edges, names = FALSE)\n    mem <- membership(communities_object)\n    is_external <- (mem[ends_matrix[,1]] != cluster_id) | (mem[ends_matrix[,2]] != cluster_id)\n    external_edges <- all_incident_edges[is_external]\n    if (length(external_edges) > 0) {\n      external_weight_sum <- sum(E(graph)$weight[external_edges], na.rm = TRUE)\n    }\n  }\n  centrality <- external_weight_sum\n  return(list(centrality = centrality, density = density, n_keywords = n_nodes_in_cluster))\n}\n\nthematic_plot_obj <- NULL\n\nif (!is.null(graph_plot_obj) && !is.null(communities) && length(unique(membership(communities))) > 0) {\n  cat(\"   - Calculating Centrality and Density for communities...\\n\")\n  \n  community_ids <- unique(membership(communities))\n  thematic_metrics <- lapply(community_ids, function(comm_id) {\n    metrics <- calculate_callon_metrics(graph_plot_obj, communities, comm_id)\n    nodes_in_comm_indices <- which(membership(communities) == comm_id)\n    community_node_names <- V(graph_plot_obj)$name[nodes_in_comm_indices]\n    community_node_freqs <- V(graph_plot_obj)$total_freq[nodes_in_comm_indices]\n    \n    if(length(community_node_names) > 0 && length(community_node_freqs) > 0 && !all(is.na(community_node_freqs))){\n      most_frequent_keyword <- community_node_names[which.max(community_node_freqs)]\n      community_label <- str_trunc(most_frequent_keyword, 30)\n    } else {\n      community_label <- paste(\"Cluster\", comm_id)\n    }\n    \n    return(tibble(\n      community_id = comm_id, label = community_label,\n      Centrality = metrics$centrality, Density = metrics$density,\n      n_keywords = metrics$n_keywords\n    ))\n  })\n  \n  thematic_data <- bind_rows(thematic_metrics) %>%\n    mutate(Centrality = as.numeric(Centrality), Density = as.numeric(Density)) %>%\n    filter(!is.na(community_id), n_keywords > 0, is.finite(Centrality), is.finite(Density))\n  \n  if(nrow(thematic_data) > 0) {\n    cat(\"   - Creating Thematic Map plot object...\\n\")\n    median_centrality <- median(thematic_data$Centrality, na.rm = TRUE)\n    median_density <- median(thematic_data$Density, na.rm = TRUE)\n    median_centrality <- ifelse(is.finite(median_centrality), median_centrality, 0)\n    median_density <- ifelse(is.finite(median_density), median_density, 0)\n    cat(\"     > Quadrant thresholds (Medians): Centrality=\", round(median_centrality,2), \", Density=\", round(median_density,2), \"\\n\")\n    \n    thematic_plot_obj <- ggplot(thematic_data, aes(x = Centrality, y = Density)) +\n      geom_hline(yintercept = median_density, linetype = \"dashed\", color = \"grey50\") +\n      geom_vline(xintercept = median_centrality, linetype = \"dashed\", color = \"grey50\") +\n      geom_point(aes(size = n_keywords), alpha = 0.7, color = \"steelblue\") +\n      geom_text_repel(aes(label = label), size = 3.0, max.overlaps = 15,\n                      box.padding = 0.4, point.padding = 0.6) +\n      scale_size_continuous(range = c(4, 12), name = \"# Keywords\\nin Theme\") +\n      ggplot2::annotate(\"text\", x = median_centrality, y = Inf, label = \"Motor Themes\", hjust = 0.5, vjust = 1.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      ggplot2::annotate(\"text\", x = -Inf, y = Inf, label = \"Niche Themes\", hjust = -0.1, vjust = 1.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      ggplot2::annotate(\"text\", x = -Inf, y = -Inf, label = \"Emerging/\\nDeclining\", hjust = -0.1, vjust = -0.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      ggplot2::annotate(\"text\", x = median_centrality, y = -Inf, label = \"Basic Themes\", hjust = 0.5, vjust = -0.5, size = 3.5, color = \"grey40\", fontface=\"bold\") +\n      labs(\n        title = \"Thematic Map (Callon's Centrality & Density)\",\n        subtitle = \"Keyword Clusters from Co-occurrence Network (Simulated Data)\",\n        x = \"Centrality (Links to other themes)\",\n        y = \"Density (Internal theme links)\"\n      ) +\n      theme_minimal(base_size = 12) +\n      theme(\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5),\n        plot.margin = margin(20, 20, 20, 20)\n      )\n    \n    # Visualize Thematic Map (Print to RStudio Plots Pane)\n    cat(\"   - Plotting Thematic Map (Check RStudio Plots Pane)...\\n\")\n    print(thematic_plot_obj)\n    \n  } else {\n    cat(\"   - Warning: No valid thematic data to plot.\\n\")\n  }\n} else {\n  cat(\"   - Skipping Thematic Map (network or communities missing).\\n\")\n}\nIn this simulated example, “ecological networks” was positioned at the center of the plot, indicating its central role within the research landscape, while “ecophylogenetics” was classified as a motor theme, reflecting its importance and well-developed nature in the field. On the other hand, “evolutionary dynamics” appeared as a peripheral theme, suggesting that it is underdeveloped or marginal in the current body of research.\nNow, let’s see the evolution of the research field, by first building a Sankey diagram, with the association between years, and the “most”used keywords in our simulated example:\n### 8. Yearly Keyword Trend Analysis\ncat(\"\\n--- 8. Generating Yearly Keyword Trend Sankey Diagram ---\\n\")\n\nsankey_plot_obj_yearly <- NULL\n\n# 8a. Count Keywords per Year\ncat(\"   - Counting keyword frequency per year (using unique paper/keyword counts)...\\n\")\nkeyword_yearly_counts <- keywords_long %>%\n  distinct(paper_id, year, keyword) %>% # Count keyword once per paper per year\n  count(year, keyword, name = \"yearly_count\") %>%\n  filter(yearly_count > 0)\n\n# 8b. Identify Top Keywords Overall (Using paper frequency calculated earlier)\ncat(\"   - Identifying top\", num_top_keywords_yearly, \"keywords overall for yearly Sankey...\\n\")\nif(exists(\"keyword_total_freq\") && inherits(keyword_total_freq, \"data.frame\") && nrow(keyword_total_freq) > 0) {\n  top_keywords_df_yearly <- keyword_total_freq\n} else {\n  cat(\"     > Warning: 'keyword_total_freq' not found. Recalculating based on yearly counts (less accurate representation of 'overall').\\n\")\n  top_keywords_df_yearly <- keyword_yearly_counts %>% group_by(keyword) %>% summarise(total_freq = sum(yearly_count)) %>% arrange(desc(total_freq))\n}\n\ntop_keywords_yearly <- top_keywords_df_yearly %>%\n  slice_head(n = num_top_keywords_yearly) %>%\n  pull(keyword)\n\nif(length(top_keywords_yearly) > 0){\n  cat(\"     > Top keywords for Yearly Sankey:\", paste(top_keywords_yearly, collapse = \", \"), \"\\n\")\n  \n  # 8c. Prepare Data for Yearly Sankey\n  sankey_data_yearly <- keyword_yearly_counts %>%\n    filter(keyword %in% top_keywords_yearly)\n  \n  if(nrow(sankey_data_yearly) == 0) {\n    cat(\"   - Warning: No yearly counts found for the top keywords. Skipping Yearly Sankey.\\n\")\n  } else {\n    cat(\"   - Preparing data for Yearly Sankey diagram...\\n\")\n    year_nodes_chr_yr <- as.character(sort(unique(sankey_data_yearly$year)))\n    keyword_nodes_sankey_yr <- unique(sankey_data_yearly$keyword)\n    # Prefix years to distinguish from keywords if necessary\n    all_node_names_yr <- c(paste0(\"Y:\", year_nodes_chr_yr), keyword_nodes_sankey_yr)\n    \n    nodes_df_yr <- data.frame(name = all_node_names_yr, stringsAsFactors = FALSE) %>%\n      mutate(id = row_number() - 1)\n    \n    links_df_yr <- sankey_data_yearly %>%\n      mutate(\n        source_name = paste0(\"Y:\", as.character(year)),\n        target_name = keyword\n      ) %>%\n      left_join(nodes_df_yr %>% select(name, source_id = id), by = c(\"source_name\" = \"name\")) %>%\n      left_join(nodes_df_yr %>% select(name, target_id = id), by = c(\"target_name\" = \"name\")) %>%\n      filter(!is.na(source_id), !is.na(target_id)) %>%\n      transmute(\n        source = source_id, target = target_id,\n        value = yearly_count, group = target_name # Color links by target keyword\n      ) %>%\n      filter(value > 0)\n    \n    if(nrow(links_df_yr) == 0) {\n      cat(\"   - Warning: Failed to create valid links for Yearly Sankey diagram. Skipping.\\n\")\n    } else {\n      # 8d. Generate Yearly Sankey Diagram Object\n      cat(\"   - Creating Yearly Sankey plot object...\\n\")\n      sankey_plot_obj_yearly <- sankeyNetwork(\n        Links = links_df_yr, Nodes = nodes_df_yr, Source = \"source\",\n        Target = \"target\", Value = \"value\", NodeID = \"name\",\n        NodeGroup = NULL, LinkGroup = \"group\", units = \"Papers\",\n        fontSize = 11, nodeWidth = 30, nodePadding = 15, sinksRight = TRUE,\n        margin = list(top=5, bottom=5, left=5, right=5)\n      )\n      \n      # 8e. Visualize Yearly Sankey (Print to RStudio Viewer Pane)\n      if (!is.null(sankey_plot_obj_yearly)) {\n        cat(\"   - Plotting Yearly Sankey Diagram (Check RStudio Viewer Pane)...\\n\")\n        sankey_title_yr <- paste0(\"Flow of Top \", num_top_keywords_yearly, \" Keywords Over Time (Yearly, Simulated)\")\n        sankey_plot_obj_yr_title <- htmlwidgets::prependContent(sankey_plot_obj_yearly,\n                                                                htmltools::h3(sankey_title_yr, style = \"text-align:center;\"))\n        print(sankey_plot_obj_yr_title)\n      } else {\n        cat (\"   - Warning: Yearly Sankey plot object could not be created.\\n\")\n      }\n    }\n  }\n} else {\n  cat(\"   - Warning: No top keywords identified for yearly Sankey. Skipping.\\n\")\n}\nAt last, let’s see how the importance of keywords has shifted over the past decades by visualizing the changing importance of the most frequently used keywords in our simulated example. To do this, we’ll build another Sankey diagram that illustrates the flow and evolution, and the importance of these keywords across time. Please have in mind , that although I will set the number of top keywords per decade to 10, the Sankey diagram may display more than 10 keywords overall because it includes the top 10 from each decade,  and if different keywords are dominant in different decades, this can lead to a larger combined set of unique keywords across the entire timeline.\n### 9. Decade-Based Keyword Evolution\ncat(\"\\n--- 9. Generating Decade-Based Keyword Evolution Sankey ---\\n\")\n\nsankey_plot_obj_decades <- NULL\n\n# 9a. Aggregate by Decade and Calculate Frequencies\ncat(\"   - Aggregating by Decade and Calculating Frequencies (using unique paper/keyword counts per decade)...\\n\")\nkeywords_decades <- keywords_long %>%\n  filter(!is.na(year)) %>%\n  mutate(decade = floor(year / 10) * 10) %>% # Calculate decade\n  select(paper_id, decade, keyword) %>%\n  distinct() # Count each keyword only once per paper within a decade\n\nkeyword_decade_counts <- keywords_decades %>%\n  count(decade, keyword, name = \"count\") %>%\n  arrange(decade, desc(count))\n\nif(nrow(keyword_decade_counts) == 0){\n  cat(\"   - Warning: No keyword counts per decade found. Skipping Decade Sankey.\\n\")\n} else {\n  cat(\"   - Decade counts calculated.\\n\")\n  \n  # 9b. Identify Top Keywords for Each Decade\n  cat(\"   - Identifying top\", num_top_keywords_per_decade, \"keywords per decade...\\n\")\n  top_keywords_per_decade <- keyword_decade_counts %>%\n    group_by(decade) %>%\n    slice_max(order_by = count, n = num_top_keywords_per_decade, with_ties = FALSE) %>%\n    ungroup()\n  \n  keywords_to_track <- unique(top_keywords_per_decade$keyword)\n  \n  if(length(keywords_to_track) == 0) {\n    cat(\"   - Warning: No top keywords identified across decades to track. Skipping Decade Sankey.\\n\")\n  } else {\n    cat(\"     > Total unique keywords to track (top\", num_top_keywords_per_decade, \"in any decade):\", length(keywords_to_track), \"\\n\")\n    \n    # Filter the counts to only include these keywords\n    sankey_base_data_dec <- keyword_decade_counts %>%\n      filter(keyword %in% keywords_to_track)\n    \n    if(nrow(sankey_base_data_dec) == 0){\n      cat(\"   - Warning: No counts found for selected keywords to track. Skipping Decade Sankey.\\n\")\n    } else {\n      \n      # 9c. Prepare Nodes and Links for Decade Sankey\n      cat(\"   - Preparing nodes and links for Decade Sankey...\\n\")\n      nodes_df_dec <- sankey_base_data_dec %>%\n        mutate(name = paste0(decade, \"s: \", keyword)) %>% # Node label: \"2000s: coevolution\"\n        select(name) %>%\n        distinct() %>%\n        mutate(id = row_number() - 1)\n      \n      decade_list <- sort(unique(sankey_base_data_dec$decade))\n      links_list_dec <- list()\n      \n      if (length(decade_list) > 1) {\n        for (i in 1:(length(decade_list) - 1)) {\n          current_decade <- decade_list[i]\n          next_decade <- decade_list[i+1]\n          \n          current_decade_data <- sankey_base_data_dec %>% filter(decade == current_decade)\n          next_decade_data <- sankey_base_data_dec %>% filter(decade == next_decade)\n          common_keywords <- intersect(current_decade_data$keyword, next_decade_data$keyword)\n          \n          if (length(common_keywords) > 0) {\n            temp_links_dec <- tibble(keyword = common_keywords) %>%\n              mutate(source_name = paste0(current_decade, \"s: \", keyword)) %>%\n              left_join(nodes_df_dec %>% select(name, source_id = id), by = c(\"source_name\" = \"name\")) %>%\n              mutate(target_name = paste0(next_decade, \"s: \", keyword)) %>%\n              left_join(nodes_df_dec %>% select(name, target_id = id), by = c(\"target_name\" = \"name\")) %>%\n              # Link value is the count in the *target* decade (flow into that decade)\n              left_join(next_decade_data %>% select(keyword, value = count), by = \"keyword\") %>%\n              filter(!is.na(source_id), !is.na(target_id), !is.na(value), value > 0) %>%\n              select(source = source_id, target = target_id, value = value, group = keyword)\n            \n            if(nrow(temp_links_dec) > 0){\n              links_list_dec[[as.character(current_decade)]] <- temp_links_dec\n            }\n          }\n        } # End for loop\n      } # End if length(decade_list) > 1\n      \n      if (length(links_list_dec) > 0) {\n        links_df_dec <- bind_rows(links_list_dec)\n      } else {\n        links_df_dec <- tibble(source = integer(), target = integer(), value = numeric(), group = character()) # Empty tibble\n      }\n      \n      if (nrow(nodes_df_dec) == 0 || nrow(links_df_dec) == 0) {\n        cat(\"   - Warning: Could not create valid nodes or links for Decade Sankey. Skipping.\\n\")\n      } else {\n        cat(\"     > Decade Nodes:\", nrow(nodes_df_dec), \"; Decade Links:\", nrow(links_df_dec), \"created.\\n\")\n        \n        # 9d. Generate Decade Sankey Diagram\n        cat(\"   - Creating Decade Sankey plot object...\\n\")\n        num_groups_dec <- length(unique(links_df_dec$group))\n        if (num_groups_dec <= 12 && num_groups_dec > 0) {\n          color_palette_dec <- RColorBrewer::brewer.pal(max(3, num_groups_dec), \"Paired\")[1:num_groups_dec]\n          color_scale_js_dec <- paste0('d3.scaleOrdinal([\"', paste(color_palette_dec, collapse = '\",\"'), '\"]);')\n        } else if (num_groups_dec > 12) {\n          color_scale_js_dec <- 'd3.scaleOrdinal(d3.schemeCategory10);'\n          cat(\"     > Warning: >12 keyword groups for Decade Sankey, colors may repeat.\\n\")\n        } else {\n          color_scale_js_dec <- 'd3.scaleOrdinal([\"#cccccc\"]);' # Default grey\n        }\n        \n        \n        sankey_plot_obj_decades <- sankeyNetwork(\n          Links = links_df_dec, Nodes = nodes_df_dec, Source = \"source\",\n          Target = \"target\", Value = \"value\", NodeID = \"name\",\n          LinkGroup = \"group\", NodeGroup = NULL, units = \"Papers\",\n          fontSize = 10, nodeWidth = 35, nodePadding = 10, # Adjusted node width/padding\n          sinksRight = FALSE, # Keep temporal flow L->R\n          colourScale = JS(color_scale_js_dec),\n          margin = list(top=5, bottom=5, left=5, right=5)\n        )\n        \n        # 9e. Visualize Decade Sankey (Print to RStudio Viewer Pane)\n        if(!is.null(sankey_plot_obj_decades)){\n          cat(\"   - Plotting Decade Sankey Diagram (Check RStudio Viewer Pane)...\\n\")\n          sankey_title_dec <- paste0(\"Evolution of Top \", num_top_keywords_per_decade, \" Keywords by Decade (Simulated)\")\n          sankey_plot_obj_dec_title <- htmlwidgets::prependContent(sankey_plot_obj_decades,\n                                                                   htmltools::h3(sankey_title_dec, style = \"text-align:center;\"))\n          print(sankey_plot_obj_dec_title)\n        } else {\n          cat(\"     > Warning: Decade Sankey plot object is NULL.\\n\")\n        }\n      } \n    } \n  } \n}\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR Code – Geekcologist\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "Understanding how scientific knowledge develops requires more than merely counting papers and citations. It requires a careful evaluation of how research topics and themes interconnect and transform over time. This is where bibliometric analysis becomes essential. As the volume of scientific journals and papers continues to grow exponentially, bibliometric analyses become indispensable for mapping and … Continue reading Mapping research landscapes and dynamics: Some basic bibliometric analyses with R",
    "meta_keywords": null,
    "og_description": "Understanding how scientific knowledge develops requires more than merely counting papers and citations. It requires a careful evaluation of how research topics and themes interconnect and transform over time. This is where bibliometric analysis becomes essential. As the volume of scientific journals and papers continues to grow exponentially, bibliometric analyses become indispensable for mapping and … Continue reading Mapping research landscapes and dynamics: Some basic bibliometric analyses with R",
    "og_image": "https://geekcologist.wordpress.com/wp-content/uploads/2025/05/word-cloud.png?w=640",
    "og_title": "Mapping research landscapes and dynamics: Some basic bibliometric analyses with R | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 20.8,
    "sitemap_lastmod": null,
    "twitter_description": "Understanding how scientific knowledge develops requires more than merely counting papers and citations. It requires a careful evaluation of how research topics and themes interconnect and transform over time. This is where bibliometric analysis becomes essential. As the volume of scientific journals and papers continues to grow exponentially, bibliometric analyses become indispensable for mapping and … Continue reading Mapping research landscapes and dynamics: Some basic bibliometric analyses with R",
    "twitter_title": "Mapping research landscapes and dynamics: Some basic bibliometric analyses with R | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/05/mapping-research-landscapes-and-dynamics-some-basic-bibliometric-analyses-with-r/",
    "word_count": 4158
  }
}