{
  "id": "77cd0d4a485ade1203101a112128ecac882eca6c",
  "url": "https://www.r-bloggers.com/2025/08/handling-missing-data-in-r-a-comprehensive-guide/",
  "created_at_utc": "2025-11-22T19:57:52Z",
  "data": null,
  "raw_original": {
    "uuid": "828e21c7-74cb-430e-8115-fa1cc7fce065",
    "created_at": "2025-11-22 19:57:52",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/08/handling-missing-data-in-r-a-comprehensive-guide/",
      "crawled_at": "2025-11-22T10:44:17.053541",
      "external_links": [
        {
          "href": "https://mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/",
          "text": "A Statistician's R Notebook"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://cran.r-project.org/package=mice",
          "text": "micepackage"
        },
        {
          "href": "https://cran.r-project.org/package=missForest",
          "text": "missForestpackage"
        },
        {
          "href": "https://mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/",
          "text": "A Statistician's R Notebook"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Handling Missing Data in R: A Comprehensive Guide | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-7-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-8-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-9-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-12-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-12-2.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/m-fatih-tuzen/",
          "text": "M. Fatih Tüzen"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-394841 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Handling Missing Data in R: A Comprehensive Guide</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">August 17, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/m-fatih-tuzen/\">M. Fatih Tüzen</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/\"> A Statistician's R Notebook</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<section class=\"level2\" data-number=\"1\" id=\"introduction\">\n<h2 class=\"anchored\" data-anchor-id=\"introduction\" data-number=\"1\"><span class=\"header-section-number\">1</span> Introduction</h2>\n<p>Missing data is one of the most common challenges in data analysis and statistical modeling.<br/>\nWhether the data originates from surveys, administrative registers, or clinical trials, it is almost inevitable that some values are absent. Ignoring this issue or handling it incorrectly can seriously bias results, reduce statistical power, and ultimately lead to misleading conclusions.</p>\n<p>In statistical notation, missing values are usually represented as <strong>NA</strong> (Not Available) in R.<br/>\nHowever, not all missing values are created equal: some are missing completely at random, while others may follow systematic patterns. Before deciding how to handle missingness, one must carefully investigate its causes and implications.</p>\n<p>In this article, we will cover:</p>\n<ul>\n<li>The theoretical foundations of missing data mechanisms (MCAR, MAR, MNAR).</li>\n<li>How to detect and visualize missing values in R.</li>\n<li>Different strategies for handling missingness, from simple imputation to advanced multiple imputation techniques.</li>\n<li>A practical workflow using the <strong>NHANES dataset</strong>, widely used in health research, to demonstrate methods in R.</li>\n<li>Best practices, pitfalls, and recommendations for applied data science.</li>\n</ul>\n<p>We will use several R packages throughout this tutorial:</p>\n<ul>\n<li><strong>tidyverse</strong>: Data wrangling and visualization</li>\n<li><strong>naniar</strong> and <strong>VIM</strong>: Tools for exploring and visualizing missing data</li>\n<li><strong>mice</strong>: Multiple imputation by chained equations</li>\n<li><strong>missForest</strong>: Random forest–based imputation for nonlinear data</li>\n</ul>\n<p>This structured approach will help you not only handle missing data but also understand the reasoning behind each method, ensuring that your analyses remain robust and reliable.</p>\n</section>\n<section class=\"level2\" data-number=\"2\" id=\"nhanes-dataset\">\n<h2 class=\"anchored\" data-anchor-id=\"nhanes-dataset\" data-number=\"2\"><span class=\"header-section-number\">2</span> NHANES Dataset</h2>\n<p>In this section, we will work with the <strong>NHANES</strong> dataset, which comes from the US National Health and Nutrition Examination Survey.<br/>\nThe dataset includes demographic, examination, and laboratory data collected from thousands of individuals.<br/>\nSince the full dataset is quite large, we will focus only on a subset of variables that are relevant for preprocessing examples.</p>\n<p>Here are the variables we will use:</p>\n<ul>\n<li><strong>ID</strong>: Unique identifier for each participant</li>\n<li><strong>Age</strong>: Age of the participant</li>\n<li><strong>Gender</strong>: Biological sex (male or female)</li>\n<li><strong>BMI</strong>: Body Mass Index</li>\n<li><strong>BPSysAve</strong>: Average systolic blood pressure</li>\n<li><strong>Diabetes</strong>: Whether the participant has been diagnosed with diabetes</li>\n</ul>\n<p>Before diving into preprocessing, let’s take a quick look at the structure of these selected variables:</p>\n<div class=\"cell\">\n<pre>library(NHANES)\nlibrary(dplyr)\n\ndata(\"NHANES\")\n\n# Select relevant variables\nnhanes_sub &lt;- NHANES |&gt; \n  select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\nglimpse(nhanes_sub)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Rows: 10,000\nColumns: 6\n$ ID       &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 51647, 51647…\n$ Age      &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, …\n$ Gender   &lt;fct&gt; male, male, male, male, female, male, male, female, female, f…\n$ BMI      &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 27.24…\n$ BPSysAve &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, 104, 134…\n$ Diabetes &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…</pre>\n</div>\n</div>\n</section>\n<section class=\"level2\" data-number=\"3\" id=\"why-missingness-matters\">\n<h2 class=\"anchored\" data-anchor-id=\"why-missingness-matters\" data-number=\"3\"><span class=\"header-section-number\">3</span> Why Missingness Matters</h2>\n<p>Missing data is not just an inconvenience — it can distort the statistical conclusions we draw from a dataset.<br/>\nThere are several critical reasons why handling missingness properly is essential:</p>\n<ul>\n<li><strong>Biased results</strong>: If the missing values are not random, analyses may systematically misrepresent the population.</li>\n<li><strong>Reduced sample size</strong>: Complete-case analysis (simply dropping missing rows) reduces data availability, weakening statistical power.</li>\n<li><strong>Model incompatibility</strong>: Many modeling techniques in R (e.g., <code>lm()</code>, <code>glm()</code>) require complete data, and will automatically drop cases with missing values, sometimes silently.</li>\n</ul>\n<section class=\"level3\" data-number=\"3.1\" id=\"a-short-case-example-bmi-missingness-and-blood-pressure\">\n<h3 class=\"anchored\" data-anchor-id=\"a-short-case-example-bmi-missingness-and-blood-pressure\" data-number=\"3.1\"><span class=\"header-section-number\">3.1</span> A Short Case Example: BMI Missingness and Blood Pressure</h3>\n<p>Suppose we want to explore how <strong>Body Mass Index (BMI)</strong> relates to <strong>Systolic Blood Pressure (BPSysAve)</strong>.<br/>\nHowever, BMI contains missing values. If we ignore them and only analyze complete cases, we may end up with biased conclusions.</p>\n<div class=\"cell\">\n<pre># How many missing in BMI?\nsum(is.na(nhanes_sub$BMI))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 366</pre>\n</div>\n<pre># Complete-case dataset (dropping missing BMI)\nnhanes_complete &lt;- nhanes_sub |&gt; \n  filter(!is.na(BMI))\n\n# Compare sample sizes\nnrow(nhanes_sub)     # original sample size</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 10000</pre>\n</div>\n<pre>nrow(nhanes_complete) # after dropping missing BMI</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 9634</pre>\n</div>\n</div>\n<p>We see that a substantial portion of the data is dropped when we remove missing BMI values. This reduction not only decreases efficiency but can also <strong>bias the estimates</strong> if those missing values are not randomly distributed.</p>\n<div class=\"cell\">\n<pre># Fit regression with complete cases only\nmodel_complete &lt;- lm(BPSysAve ~ BMI + Age + Gender, data = nhanes_complete)\nsummary(model_complete)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\nCall:\nlm(formula = BPSysAve ~ BMI + Age + Gender, data = nhanes_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.281  -8.652  -0.955   7.560 102.790 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 90.016503   0.677618  132.84   &lt;2e-16 ***\nBMI          0.328076   0.023228   14.12   &lt;2e-16 ***\nAge          0.412758   0.008076   51.11   &lt;2e-16 ***\nGendermale   4.346847   0.313476   13.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.43 on 8483 degrees of freedom\n  (1147 observations deleted due to missingness)\nMultiple R-squared:  0.2969,    Adjusted R-squared:  0.2966 \nF-statistic:  1194 on 3 and 8483 DF,  p-value: &lt; 2.2e-16</pre>\n</div>\n</div>\n<p><strong>Interpretation</strong>:</p>\n<ul>\n<li>The model only uses complete cases, ignoring potentially informative missingness.</li>\n<li>If BMI is more often missing in certain subgroups (e.g., older adults or females), then the relationship estimated here does not represent the whole population.</li>\n<li>In later sections, we will see how different imputation strategies can mitigate this problem.</li>\n</ul>\n</section>\n</section>\n<section class=\"level2\" data-number=\"4\" id=\"missing-data-mechanisms\">\n<h2 class=\"anchored\" data-anchor-id=\"missing-data-mechanisms\" data-number=\"4\"><span class=\"header-section-number\">4</span> Missing Data Mechanisms</h2>\n<p>One of the most crucial aspects of handling missing data is to understand <strong>why</strong> the data are missing.<br/>\nThe mechanism behind missingness determines whether our chosen method will yield unbiased and efficient estimates.</p>\n<section class=\"level3\" data-number=\"4.1\" id=\"types-of-missing-data-mechanisms\">\n<h3 class=\"anchored\" data-anchor-id=\"types-of-missing-data-mechanisms\" data-number=\"4.1\"><span class=\"header-section-number\">4.1</span> Types of Missing Data Mechanisms</h3>\n<ul>\n<li><p><strong>MCAR (Missing Completely At Random)</strong><br/>\nThe probability of a value being missing does not depend on either the observed or the unobserved data.<br/>\n→ Example: A lab machine randomly fails for some patients, regardless of their characteristics.<br/>\n→ Implication: Complete-case analysis is valid (though less efficient).</p></li>\n<li><p><strong>MAR (Missing At Random)</strong><br/>\nThe probability of missingness depends only on the <strong>observed</strong> data, not on the missing values themselves.<br/>\n→ Example: People with lower income are less likely to report their weight, but we observe income.<br/>\n→ Implication: Multiple imputation or likelihood-based methods can recover unbiased estimates.</p></li>\n<li><p><strong>MNAR (Missing Not At Random)</strong><br/>\nThe probability of missingness depends on the <strong>unobserved</strong> value itself.<br/>\n→ Example: People with higher BMI are less likely to report their weight.<br/>\n→ Implication: Strong assumptions or external information are needed; imputation under MAR will still be biased.</p></li>\n</ul>\n</section>\n<section class=\"level3\" data-number=\"4.2\" id=\"what-each-mechanism-implies-with-nhanes-intuition\">\n<h3 class=\"anchored\" data-anchor-id=\"what-each-mechanism-implies-with-nhanes-intuition\" data-number=\"4.2\"><span class=\"header-section-number\">4.2</span> What each mechanism implies (with NHANES intuition)</h3>\n<ul>\n<li><p><strong>MCAR</strong> — e.g., random device failure that occasionally prevents recording <code>BMI</code>.<br/>\n<em>Implication:</em> Complete-case analysis (dropping rows) is unbiased but wastes data.</p></li>\n<li><p><strong>MAR</strong> — e.g., <code>BMI</code> missingness varies by observed <strong>Age</strong> or <strong>Gender</strong>.<br/>\n<em>Implication:</em> Likelihood-based methods or <strong>Multiple Imputation (MI)</strong> are valid if those predictors are in the imputation model.</p></li>\n<li><p><strong>MNAR</strong> — e.g., people with <strong>very high BMI</strong> systematically do not report it.<br/>\n<em>Implication:</em> MAR-based methods still biased; requires <strong>sensitivity analysis</strong> or explicit MNAR models.</p></li>\n</ul>\n</section>\n<section class=\"level3\" data-number=\"4.3\" id=\"quick-nhanes-checks-that-suggest-a-mechanism\">\n<h3 class=\"anchored\" data-anchor-id=\"quick-nhanes-checks-that-suggest-a-mechanism\" data-number=\"4.3\"><span class=\"header-section-number\">4.3</span> Quick NHANES checks that suggest a mechanism</h3>\n<p>Below we do two simple diagnostics on our working subset <code>nhanes_sub</code><br/>\n(defined earlier as: <code>NHANES |&gt; select(ID, Age, Gender, BMI, BPSysAve, Diabetes)</code>).</p>\n<div class=\"cell\">\n<pre># Packages we already use\nlibrary(dplyr)\nlibrary(knitr)\n\n# 1) Overall BMI missingness\nnhanes_sub |&gt;\n  summarise(pct_missing_BMI = mean(is.na(BMI)) * 100) |&gt;\n  mutate(pct_missing_BMI = round(pct_missing_BMI, 1)) |&gt;\n  kable(caption = \"Overall BMI missingness (%)\")</pre>\n<div class=\"cell-output-display\">\n<table class=\"caption-top table table-sm table-striped small\">\n<caption>Overall BMI missingness (%)</caption>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: right;\">pct_missing_BMI</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: right;\">3.7</td>\n</tr>\n</tbody>\n</table>\n</div>\n<pre># 2) Does BMI missingness vary by observed variables? (MAR hint)\n#    - By Gender\nby_gender &lt;- nhanes_sub |&gt;\n  group_by(Gender) |&gt;\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |&gt;\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n#    - By Age groups (bins)\nby_age &lt;- nhanes_sub |&gt;\n  mutate(AgeBand = cut(Age, breaks = c(0, 30, 45, 60, Inf),\n                       labels = c(\"&lt;=30\", \"31–45\", \"46–60\", \"60+\"),\n                       right = FALSE)) |&gt;\n  group_by(AgeBand) |&gt;\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |&gt;\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n# Show summaries nicely\nkable(by_gender, caption = \"BMI missingness by Gender (%)\")</pre>\n<div class=\"cell-output-display\">\n<table class=\"caption-top table table-sm table-striped small\">\n<caption>BMI missingness by Gender (%)</caption>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Gender</th>\n<th style=\"text-align: right;\">pct_miss_BMI</th>\n<th style=\"text-align: right;\">n</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">female</td>\n<td style=\"text-align: right;\">3.6</td>\n<td style=\"text-align: right;\">5020</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">male</td>\n<td style=\"text-align: right;\">3.8</td>\n<td style=\"text-align: right;\">4980</td>\n</tr>\n</tbody>\n</table>\n</div>\n<pre>kable(by_age,    caption = \"BMI missingness by Age band (%)\")</pre>\n<div class=\"cell-output-display\">\n<table class=\"caption-top table table-sm table-striped small\">\n<caption>BMI missingness by Age band (%)</caption>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">AgeBand</th>\n<th style=\"text-align: right;\">pct_miss_BMI</th>\n<th style=\"text-align: right;\">n</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">&lt;=30</td>\n<td style=\"text-align: right;\">7.6</td>\n<td style=\"text-align: right;\">4121</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">31–45</td>\n<td style=\"text-align: right;\">0.5</td>\n<td style=\"text-align: right;\">2049</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">46–60</td>\n<td style=\"text-align: right;\">0.6</td>\n<td style=\"text-align: right;\">1991</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">60+</td>\n<td style=\"text-align: right;\">1.7</td>\n<td style=\"text-align: right;\">1839</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<p><strong>Interpretation:</strong></p>\n<ul>\n<li><p>If <code>pct_miss_BMI</code> is <strong>similar across groups</strong>, MCAR is more plausible.</p></li>\n<li><p>If missingness <strong>changes with Age or Gender</strong>, <strong>MAR</strong> is more plausible (we must include those predictors in imputation).</p></li>\n<li><p>These are <em>indicators</em>, not proofs; true <strong>MNAR</strong> needs external info or sensitivity analyses.</p></li>\n</ul>\n<p><strong>Which methods are valid under which mechanism?</strong></p>\n<table class=\"caption-top table\">\n<colgroup>\n<col style=\"width: 20%\"/>\n<col style=\"width: 27%\"/>\n<col style=\"width: 30%\"/>\n<col style=\"width: 20%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Mechanism</th>\n<th>Example (NHANES context)</th>\n<th>Valid methods</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><strong>MCAR</strong></td>\n<td>Random loss of <code>BMI</code> records</td>\n<td>Complete-case, single imputation, MI</td>\n<td>Unbiased but may waste data</td>\n</tr>\n<tr class=\"even\">\n<td><strong>MAR</strong></td>\n<td><code>BMI</code> missingness varies by observed <code>Age</code>, <code>Gender</code></td>\n<td><strong>Multiple Imputation (MICE)</strong>, likelihood/EM, missForest</td>\n<td>Include strong predictors of missingness</td>\n</tr>\n<tr class=\"odd\">\n<td><strong>MNAR</strong></td>\n<td>People with very high <code>BMI</code> hide it</td>\n<td>Sensitivity analysis, selection/pattern-mixture models</td>\n<td>MAR-based MI alone is biased</td>\n</tr>\n</tbody>\n</table>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nOptional: Little’s MCAR Test\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p>Little’s MCAR test is a statistical procedure used to examine whether data are <strong>Missing Completely at Random (MCAR)</strong>.</p>\n<p>⚠️ However, this test comes with important caveats:<br/>\n- It can be overly sensitive in <strong>large samples</strong>, flagging trivial deviations.<br/>\n- In <strong>small samples</strong>, its power is often too low to detect meaningful departures from MCAR.</p>\n<p>Because of these limitations, it should be treated only as a <strong>supporting tool</strong> rather than a definitive test when diagnosing missingness mechanisms.</p>\n</div>\n</div>\n</section>\n</section>\n<section class=\"level2\" data-number=\"5\" id=\"detecting-missing-data\">\n<h2 class=\"anchored\" data-anchor-id=\"detecting-missing-data\" data-number=\"5\"><span class=\"header-section-number\">5</span> Detecting Missing Data</h2>\n<p>Before applying any imputation or modeling technique, it is essential to explore the <strong>extent and structure of missingness</strong> in the dataset. The <code>nhanes_sub</code> data frame, derived from the NHANES dataset, will be used for illustration.</p>\n<section class=\"level3\" data-number=\"5.1\" id=\"simple-counts-and-summaries\">\n<h3 class=\"anchored\" data-anchor-id=\"simple-counts-and-summaries\" data-number=\"5.1\"><span class=\"header-section-number\">5.1</span> Simple Counts and Summaries</h3>\n<p>The first step is to quantify how many values are missing per variable.</p>\n<div class=\"cell\">\n<pre># Count missing values for each variable\nnhanes_sub %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) </pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre># A tibble: 1 × 6\n     ID   Age Gender   BMI BPSysAve Diabetes\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n1     0     0      0   366     1449      142</pre>\n</div>\n</div>\n<p>The output shows the number of missing values in each column, making it easy to spot problematic variables. Another quick check is to identify how many <strong>complete vs. incomplete cases</strong> exist:</p>\n<div class=\"cell\">\n<pre>sum(complete.cases(nhanes_sub))       # number of complete rows</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 8482</pre>\n</div>\n<pre>sum(!complete.cases(nhanes_sub))      # number of incomplete rows</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 1518</pre>\n</div>\n</div>\n<p>This gives us an idea of the proportion of observations that would be lost if we opted for <strong>listwise deletion</strong>.</p>\n</section>\n<section class=\"level3\" data-number=\"5.2\" id=\"visualizing-missingness\">\n<h3 class=\"anchored\" data-anchor-id=\"visualizing-missingness\" data-number=\"5.2\"><span class=\"header-section-number\">5.2</span> Visualizing Missingness</h3>\n<p>Textual summaries are informative, but missing data often has <strong>patterns</strong> that are better revealed visually. Several R packages support this task:</p>\n<section class=\"level4\" data-number=\"5.2.1\" id=\"naniar\">\n<h4 class=\"anchored\" data-anchor-id=\"naniar\" data-number=\"5.2.1\"><span class=\"header-section-number\">5.2.1</span> <code>naniar</code></h4>\n<div class=\"cell\">\n<pre>library(naniar)\nlibrary(ggplot2)\n\n# Visualize missing values by variable\ngg_miss_var(nhanes_sub, show_pct = TRUE) +\n  labs(title = \"Missing Values by Variable in NHANES Subset\",\n       x = \"Variables\",\n       y = \"Proportion of Missing Values\")</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-7-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-7-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<ul>\n<li><p>Each bar corresponds to a variable.</p></li>\n<li><p>The <strong>height of the bar</strong> shows how many observations are missing for that variable.</p></li>\n<li><p>With <code>show_pct = TRUE</code>, the proportion of missing values is also displayed, making it easier to compare across variables.</p></li>\n<li><p>Variables with tall bars clearly have higher missingness (e.g., BMI or blood pressure variables often stand out in this dataset).</p></li>\n</ul>\n</section>\n<section class=\"level4\" data-number=\"5.2.2\" id=\"vim\">\n<h4 class=\"anchored\" data-anchor-id=\"vim\" data-number=\"5.2.2\"><span class=\"header-section-number\">5.2.2</span> <code>VIM</code></h4>\n<div class=\"cell\">\n<pre>library(VIM)\n\naggr(nhanes_sub, numbers = TRUE, prop = FALSE, sortVar = TRUE)</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-8-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-8-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\n Variables sorted by number of missings: \n Variable Count\n BPSysAve  1449\n      BMI   366\n Diabetes   142\n       ID     0\n      Age     0\n   Gender     0</pre>\n</div>\n</div>\n<p>This aggregated visualization shows the proportion of missing values per variable and the combinations of missingness across variables.</p>\n</section>\n<section class=\"level4\" data-number=\"5.2.3\" id=\"visdat\">\n<h4 class=\"anchored\" data-anchor-id=\"visdat\" data-number=\"5.2.3\"><span class=\"header-section-number\">5.2.3</span> <code>visdat</code></h4>\n<div class=\"cell\">\n<pre>library(visdat)\n\nvis_dat(nhanes_sub)</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-9-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-9-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p>This function displays the data type of each variable and overlays missingness, helping to identify whether missing values cluster in certain variable types (e.g., numeric vs. categorical).</p>\n</section>\n</section>\n<section class=\"level3\" data-number=\"5.3\" id=\"interpreting-the-patterns\">\n<h3 class=\"anchored\" data-anchor-id=\"interpreting-the-patterns\" data-number=\"5.3\"><span class=\"header-section-number\">5.3</span> Interpreting the Patterns</h3>\n<ul>\n<li><strong>Random scatter of missing values</strong> across rows/columns may indicate <strong>MCAR</strong> (though formal testing is required).</li>\n<li><strong>Systematic patterns</strong> (e.g., older participants more likely to have missing BMI) hint at <strong>MAR</strong>.</li>\n<li><strong>Blocks of missingness</strong> (entire variables missing for subgroups) may suggest <strong>MNAR</strong> or structural missingness.</li>\n</ul>\n</section>\n</section>\n<section class=\"level2\" data-number=\"6\" id=\"handling-missing-data-methods\">\n<h2 class=\"anchored\" data-anchor-id=\"handling-missing-data-methods\" data-number=\"6\"><span class=\"header-section-number\">6</span> Handling Missing Data — Methods</h2>\n<p>In this section we review the main families of methods, show <strong>when</strong> each is appropriate, and demonstrate them on <code>nhanes_sub</code>. We will explicitly call out the <strong>trade-offs</strong> so readers can choose deliberately—not by habit.</p>\n<hr/>\n<section class=\"level3\" data-number=\"6.1\" id=\"deletion\">\n<h3 class=\"anchored\" data-anchor-id=\"deletion\" data-number=\"6.1\"><span class=\"header-section-number\">6.1</span> Deletion</h3>\n<p><strong>Listwise deletion (complete-case)</strong> removes any row that contains <em>at least one</em> missing value.<br/>\n<strong>Pairwise deletion</strong> uses all available pairs to compute correlations/covariances, which can later lead to <strong>non–positive-definite</strong> covariance matrices and failures in modeling.</p>\n<ul>\n<li><p><strong>Pros</strong> - Simple; widely implemented by default (often silently). - Unbiased <em>only</em> under <strong>MCAR</strong>.</p></li>\n<li><p><strong>Cons</strong> - Wastes data; reduces power. - Biased under <strong>MAR/MNAR</strong>; can change the sample composition.</p></li>\n</ul>\n<div class=\"cell\">\n<pre># How many rows would we lose if we required complete cases for these variables?\nn_total &lt;- nrow(nhanes_sub)\nn_cc    &lt;- nhanes_sub |&gt; stats::complete.cases() |&gt; sum()\n\ncbind(\n  total_rows    = n_total,\n  complete_cases= n_cc,\n  lost_rows     = n_total - n_cc,\n  lost_pct      = round((n_total - n_cc) / n_total * 100, 1)\n)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>     total_rows complete_cases lost_rows lost_pct\n[1,]      10000           8482      1518     15.2</pre>\n</div>\n</div>\n<p><strong>Interpretation:</strong> If the lost percentage is non-trivial (e.g., &gt;5–10%), listwise deletion both <strong>shrinks power</strong> and <strong>risks bias</strong> unless MCAR truly holds. Pairwise deletion is <strong>not recommended</strong> for modeling because it can yield inconsistent covariance structures.</p>\n</section>\n<section class=\"level3\" data-number=\"6.2\" id=\"simple-imputation\">\n<h3 class=\"anchored\" data-anchor-id=\"simple-imputation\" data-number=\"6.2\"><span class=\"header-section-number\">6.2</span> Simple Imputation</h3>\n<p><strong>Idea.</strong> Fill missing values with a single plausible value (one pass). Fast and convenient, but it <strong>underestimates uncertainty</strong> (standard errors too small) and can <strong>distort distributions</strong>.</p>\n<p><strong>Typical choices</strong></p>\n<ul>\n<li><p><strong>Mean/Median/Mode</strong> (baselines; median is more robust to skew)</p></li>\n<li><p><strong>k-Nearest Neighbors (kNN)</strong> (borrows information from similar rows)</p></li>\n<li><p><strong>Hot-deck</strong> (donor-based; similar spirit to kNN)</p></li>\n</ul>\n<section class=\"level4\" data-number=\"6.2.1\" id=\"median-numeric-mode-categorical-baselines\">\n<h4 class=\"anchored\" data-anchor-id=\"median-numeric-mode-categorical-baselines\" data-number=\"6.2.1\"><span class=\"header-section-number\">6.2.1</span> Median (numeric) + Mode (categorical) baselines</h4>\n<div class=\"cell\">\n<pre>set.seed(2025)\n\n# Create a median-imputed BMI for illustration (only if BMI is missing)\nnh_med &lt;- nhanes_sub |&gt;\n  mutate(\n    BMI_med = ifelse(is.na(BMI), stats::median(BMI, na.rm = TRUE), BMI)\n  )\n\n# Compare how many BMI were imputed\nsum(is.na(nhanes_sub$BMI))           # original missing BMI count</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 366</pre>\n</div>\n<pre>sum(is.na(nh_med$BMI_med))           # should be 0</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 0</pre>\n</div>\n</div>\n<p><strong>Distribution distortion (variance shrinkage).</strong></p>\n<div class=\"cell\">\n<pre>library(ggplot2)\n\n# Compare BMI distribution: complete-case vs median-imputed\np_cc  &lt;- nhanes_sub |&gt;\n  filter(!is.na(BMI)) |&gt;\n  ggplot(aes(x = BMI)) +\n  geom_density() +\n  labs(title = \"BMI density — complete cases\")\n\np_med &lt;- nh_med |&gt;\n  ggplot(aes(x = BMI_med)) +\n  geom_density() +\n  labs(title = \"BMI density — median-imputed\")\n\np_cc; p_med</pre>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-12-1.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n<div class=\"cell-output-display\">\n<div>\n<figure class=\"figure\">\n<p><img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-12-2.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-12-2.png?w=450&amp;ssl=1\"/></noscript></p>\n</figure>\n</div>\n</div>\n</div>\n<p><strong>Interpretation:</strong> Median imputation <strong>spikes</strong> the distribution around the median and <strong>reduces variance</strong>. This can attenuate real relationships that depend on dispersion.</p>\n</section>\n<section class=\"level4\" data-number=\"6.2.2\" id=\"knn-donor-based-imputation\">\n<h4 class=\"anchored\" data-anchor-id=\"knn-donor-based-imputation\" data-number=\"6.2.2\"><span class=\"header-section-number\">6.2.2</span> kNN (donor-based) imputation</h4>\n<div class=\"cell\">\n<pre># kNN imputation with VIM::kNN (works on data frames; chooses donors by similarity)\nlibrary(VIM)\n\n# We impute only BMI here; set k=5 as a reasonable starting point.\nnh_knn &lt;- nhanes_sub |&gt;\n  select(Age, Gender, BMI, BPSysAve, Diabetes) |&gt;\n  VIM::kNN(k = 5, imp_var = FALSE)  # imp_var=FALSE avoids extra *_imp columns\n\n# Check imputation effect\nsum(is.na(nhanes_sub$BMI))   # original missing BMI</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 366</pre>\n</div>\n<pre>sum(is.na(nh_knn$BMI))       # after kNN (should be 0)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 0</pre>\n</div>\n</div>\n<p><strong>Interpretation:</strong> kNN preserves local structure better than mean/median, but it is still <strong>single imputation</strong> → uncertainty is <strong>not</strong> propagated. Choice of <strong>k</strong> and included predictors matters.</p>\n<blockquote class=\"blockquote\">\n<p><strong>Rule of thumb.</strong> Simple methods are acceptable for quick EDA or as baselines. For principled inference under MAR, prefer <strong>Multiple Imputation</strong>.</p>\n</blockquote>\n</section>\n</section>\n<section class=\"level3\" data-number=\"6.3\" id=\"advanced-methods\">\n<h3 class=\"anchored\" data-anchor-id=\"advanced-methods\" data-number=\"6.3\"><span class=\"header-section-number\">6.3</span> Advanced Methods</h3>\n<section class=\"level4\" data-number=\"6.3.1\" id=\"multiple-imputation-with-mice\">\n<h4 class=\"anchored\" data-anchor-id=\"multiple-imputation-with-mice\" data-number=\"6.3.1\"><span class=\"header-section-number\">6.3.1</span> Multiple Imputation with <code>mice</code></h4>\n<p>So far, we have seen that missing values exist in several variables of our dataset. A common and powerful approach to handle missingness is <strong>Multiple Imputation by Chained Equations (MICE)</strong>. The <code>mice</code> package in R is widely used for this purpose. The idea is simple:</p>\n<ul>\n<li>Instead of filling in missing values once, MICE creates <strong>multiple complete datasets</strong> by imputing values several times.</li>\n<li>Each dataset is then analyzed separately.</li>\n<li>Finally, results are pooled together to account for the variability introduced by missingness.</li>\n</ul>\n<p>Let’s try this approach on our subset of the <code>NHANES</code> data:</p>\n<div class=\"cell\">\n<pre>library(mice)\n\n# Create imputations\nimp &lt;- mice(nhanes_sub, m = 3, seed = 123)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\n iter imp variable\n  1   1  BMI  BPSysAve  Diabetes\n  1   2  BMI  BPSysAve  Diabetes\n  1   3  BMI  BPSysAve  Diabetes\n  2   1  BMI  BPSysAve  Diabetes\n  2   2  BMI  BPSysAve  Diabetes\n  2   3  BMI  BPSysAve  Diabetes\n  3   1  BMI  BPSysAve  Diabetes\n  3   2  BMI  BPSysAve  Diabetes\n  3   3  BMI  BPSysAve  Diabetes\n  4   1  BMI  BPSysAve  Diabetes\n  4   2  BMI  BPSysAve  Diabetes\n  4   3  BMI  BPSysAve  Diabetes\n  5   1  BMI  BPSysAve  Diabetes\n  5   2  BMI  BPSysAve  Diabetes\n  5   3  BMI  BPSysAve  Diabetes</pre>\n</div>\n<pre># Look at a summary\nimp</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>Class: mids\nNumber of multiple imputations:  3 \nImputation methods:\n      ID      Age   Gender      BMI BPSysAve Diabetes \n      \"\"       \"\"       \"\"    \"pmm\"    \"pmm\" \"logreg\" \nPredictorMatrix:\n         ID Age Gender BMI BPSysAve Diabetes\nID        0   1      1   1        1        1\nAge       1   0      1   1        1        1\nGender    1   1      0   1        1        1\nBMI       1   1      1   0        1        1\nBPSysAve  1   1      1   1        0        1\nDiabetes  1   1      1   1        1        0</pre>\n</div>\n</div>\n<p>The output shows:</p>\n<ul>\n<li><p><code>m = 3</code>: number of imputed datasets created.</p></li>\n<li><p>For each variable with missingness, the method used for imputation.</p></li>\n<li><p>How many iterations were performed in the algorithm.</p></li>\n</ul>\n<p>We can take a quick look at the imputed values:</p>\n<div class=\"cell\">\n<pre># Inspect first few imputations for BMI\nhead(imp$imp$BMI)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>        1     2     3\n61  43.00 17.70 12.90\n161 16.70 13.50 18.59\n210 16.28 24.00 23.10\n309 24.00 37.32 26.20\n310 24.00 28.55 26.30\n320 25.10 32.25 29.20</pre>\n</div>\n</div>\n<p>This shows different plausible values for missing BMI observations across the three imputed datasets. Each dataset gives slightly different results, which is expected and important for reflecting uncertainty.</p>\n<p>Once we have these imputations, we can <strong>complete the dataset</strong>:</p>\n<div class=\"cell\">\n<pre># Extract the first imputed dataset\nnhanes_completed &lt;- complete(imp, 1)\n\nhead(nhanes_completed)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>     ID Age Gender   BMI BPSysAve Diabetes\n1 51624  34   male 32.22      113       No\n2 51624  34   male 32.22      113       No\n3 51624  34   male 32.22      113       No\n4 51625   4   male 15.30       92       No\n5 51630  49 female 30.57      112       No\n6 51638   9   male 16.82       86       No</pre>\n</div>\n</div>\n<p>Now we have a complete dataset with no missing values. In practice, we would analyze all imputed datasets and then combine results using Rubin’s rules, but the key takeaway here is:</p>\n<ul>\n<li><p><code>mice()</code> provides multiple versions of the data,</p></li>\n<li><p>imputations are based on relationships among variables,</p></li>\n<li><p>and the method preserves uncertainty rather than hiding it.</p></li>\n</ul>\n<div class=\"callout callout-style-default callout-note callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nMICE Essentials: Key Arguments\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<ul>\n<li><p><strong>method</strong>: Specifies the imputation model for each variable.</p>\n<ul>\n<li><code>pmm</code>: predictive mean matching (continuous variables)</li>\n<li><code>logreg</code>: logistic regression (binary)</li>\n<li><code>polyreg</code>: multinomial regression (nominal categorical)</li>\n<li><code>polr</code>: proportional odds model (ordered categorical)</li>\n</ul>\n<p><em>Rule of thumb</em>: If a factor has &gt;2 levels, prefer <code>polyreg</code> (nominal) or <code>polr</code> (ordered) instead of <code>logreg</code>. Always check the actual levels of variables such as <code>Gender</code> or <code>Diabetes</code> in your data before setting methods.</p></li>\n<li><p><strong>predictorMatrix</strong>: Controls which variables are used to predict others.</p>\n<ul>\n<li>Rows = target variables (to be imputed)</li>\n<li>Columns = predictor variables</li>\n</ul></li>\n<li><p><strong>m</strong>: Number of multiple imputations to generate (commonly 5–20).</p>\n<ul>\n<li>More imputations recommended for high missingness.</li>\n</ul></li>\n<li><p><strong>maxit</strong>: Number of iterations of the chained equations (often 5–10).</p></li>\n<li><p><strong>seed</strong>: Random seed for reproducibility.</p>\n<ul>\n<li>Always set when writing tutorials or reports.</li>\n</ul></li>\n</ul>\n</div>\n</div>\n</section>\n<section class=\"level4\" data-number=\"6.3.2\" id=\"multiple-imputation-with-missforest\">\n<h4 class=\"anchored\" data-anchor-id=\"multiple-imputation-with-missforest\" data-number=\"6.3.2\"><span class=\"header-section-number\">6.3.2</span> Multiple Imputation with missForest</h4>\n<p>The <strong>missForest</strong> package provides a non-parametric imputation method based on random forests.<br/>\nUnlike <code>mice</code>, which generates multiple imputations, <code>missForest</code> creates a <strong>single completed dataset</strong> by iteratively predicting missing values using random forest models. It works well with both continuous and categorical variables and can capture nonlinear relationships.</p>\n<p>We will use the same <code>nhanes_sub</code> dataset as before:</p>\n<div class=\"cell\">\n<pre>library(dplyr)\nlibrary(missForest)\n\n# Start from the existing subset:\n# nhanes_sub &lt;- NHANES |&gt; select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\n# 1) Keep only model-relevant columns (drop pure identifier)\n# 2) Convert character variables to factors (missForest expects factors, not raw character)\n# 3) Coerce to base data.frame to avoid tibble-related method dispatch issues\nmf_input &lt;- nhanes_sub |&gt;\n  select(Age, Gender, BMI, BPSysAve, Diabetes) |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;\n  as.data.frame()\n\nset.seed(123)\nmf_fit &lt;- missForest(\n  mf_input,\n  ntree   = 200,    # more trees -&gt; stabler imputations\n  maxiter = 5,      # outer iterations (default 10; 5 is fine for demo)\n  verbose = FALSE\n)\n\n# Completed data and OOB error\nmf_imputed &lt;- mf_fit$ximp\nmf_oob     &lt;- mf_fit$OOBerror\n\n# Quick checks\nsum(is.na(mf_input$BMI))</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 366</pre>\n</div>\n<pre>sum(is.na(mf_imputed$BMI))   # should go to 0</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] 0</pre>\n</div>\n<pre>mf_oob</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>     NRMSE        PFC \n0.17890307 0.02667884 </pre>\n</div>\n</div>\n<p>The function returns a list with two key elements:</p>\n<ul>\n<li><p><code>ximp</code>: the completed dataset after imputation.</p></li>\n<li><p><code>OOBerror</code>: the estimated imputation error (normalized root mean squared error for continuous variables and proportion of falsely classified entries for categorical variables).</p></li>\n</ul>\n<p><strong>Interpretation:</strong></p>\n<ul>\n<li><p>The <strong>completed dataset</strong> (<code>ximp</code>) replaces all missing values with imputed estimates.</p></li>\n<li><p><strong>NRMSE (Normalized Root Mean Squared Error):</strong> <code>0.1789</code></p>\n<ul>\n<li>This value reflects the imputation error for continuous variables (e.g., <code>Age</code>, <code>BMI</code>, <code>BPSysAve</code>).</li>\n<li>Since it is normalized, values closer to <strong>0</strong> indicate better accuracy. Here, an error of ~0.18 suggests that the imputed values are quite close to the true (non-missing) values.</li>\n</ul></li>\n<li><p><strong>PFC (Proportion of Falsely Classified):</strong> <code>0.0267</code></p>\n<ul>\n<li>This metric evaluates categorical variables (e.g., <code>Gender</code>, <code>Diabetes</code>).</li>\n<li>A value of ~0.027 means only about <strong>2.7% of categorical imputations were misclassified</strong>, which is a strong performance.</li>\n</ul></li>\n</ul>\n<p>✅ <strong>Interpretation:</strong><br/>\nThe results indicate that <code>missForest</code> produced high-quality imputations: continuous variables are imputed with relatively low error, and categorical variables with very low misclassification. In practical terms, this means the dataset after imputation is reliable and close to the original data distribution.</p>\n<p><strong>Pros and Cons of <code>missForest</code></strong></p>\n<p><strong>Advantages:</strong></p>\n<ul>\n<li>Handles mixed data types (continuous + categorical).</li>\n<li>Captures nonlinearities and complex interactions.</li>\n<li>No need to specify an explicit imputation model.</li>\n</ul>\n<p><strong>Limitations:</strong></p>\n<ul>\n<li>Produces only a <strong>single imputed dataset</strong>, so uncertainty is not directly quantified (unlike <code>mice</code>).</li>\n<li>Computationally more expensive for very large datasets.</li>\n</ul>\n</section>\n</section>\n</section>\n<section class=\"level2\" data-number=\"7\" id=\"single-vs.-multiple-imputation\">\n<h2 class=\"anchored\" data-anchor-id=\"single-vs.-multiple-imputation\" data-number=\"7\"><span class=\"header-section-number\">7</span> Single vs. Multiple Imputation</h2>\n<p>One critical distinction in handling missing data is <strong>single imputation</strong> vs. <strong>multiple imputation (MI)</strong>.</p>\n<ul>\n<li><p><strong>Single imputation</strong> (mean, median, regression, etc.) fills each missing value once. While simple, it <strong>ignores uncertainty</strong>, treating imputed values as if they were observed.</p></li>\n<li><p><strong>Multiple imputation</strong> generates <strong>several plausible versions of the dataset</strong> (e.g., 5–10). Each dataset is analyzed separately, and results are then combined (pooled). This approach accounts for <strong>variability due to missingness</strong> and produces more reliable inferences.</p></li>\n</ul>\n<p>Let’s illustrate with our <code>nhanes_sub</code> dataset:</p>\n<div class=\"cell\">\n<pre># Complete-case analysis (ignores missing data)\nlm_cc &lt;- lm(BMI ~ Age + Gender + BPSysAve + Diabetes,\n            data = nhanes_sub, na.action = na.omit)\n\n# Single imputation (mean imputation for BMI)\nnhanes_single &lt;- nhanes_sub |&gt; \n  mutate(BMI = ifelse(is.na(BMI), mean(BMI, na.rm = TRUE), BMI))\n\nlm_si &lt;- lm(BMI ~ Age + Gender + BPSysAve + Diabetes,\n            data = nhanes_single)\n\n# Multiple imputation with mice\nimp &lt;- mice(nhanes_sub, m = 5, method = \"pmm\", seed = 123)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\n iter imp variable\n  1   1  BMI  BPSysAve  Diabetes\n  1   2  BMI  BPSysAve  Diabetes\n  1   3  BMI  BPSysAve  Diabetes\n  1   4  BMI  BPSysAve  Diabetes\n  1   5  BMI  BPSysAve  Diabetes\n  2   1  BMI  BPSysAve  Diabetes\n  2   2  BMI  BPSysAve  Diabetes\n  2   3  BMI  BPSysAve  Diabetes\n  2   4  BMI  BPSysAve  Diabetes\n  2   5  BMI  BPSysAve  Diabetes\n  3   1  BMI  BPSysAve  Diabetes\n  3   2  BMI  BPSysAve  Diabetes\n  3   3  BMI  BPSysAve  Diabetes\n  3   4  BMI  BPSysAve  Diabetes\n  3   5  BMI  BPSysAve  Diabetes\n  4   1  BMI  BPSysAve  Diabetes\n  4   2  BMI  BPSysAve  Diabetes\n  4   3  BMI  BPSysAve  Diabetes\n  4   4  BMI  BPSysAve  Diabetes\n  4   5  BMI  BPSysAve  Diabetes\n  5   1  BMI  BPSysAve  Diabetes\n  5   2  BMI  BPSysAve  Diabetes\n  5   3  BMI  BPSysAve  Diabetes\n  5   4  BMI  BPSysAve  Diabetes\n  5   5  BMI  BPSysAve  Diabetes</pre>\n</div>\n<pre>lm_mi &lt;- with(imp, lm(BMI ~ Age + Gender + BPSysAve + Diabetes))\npooled &lt;- pool(lm_mi)</pre>\n</div>\n<div class=\"cell\">\n<pre>summary(lm_cc)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\nCall:\nlm(formula = BMI ~ Age + Gender + BPSysAve + Diabetes, data = nhanes_sub, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.771  -4.640  -1.053   3.553  53.003 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.488597   0.513225  34.076   &lt;2e-16 ***\nAge          0.047930   0.004273  11.217   &lt;2e-16 ***\nGendermale  -0.278756   0.144799  -1.925   0.0542 .  \nBPSysAve     0.067504   0.004903  13.767   &lt;2e-16 ***\nDiabetesYes  3.789606   0.265240  14.287   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.588 on 8477 degrees of freedom\n  (1518 observations deleted due to missingness)\nMultiple R-squared:  0.1136,    Adjusted R-squared:  0.1132 \nF-statistic: 271.5 on 4 and 8477 DF,  p-value: &lt; 2.2e-16</pre>\n</div>\n<pre>summary(lm_si)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>\nCall:\nlm(formula = BMI ~ Age + Gender + BPSysAve + Diabetes, data = nhanes_single)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.657  -4.634  -1.029   3.533  53.004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.600406   0.508667  34.601   &lt;2e-16 ***\nAge          0.047159   0.004237  11.129   &lt;2e-16 ***\nGendermale  -0.287221   0.143820  -1.997   0.0458 *  \nBPSysAve     0.066791   0.004858  13.748   &lt;2e-16 ***\nDiabetesYes  3.725941   0.262781  14.179   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.57 on 8541 degrees of freedom\n  (1454 observations deleted due to missingness)\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1114 \nF-statistic: 268.8 on 4 and 8541 DF,  p-value: &lt; 2.2e-16</pre>\n</div>\n<pre>summary(pooled)</pre>\n<div class=\"cell-output cell-output-stdout\">\n<pre>         term    estimate   std.error statistic       df       p.value\n1 (Intercept) 14.66559186 0.487686304 30.071773 1253.210 5.075329e-150\n2         Age  0.10367759 0.003748806 27.656164 3916.798 6.073157e-154\n3  Gendermale -0.30521870 0.134707691 -2.265785 3727.382  2.352165e-02\n4    BPSysAve  0.06754332 0.004761820 14.184349 1160.000  3.121286e-42\n5 DiabetesYes  3.23549109 0.260299313 12.429887 8866.664  3.529334e-35</pre>\n</div>\n</div>\n<p>We applied three different approaches to handle missing BMI values in the <code>nhanes_sub</code> dataset, modeling <strong>BMI ~ Age + Gender + BPSysAve + Diabetes</strong>. Here is what we found:</p>\n<p><strong>1. Complete-Case Analysis (CCA)</strong></p>\n<ul>\n<li><p><strong>What we did:</strong> We dropped all observations with missing values (<code>na.omit</code>).</p></li>\n<li><p><strong>Result:</strong></p>\n<ul>\n<li><p><strong>Coefficients:</strong> Age (0.048), BPSysAve (0.068), DiabetesYes (+3.79), Gender slightly negative.</p></li>\n<li><p><strong>Standard errors:</strong> Relatively large because ~1500 observations were discarded.</p></li>\n<li><p><strong>R²:</strong> 0.114 — fairly low.</p></li>\n</ul></li>\n<li><p><strong>Takeaway:</strong> CCA wastes data and may bias estimates if missingness is not MCAR (Missing Completely at Random).</p></li>\n</ul>\n<p><strong>2. Single Imputation (Mean Substitution for BMI)</strong></p>\n<ul>\n<li><p><strong>What we did:</strong> Replaced missing BMI values with the mean BMI.</p></li>\n<li><p><strong>Result:</strong></p>\n<ul>\n<li><p><strong>Coefficients:</strong> Very close to CCA (Age 0.047, BPSysAve 0.067, DiabetesYes +3.73).</p></li>\n<li><p><strong>Gender</strong> effect became just significant (<em>p</em> = 0.045).</p></li>\n<li><p><strong>Residual SE</strong> decreased slightly (6.57).</p></li>\n</ul></li>\n<li><p><strong>Takeaway:</strong> Looks “better” because all observations are retained, but this approach <strong>ignores imputation uncertainty</strong> and artificially stabilizes estimates. Standard errors are underestimated, leading to overconfidence.</p></li>\n</ul>\n<p><strong>3. Multiple Imputation (MI with <code>mice</code>, m = 5, method = “pmm”)</strong></p>\n<ul>\n<li><p><strong>What we did:</strong> Generated 5 imputed datasets using Predictive Mean Matching (PMM), fit the same model in each, and pooled results.</p></li>\n<li><p><strong>Result:</strong></p>\n<ul>\n<li><p><strong>Coefficients:</strong> Age effect doubled (0.104), intercept dropped (14.7 vs. ~17.5), Diabetes effect slightly smaller (+3.24), Gender effect remained modest but significant (<em>p</em> = 0.023).</p></li>\n<li><p><strong>Standard errors:</strong> Properly adjusted upwards — reflecting real uncertainty in imputed BMI values.</p></li>\n<li><p><strong>Inference:</strong> Despite differences in point estimates, the conclusions are more <strong>statistically honest</strong>.</p></li>\n</ul></li>\n<li><p><strong>Takeaway:</strong> MI balances efficiency (uses all data) and validity (acknowledges missingness uncertainty).</p></li>\n</ul>\n<p><strong>🔑 Overall Comparison</strong></p>\n<table class=\"caption-top table\">\n<colgroup>\n<col style=\"width: 19%\"/>\n<col style=\"width: 16%\"/>\n<col style=\"width: 22%\"/>\n<col style=\"width: 22%\"/>\n<col style=\"width: 19%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Method</th>\n<th>Keeps All Data</th>\n<th>Coefficients Similar?</th>\n<th>SE Adjusted for Uncertainty?</th>\n<th>Main Issue</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>Complete Case (CCA)</td>\n<td>❌ (~1500 rows lost)</td>\n<td>Yes, but less precise</td>\n<td>✅ (but biased if MAR/MNAR)</td>\n<td>Data loss, possible bias</td>\n</tr>\n<tr class=\"even\">\n<td>Single Imputation (SI)</td>\n<td>✅</td>\n<td>Similar to CCA</td>\n<td>❌ Underestimated</td>\n<td>Overconfident inference</td>\n</tr>\n<tr class=\"odd\">\n<td>Multiple Imputation (MI)</td>\n<td>✅</td>\n<td>Somewhat different (esp. Age)</td>\n<td>✅ Properly adjusted</td>\n<td>More computation needed</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Interpretation:</strong></p>\n<ul>\n<li><p><strong>Complete-case</strong> drops too much data and risks bias.</p></li>\n<li><p><strong>Single imputation</strong> keeps the data but gives <em>too much confidence</em> in results.</p></li>\n<li><p><strong>Multiple imputation</strong> changes some coefficients (notably Age) and reports more realistic uncertaint</p></li>\n</ul>\n<p>👉 <strong>Lesson:</strong> If your goal is valid inference, especially in epidemiological or social science settings, <strong>multiple imputation is the gold standard</strong>.</p>\n</section>\n<section class=\"level2\" data-number=\"8\" id=\"comparison-of-common-imputation-methods\">\n<h2 class=\"anchored\" data-anchor-id=\"comparison-of-common-imputation-methods\" data-number=\"8\"><span class=\"header-section-number\">8</span> Comparison of Common Imputation Methods</h2>\n<table class=\"caption-top table\">\n<colgroup>\n<col style=\"width: 22%\"/>\n<col style=\"width: 31%\"/>\n<col style=\"width: 23%\"/>\n<col style=\"width: 22%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th>Method</th>\n<th>Description</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>Listwise Deletion</td>\n<td>Removes all observations containing missing values</td>\n<td>Very simple, quick to implement</td>\n<td>Substantial data loss, potential bias</td>\n</tr>\n<tr class=\"even\">\n<td>Mean / Median / Mode</td>\n<td>Replaces missing values with a fixed statistic</td>\n<td>Easy to apply, preserves sample size</td>\n<td>Reduces variance, distorts relationships</td>\n</tr>\n<tr class=\"odd\">\n<td>LOCF (Last Observation Carried Forward)</td>\n<td>Uses the last available value (mainly time series)</td>\n<td>Useful in longitudinal data, preserves continuity</td>\n<td>Ignores trends, underestimates variability</td>\n</tr>\n<tr class=\"even\">\n<td>Linear Interpolation</td>\n<td>Estimates missing values by connecting known data points</td>\n<td>Maintains trends, intuitive</td>\n<td>Fails with sudden changes or nonlinear patterns</td>\n</tr>\n<tr class=\"odd\">\n<td>KNN Imputation</td>\n<td>Predicts missing values using nearest neighbors</td>\n<td>Preserves multivariate structure, flexible</td>\n<td>Computationally expensive, sensitive to k choice</td>\n</tr>\n<tr class=\"even\">\n<td>MICE (Multiple Imputation by Chained Equations)</td>\n<td>Iterative regression-based multiple imputation</td>\n<td>Accounts for uncertainty, widely used in research</td>\n<td>Time-consuming, requires expertise</td>\n</tr>\n<tr class=\"odd\">\n<td>missForest</td>\n<td>Uses Random Forest to impute missing values</td>\n<td>Handles nonlinearities and interactions</td>\n<td>Black-box method, computationally intensive</td>\n</tr>\n<tr class=\"even\">\n<td>EM Algorithm</td>\n<td>Iterative expectation-maximization for likelihood-based estimation</td>\n<td>Statistically principled, robust in theory</td>\n<td>Requires strong assumptions, advanced knowledge</td>\n</tr>\n</tbody>\n</table>\n<p>No single imputation method is universally optimal—each comes with trade-offs between simplicity, accuracy, and interpretability. For instance, <strong>listwise deletion</strong> is tempting for its ease but can heavily bias results if missingness is not random. Simple <strong>mean or median imputation</strong> keeps the dataset intact but artificially reduces variability and masks true correlations. More advanced techniques such as <strong>MICE, missForest, and EM</strong> provide statistically sound imputations that preserve uncertainty and relationships, but they demand more computational resources and methodological expertise.</p>\n<p>In practice:</p>\n<ul>\n<li><p><strong>Exploratory analysis</strong> often starts with simple methods (e.g., median replacement) to get a sense of the data.</p></li>\n<li><p><strong>Time series data</strong> may rely on <strong>LOCF or interpolation</strong>.</p></li>\n<li><p><strong>Complex survey or clinical datasets</strong> typically benefit from advanced approaches like <strong>MICE</strong> or <strong>missForest</strong>, which better respect the multivariate nature of the data.</p></li>\n</ul>\n<p>Ultimately, the choice depends on the <strong>data structure, missingness mechanism (MCAR, MAR, MNAR), and analytical goals</strong>.</p>\n</section>\n<section class=\"level2\" data-number=\"9\" id=\"conclusion\">\n<h2 class=\"anchored\" data-anchor-id=\"conclusion\" data-number=\"9\"><span class=\"header-section-number\">9</span> Conclusion</h2>\n<p>There is <strong>no one-size-fits-all</strong> solution for missing data. The right approach depends on your <strong>goal (prediction vs. inference)</strong>, the <strong>missingness mechanism (MCAR/MAR/MNAR)</strong>, your <strong>data structure</strong> (cross-sectional vs. longitudinal), and <strong>practical constraints</strong> (time, compute, expertise).</p>\n<section class=\"level3\" data-number=\"9.1\" id=\"what-our-nhanes-walkthrough-showed\">\n<h3 class=\"anchored\" data-anchor-id=\"what-our-nhanes-walkthrough-showed\" data-number=\"9.1\"><span class=\"header-section-number\">9.1</span> What our NHANES walkthrough showed</h3>\n<ul>\n<li><strong>Complete-case analysis</strong> is simple but wastes data and can bias results unless MCAR is plausible.</li>\n<li><strong>Single imputation</strong> (mean/median, kNN, missForest run once) keeps all rows but <strong>underestimates uncertainty</strong>, yielding overconfident inferences.</li>\n<li><strong>Multiple imputation (MICE)</strong> typically strikes the best balance for <strong>inference under MAR</strong>: it preserves multivariate structure and <strong>propagates uncertainty</strong> (via pooling), producing more honest standard errors and CIs.</li>\n<li><strong>Nonparametric imputers</strong> like <strong>missForest</strong> are strong for <strong>predictive accuracy</strong> on complex, nonlinear structure, but they do <strong>not</strong> capture imputation uncertainty by themselves.</li>\n</ul>\n</section>\n<section class=\"level3\" data-number=\"9.2\" id=\"practical-guidance-decision-oriented\">\n<h3 class=\"anchored\" data-anchor-id=\"practical-guidance-decision-oriented\" data-number=\"9.2\"><span class=\"header-section-number\">9.2</span> Practical guidance (decision-oriented)</h3>\n<ul>\n<li><strong>If your main task is prediction</strong> and interpretability is secondary → a good single-imputation engine (e.g., <strong>missForest</strong>) can be effective, with careful validation.</li>\n<li><strong>If your main task is inference</strong> (effect sizes, CIs, p-values) and <strong>MAR is reasonable</strong> → prefer <strong>MICE</strong>; include strong predictors of both the outcome and missingness; check diagnostics.</li>\n<li><strong>If you suspect MNAR</strong> → acknowledge this explicitly and consider <strong>sensitivity analyses</strong> (pattern-mixture/selection models) rather than assuming MAR.</li>\n</ul>\n</section>\n<section class=\"level3\" data-number=\"9.3\" id=\"reporting-checklist-make-your-analysis-reproducible-credible\">\n<h3 class=\"anchored\" data-anchor-id=\"reporting-checklist-make-your-analysis-reproducible-credible\" data-number=\"9.3\"><span class=\"header-section-number\">9.3</span> Reporting checklist (make your analysis reproducible &amp; credible)</h3>\n<ul>\n<li>% missing <strong>by variable</strong> and <strong>by key subgroups</strong> (e.g., Age, Gender).</li>\n<li>Your <strong>assumed mechanism</strong> (MCAR/MAR/MNAR) and why it’s plausible.</li>\n<li>The <strong>method(s)</strong> used (e.g., MICE with <code>pmm</code>, <code>m</code>, <code>maxit</code>, <code>predictorMatrix</code>; or missForest with <code>ntree</code>, <code>maxiter</code>).</li>\n<li><strong>Diagnostics</strong> (trace/density/strip plots for MICE; OOB error for missForest).</li>\n<li>For MI: <strong>pooled estimates</strong> with standard errors/intervals; clarify how pooling was performed.</li>\n<li><strong>Limitations</strong> (e.g., potential MNAR, model misspecification, small-sample caveats).</li>\n</ul>\n<div class=\"callout callout-style-default callout-tip callout-titled\">\n<div class=\"callout-header d-flex align-content-center\">\n<div class=\"callout-icon-container\">\n<i class=\"callout-icon\"></i>\n</div>\n<div class=\"callout-title-container flex-fill\">\nTip\n</div>\n</div>\n<div class=\"callout-body-container callout-body\">\n<p><strong>Rule of thumb.</strong> Use simple methods for quick EDA; use <strong>MICE</strong> for publication-grade inference under MAR; use <strong>missForest</strong> when you primarily need strong <strong>predictive performance</strong> on mixed/complex data.</p>\n</div>\n</div>\n</section>\n<section class=\"level3\" data-number=\"9.4\" id=\"common-pitfalls-to-avoid\">\n<h3 class=\"anchored\" data-anchor-id=\"common-pitfalls-to-avoid\" data-number=\"9.4\"><span class=\"header-section-number\">9.4</span> Common pitfalls to avoid</h3>\n<ul>\n<li>Treating imputed values as if they were observed “truth” (single imputation + significance testing).</li>\n<li>Imputing the <strong>outcome</strong> itself (generally avoid; let it <em>inform</em> predictor imputations instead).</li>\n<li>Ignoring <strong>leakage</strong>: fit imputers <strong>within</strong> resampling folds/splits, not on the full data.</li>\n<li>Omitting key covariates that explain missingness (weakens the MAR assumption and the imputer).</li>\n</ul>\n</section>\n<section class=\"level3\" data-number=\"9.5\" id=\"where-to-go-next\">\n<h3 class=\"anchored\" data-anchor-id=\"where-to-go-next\" data-number=\"9.5\"><span class=\"header-section-number\">9.5</span> Where to go next</h3>\n<ul>\n<li><strong>Leakage-free pipelines</strong> with <code>tidymodels::recipes</code> (train/test split done right).</li>\n<li><strong>Sensitivity analyses</strong> for MNAR.</li>\n<li><strong>Robustness checks</strong> (alternative imputation models, different <code>m</code>, predictor sets).</li>\n</ul>\n<p><strong>Bottom line:</strong> Choose methods intentionally, <strong>justify assumptions</strong>, show diagnostics, and <strong>report pooled results</strong> when using MI. Good missing-data practice is less about one magic function and more about transparent, principled workflow.</p>\n</section>\n</section>\n<section class=\"level2\" data-number=\"10\" id=\"references\">\n<h2 class=\"anchored\" data-anchor-id=\"references\" data-number=\"10\"><span class=\"header-section-number\">10</span> References</h2>\n<ul>\n<li><p>Allison, P. D. (2001). <em>Missing Data</em>. Sage Publications.</p></li>\n<li><p>Enders, C. K. (2010). <em>Applied Missing Data Analysis</em>. The Guilford Press.</p></li>\n<li><p>Little, R. J. A., &amp; Rubin, D. B. (2002). <em>Statistical Analysis with Missing Data</em> (2nd ed.). Wiley.</p></li>\n<li><p>van Buuren, S. (2018). <em>Flexible Imputation of Missing Data</em> (2nd ed.). Chapman &amp; Hall/CRC.</p></li>\n<li><p>Stekhoven, D. J., &amp; Bühlmann, P. (2012). “MissForest—Nonparametric Missing Value Imputation for Mixed-Type Data.” <em>Bioinformatics</em>, 28(1), 112–118. https://doi.org/10.1093/bioinformatics/btr597</p></li>\n<li><p>Rubin, D. B. (1987). <em>Multiple Imputation for Nonresponse in Surveys</em>. Wiley.</p></li>\n<li><p>Schafer, J. L. (1997). <em>Analysis of Incomplete Multivariate Data</em>. Chapman &amp; Hall/CRC.</p></li>\n<li><p>R Documentation: <a href=\"https://cran.r-project.org/package=mice\" rel=\"nofollow\" target=\"_blank\"><code>mice</code> package</a></p></li>\n<li><p>R Documentation: <a href=\"https://cran.r-project.org/package=missForest\" rel=\"nofollow\" target=\"_blank\"><code>missForest</code> package</a></p></li>\n</ul>\n<!-- -->\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/\"> A Statistician's R Notebook</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Handling Missing Data in R: A Comprehensive Guide\nPosted on\nAugust 17, 2025\nby\nM. Fatih Tüzen\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nA Statistician's R Notebook\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\n1\nIntroduction\nMissing data is one of the most common challenges in data analysis and statistical modeling.\nWhether the data originates from surveys, administrative registers, or clinical trials, it is almost inevitable that some values are absent. Ignoring this issue or handling it incorrectly can seriously bias results, reduce statistical power, and ultimately lead to misleading conclusions.\nIn statistical notation, missing values are usually represented as\nNA\n(Not Available) in R.\nHowever, not all missing values are created equal: some are missing completely at random, while others may follow systematic patterns. Before deciding how to handle missingness, one must carefully investigate its causes and implications.\nIn this article, we will cover:\nThe theoretical foundations of missing data mechanisms (MCAR, MAR, MNAR).\nHow to detect and visualize missing values in R.\nDifferent strategies for handling missingness, from simple imputation to advanced multiple imputation techniques.\nA practical workflow using the\nNHANES dataset\n, widely used in health research, to demonstrate methods in R.\nBest practices, pitfalls, and recommendations for applied data science.\nWe will use several R packages throughout this tutorial:\ntidyverse\n: Data wrangling and visualization\nnaniar\nand\nVIM\n: Tools for exploring and visualizing missing data\nmice\n: Multiple imputation by chained equations\nmissForest\n: Random forest–based imputation for nonlinear data\nThis structured approach will help you not only handle missing data but also understand the reasoning behind each method, ensuring that your analyses remain robust and reliable.\n2\nNHANES Dataset\nIn this section, we will work with the\nNHANES\ndataset, which comes from the US National Health and Nutrition Examination Survey.\nThe dataset includes demographic, examination, and laboratory data collected from thousands of individuals.\nSince the full dataset is quite large, we will focus only on a subset of variables that are relevant for preprocessing examples.\nHere are the variables we will use:\nID\n: Unique identifier for each participant\nAge\n: Age of the participant\nGender\n: Biological sex (male or female)\nBMI\n: Body Mass Index\nBPSysAve\n: Average systolic blood pressure\nDiabetes\n: Whether the participant has been diagnosed with diabetes\nBefore diving into preprocessing, let’s take a quick look at the structure of these selected variables:\nlibrary(NHANES)\nlibrary(dplyr)\n\ndata(\"NHANES\")\n\n# Select relevant variables\nnhanes_sub <- NHANES |> \n  select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\nglimpse(nhanes_sub)\nRows: 10,000\nColumns: 6\n$ ID       <int> 51624, 51624, 51624, 51625, 51630, 51638, 51646, 51647, 51647…\n$ Age      <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, …\n$ Gender   <fct> male, male, male, male, female, male, male, female, female, f…\n$ BMI      <dbl> 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 27.24…\n$ BPSysAve <int> 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, 104, 134…\n$ Diabetes <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n3\nWhy Missingness Matters\nMissing data is not just an inconvenience — it can distort the statistical conclusions we draw from a dataset.\nThere are several critical reasons why handling missingness properly is essential:\nBiased results\n: If the missing values are not random, analyses may systematically misrepresent the population.\nReduced sample size\n: Complete-case analysis (simply dropping missing rows) reduces data availability, weakening statistical power.\nModel incompatibility\n: Many modeling techniques in R (e.g.,\nlm()\n,\nglm()\n) require complete data, and will automatically drop cases with missing values, sometimes silently.\n3.1\nA Short Case Example: BMI Missingness and Blood Pressure\nSuppose we want to explore how\nBody Mass Index (BMI)\nrelates to\nSystolic Blood Pressure (BPSysAve)\n.\nHowever, BMI contains missing values. If we ignore them and only analyze complete cases, we may end up with biased conclusions.\n# How many missing in BMI?\nsum(is.na(nhanes_sub$BMI))\n[1] 366\n# Complete-case dataset (dropping missing BMI)\nnhanes_complete <- nhanes_sub |> \n  filter(!is.na(BMI))\n\n# Compare sample sizes\nnrow(nhanes_sub)     # original sample size\n[1] 10000\nnrow(nhanes_complete) # after dropping missing BMI\n[1] 9634\nWe see that a substantial portion of the data is dropped when we remove missing BMI values. This reduction not only decreases efficiency but can also\nbias the estimates\nif those missing values are not randomly distributed.\n# Fit regression with complete cases only\nmodel_complete <- lm(BPSysAve ~ BMI + Age + Gender, data = nhanes_complete)\nsummary(model_complete)\nCall:\nlm(formula = BPSysAve ~ BMI + Age + Gender, data = nhanes_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.281  -8.652  -0.955   7.560 102.790 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 90.016503   0.677618  132.84   <2e-16 ***\nBMI          0.328076   0.023228   14.12   <2e-16 ***\nAge          0.412758   0.008076   51.11   <2e-16 ***\nGendermale   4.346847   0.313476   13.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.43 on 8483 degrees of freedom\n  (1147 observations deleted due to missingness)\nMultiple R-squared:  0.2969,    Adjusted R-squared:  0.2966 \nF-statistic:  1194 on 3 and 8483 DF,  p-value: < 2.2e-16\nInterpretation\n:\nThe model only uses complete cases, ignoring potentially informative missingness.\nIf BMI is more often missing in certain subgroups (e.g., older adults or females), then the relationship estimated here does not represent the whole population.\nIn later sections, we will see how different imputation strategies can mitigate this problem.\n4\nMissing Data Mechanisms\nOne of the most crucial aspects of handling missing data is to understand\nwhy\nthe data are missing.\nThe mechanism behind missingness determines whether our chosen method will yield unbiased and efficient estimates.\n4.1\nTypes of Missing Data Mechanisms\nMCAR (Missing Completely At Random)\nThe probability of a value being missing does not depend on either the observed or the unobserved data.\n→ Example: A lab machine randomly fails for some patients, regardless of their characteristics.\n→ Implication: Complete-case analysis is valid (though less efficient).\nMAR (Missing At Random)\nThe probability of missingness depends only on the\nobserved\ndata, not on the missing values themselves.\n→ Example: People with lower income are less likely to report their weight, but we observe income.\n→ Implication: Multiple imputation or likelihood-based methods can recover unbiased estimates.\nMNAR (Missing Not At Random)\nThe probability of missingness depends on the\nunobserved\nvalue itself.\n→ Example: People with higher BMI are less likely to report their weight.\n→ Implication: Strong assumptions or external information are needed; imputation under MAR will still be biased.\n4.2\nWhat each mechanism implies (with NHANES intuition)\nMCAR\n— e.g., random device failure that occasionally prevents recording\nBMI\n.\nImplication:\nComplete-case analysis (dropping rows) is unbiased but wastes data.\nMAR\n— e.g.,\nBMI\nmissingness varies by observed\nAge\nor\nGender\n.\nImplication:\nLikelihood-based methods or\nMultiple Imputation (MI)\nare valid if those predictors are in the imputation model.\nMNAR\n— e.g., people with\nvery high BMI\nsystematically do not report it.\nImplication:\nMAR-based methods still biased; requires\nsensitivity analysis\nor explicit MNAR models.\n4.3\nQuick NHANES checks that suggest a mechanism\nBelow we do two simple diagnostics on our working subset\nnhanes_sub\n(defined earlier as:\nNHANES |> select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n).\n# Packages we already use\nlibrary(dplyr)\nlibrary(knitr)\n\n# 1) Overall BMI missingness\nnhanes_sub |>\n  summarise(pct_missing_BMI = mean(is.na(BMI)) * 100) |>\n  mutate(pct_missing_BMI = round(pct_missing_BMI, 1)) |>\n  kable(caption = \"Overall BMI missingness (%)\")\nOverall BMI missingness (%)\npct_missing_BMI\n3.7\n# 2) Does BMI missingness vary by observed variables? (MAR hint)\n#    - By Gender\nby_gender <- nhanes_sub |>\n  group_by(Gender) |>\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |>\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n#    - By Age groups (bins)\nby_age <- nhanes_sub |>\n  mutate(AgeBand = cut(Age, breaks = c(0, 30, 45, 60, Inf),\n                       labels = c(\"<=30\", \"31–45\", \"46–60\", \"60+\"),\n                       right = FALSE)) |>\n  group_by(AgeBand) |>\n  summarise(pct_miss_BMI = mean(is.na(BMI)) * 100,\n            n = n(), .groups = \"drop\") |>\n  mutate(pct_miss_BMI = round(pct_miss_BMI, 1))\n\n# Show summaries nicely\nkable(by_gender, caption = \"BMI missingness by Gender (%)\")\nBMI missingness by Gender (%)\nGender\npct_miss_BMI\nn\nfemale\n3.6\n5020\nmale\n3.8\n4980\nkable(by_age,    caption = \"BMI missingness by Age band (%)\")\nBMI missingness by Age band (%)\nAgeBand\npct_miss_BMI\nn\n<=30\n7.6\n4121\n31–45\n0.5\n2049\n46–60\n0.6\n1991\n60+\n1.7\n1839\nInterpretation:\nIf\npct_miss_BMI\nis\nsimilar across groups\n, MCAR is more plausible.\nIf missingness\nchanges with Age or Gender\n,\nMAR\nis more plausible (we must include those predictors in imputation).\nThese are\nindicators\n, not proofs; true\nMNAR\nneeds external info or sensitivity analyses.\nWhich methods are valid under which mechanism?\nMechanism\nExample (NHANES context)\nValid methods\nNotes\nMCAR\nRandom loss of\nBMI\nrecords\nComplete-case, single imputation, MI\nUnbiased but may waste data\nMAR\nBMI\nmissingness varies by observed\nAge\n,\nGender\nMultiple Imputation (MICE)\n, likelihood/EM, missForest\nInclude strong predictors of missingness\nMNAR\nPeople with very high\nBMI\nhide it\nSensitivity analysis, selection/pattern-mixture models\nMAR-based MI alone is biased\nOptional: Little’s MCAR Test\nLittle’s MCAR test is a statistical procedure used to examine whether data are\nMissing Completely at Random (MCAR)\n.\n⚠️ However, this test comes with important caveats:\n- It can be overly sensitive in\nlarge samples\n, flagging trivial deviations.\n- In\nsmall samples\n, its power is often too low to detect meaningful departures from MCAR.\nBecause of these limitations, it should be treated only as a\nsupporting tool\nrather than a definitive test when diagnosing missingness mechanisms.\n5\nDetecting Missing Data\nBefore applying any imputation or modeling technique, it is essential to explore the\nextent and structure of missingness\nin the dataset. The\nnhanes_sub\ndata frame, derived from the NHANES dataset, will be used for illustration.\n5.1\nSimple Counts and Summaries\nThe first step is to quantify how many values are missing per variable.\n# Count missing values for each variable\nnhanes_sub %>%\n  summarise(across(everything(), ~ sum(is.na(.))))\n# A tibble: 1 × 6\n     ID   Age Gender   BMI BPSysAve Diabetes\n  <int> <int>  <int> <int>    <int>    <int>\n1     0     0      0   366     1449      142\nThe output shows the number of missing values in each column, making it easy to spot problematic variables. Another quick check is to identify how many\ncomplete vs. incomplete cases\nexist:\nsum(complete.cases(nhanes_sub))       # number of complete rows\n[1] 8482\nsum(!complete.cases(nhanes_sub))      # number of incomplete rows\n[1] 1518\nThis gives us an idea of the proportion of observations that would be lost if we opted for\nlistwise deletion\n.\n5.2\nVisualizing Missingness\nTextual summaries are informative, but missing data often has\npatterns\nthat are better revealed visually. Several R packages support this task:\n5.2.1\nnaniar\nlibrary(naniar)\nlibrary(ggplot2)\n\n# Visualize missing values by variable\ngg_miss_var(nhanes_sub, show_pct = TRUE) +\n  labs(title = \"Missing Values by Variable in NHANES Subset\",\n       x = \"Variables\",\n       y = \"Proportion of Missing Values\")\nEach bar corresponds to a variable.\nThe\nheight of the bar\nshows how many observations are missing for that variable.\nWith\nshow_pct = TRUE\n, the proportion of missing values is also displayed, making it easier to compare across variables.\nVariables with tall bars clearly have higher missingness (e.g., BMI or blood pressure variables often stand out in this dataset).\n5.2.2\nVIM\nlibrary(VIM)\n\naggr(nhanes_sub, numbers = TRUE, prop = FALSE, sortVar = TRUE)\nVariables sorted by number of missings: \n Variable Count\n BPSysAve  1449\n      BMI   366\n Diabetes   142\n       ID     0\n      Age     0\n   Gender     0\nThis aggregated visualization shows the proportion of missing values per variable and the combinations of missingness across variables.\n5.2.3\nvisdat\nlibrary(visdat)\n\nvis_dat(nhanes_sub)\nThis function displays the data type of each variable and overlays missingness, helping to identify whether missing values cluster in certain variable types (e.g., numeric vs. categorical).\n5.3\nInterpreting the Patterns\nRandom scatter of missing values\nacross rows/columns may indicate\nMCAR\n(though formal testing is required).\nSystematic patterns\n(e.g., older participants more likely to have missing BMI) hint at\nMAR\n.\nBlocks of missingness\n(entire variables missing for subgroups) may suggest\nMNAR\nor structural missingness.\n6\nHandling Missing Data — Methods\nIn this section we review the main families of methods, show\nwhen\neach is appropriate, and demonstrate them on\nnhanes_sub\n. We will explicitly call out the\ntrade-offs\nso readers can choose deliberately—not by habit.\n6.1\nDeletion\nListwise deletion (complete-case)\nremoves any row that contains\nat least one\nmissing value.\nPairwise deletion\nuses all available pairs to compute correlations/covariances, which can later lead to\nnon–positive-definite\ncovariance matrices and failures in modeling.\nPros\n- Simple; widely implemented by default (often silently). - Unbiased\nonly\nunder\nMCAR\n.\nCons\n- Wastes data; reduces power. - Biased under\nMAR/MNAR\n; can change the sample composition.\n# How many rows would we lose if we required complete cases for these variables?\nn_total <- nrow(nhanes_sub)\nn_cc    <- nhanes_sub |> stats::complete.cases() |> sum()\n\ncbind(\n  total_rows    = n_total,\n  complete_cases= n_cc,\n  lost_rows     = n_total - n_cc,\n  lost_pct      = round((n_total - n_cc) / n_total * 100, 1)\n)\ntotal_rows complete_cases lost_rows lost_pct\n[1,]      10000           8482      1518     15.2\nInterpretation:\nIf the lost percentage is non-trivial (e.g., >5–10%), listwise deletion both\nshrinks power\nand\nrisks bias\nunless MCAR truly holds. Pairwise deletion is\nnot recommended\nfor modeling because it can yield inconsistent covariance structures.\n6.2\nSimple Imputation\nIdea.\nFill missing values with a single plausible value (one pass). Fast and convenient, but it\nunderestimates uncertainty\n(standard errors too small) and can\ndistort distributions\n.\nTypical choices\nMean/Median/Mode\n(baselines; median is more robust to skew)\nk-Nearest Neighbors (kNN)\n(borrows information from similar rows)\nHot-deck\n(donor-based; similar spirit to kNN)\n6.2.1\nMedian (numeric) + Mode (categorical) baselines\nset.seed(2025)\n\n# Create a median-imputed BMI for illustration (only if BMI is missing)\nnh_med <- nhanes_sub |>\n  mutate(\n    BMI_med = ifelse(is.na(BMI), stats::median(BMI, na.rm = TRUE), BMI)\n  )\n\n# Compare how many BMI were imputed\nsum(is.na(nhanes_sub$BMI))           # original missing BMI count\n[1] 366\nsum(is.na(nh_med$BMI_med))           # should be 0\n[1] 0\nDistribution distortion (variance shrinkage).\nlibrary(ggplot2)\n\n# Compare BMI distribution: complete-case vs median-imputed\np_cc  <- nhanes_sub |>\n  filter(!is.na(BMI)) |>\n  ggplot(aes(x = BMI)) +\n  geom_density() +\n  labs(title = \"BMI density — complete cases\")\n\np_med <- nh_med |>\n  ggplot(aes(x = BMI_med)) +\n  geom_density() +\n  labs(title = \"BMI density — median-imputed\")\n\np_cc; p_med\nInterpretation:\nMedian imputation\nspikes\nthe distribution around the median and\nreduces variance\n. This can attenuate real relationships that depend on dispersion.\n6.2.2\nkNN (donor-based) imputation\n# kNN imputation with VIM::kNN (works on data frames; chooses donors by similarity)\nlibrary(VIM)\n\n# We impute only BMI here; set k=5 as a reasonable starting point.\nnh_knn <- nhanes_sub |>\n  select(Age, Gender, BMI, BPSysAve, Diabetes) |>\n  VIM::kNN(k = 5, imp_var = FALSE)  # imp_var=FALSE avoids extra *_imp columns\n\n# Check imputation effect\nsum(is.na(nhanes_sub$BMI))   # original missing BMI\n[1] 366\nsum(is.na(nh_knn$BMI))       # after kNN (should be 0)\n[1] 0\nInterpretation:\nkNN preserves local structure better than mean/median, but it is still\nsingle imputation\n→ uncertainty is\nnot\npropagated. Choice of\nk\nand included predictors matters.\nRule of thumb.\nSimple methods are acceptable for quick EDA or as baselines. For principled inference under MAR, prefer\nMultiple Imputation\n.\n6.3\nAdvanced Methods\n6.3.1\nMultiple Imputation with\nmice\nSo far, we have seen that missing values exist in several variables of our dataset. A common and powerful approach to handle missingness is\nMultiple Imputation by Chained Equations (MICE)\n. The\nmice\npackage in R is widely used for this purpose. The idea is simple:\nInstead of filling in missing values once, MICE creates\nmultiple complete datasets\nby imputing values several times.\nEach dataset is then analyzed separately.\nFinally, results are pooled together to account for the variability introduced by missingness.\nLet’s try this approach on our subset of the\nNHANES\ndata:\nlibrary(mice)\n\n# Create imputations\nimp <- mice(nhanes_sub, m = 3, seed = 123)\niter imp variable\n  1   1  BMI  BPSysAve  Diabetes\n  1   2  BMI  BPSysAve  Diabetes\n  1   3  BMI  BPSysAve  Diabetes\n  2   1  BMI  BPSysAve  Diabetes\n  2   2  BMI  BPSysAve  Diabetes\n  2   3  BMI  BPSysAve  Diabetes\n  3   1  BMI  BPSysAve  Diabetes\n  3   2  BMI  BPSysAve  Diabetes\n  3   3  BMI  BPSysAve  Diabetes\n  4   1  BMI  BPSysAve  Diabetes\n  4   2  BMI  BPSysAve  Diabetes\n  4   3  BMI  BPSysAve  Diabetes\n  5   1  BMI  BPSysAve  Diabetes\n  5   2  BMI  BPSysAve  Diabetes\n  5   3  BMI  BPSysAve  Diabetes\n# Look at a summary\nimp\nClass: mids\nNumber of multiple imputations:  3 \nImputation methods:\n      ID      Age   Gender      BMI BPSysAve Diabetes \n      \"\"       \"\"       \"\"    \"pmm\"    \"pmm\" \"logreg\" \nPredictorMatrix:\n         ID Age Gender BMI BPSysAve Diabetes\nID        0   1      1   1        1        1\nAge       1   0      1   1        1        1\nGender    1   1      0   1        1        1\nBMI       1   1      1   0        1        1\nBPSysAve  1   1      1   1        0        1\nDiabetes  1   1      1   1        1        0\nThe output shows:\nm = 3\n: number of imputed datasets created.\nFor each variable with missingness, the method used for imputation.\nHow many iterations were performed in the algorithm.\nWe can take a quick look at the imputed values:\n# Inspect first few imputations for BMI\nhead(imp$imp$BMI)\n1     2     3\n61  43.00 17.70 12.90\n161 16.70 13.50 18.59\n210 16.28 24.00 23.10\n309 24.00 37.32 26.20\n310 24.00 28.55 26.30\n320 25.10 32.25 29.20\nThis shows different plausible values for missing BMI observations across the three imputed datasets. Each dataset gives slightly different results, which is expected and important for reflecting uncertainty.\nOnce we have these imputations, we can\ncomplete the dataset\n:\n# Extract the first imputed dataset\nnhanes_completed <- complete(imp, 1)\n\nhead(nhanes_completed)\nID Age Gender   BMI BPSysAve Diabetes\n1 51624  34   male 32.22      113       No\n2 51624  34   male 32.22      113       No\n3 51624  34   male 32.22      113       No\n4 51625   4   male 15.30       92       No\n5 51630  49 female 30.57      112       No\n6 51638   9   male 16.82       86       No\nNow we have a complete dataset with no missing values. In practice, we would analyze all imputed datasets and then combine results using Rubin’s rules, but the key takeaway here is:\nmice()\nprovides multiple versions of the data,\nimputations are based on relationships among variables,\nand the method preserves uncertainty rather than hiding it.\nMICE Essentials: Key Arguments\nmethod\n: Specifies the imputation model for each variable.\npmm\n: predictive mean matching (continuous variables)\nlogreg\n: logistic regression (binary)\npolyreg\n: multinomial regression (nominal categorical)\npolr\n: proportional odds model (ordered categorical)\nRule of thumb\n: If a factor has >2 levels, prefer\npolyreg\n(nominal) or\npolr\n(ordered) instead of\nlogreg\n. Always check the actual levels of variables such as\nGender\nor\nDiabetes\nin your data before setting methods.\npredictorMatrix\n: Controls which variables are used to predict others.\nRows = target variables (to be imputed)\nColumns = predictor variables\nm\n: Number of multiple imputations to generate (commonly 5–20).\nMore imputations recommended for high missingness.\nmaxit\n: Number of iterations of the chained equations (often 5–10).\nseed\n: Random seed for reproducibility.\nAlways set when writing tutorials or reports.\n6.3.2\nMultiple Imputation with missForest\nThe\nmissForest\npackage provides a non-parametric imputation method based on random forests.\nUnlike\nmice\n, which generates multiple imputations,\nmissForest\ncreates a\nsingle completed dataset\nby iteratively predicting missing values using random forest models. It works well with both continuous and categorical variables and can capture nonlinear relationships.\nWe will use the same\nnhanes_sub\ndataset as before:\nlibrary(dplyr)\nlibrary(missForest)\n\n# Start from the existing subset:\n# nhanes_sub <- NHANES |> select(ID, Age, Gender, BMI, BPSysAve, Diabetes)\n\n# 1) Keep only model-relevant columns (drop pure identifier)\n# 2) Convert character variables to factors (missForest expects factors, not raw character)\n# 3) Coerce to base data.frame to avoid tibble-related method dispatch issues\nmf_input <- nhanes_sub |>\n  select(Age, Gender, BMI, BPSysAve, Diabetes) |>\n  mutate(across(where(is.character), as.factor)) |>\n  as.data.frame()\n\nset.seed(123)\nmf_fit <- missForest(\n  mf_input,\n  ntree   = 200,    # more trees -> stabler imputations\n  maxiter = 5,      # outer iterations (default 10; 5 is fine for demo)\n  verbose = FALSE\n)\n\n# Completed data and OOB error\nmf_imputed <- mf_fit$ximp\nmf_oob     <- mf_fit$OOBerror\n\n# Quick checks\nsum(is.na(mf_input$BMI))\n[1] 366\nsum(is.na(mf_imputed$BMI))   # should go to 0\n[1] 0\nmf_oob\nNRMSE        PFC \n0.17890307 0.02667884\nThe function returns a list with two key elements:\nximp\n: the completed dataset after imputation.\nOOBerror\n: the estimated imputation error (normalized root mean squared error for continuous variables and proportion of falsely classified entries for categorical variables).\nInterpretation:\nThe\ncompleted dataset\n(\nximp\n) replaces all missing values with imputed estimates.\nNRMSE (Normalized Root Mean Squared Error):\n0.1789\nThis value reflects the imputation error for continuous variables (e.g.,\nAge\n,\nBMI\n,\nBPSysAve\n).\nSince it is normalized, values closer to\n0\nindicate better accuracy. Here, an error of ~0.18 suggests that the imputed values are quite close to the true (non-missing) values.\nPFC (Proportion of Falsely Classified):\n0.0267\nThis metric evaluates categorical variables (e.g.,\nGender\n,\nDiabetes\n).\nA value of ~0.027 means only about\n2.7% of categorical imputations were misclassified\n, which is a strong performance.\n✅\nInterpretation:\nThe results indicate that\nmissForest\nproduced high-quality imputations: continuous variables are imputed with relatively low error, and categorical variables with very low misclassification. In practical terms, this means the dataset after imputation is reliable and close to the original data distribution.\nPros and Cons of\nmissForest\nAdvantages:\nHandles mixed data types (continuous + categorical).\nCaptures nonlinearities and complex interactions.\nNo need to specify an explicit imputation model.\nLimitations:\nProduces only a\nsingle imputed dataset\n, so uncertainty is not directly quantified (unlike\nmice\n).\nComputationally more expensive for very large datasets.\n7\nSingle vs. Multiple Imputation\nOne critical distinction in handling missing data is\nsingle imputation\nvs.\nmultiple imputation (MI)\n.\nSingle imputation\n(mean, median, regression, etc.) fills each missing value once. While simple, it\nignores uncertainty\n, treating imputed values as if they were observed.\nMultiple imputation\ngenerates\nseveral plausible versions of the dataset\n(e.g., 5–10). Each dataset is analyzed separately, and results are then combined (pooled). This approach accounts for\nvariability due to missingness\nand produces more reliable inferences.\nLet’s illustrate with our\nnhanes_sub\ndataset:\n# Complete-case analysis (ignores missing data)\nlm_cc <- lm(BMI ~ Age + Gender + BPSysAve + Diabetes,\n            data = nhanes_sub, na.action = na.omit)\n\n# Single imputation (mean imputation for BMI)\nnhanes_single <- nhanes_sub |> \n  mutate(BMI = ifelse(is.na(BMI), mean(BMI, na.rm = TRUE), BMI))\n\nlm_si <- lm(BMI ~ Age + Gender + BPSysAve + Diabetes,\n            data = nhanes_single)\n\n# Multiple imputation with mice\nimp <- mice(nhanes_sub, m = 5, method = \"pmm\", seed = 123)\niter imp variable\n  1   1  BMI  BPSysAve  Diabetes\n  1   2  BMI  BPSysAve  Diabetes\n  1   3  BMI  BPSysAve  Diabetes\n  1   4  BMI  BPSysAve  Diabetes\n  1   5  BMI  BPSysAve  Diabetes\n  2   1  BMI  BPSysAve  Diabetes\n  2   2  BMI  BPSysAve  Diabetes\n  2   3  BMI  BPSysAve  Diabetes\n  2   4  BMI  BPSysAve  Diabetes\n  2   5  BMI  BPSysAve  Diabetes\n  3   1  BMI  BPSysAve  Diabetes\n  3   2  BMI  BPSysAve  Diabetes\n  3   3  BMI  BPSysAve  Diabetes\n  3   4  BMI  BPSysAve  Diabetes\n  3   5  BMI  BPSysAve  Diabetes\n  4   1  BMI  BPSysAve  Diabetes\n  4   2  BMI  BPSysAve  Diabetes\n  4   3  BMI  BPSysAve  Diabetes\n  4   4  BMI  BPSysAve  Diabetes\n  4   5  BMI  BPSysAve  Diabetes\n  5   1  BMI  BPSysAve  Diabetes\n  5   2  BMI  BPSysAve  Diabetes\n  5   3  BMI  BPSysAve  Diabetes\n  5   4  BMI  BPSysAve  Diabetes\n  5   5  BMI  BPSysAve  Diabetes\nlm_mi <- with(imp, lm(BMI ~ Age + Gender + BPSysAve + Diabetes))\npooled <- pool(lm_mi)\nsummary(lm_cc)\nCall:\nlm(formula = BMI ~ Age + Gender + BPSysAve + Diabetes, data = nhanes_sub, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.771  -4.640  -1.053   3.553  53.003 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.488597   0.513225  34.076   <2e-16 ***\nAge          0.047930   0.004273  11.217   <2e-16 ***\nGendermale  -0.278756   0.144799  -1.925   0.0542 .  \nBPSysAve     0.067504   0.004903  13.767   <2e-16 ***\nDiabetesYes  3.789606   0.265240  14.287   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.588 on 8477 degrees of freedom\n  (1518 observations deleted due to missingness)\nMultiple R-squared:  0.1136,    Adjusted R-squared:  0.1132 \nF-statistic: 271.5 on 4 and 8477 DF,  p-value: < 2.2e-16\nsummary(lm_si)\nCall:\nlm(formula = BMI ~ Age + Gender + BPSysAve + Diabetes, data = nhanes_single)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.657  -4.634  -1.029   3.533  53.004 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.600406   0.508667  34.601   <2e-16 ***\nAge          0.047159   0.004237  11.129   <2e-16 ***\nGendermale  -0.287221   0.143820  -1.997   0.0458 *  \nBPSysAve     0.066791   0.004858  13.748   <2e-16 ***\nDiabetesYes  3.725941   0.262781  14.179   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.57 on 8541 degrees of freedom\n  (1454 observations deleted due to missingness)\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1114 \nF-statistic: 268.8 on 4 and 8541 DF,  p-value: < 2.2e-16\nsummary(pooled)\nterm    estimate   std.error statistic       df       p.value\n1 (Intercept) 14.66559186 0.487686304 30.071773 1253.210 5.075329e-150\n2         Age  0.10367759 0.003748806 27.656164 3916.798 6.073157e-154\n3  Gendermale -0.30521870 0.134707691 -2.265785 3727.382  2.352165e-02\n4    BPSysAve  0.06754332 0.004761820 14.184349 1160.000  3.121286e-42\n5 DiabetesYes  3.23549109 0.260299313 12.429887 8866.664  3.529334e-35\nWe applied three different approaches to handle missing BMI values in the\nnhanes_sub\ndataset, modeling\nBMI ~ Age + Gender + BPSysAve + Diabetes\n. Here is what we found:\n1. Complete-Case Analysis (CCA)\nWhat we did:\nWe dropped all observations with missing values (\nna.omit\n).\nResult:\nCoefficients:\nAge (0.048), BPSysAve (0.068), DiabetesYes (+3.79), Gender slightly negative.\nStandard errors:\nRelatively large because ~1500 observations were discarded.\nR²:\n0.114 — fairly low.\nTakeaway:\nCCA wastes data and may bias estimates if missingness is not MCAR (Missing Completely at Random).\n2. Single Imputation (Mean Substitution for BMI)\nWhat we did:\nReplaced missing BMI values with the mean BMI.\nResult:\nCoefficients:\nVery close to CCA (Age 0.047, BPSysAve 0.067, DiabetesYes +3.73).\nGender\neffect became just significant (\np\n= 0.045).\nResidual SE\ndecreased slightly (6.57).\nTakeaway:\nLooks “better” because all observations are retained, but this approach\nignores imputation uncertainty\nand artificially stabilizes estimates. Standard errors are underestimated, leading to overconfidence.\n3. Multiple Imputation (MI with\nmice\n, m = 5, method = “pmm”)\nWhat we did:\nGenerated 5 imputed datasets using Predictive Mean Matching (PMM), fit the same model in each, and pooled results.\nResult:\nCoefficients:\nAge effect doubled (0.104), intercept dropped (14.7 vs. ~17.5), Diabetes effect slightly smaller (+3.24), Gender effect remained modest but significant (\np\n= 0.023).\nStandard errors:\nProperly adjusted upwards — reflecting real uncertainty in imputed BMI values.\nInference:\nDespite differences in point estimates, the conclusions are more\nstatistically honest\n.\nTakeaway:\nMI balances efficiency (uses all data) and validity (acknowledges missingness uncertainty).\n🔑 Overall Comparison\nMethod\nKeeps All Data\nCoefficients Similar?\nSE Adjusted for Uncertainty?\nMain Issue\nComplete Case (CCA)\n❌ (~1500 rows lost)\nYes, but less precise\n✅ (but biased if MAR/MNAR)\nData loss, possible bias\nSingle Imputation (SI)\n✅\nSimilar to CCA\n❌ Underestimated\nOverconfident inference\nMultiple Imputation (MI)\n✅\nSomewhat different (esp. Age)\n✅ Properly adjusted\nMore computation needed\nInterpretation:\nComplete-case\ndrops too much data and risks bias.\nSingle imputation\nkeeps the data but gives\ntoo much confidence\nin results.\nMultiple imputation\nchanges some coefficients (notably Age) and reports more realistic uncertaint\n👉\nLesson:\nIf your goal is valid inference, especially in epidemiological or social science settings,\nmultiple imputation is the gold standard\n.\n8\nComparison of Common Imputation Methods\nMethod\nDescription\nAdvantages\nDisadvantages\nListwise Deletion\nRemoves all observations containing missing values\nVery simple, quick to implement\nSubstantial data loss, potential bias\nMean / Median / Mode\nReplaces missing values with a fixed statistic\nEasy to apply, preserves sample size\nReduces variance, distorts relationships\nLOCF (Last Observation Carried Forward)\nUses the last available value (mainly time series)\nUseful in longitudinal data, preserves continuity\nIgnores trends, underestimates variability\nLinear Interpolation\nEstimates missing values by connecting known data points\nMaintains trends, intuitive\nFails with sudden changes or nonlinear patterns\nKNN Imputation\nPredicts missing values using nearest neighbors\nPreserves multivariate structure, flexible\nComputationally expensive, sensitive to k choice\nMICE (Multiple Imputation by Chained Equations)\nIterative regression-based multiple imputation\nAccounts for uncertainty, widely used in research\nTime-consuming, requires expertise\nmissForest\nUses Random Forest to impute missing values\nHandles nonlinearities and interactions\nBlack-box method, computationally intensive\nEM Algorithm\nIterative expectation-maximization for likelihood-based estimation\nStatistically principled, robust in theory\nRequires strong assumptions, advanced knowledge\nNo single imputation method is universally optimal—each comes with trade-offs between simplicity, accuracy, and interpretability. For instance,\nlistwise deletion\nis tempting for its ease but can heavily bias results if missingness is not random. Simple\nmean or median imputation\nkeeps the dataset intact but artificially reduces variability and masks true correlations. More advanced techniques such as\nMICE, missForest, and EM\nprovide statistically sound imputations that preserve uncertainty and relationships, but they demand more computational resources and methodological expertise.\nIn practice:\nExploratory analysis\noften starts with simple methods (e.g., median replacement) to get a sense of the data.\nTime series data\nmay rely on\nLOCF or interpolation\n.\nComplex survey or clinical datasets\ntypically benefit from advanced approaches like\nMICE\nor\nmissForest\n, which better respect the multivariate nature of the data.\nUltimately, the choice depends on the\ndata structure, missingness mechanism (MCAR, MAR, MNAR), and analytical goals\n.\n9\nConclusion\nThere is\nno one-size-fits-all\nsolution for missing data. The right approach depends on your\ngoal (prediction vs. inference)\n, the\nmissingness mechanism (MCAR/MAR/MNAR)\n, your\ndata structure\n(cross-sectional vs. longitudinal), and\npractical constraints\n(time, compute, expertise).\n9.1\nWhat our NHANES walkthrough showed\nComplete-case analysis\nis simple but wastes data and can bias results unless MCAR is plausible.\nSingle imputation\n(mean/median, kNN, missForest run once) keeps all rows but\nunderestimates uncertainty\n, yielding overconfident inferences.\nMultiple imputation (MICE)\ntypically strikes the best balance for\ninference under MAR\n: it preserves multivariate structure and\npropagates uncertainty\n(via pooling), producing more honest standard errors and CIs.\nNonparametric imputers\nlike\nmissForest\nare strong for\npredictive accuracy\non complex, nonlinear structure, but they do\nnot\ncapture imputation uncertainty by themselves.\n9.2\nPractical guidance (decision-oriented)\nIf your main task is prediction\nand interpretability is secondary → a good single-imputation engine (e.g.,\nmissForest\n) can be effective, with careful validation.\nIf your main task is inference\n(effect sizes, CIs, p-values) and\nMAR is reasonable\n→ prefer\nMICE\n; include strong predictors of both the outcome and missingness; check diagnostics.\nIf you suspect MNAR\n→ acknowledge this explicitly and consider\nsensitivity analyses\n(pattern-mixture/selection models) rather than assuming MAR.\n9.3\nReporting checklist (make your analysis reproducible & credible)\n% missing\nby variable\nand\nby key subgroups\n(e.g., Age, Gender).\nYour\nassumed mechanism\n(MCAR/MAR/MNAR) and why it’s plausible.\nThe\nmethod(s)\nused (e.g., MICE with\npmm\n,\nm\n,\nmaxit\n,\npredictorMatrix\n; or missForest with\nntree\n,\nmaxiter\n).\nDiagnostics\n(trace/density/strip plots for MICE; OOB error for missForest).\nFor MI:\npooled estimates\nwith standard errors/intervals; clarify how pooling was performed.\nLimitations\n(e.g., potential MNAR, model misspecification, small-sample caveats).\nTip\nRule of thumb.\nUse simple methods for quick EDA; use\nMICE\nfor publication-grade inference under MAR; use\nmissForest\nwhen you primarily need strong\npredictive performance\non mixed/complex data.\n9.4\nCommon pitfalls to avoid\nTreating imputed values as if they were observed “truth” (single imputation + significance testing).\nImputing the\noutcome\nitself (generally avoid; let it\ninform\npredictor imputations instead).\nIgnoring\nleakage\n: fit imputers\nwithin\nresampling folds/splits, not on the full data.\nOmitting key covariates that explain missingness (weakens the MAR assumption and the imputer).\n9.5\nWhere to go next\nLeakage-free pipelines\nwith\ntidymodels::recipes\n(train/test split done right).\nSensitivity analyses\nfor MNAR.\nRobustness checks\n(alternative imputation models, different\nm\n, predictor sets).\nBottom line:\nChoose methods intentionally,\njustify assumptions\n, show diagnostics, and\nreport pooled results\nwhen using MI. Good missing-data practice is less about one magic function and more about transparent, principled workflow.\n10\nReferences\nAllison, P. D. (2001).\nMissing Data\n. Sage Publications.\nEnders, C. K. (2010).\nApplied Missing Data Analysis\n. The Guilford Press.\nLittle, R. J. A., & Rubin, D. B. (2002).\nStatistical Analysis with Missing Data\n(2nd ed.). Wiley.\nvan Buuren, S. (2018).\nFlexible Imputation of Missing Data\n(2nd ed.). Chapman & Hall/CRC.\nStekhoven, D. J., & Bühlmann, P. (2012). “MissForest—Nonparametric Missing Value Imputation for Mixed-Type Data.”\nBioinformatics\n, 28(1), 112–118. https://doi.org/10.1093/bioinformatics/btr597\nRubin, D. B. (1987).\nMultiple Imputation for Nonresponse in Surveys\n. Wiley.\nSchafer, J. L. (1997).\nAnalysis of Incomplete Multivariate Data\n. Chapman & Hall/CRC.\nR Documentation:\nmice\npackage\nR Documentation:\nmissForest\npackage\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nA Statistician's R Notebook\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "1 Introduction Missing data is one of the most common challenges in data analysis and statistical modeling. Whether the data originates from surveys, administrative registers, or clinical trials, it is almost inevitable that some values are abs...",
      "meta_keywords": null,
      "og_description": "1 Introduction Missing data is one of the most common challenges in data analysis and statistical modeling. Whether the data originates from surveys, administrative registers, or clinical trials, it is almost inevitable that some values are abs...",
      "og_image": "https://mfatihtuzen.netlify.app/posts/2025-08-18_missing_values/index_files/figure-html/unnamed-chunk-7-1.png",
      "og_title": "Handling Missing Data in R: A Comprehensive Guide | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 27.7,
      "sitemap_lastmod": null,
      "twitter_description": "1 Introduction Missing data is one of the most common challenges in data analysis and statistical modeling. Whether the data originates from surveys, administrative registers, or clinical trials, it is almost inevitable that some values are abs...",
      "twitter_title": "Handling Missing Data in R: A Comprehensive Guide | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/08/handling-missing-data-in-r-a-comprehensive-guide/",
      "word_count": 5539
    }
  }
}