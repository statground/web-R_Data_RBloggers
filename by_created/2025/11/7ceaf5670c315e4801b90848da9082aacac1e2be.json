{
  "id": "7ceaf5670c315e4801b90848da9082aacac1e2be",
  "url": "https://www.r-bloggers.com/2009/09/polynomial-regression-techniques/",
  "created_at_utc": "2025-11-17T20:40:25Z",
  "data": null,
  "raw_original": {
    "uuid": "205bd02b-3837-4384-ae18-1b16b669c83e",
    "created_at": "2025-11-17 20:40:25",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2009/09/polynomial-regression-techniques/",
      "crawled_at": "2025-11-17T10:26:29.892690",
      "external_links": [
        {
          "href": "https://statistic-on-air.blogspot.com/2009/09/polynomial-regression-techniques.html",
          "text": "Statistic on aiR"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.jpg",
          "text": null
        },
        {
          "href": "https://i2.wp.com/img245.imageshack.us/img245/9274/pol2d.jpg",
          "text": null
        },
        {
          "href": "https://i1.wp.com/img269.imageshack.us/img269/6797/pol3k.jpg",
          "text": null
        },
        {
          "href": "https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.jpg",
          "text": null
        },
        {
          "href": "https://i1.wp.com/img269.imageshack.us/img269/261/pol5e.jpg",
          "text": null
        },
        {
          "href": "https://statistic-on-air.blogspot.com/2009/09/polynomial-regression-techniques.html",
          "text": "Statistic on aiR"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Polynomial regression techniques | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/img245.imageshack.us/img245/9274/pol2d.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/img245.imageshack.us/img245/9274/pol2d.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/img269.imageshack.us/img269/6797/pol3k.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/img269.imageshack.us/img269/6797/pol3k.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/img269.imageshack.us/img269/261/pol5e.th.jpg?w=578"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/img269.imageshack.us/img269/261/pol5e.th.jpg?w=578"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/todos-logos/",
          "text": "Todos Logos"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-612 post type-post status-publish format-standard hentry\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Polynomial regression techniques</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">September 5, 2009</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/todos-logos/\">Todos Logos</a></span>  in Uncategorized | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<p class=\"syndicated-attribution\"><!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://statistic-on-air.blogspot.com/2009/09/polynomial-regression-techniques.html\"> Statistic on aiR</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div></p>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.0-->Suppose we want to create a polynomial that can approximate better the following dataset on the population of a certain Italian city over 10 years. The table summarizes the data:<br/><br/>$$\\begin{tabular}{|1|1|}\\hline Year &amp; Population\\\\ \\hline 1959&amp;4835\\\\ 1960&amp;4970\\\\ 1961&amp;5085\\\\ 1962&amp;5160\\\\ 1963&amp;5310\\\\ 1964&amp;5260\\\\ 1965&amp;5235\\\\ 1966&amp;5255\\\\ 1967&amp;5235\\\\ 1968&amp;5210\\\\ 1969&amp;5175\\\\ \\hline \\end{tabular}$$<br/><br/><span class=\"fullpost\">First we import the data into R:<br/><br/><pre>\nYear &lt;- c(1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969)\nPopulation &lt;- c(4835, 4970, 5085, 5160, 5310, 5260, 5235, 5255, 5235, 5210, 5175)</pre><br/><br/>Now we create the dataframe named sample1:<br/><br/><pre>\nsample1 &lt;- data.frame(Year, Population)\nsample1\n\n   Year Population\n1  1959       4835\n2  1960       4970\n3  1961       5085\n4  1962       5160\n5  1963       5310\n6  1964       5260\n7  1965       5235\n8  1966       5255\n9  1967       5235\n10 1968       5210\n11 1969       5175</pre><br/><br/>At this point may be useful to chart these values, to observe the trend and take an idea of the final polynomial function. For convenience we modify the column <code>Year</code>, creating a neighborhood of zero, thus:<br/><br/><pre>\nsample1$Year &lt;- sample1$Year - 1964\nsample1\n\n   Year Population\n1    -5       4835\n2    -4       4970\n3    -3       5085\n4    -2       5160\n5    -1       5310\n6     0       5260\n7     1       5235\n8     2       5255\n9     3       5235\n10    4       5210\n11    5       5175</pre><br/><br/>Put the values on a chart<br/><br/><pre>\nplot(sample1$Year, sample1$Population, type=\"b\")</pre><br/><br/><center><a href=\"https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.jpg\" rel=\"nofollow\" target=\"_blank\"><img border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.th.jpg?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.th.jpg?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img border=\"0\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/img137.imageshack.us/img137/7946/pol1.th.jpg?w=578\"/></noscript></a></center><br/><br/>At this point we can start with the search for a polynomial model that adequately approximates our data. First, we specify that we want a polynomial function of X, ie a <em>raw polynomial</em> , is different from the <em>orthogonal polynomial</em>. This is an important addition because the controls and the results will change in the two cases R. So we want a function of X like:<br/><br/>$$f(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+ ... +\\beta_nx^n$$<br/><br/>At what degree of the polynomial stop? Depends on the degree of precision that we seek. The greater the degree of the polynomial, the greater the accuracy of the model, but the greater the difficulty in calculating; we must also verify the significance of coefficients that are found. But let's get straight to the point.<br/><br/>In R for <b>fitting a polynomial regression model</b> (not orthogonal), there are two methods, among them identical. Suppose we seek the values of beta coefficients for a polynomial of degree 1, then 2nd degree, and 3rd degree:<br/><br/><pre>\nfit1 &lt;- lm(sample1$Population ~ sample1$Year)\nfit2 &lt;- lm(sample1$Population ~ sample1$Year + I(sample1$Year^2))\nfit3 &lt;- lm(sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3))</pre><br/><br/>Or we can write more quickly, for polynomials of degree 2 and 3:<br/><br/><pre>\nfit2b &lt;- lm(sample1$Population ~ poly(sample1$Year, 2, raw=TRUE))\nfit3b &lt;- lm(sample1$Population ~ poly(sample1$Year, 3, raw=TRUE))</pre><br/><br/>The function <code>poly</code> is useful if you want to get a polynomial of high degree, because it avoids explicitly write the formula. If we specify <code>raw=TRUE</code>, the two methods provide the same output, but if we do not specify <code>raw=TRUE</code> (or rgb(153, 0, 0);\"&gt;raw=F), the function <code>poly</code> give us the values of the beta parameters of an orthogonal polynomials, which is different from the general formula I wrote above, although the models are both effective.<br/><br/>Let's look at the output. <br/><br/><pre>\nsummary(fit2)\n## or summary(fit2b)\n\nCall:\nlm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.888 -18.834  -3.159   2.040  86.748 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5263.159     17.655 298.110  &lt; 2e-16 ***\nsample1$Year        29.318      3.696   7.933 4.64e-05 ***\nI(sample1$Year^2)  -10.589      1.323  -8.002 4.36e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\nResidual standard error: 38.76 on 8 degrees of freedom\nMultiple R-squared: 0.9407,     Adjusted R-squared: 0.9259 \nF-statistic: 63.48 on 2 and 8 DF,  p-value: 1.235e-05 </pre><br/><br/>The output of <code>summary(fit2b)</code> is the same. We obtained the values of beta0 (5263,159), beta1 (29,318) and beta2 (-10,589), which appear to be significant AII 3. The equation of polynomial of degree 2 of our model is:<br/><br/>$$f(x)=5263.1597+29.318x-10.589x^2$$<br/><br/>If we want a polynomial of 3rd degree, we have:<br/><br/><pre>\nsummary(fit3)\n## of summary(fit3b)\n\nCall:\nlm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2) + \n    I(sample1$Year^3))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.774 -14.802  -1.253   3.199  72.634 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5263.1585    15.0667 349.324 4.16e-16 ***\nsample1$Year        14.3638     8.1282   1.767   0.1205    \nI(sample1$Year^2)  -10.5886     1.1293  -9.376 3.27e-05 ***\nI(sample1$Year^3)    0.8401     0.4209   1.996   0.0861 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\nResidual standard error: 33.08 on 7 degrees of freedom\nMultiple R-squared: 0.9622,     Adjusted R-squared: 0.946 \nF-statistic: 59.44 on 3 and 7 DF,  p-value: 2.403e-05 </pre><br/><br/>The equation is:<br/><br/>$$f(x)=5263.1585+14.3638x-10.5886x^2+0.8401x^3$$<br/><br/>In the latter case, however, the coefficients beta1 and beta3 are not significant, then the best model is the polynomial of 2nd degree. Furthermore look at the Multiple R-squared: in the 2nd degree model it is 94.07%, while in the 3rd degree model it is 96.22%. It seems that there has been an increase in accuracy of the model, but it is a significant increase? We can compare the two model with an ANOVA table:<br/><br/><pre>\nanova(fit2, fit3)\n\nAnalysis of Variance Table\n\nModel 1: sample1$Population ~ sample1$Year + I(sample1$Year^2)\nModel 2: sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3)\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)  \n1      8 12019.8                             \n2      7  7659.5  1    4360.3 3.9848 0.0861 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 </pre><br/><br/>Since the p-value is greater than 0.05, we accept the null hypothesis: there wasn't a significant improvement of the model.<br/><br/>The biggest problem now is to represent graphically the result. In fact, R does not exist (as far as I know) a function for plotting polynomials found. We must therefore proceed with graphic artifacts still valid, but somewhat laborious.<br/><br/>First, we plotted the values, with the command seen before. This time only display the lines and not points, for convenience graphics:<br/><br/><pre>\nplot(sample1$Year, sample1$Population, type=\"l\", lwd=3)</pre><br/><br/>Now add to this chart the progress of the 2nd degree polynomial, in this way:<br/><br/><pre>\npoints(sample1$Year, predict(fit2), type=\"l\", col=\"red\", lwd=2)</pre><br/><br/>The function <code>predict()</code> compute the Y values given the X values. The the coordinates are linked with lines. Is not plotted the continuous, but the discrete. With a few values, this method is highly debilitating.<br/><br/><center><a href=\"https://i2.wp.com/img245.imageshack.us/img245/9274/pol2d.jpg\" rel=\"nofollow\" target=\"_blank\"><img border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i1.wp.com/img245.imageshack.us/img245/9274/pol2d.th.jpg?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/img245.imageshack.us/img245/9274/pol2d.th.jpg?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img border=\"0\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/img245.imageshack.us/img245/9274/pol2d.th.jpg?w=578\"/></noscript></a></center><br/><br/>Let's add the graph of the polynomial of 3rd degree:<br/><br/><pre>\npoints(sample1&amp;Year, predict(fit3), type=\"l\", col=\"blue\", lwd=2)</pre><br/><br/><center><a href=\"https://i1.wp.com/img269.imageshack.us/img269/6797/pol3k.jpg\" rel=\"nofollow\" target=\"_blank\"><img border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i2.wp.com/img269.imageshack.us/img269/6797/pol3k.th.jpg?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/img269.imageshack.us/img269/6797/pol3k.th.jpg?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img border=\"0\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/img269.imageshack.us/img269/6797/pol3k.th.jpg?w=578\"/></noscript></a></center><br/><br/>As you can see the two models have very similar trends.<br/><br/>If we would instead obtain the graph of continuous functions obtained, we proceed in this manner. First you create the polynomial equation we previously found:<br/><br/><pre>\npol2 &lt;- function(x) fit2$coefficient[3]*x^2 + fit2$coefficient[2]*x + fit2$coefficient[1]</pre><br/><br/>Remember that:<br/>- coefficient[1] = beta0<br/>- coefficient[2] = beta1<br/>- coefficient[3] = beta2<br/>and so on.<br/><br/>At this point we plotted the coordinates of sample1 and then the created curve with <code>curve(x)</code>:<br/><br/><pre>\nplot(sample1$Year, sample1$Population, type=\"p\", lwd=3)\npol2 &lt;- function(x) fit2$coefficient[3]*x^2 + fit2$coefficient[2]*x + fit2$coefficient[1]\ncurve(pol2, col=\"red\", lwd=2)</pre><br/><br/>The point, however, disappear, but we can replace them with the command <code>points</code>:<br/><br/><pre>\npoints(sample1$Year, sample1$Population, type=\"p\", lwd=3)</pre><br/><br/>A note: you must follow the order of commands as I have described, otherwise the function <code>curve</code> creates a wrong graph. So summarizing the commands to get the continuous function, and the experimental points on the same graph are the following:<br/><br/><pre>\nplot(sample1$Year, sample1$Population, type=\"p\", lwd=3)\npol2 &lt;- function(x) fit2$coefficient[3]*x^2 + fit2$coefficient[2]*x + fit2$coefficient[1]\ncurve(pol2, col=\"red\", lwd=2)\npoints(sample1$Year, sample1$Population, type=\"p\", lwd=3)</pre><br/><br/>The graph we get is the following:<br/><br/><center><a href=\"https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.jpg\" rel=\"nofollow\" target=\"_blank\"><img border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.th.jpg?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.th.jpg?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img border=\"0\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/img245.imageshack.us/img245/4396/pol4.th.jpg?w=578\"/></noscript></a></center><br/><br/>Now draw the graph of the polynomial of 3rd degree:<br/><br/><pre>\nplot(sample1$Year, sample1$Population, type=\"p\", lwd=3)\npol3 &lt;- function(x) fit3$coefficient[4]*x^3 + fit3$coefficient[3]*x^2 + fit3$coefficient[2]*x + fit3$coefficient[1]\ncurve(pol3, col=\"red\", lwd=2)\npoints(sample1$Year, sample1$Population, type=\"p\", lwd=3)</pre><br/><br/><center><a href=\"https://i1.wp.com/img269.imageshack.us/img269/261/pol5e.jpg\" rel=\"nofollow\" target=\"_blank\"><img border=\"0\" class=\"jetpack-lazy-image\" data-lazy-src=\"https://i2.wp.com/img269.imageshack.us/img269/261/pol5e.th.jpg?w=578&amp;is-pending-load=1\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/img269.imageshack.us/img269/261/pol5e.th.jpg?w=578\" srcset=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"/><noscript><img border=\"0\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/img269.imageshack.us/img269/261/pol5e.th.jpg?w=578\"/></noscript></a></center><br/></span>\n\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.0-->\n<p class=\"syndicated-attribution\"><div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://statistic-on-air.blogspot.com/2009/09/polynomial-regression-techniques.html\"> Statistic on aiR</a></strong>.</div>\n<hr>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</hr></div></p> </div>\n</article>",
      "main_text": "Polynomial regression techniques\nPosted on\nSeptember 5, 2009\nby\nTodos Logos\nin Uncategorized | 0 Comments\n[This article was first published on\nStatistic on aiR\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nSuppose we want to create a polynomial that can approximate better the following dataset on the population of a certain Italian city over 10 years. The table summarizes the data:\n$$\\begin{tabular}{|1|1|}\\hline Year & Population\\\\ \\hline 1959&4835\\\\ 1960&4970\\\\ 1961&5085\\\\ 1962&5160\\\\ 1963&5310\\\\ 1964&5260\\\\ 1965&5235\\\\ 1966&5255\\\\ 1967&5235\\\\ 1968&5210\\\\ 1969&5175\\\\ \\hline \\end{tabular}$$\nFirst we import the data into R:\nYear <- c(1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969)\nPopulation <- c(4835, 4970, 5085, 5160, 5310, 5260, 5235, 5255, 5235, 5210, 5175)\nNow we create the dataframe named sample1:\nsample1 <- data.frame(Year, Population)\nsample1\n\n   Year Population\n1  1959       4835\n2  1960       4970\n3  1961       5085\n4  1962       5160\n5  1963       5310\n6  1964       5260\n7  1965       5235\n8  1966       5255\n9  1967       5235\n10 1968       5210\n11 1969       5175\nAt this point may be useful to chart these values, to observe the trend and take an idea of the final polynomial function. For convenience we modify the column\nYear\n, creating a neighborhood of zero, thus:\nsample1$Year <- sample1$Year - 1964\nsample1\n\n   Year Population\n1    -5       4835\n2    -4       4970\n3    -3       5085\n4    -2       5160\n5    -1       5310\n6     0       5260\n7     1       5235\n8     2       5255\n9     3       5235\n10    4       5210\n11    5       5175\nPut the values on a chart\nplot(sample1$Year, sample1$Population, type=\"b\")\nAt this point we can start with the search for a polynomial model that adequately approximates our data. First, we specify that we want a polynomial function of X, ie a\nraw polynomial\n, is different from the\northogonal polynomial\n. This is an important addition because the controls and the results will change in the two cases R. So we want a function of X like:\n$$f(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+ ... +\\beta_nx^n$$\nAt what degree of the polynomial stop? Depends on the degree of precision that we seek. The greater the degree of the polynomial, the greater the accuracy of the model, but the greater the difficulty in calculating; we must also verify the significance of coefficients that are found. But let's get straight to the point.\nIn R for\nfitting a polynomial regression model\n(not orthogonal), there are two methods, among them identical. Suppose we seek the values of beta coefficients for a polynomial of degree 1, then 2nd degree, and 3rd degree:\nfit1 <- lm(sample1$Population ~ sample1$Year)\nfit2 <- lm(sample1$Population ~ sample1$Year + I(sample1$Year^2))\nfit3 <- lm(sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3))\nOr we can write more quickly, for polynomials of degree 2 and 3:\nfit2b <- lm(sample1$Population ~ poly(sample1$Year, 2, raw=TRUE))\nfit3b <- lm(sample1$Population ~ poly(sample1$Year, 3, raw=TRUE))\nThe function\npoly\nis useful if you want to get a polynomial of high degree, because it avoids explicitly write the formula. If we specify\nraw=TRUE\n, the two methods provide the same output, but if we do not specify\nraw=TRUE\n(or rgb(153, 0, 0);\">raw=F\n), the function\npoly\ngive us the values of the beta parameters of an orthogonal polynomials, which is different from the general formula I wrote above, although the models are both effective.\nLet's look at the output.\nsummary(fit2)\n## or summary(fit2b)\n\nCall:\nlm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.888 -18.834  -3.159   2.040  86.748 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5263.159     17.655 298.110  < 2e-16 ***\nsample1$Year        29.318      3.696   7.933 4.64e-05 ***\nI(sample1$Year^2)  -10.589      1.323  -8.002 4.36e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\nResidual standard error: 38.76 on 8 degrees of freedom\nMultiple R-squared: 0.9407,     Adjusted R-squared: 0.9259 \nF-statistic: 63.48 on 2 and 8 DF,  p-value: 1.235e-05\nThe output of\nsummary(fit2b)\nis the same. We obtained the values of beta0 (5263,159), beta1 (29,318) and beta2 (-10,589), which appear to be significant AII 3. The equation of polynomial of degree 2 of our model is:\n$$f(x)=5263.1597+29.318x-10.589x^2$$\nIf we want a polynomial of 3rd degree, we have:\nsummary(fit3)\n## of summary(fit3b)\n\nCall:\nlm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2) + \n    I(sample1$Year^3))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.774 -14.802  -1.253   3.199  72.634 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5263.1585    15.0667 349.324 4.16e-16 ***\nsample1$Year        14.3638     8.1282   1.767   0.1205    \nI(sample1$Year^2)  -10.5886     1.1293  -9.376 3.27e-05 ***\nI(sample1$Year^3)    0.8401     0.4209   1.996   0.0861 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\nResidual standard error: 33.08 on 7 degrees of freedom\nMultiple R-squared: 0.9622,     Adjusted R-squared: 0.946 \nF-statistic: 59.44 on 3 and 7 DF,  p-value: 2.403e-05\nThe equation is:\n$$f(x)=5263.1585+14.3638x-10.5886x^2+0.8401x^3$$\nIn the latter case, however, the coefficients beta1 and beta3 are not significant, then the best model is the polynomial of 2nd degree. Furthermore look at the Multiple R-squared: in the 2nd degree model it is 94.07%, while in the 3rd degree model it is 96.22%. It seems that there has been an increase in accuracy of the model, but it is a significant increase? We can compare the two model with an ANOVA table:\nanova(fit2, fit3)\n\nAnalysis of Variance Table\n\nModel 1: sample1$Population ~ sample1$Year + I(sample1$Year^2)\nModel 2: sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3)\n  Res.Df     RSS Df Sum of Sq      F Pr(>F)  \n1      8 12019.8                             \n2      7  7659.5  1    4360.3 3.9848 0.0861 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nSince the p-value is greater than 0.05, we accept the null hypothesis: there wasn't a significant improvement of the model.\nThe biggest problem now is to represent graphically the result. In fact, R does not exist (as far as I know) a function for plotting polynomials found. We must therefore proceed with graphic artifacts still valid, but somewhat laborious.\nFirst, we plotted the values, with the command seen before. This time only display the lines and not points, for convenience graphics:\nplot(sample1$Year, sample1$Population, type=\"l\", lwd=3)\nNow add to this chart the progress of the 2nd degree polynomial, in this way:\npoints(sample1$Year, predict(fit2), type=\"l\", col=\"red\", lwd=2)\nThe function\npredict()\ncompute the Y values given the X values. The the coordinates are linked with lines. Is not plotted the continuous, but the discrete. With a few values, this method is highly debilitating.\nLet's add the graph of the polynomial of 3rd degree:\npoints(sample1&Year, predict(fit3), type=\"l\", col=\"blue\", lwd=2)\nAs you can see the two models have very similar trends.\nIf we would instead obtain the graph of continuous functions obtained, we proceed in this manner. First you create the polynomial equation we previously found:\npol2 <- function(x) fit2$coefficient[3]*x^2 + fit2$coefficient[2]*x + fit2$coefficient[1]\nRemember that:\n- coefficient[1] = beta0\n- coefficient[2] = beta1\n- coefficient[3] = beta2\nand so on.\nAt this point we plotted the coordinates of sample1 and then the created curve with\ncurve(x)\n:\nplot(sample1$Year, sample1$Population, type=\"p\", lwd=3)\npol2 <- function(x) fit2$coefficient[3]*x^2 + fit2$coefficient[2]*x + fit2$coefficient[1]\ncurve(pol2, col=\"red\", lwd=2)\nThe point, however, disappear, but we can replace them with the command\npoints\n:\npoints(sample1$Year, sample1$Population, type=\"p\", lwd=3)\nA note: you must follow the order of commands as I have described, otherwise the function\ncurve\ncreates a wrong graph. So summarizing the commands to get the continuous function, and the experimental points on the same graph are the following:\nplot(sample1$Year, sample1$Population, type=\"p\", lwd=3)\npol2 <- function(x) fit2$coefficient[3]*x^2 + fit2$coefficient[2]*x + fit2$coefficient[1]\ncurve(pol2, col=\"red\", lwd=2)\npoints(sample1$Year, sample1$Population, type=\"p\", lwd=3)\nThe graph we get is the following:\nNow draw the graph of the polynomial of 3rd degree:\nplot(sample1$Year, sample1$Population, type=\"p\", lwd=3)\npol3 <- function(x) fit3$coefficient[4]*x^3 + fit3$coefficient[3]*x^2 + fit3$coefficient[2]*x + fit3$coefficient[1]\ncurve(pol3, col=\"red\", lwd=2)\npoints(sample1$Year, sample1$Population, type=\"p\", lwd=3)\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nStatistic on aiR\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Suppose we want to create a polynomial that can approximate better the following dataset on the population of a certain Italian city over 10 years. The table summarizes the data:$$\\begin{tabular}{|1|1|}\\hline Year & Population\\ \\hline 1959&4835\\ 1960&4970\\ 1961&5085\\ 1962&5160\\ 1963&5310\\ 1964&5260\\ 1965&5235\\ 1966&5255\\ 1967&5235\\ 1968&5210\\ 1969&5175\\ \\hline \\end{tabular}$$First we import the data into R:Year Population Now we create the dataframe named sample1:sample1 sample1 Year Population1 1959 48352 1960 49703 1961 50854 1962 51605 1963 53106 1964 52607 1965 52358 1966 52559 1967 523510 1968 521011 1969 5175At this point may be useful to chart these values, to observe the trend and take an idea of the final polynomial function. For convenience we modify the column Year, creating a neighborhood of zero, thus:sample1$Year sample1 Year Population1 -5 48352 -4 49703 -3 50854 -2 51605 -1 53106 0 52607 1 52358 2 52559 3 523510 4 521011 5 5175Put the values on a chartplot(sample1$Year, sample1$Population, type=\"b\")At this point we can start with the search for a polynomial model that adequately approximates our data. First, we specify that we want a polynomial function of X, ie a raw polynomial , is different from the orthogonal polynomial. This is an important addition because the controls and the results will change in the two cases R. So we want a function of X like:$$f(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+ ... +\\beta_nx^n$$At what degree of the polynomial stop? Depends on the degree of precision that we seek. The greater the degree of the polynomial, the greater the accuracy of the model, but the greater the difficulty in calculating; we must also verify the significance of coefficients that are found. But let's get straight to the point.In R for fitting a polynomial regression model (not orthogonal), there are two methods, among them identical. Suppose we seek the values of beta coefficients for a polynomial of degree 1, then 2nd degree, and 3rd degree:fit1 fit2 fit3 Or we can write more quickly, for polynomials of degree 2 and 3:fit2b fit3b The function poly is useful if you want to get a polynomial of high degree, because it avoids explicitly write the formula. If we specify raw=TRUE, the two methods provide the same output, but if we do not specify raw=TRUE (or rgb(153, 0, 0);\">raw=F), the function poly give us the values of the beta parameters of an orthogonal polynomials, which is different from the general formula I wrote above, although the models are both effective.Let's look at the output. summary(fit2)## or summary(fit2b)Call:lm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2))Residuals: Min 1Q Median 3Q Max -46.888 -18.834 -3.159 2.040 86.748 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5263.159 17.655 298.110 sample1$Year 29.318 3.696 7.933 4.64e-05 ***I(sample1$Year^2) -10.589 1.323 -8.002 4.36e-05 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 38.76 on 8 degrees of freedomMultiple R-squared: 0.9407, Adjusted R-squared: 0.9259 F-statistic: 63.48 on 2 and 8 DF, p-value: 1.235e-05 The output of summary(fit2b) is the same. We obtained the values of beta0 (5263,159), beta1 (29,318) and beta2 (-10,589), which appear to be significant AII 3. The equation of polynomial of degree 2 of our model is:$$f(x)=5263.1597+29.318x-10.589x^2$$If we want a polynomial of 3rd degree, we have:summary(fit3)## of summary(fit3b)Call:lm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3))Residuals: Min 1Q Median 3Q Max -32.774 -14.802 -1.253 3.199 72.634 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5263.1585 15.0667 349.324 4.16e-16 ***sample1$Year 14.3638 8.1282 1.767 0.1205 I(sample1$Year^2) -10.5886 1.1293 -9.376 3.27e-05 ***I(sample1$Year^3) 0.8401 0.4209 1.996 0.0861 . ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 33.08 on 7 degrees of freedomMultiple R-squared: 0.9622, Adjusted R-squared: 0.946 F-statistic: 59.44 on 3 and 7 DF, p-value: 2.403e-05 The equation is:$$f(x)=5263.1585+14.3638x-10.5886x^2+0.8401x^3$$In the latter case, however, the coefficients beta1 and beta3 are not significant, then the best model is the polynomial of 2nd degree. Furthermore look at the Multiple R-squared: in the 2nd degree model it is 94.07%, while in the 3rd degree model it is 96.22%. It seems that there has been an increase in accuracy of the model, but it is a significant increase? We can compare the two model with an ANOVA table:anova(fit2, fit3)Analysis of Variance TableModel 1: sample1$Population ~ sample1$Year + I(sample1$Year^2)Model 2: sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3) Res.Df RSS Df Sum of Sq F Pr(>F) 1 8 12019.8 2 7 7659.5 1 4360.3 3.9848 0.0861 .---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Since the p-value is greater than 0.05, we accept the null hypothesis: there wasn't a significant improvement of the model.The biggest problem now is to represent graphically the result. In fact, R does not exist (as far as I know) a function for plotting polynomials found. We must therefore proceed with graphic artifacts still valid, but somewhat laborious.First, we plotted the values, with the command seen before. This time only display the lines and not points, for convenience graphics:plot(sample1$Year, sample1$Population, type=\"l\", lwd=3)Now add to this chart the progress of the 2nd degree polynomial, in this way:points(sample1$Year, predict(fit2), type=\"l\", col=\"red\", lwd=2)The function predict() compute the Y values given the X values. The the coordinates are linked with lines. Is not plotted the continuous, but the discrete. With a few values, this method is highly debilitating.Let's add the graph of the polynomial of 3rd degree:points(sample1&Year, predict(fit3), type=\"l\", col=\"blue\", lwd=2)As you can see the two models have very similar trends.If we would instead obtain the graph of continuous functions obtained, we proceed in this manner. First you create the polynomial equation we previously found:pol2 Remember that:- coefficient[1] = beta0- coefficient[2] = beta1- coefficient[3] = beta2and so on.At this point we plotted the coordinates of sample1 and then the created curve with curve(x):plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol2 curve(pol2, col=\"red\", lwd=2)The point, however, disappear, but we can replace them with the command points:points(sample1$Year, sample1$Population, type=\"p\", lwd=3)A note: you must follow the order of commands as I have described, otherwise the function curve creates a wrong graph. So summarizing the commands to get the continuous function, and the experimental points on the same graph are the following:plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol2 curve(pol2, col=\"red\", lwd=2)points(sample1$Year, sample1$Population, type=\"p\", lwd=3)The graph we get is the following:Now draw the graph of the polynomial of 3rd degree:plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol3 curve(pol3, col=\"red\", lwd=2)points(sample1$Year, sample1$Population, type=\"p\", lwd=3)",
      "meta_keywords": null,
      "og_description": "Suppose we want to create a polynomial that can approximate better the following dataset on the population of a certain Italian city over 10 years. The table summarizes the data:$$\\begin{tabular}{|1|1|}\\hline Year & Population\\ \\hline 1959&4835\\ 1960&4970\\ 1961&5085\\ 1962&5160\\ 1963&5310\\ 1964&5260\\ 1965&5235\\ 1966&5255\\ 1967&5235\\ 1968&5210\\ 1969&5175\\ \\hline \\end{tabular}$$First we import the data into R:Year Population Now we create the dataframe named sample1:sample1 sample1 Year Population1 1959 48352 1960 49703 1961 50854 1962 51605 1963 53106 1964 52607 1965 52358 1966 52559 1967 523510 1968 521011 1969 5175At this point may be useful to chart these values, to observe the trend and take an idea of the final polynomial function. For convenience we modify the column Year, creating a neighborhood of zero, thus:sample1$Year sample1 Year Population1 -5 48352 -4 49703 -3 50854 -2 51605 -1 53106 0 52607 1 52358 2 52559 3 523510 4 521011 5 5175Put the values on a chartplot(sample1$Year, sample1$Population, type=\"b\")At this point we can start with the search for a polynomial model that adequately approximates our data. First, we specify that we want a polynomial function of X, ie a raw polynomial , is different from the orthogonal polynomial. This is an important addition because the controls and the results will change in the two cases R. So we want a function of X like:$$f(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+ ... +\\beta_nx^n$$At what degree of the polynomial stop? Depends on the degree of precision that we seek. The greater the degree of the polynomial, the greater the accuracy of the model, but the greater the difficulty in calculating; we must also verify the significance of coefficients that are found. But let's get straight to the point.In R for fitting a polynomial regression model (not orthogonal), there are two methods, among them identical. Suppose we seek the values of beta coefficients for a polynomial of degree 1, then 2nd degree, and 3rd degree:fit1 fit2 fit3 Or we can write more quickly, for polynomials of degree 2 and 3:fit2b fit3b The function poly is useful if you want to get a polynomial of high degree, because it avoids explicitly write the formula. If we specify raw=TRUE, the two methods provide the same output, but if we do not specify raw=TRUE (or rgb(153, 0, 0);\">raw=F), the function poly give us the values of the beta parameters of an orthogonal polynomials, which is different from the general formula I wrote above, although the models are both effective.Let's look at the output. summary(fit2)## or summary(fit2b)Call:lm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2))Residuals: Min 1Q Median 3Q Max -46.888 -18.834 -3.159 2.040 86.748 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5263.159 17.655 298.110 sample1$Year 29.318 3.696 7.933 4.64e-05 ***I(sample1$Year^2) -10.589 1.323 -8.002 4.36e-05 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 38.76 on 8 degrees of freedomMultiple R-squared: 0.9407, Adjusted R-squared: 0.9259 F-statistic: 63.48 on 2 and 8 DF, p-value: 1.235e-05 The output of summary(fit2b) is the same. We obtained the values of beta0 (5263,159), beta1 (29,318) and beta2 (-10,589), which appear to be significant AII 3. The equation of polynomial of degree 2 of our model is:$$f(x)=5263.1597+29.318x-10.589x^2$$If we want a polynomial of 3rd degree, we have:summary(fit3)## of summary(fit3b)Call:lm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3))Residuals: Min 1Q Median 3Q Max -32.774 -14.802 -1.253 3.199 72.634 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5263.1585 15.0667 349.324 4.16e-16 ***sample1$Year 14.3638 8.1282 1.767 0.1205 I(sample1$Year^2) -10.5886 1.1293 -9.376 3.27e-05 ***I(sample1$Year^3) 0.8401 0.4209 1.996 0.0861 . ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 33.08 on 7 degrees of freedomMultiple R-squared: 0.9622, Adjusted R-squared: 0.946 F-statistic: 59.44 on 3 and 7 DF, p-value: 2.403e-05 The equation is:$$f(x)=5263.1585+14.3638x-10.5886x^2+0.8401x^3$$In the latter case, however, the coefficients beta1 and beta3 are not significant, then the best model is the polynomial of 2nd degree. Furthermore look at the Multiple R-squared: in the 2nd degree model it is 94.07%, while in the 3rd degree model it is 96.22%. It seems that there has been an increase in accuracy of the model, but it is a significant increase? We can compare the two model with an ANOVA table:anova(fit2, fit3)Analysis of Variance TableModel 1: sample1$Population ~ sample1$Year + I(sample1$Year^2)Model 2: sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3) Res.Df RSS Df Sum of Sq F Pr(>F) 1 8 12019.8 2 7 7659.5 1 4360.3 3.9848 0.0861 .---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Since the p-value is greater than 0.05, we accept the null hypothesis: there wasn't a significant improvement of the model.The biggest problem now is to represent graphically the result. In fact, R does not exist (as far as I know) a function for plotting polynomials found. We must therefore proceed with graphic artifacts still valid, but somewhat laborious.First, we plotted the values, with the command seen before. This time only display the lines and not points, for convenience graphics:plot(sample1$Year, sample1$Population, type=\"l\", lwd=3)Now add to this chart the progress of the 2nd degree polynomial, in this way:points(sample1$Year, predict(fit2), type=\"l\", col=\"red\", lwd=2)The function predict() compute the Y values given the X values. The the coordinates are linked with lines. Is not plotted the continuous, but the discrete. With a few values, this method is highly debilitating.Let's add the graph of the polynomial of 3rd degree:points(sample1&Year, predict(fit3), type=\"l\", col=\"blue\", lwd=2)As you can see the two models have very similar trends.If we would instead obtain the graph of continuous functions obtained, we proceed in this manner. First you create the polynomial equation we previously found:pol2 Remember that:- coefficient[1] = beta0- coefficient[2] = beta1- coefficient[3] = beta2and so on.At this point we plotted the coordinates of sample1 and then the created curve with curve(x):plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol2 curve(pol2, col=\"red\", lwd=2)The point, however, disappear, but we can replace them with the command points:points(sample1$Year, sample1$Population, type=\"p\", lwd=3)A note: you must follow the order of commands as I have described, otherwise the function curve creates a wrong graph. So summarizing the commands to get the continuous function, and the experimental points on the same graph are the following:plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol2 curve(pol2, col=\"red\", lwd=2)points(sample1$Year, sample1$Population, type=\"p\", lwd=3)The graph we get is the following:Now draw the graph of the polynomial of 3rd degree:plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol3 curve(pol3, col=\"red\", lwd=2)points(sample1$Year, sample1$Population, type=\"p\", lwd=3)",
      "og_image": "https://img137.imageshack.us/img137/7946/pol1.th.jpg",
      "og_title": "Polynomial regression techniques | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 8.1,
      "sitemap_lastmod": "2009-09-05T18:27:46+00:00",
      "twitter_description": "Suppose we want to create a polynomial that can approximate better the following dataset on the population of a certain Italian city over 10 years. The table summarizes the data:$$\\begin{tabular}{|1|1|}\\hline Year & Population\\ \\hline 1959&4835\\ 1960&4970\\ 1961&5085\\ 1962&5160\\ 1963&5310\\ 1964&5260\\ 1965&5235\\ 1966&5255\\ 1967&5235\\ 1968&5210\\ 1969&5175\\ \\hline \\end{tabular}$$First we import the data into R:Year Population Now we create the dataframe named sample1:sample1 sample1 Year Population1 1959 48352 1960 49703 1961 50854 1962 51605 1963 53106 1964 52607 1965 52358 1966 52559 1967 523510 1968 521011 1969 5175At this point may be useful to chart these values, to observe the trend and take an idea of the final polynomial function. For convenience we modify the column Year, creating a neighborhood of zero, thus:sample1$Year sample1 Year Population1 -5 48352 -4 49703 -3 50854 -2 51605 -1 53106 0 52607 1 52358 2 52559 3 523510 4 521011 5 5175Put the values on a chartplot(sample1$Year, sample1$Population, type=\"b\")At this point we can start with the search for a polynomial model that adequately approximates our data. First, we specify that we want a polynomial function of X, ie a raw polynomial , is different from the orthogonal polynomial. This is an important addition because the controls and the results will change in the two cases R. So we want a function of X like:$$f(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+ ... +\\beta_nx^n$$At what degree of the polynomial stop? Depends on the degree of precision that we seek. The greater the degree of the polynomial, the greater the accuracy of the model, but the greater the difficulty in calculating; we must also verify the significance of coefficients that are found. But let's get straight to the point.In R for fitting a polynomial regression model (not orthogonal), there are two methods, among them identical. Suppose we seek the values of beta coefficients for a polynomial of degree 1, then 2nd degree, and 3rd degree:fit1 fit2 fit3 Or we can write more quickly, for polynomials of degree 2 and 3:fit2b fit3b The function poly is useful if you want to get a polynomial of high degree, because it avoids explicitly write the formula. If we specify raw=TRUE, the two methods provide the same output, but if we do not specify raw=TRUE (or rgb(153, 0, 0);\">raw=F), the function poly give us the values of the beta parameters of an orthogonal polynomials, which is different from the general formula I wrote above, although the models are both effective.Let's look at the output. summary(fit2)## or summary(fit2b)Call:lm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2))Residuals: Min 1Q Median 3Q Max -46.888 -18.834 -3.159 2.040 86.748 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5263.159 17.655 298.110 sample1$Year 29.318 3.696 7.933 4.64e-05 ***I(sample1$Year^2) -10.589 1.323 -8.002 4.36e-05 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 38.76 on 8 degrees of freedomMultiple R-squared: 0.9407, Adjusted R-squared: 0.9259 F-statistic: 63.48 on 2 and 8 DF, p-value: 1.235e-05 The output of summary(fit2b) is the same. We obtained the values of beta0 (5263,159), beta1 (29,318) and beta2 (-10,589), which appear to be significant AII 3. The equation of polynomial of degree 2 of our model is:$$f(x)=5263.1597+29.318x-10.589x^2$$If we want a polynomial of 3rd degree, we have:summary(fit3)## of summary(fit3b)Call:lm(formula = sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3))Residuals: Min 1Q Median 3Q Max -32.774 -14.802 -1.253 3.199 72.634 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5263.1585 15.0667 349.324 4.16e-16 ***sample1$Year 14.3638 8.1282 1.767 0.1205 I(sample1$Year^2) -10.5886 1.1293 -9.376 3.27e-05 ***I(sample1$Year^3) 0.8401 0.4209 1.996 0.0861 . ---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 33.08 on 7 degrees of freedomMultiple R-squared: 0.9622, Adjusted R-squared: 0.946 F-statistic: 59.44 on 3 and 7 DF, p-value: 2.403e-05 The equation is:$$f(x)=5263.1585+14.3638x-10.5886x^2+0.8401x^3$$In the latter case, however, the coefficients beta1 and beta3 are not significant, then the best model is the polynomial of 2nd degree. Furthermore look at the Multiple R-squared: in the 2nd degree model it is 94.07%, while in the 3rd degree model it is 96.22%. It seems that there has been an increase in accuracy of the model, but it is a significant increase? We can compare the two model with an ANOVA table:anova(fit2, fit3)Analysis of Variance TableModel 1: sample1$Population ~ sample1$Year + I(sample1$Year^2)Model 2: sample1$Population ~ sample1$Year + I(sample1$Year^2) + I(sample1$Year^3) Res.Df RSS Df Sum of Sq F Pr(>F) 1 8 12019.8 2 7 7659.5 1 4360.3 3.9848 0.0861 .---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Since the p-value is greater than 0.05, we accept the null hypothesis: there wasn't a significant improvement of the model.The biggest problem now is to represent graphically the result. In fact, R does not exist (as far as I know) a function for plotting polynomials found. We must therefore proceed with graphic artifacts still valid, but somewhat laborious.First, we plotted the values, with the command seen before. This time only display the lines and not points, for convenience graphics:plot(sample1$Year, sample1$Population, type=\"l\", lwd=3)Now add to this chart the progress of the 2nd degree polynomial, in this way:points(sample1$Year, predict(fit2), type=\"l\", col=\"red\", lwd=2)The function predict() compute the Y values given the X values. The the coordinates are linked with lines. Is not plotted the continuous, but the discrete. With a few values, this method is highly debilitating.Let's add the graph of the polynomial of 3rd degree:points(sample1&Year, predict(fit3), type=\"l\", col=\"blue\", lwd=2)As you can see the two models have very similar trends.If we would instead obtain the graph of continuous functions obtained, we proceed in this manner. First you create the polynomial equation we previously found:pol2 Remember that:- coefficient[1] = beta0- coefficient[2] = beta1- coefficient[3] = beta2and so on.At this point we plotted the coordinates of sample1 and then the created curve with curve(x):plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol2 curve(pol2, col=\"red\", lwd=2)The point, however, disappear, but we can replace them with the command points:points(sample1$Year, sample1$Population, type=\"p\", lwd=3)A note: you must follow the order of commands as I have described, otherwise the function curve creates a wrong graph. So summarizing the commands to get the continuous function, and the experimental points on the same graph are the following:plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol2 curve(pol2, col=\"red\", lwd=2)points(sample1$Year, sample1$Population, type=\"p\", lwd=3)The graph we get is the following:Now draw the graph of the polynomial of 3rd degree:plot(sample1$Year, sample1$Population, type=\"p\", lwd=3)pol3 curve(pol3, col=\"red\", lwd=2)points(sample1$Year, sample1$Population, type=\"p\", lwd=3)",
      "twitter_title": "Polynomial regression techniques | R-bloggers",
      "url": "https://www.r-bloggers.com/2009/09/polynomial-regression-techniques/",
      "word_count": 1615
    }
  }
}