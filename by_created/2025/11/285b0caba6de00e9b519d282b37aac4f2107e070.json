{
  "id": "285b0caba6de00e9b519d282b37aac4f2107e070",
  "url": "https://www.r-bloggers.com/2023/10/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/",
  "created_at_utc": "2025-11-17T20:39:37Z",
  "data": null,
  "raw_original": {
    "uuid": "43dfee24-a8aa-4d2c-8dfb-4443f3164f99",
    "created_at": "2025-11-17 20:39:37",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2023/10/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/",
      "crawled_at": "2025-11-17T10:09:11.701780",
      "external_links": [
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/",
          "text": "R, Econometrics, High Performance"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/",
          "text": "collapse"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/",
          "text": "website"
        },
        {
          "href": "https://raw.githubusercontent.com/SebKrantz/collapse/master/misc/collapse%20cheat%20sheet/collapse_cheat_sheet.pdf",
          "text": "cheat sheet"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html",
          "text": "vignette"
        },
        {
          "href": "https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.join.html",
          "text": "polars"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fn1",
          "text": "1"
        },
        {
          "href": "https://www.rug.nl/ggdc/structuralchange/previous-sector-database/10-sector-2014",
          "text": "Groningen Growth and Development Centre 10-Sector Database"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/pivot.html",
          "text": "documentation"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/collapse-options.html",
          "text": "global configurability"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html",
          "text": "Fast Statistical Functions"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/fast-grouping-ordering.html",
          "text": "Grouping and Ordering"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/fast-data-manipulation.html",
          "text": "Data Manipulation"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/time-series-panel-series.html",
          "text": "Time Series"
        },
        {
          "href": "https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html",
          "text": "Fast Statistical Functions"
        },
        {
          "href": "https://github.com/oracle/fastr",
          "text": "FastR/Graal VM"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fn2",
          "text": "2"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fn3",
          "text": "3"
        },
        {
          "href": "https://twitter.com/collapse_R",
          "text": "Twitter"
        },
        {
          "href": "https://duckdb.org/2023/04/14/h2oai.html#results",
          "text": "DuckDB database like ops benchmark"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fnref1",
          "text": "↩︎"
        },
        {
          "href": "https://github.com/brodieG/r2c",
          "text": "r2c"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fnref2",
          "text": "↩︎"
        },
        {
          "href": "https://duckdblabs.github.io/db-benchmark/",
          "text": "benchmarks"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fnref3",
          "text": "↩︎"
        },
        {
          "href": "https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/",
          "text": "R, Econometrics, High Performance"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Releasing collapse 2.0: Blazing Fast Joins, Reshaping, and Enhanced R | R-bloggers",
      "images": [],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/r-econometrics-high-performance/",
          "text": "R, Econometrics, High Performance"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-379200 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Releasing collapse 2.0: Blazing Fast Joins, Reshaping, and Enhanced R</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">October 14, 2023</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-econometrics-high-performance/\">R, Econometrics, High Performance</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \r\n<div style=\"min-height: 30px;\">\r\n[social4i size=\"small\" align=\"align-left\"]\r\n</div>\r\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\r\n[This article was first published on  <strong><a href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/\"> R, Econometrics, High Performance</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<p>I’m excited to announce the release of <a href=\"https://sebkrantz.github.io/collapse/\" rel=\"nofollow\" target=\"_blank\"><em>collapse</em></a> 2.0, adding blazing fast joins, pivots, flexible namespace and many other features, including a brand new <a href=\"https://sebkrantz.github.io/collapse/\" rel=\"nofollow\" target=\"_blank\"><em>website</em></a>, an updated <a href=\"https://raw.githubusercontent.com/SebKrantz/collapse/master/misc/collapse%20cheat%20sheet/collapse_cheat_sheet.pdf\" rel=\"nofollow\" target=\"_blank\">cheat sheet</a>, and a new <a href=\"https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html\" rel=\"nofollow\" target=\"_blank\">vignette</a> aimed at <em>tidyverse</em> users.</p>\n<p>In the 3.5 years after the first release of <em>collapse</em> 1.0 to CRAN in March 2020, the package has seen 10 major updates, and become a remarkable piece of statistical software that is robust, stable, lightweight, fast, statistically advanced, comprehensive, flexible, class-agnostic, verbose and well-documented. It is profoundly able to deal with rich (multi-level, irregular, weighted, nested, labelled, and missing) scientific data, and can enhance the workflow of every R user.</p>\n<p>The addition of rich, fast, and verbose joins and pivots in this release, together with secure interactive namespace masking and extensive global configurability, should enable many R users to use it as a workhorse package for data manipulation and statistical computing tasks.</p>\n<p>In this post, I briefly introduce the core new features of this release and end with some reflections on why I created the package and think that its approach towards speeding up and enriching R is more encompassing than others.</p>\n<div class=\"section level1\" id=\"fast-class-agnostic-and-verbose-table-joins\">\n<h1>Fast, Class-Agnostic, and Verbose Table Joins</h1>\n<p>Joins in <em>collapse</em> has been a much-requested feature. Still, I was long hesitant to take them on because they are complex, high-risk, operations, and I was unsure on how to go about them or provide an implementation that would satisfy my own ambitious demands to their performance, generality and verbosity.</p>\n<p>I am glad that, following repeated requests, I have overcome these hesitations and designed an implementation – inspired by <a href=\"https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.join.html\" rel=\"nofollow\" target=\"_blank\"><em>polars</em></a> – that I am very satisfied with. <em>collapse</em>’s join function is simply called <code>join()</code>, and provides 6 types of joins (left, inner, full, right, semi and anti), controlled by a <code>how</code> argument – the default being a left join. It also provides two separate join algorithms: a vectorized hash join (the default, <code>sort = FALSE</code>) and a sort-merge-join (<code>sort = TRUE</code>). The join-column argument is called <code>on</code>, and, if left empty, selects columns present in both datasets. An example with generated data follows:</p>\n<pre>library(collapse)\ndf1 &lt;- data.frame(\n  id1 = c(1, 1, 2, 3),\n  id2 = c(\"a\", \"b\", \"b\", \"c\"),\n  name = c(\"John\", \"Jane\", \"Bob\", \"Carl\"),\n  age = c(35, 28, 42, 50)\n)\ndf2 &lt;- data.frame(\n  id1 = c(1, 2, 3, 3),\n  id2 = c(\"a\", \"b\", \"c\", \"e\"),\n  salary = c(60000, 55000, 70000, 80000),\n  dept = c(\"IT\", \"Marketing\", \"Sales\", \"IT\")\n)\n\n# Different types of joins\nfor (i in c(\"left\",\"inner\",\"right\",\"full\",\"semi\",\"anti\"))\n    join(df1, df2, how = i) |&gt; print()\n## left join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   1   b Jane  28     NA      &lt;NA&gt;\n## 3   2   b  Bob  42  55000 Marketing\n## 4   3   c Carl  50  70000     Sales\n## inner join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   2   b  Bob  42  55000 Marketing\n## 3   3   c Carl  50  70000     Sales\n## right join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   2   b  Bob  42  55000 Marketing\n## 3   3   c Carl  50  70000     Sales\n## 4   3   e &lt;NA&gt;  NA  80000        IT\n## full join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   1   b Jane  28     NA      &lt;NA&gt;\n## 3   2   b  Bob  42  55000 Marketing\n## 4   3   c Carl  50  70000     Sales\n## 5   3   e &lt;NA&gt;  NA  80000        IT\n## semi join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age\n## 1   1   a John  35\n## 2   2   b  Bob  42\n## 3   3   c Carl  50\n## anti join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age\n## 1   1   b Jane  28</pre>\n<p>Notice how, by default (<code>verbose = 1</code>), a compact summary of the operation is printed, indicating the type of join, the datasets and columns, and the number and percentage of records from each dataset matched in the join operation. <code>join()</code> also preserves the attributes of the first argument (<code>x</code>) and the order of columns and rows (default <code>keep.col.order = TRUE</code>, <code>sort = FALSE</code>) in it. We can thus think of a join operation as adding columns to a data frame-like object (<code>x</code>) from another similar object (<code>y</code>) .</p>\n<p>There are several additional options to increase verbosity and assimilate the join operation:</p>\n<pre># verbose = 2 also shows classes, allowing you to detect implicit conversions (inside fmatch())\njoin(df1, df2, how = \"left\", verbose = 2)\n## left join: df1[id1:numeric, id2:character] 3/4 (75%) &lt;m:m&gt; df2[id1:numeric, id2:character] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   1   b Jane  28     NA      &lt;NA&gt;\n## 3   2   b  Bob  42  55000 Marketing\n## 4   3   c Carl  50  70000     Sales\n\n# Adding join column: useful especially for full join\njoin(df1, df2, how = \"full\", column = TRUE)\n## full join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept   .join\n## 1   1   a John  35  60000        IT matched\n## 2   1   b Jane  28     NA      &lt;NA&gt;     df1\n## 3   2   b  Bob  42  55000 Marketing matched\n## 4   3   c Carl  50  70000     Sales matched\n## 5   3   e &lt;NA&gt;  NA  80000        IT     df2\n\n# Custom column + rearranging\njoin(df1, df2, how = \"full\", column = list(\"join\", c(\"x\", \"y\", \"x_y\")), \n     keep.col.order = FALSE)\n## full join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n##   id1 id2 join name age salary      dept\n## 1   1   a  x_y John  35  60000        IT\n## 2   1   b    x Jane  28     NA      &lt;NA&gt;\n## 3   2   b  x_y  Bob  42  55000 Marketing\n## 4   3   c  x_y Carl  50  70000     Sales\n## 5   3   e    y &lt;NA&gt;  NA  80000        IT\n\n# Attaching match attribute\nstr(join(df1, df2, attr = TRUE))\n## left join: df1[id1, id2] 3/4 (75%) &lt;m:m&gt; df2[id1, id2] 3/4 (75%)\n## 'data.frame':\t4 obs. of  6 variables:\n##  $ id1   : num  1 1 2 3\n##  $ id2   : chr  \"a\" \"b\" \"b\" \"c\"\n##  $ name  : chr  \"John\" \"Jane\" \"Bob\" \"Carl\"\n##  $ age   : num  35 28 42 50\n##  $ salary: num  60000 NA 55000 70000\n##  $ dept  : chr  \"IT\" NA \"Marketing\" \"Sales\"\n##  - attr(*, \"join.match\")=List of 3\n##   ..$ call   : language join(x = df1, y = df2, attr = TRUE)\n##   ..$ on.cols:List of 2\n##   .. ..$ x: chr [1:2] \"id1\" \"id2\"\n##   .. ..$ y: chr [1:2] \"id1\" \"id2\"\n##   ..$ match  : 'qG' int [1:4] 1 NA 2 3\n##   .. ..- attr(*, \"N.nomatch\")= int 1\n##   .. ..- attr(*, \"N.groups\")= int 4\n##   .. ..- attr(*, \"N.distinct\")= int 3</pre>\n<p>Finally, it is possible to validate the join operation to be either one of <code>\"m:m\"</code> (default, no checks), <code>\"1:m\"</code>, <code>\"m:1\"</code> or <code>\"1:1\"</code>. For example:</p>\n<pre>join(df1, rowbind(df2, df2), validate = \"1:1\")\n## Error in join(df1, rowbind(df2, df2), validate = \"1:1\"): Join is not 1:1: df1 (x) is unique on the join columns; rowbind (y) is not unique on the join columns</pre>\n<p>Another check being automatically executed inside the workhorse function <code>fmatch()</code> (if <code>sort = FALSE</code>) is for overidentified join conditions, i.e., if the records are more than identified by the join columns. For example if we added <code>\"name\"</code> and <code>\"dept\"</code> to the join condition, this would issue a warning as the match is already identified by <code>\"id1\"</code> and <code>\"id2\"</code>:</p>\n<pre>join(df1, df2, on = c(\"id1\", \"id2\", \"name\" = \"dept\"), how = \"left\")\n## Warning in fmatch(x[ixon], y[iyon], nomatch = NA_integer_, count = count, : Overidentified match/join: the first 2 of 3 columns uniquely match the records. With overid &gt; 0, fmatch() continues to\n## match columns. Consider removing columns or setting overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning.\n## left join: df1[id1, id2, name] 0/4 (0%) &lt;m:m&gt; df2[id1, id2, dept] 0/4 (0%)\n##   id1 id2 name age salary\n## 1   1   a John  35     NA\n## 2   1   b Jane  28     NA\n## 3   2   b  Bob  42     NA\n## 4   3   c Carl  50     NA</pre>\n<p>The warning can be silenced by passing <code>overid = 2</code> to <code>join()</code>. To see better where this may be useful, consider the following example using <code>fmatch()</code>.</p>\n<pre>df1 &lt;- data.frame(\n  id1 = c(1, 1, 2, 3),\n  id2 = c(\"a\", \"b\", \"b\", \"c\"),\n  name = c(\"John\", \"Bob\", \"Jane\", \"Carl\")\n)\ndf2 &lt;- data.frame(\n  id1 = c(1, 2, 3, 3),\n  id2 = c(\"a\", \"b\", \"c\", \"e\"),\n  name = c(\"John\", \"Janne\", \"Carl\", \"Lynne\")\n)\n\n# This gives an overidentification warning: columns 1:2 identify the data\nfmatch(df1, df2)\n## Warning in fmatch(df1, df2): Overidentified match/join: the first 2 of 3 columns uniquely match the records. With overid &gt; 0, fmatch() continues to match columns. Consider removing columns or setting\n## overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning.\n## [1]  1 NA NA  3\n# This just runs through without warning\nfmatch(df1, df2, overid = 2)\n## [1]  1 NA NA  3\n# This terminates computation after first 2 columns\nfmatch(df1, df2, overid = 0)\n## [1]  1 NA  2  3\nfmatch(df1[1:2], df2[1:2])  # Same thing!\n## [1]  1 NA  2  3\n# -&gt; note that here we get an additional match based on the unique ids,\n# which we didn't get before because \"Jane\" != \"Janne\"</pre>\n<p>So, in summary, the implementation of joins on <em>collapse</em> as provided by the <code>join()</code> function is not only blazing fast<a class=\"footnote-ref\" href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fn1\" id=\"fnref1\" rel=\"nofollow\" target=\"_blank\"><sup>1</sup></a> and class-agnostic but also allows you to verify all aspects of this high-risk operation.</p>\n</div>\n<div class=\"section level1\" id=\"advanced-pivots\">\n<h1>Advanced Pivots</h1>\n<p>The second big addition in <em>collapse</em> 2.0 is <code>pivot()</code>, which provides advanced data reshaping capabilities in a single parsimonious API. Notably, it supports longer-, wider-, and recast-pivoting functionality and can accommodate variable labels in the reshaping process.</p>\n<p>Fortunately, <em>collapse</em> supplies a perfect test dataset to illustrate these capabilities: the 2014 <a href=\"https://www.rug.nl/ggdc/structuralchange/previous-sector-database/10-sector-2014\" rel=\"nofollow\" target=\"_blank\">Groningen Growth and Development Centre 10-Sector Database</a>, which provides sectoral employment and value-added series for 10 broad sectors in 43 countries:</p>\n<pre>head(GGDC10S)\n##   Country Regioncode             Region Variable Year      AGR      MIN       MAN        PU       CON      WRT      TRA     FIRE      GOV      OTH      SUM\n## 1     BWA        SSA Sub-saharan Africa       VA 1960       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 2     BWA        SSA Sub-saharan Africa       VA 1961       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 3     BWA        SSA Sub-saharan Africa       VA 1962       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 4     BWA        SSA Sub-saharan Africa       VA 1963       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 5     BWA        SSA Sub-saharan Africa       VA 1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229\n## 6     BWA        SSA Sub-saharan Africa       VA 1965 15.72700 2.495768 1.0181992 0.1350976 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710\n\nnamlab(GGDC10S, N = TRUE, Ndistinct = TRUE)\n##      Variable    N Ndist                                                 Label\n## 1     Country 5027    43                                               Country\n## 2  Regioncode 5027     6                                           Region code\n## 3      Region 5027     6                                                Region\n## 4    Variable 5027     2                                              Variable\n## 5        Year 5027    67                                                  Year\n## 6         AGR 4364  4353                                          Agriculture \n## 7         MIN 4355  4224                                                Mining\n## 8         MAN 4355  4353                                         Manufacturing\n## 9          PU 4354  4237                                             Utilities\n## 10        CON 4355  4339                                          Construction\n## 11        WRT 4355  4344                         Trade, restaurants and hotels\n## 12        TRA 4355  4334                  Transport, storage and communication\n## 13       FIRE 4355  4349 Finance, insurance, real estate and business services\n## 14        GOV 3482  3470                                   Government services\n## 15        OTH 4248  4238               Community, social and personal services\n## 16        SUM 4364  4364                               Summation of sector GDP</pre>\n<p>Evidently, the data is supplied in a format where two variables, employment and value-added, are stacked in each sector column. The data is also labeled, with descriptions attached as <code>\"label\"</code> attributes (retrievable using <code>vlabels()</code> or, together with names, using <code>namlab()</code>).</p>\n<p>There are 3 different ways to reshape this data to make it easier to analyze. The first is to simply melt it into a long frame, e.g. for plotting with <code>ggplot2</code>:</p>\n<pre># Pivot Longer\npivot(GGDC10S, ids = 1:5, \n      names = list(variable = \"Sectorcode\", value = \"Value\"), \n      labels = \"Sector\", how = \"longer\", na.rm = TRUE) |&gt; head()\n##   Country Regioncode             Region Variable Year Sectorcode       Sector    Value\n## 1     BWA        SSA Sub-saharan Africa       VA 1964        AGR Agriculture  16.30154\n## 2     BWA        SSA Sub-saharan Africa       VA 1965        AGR Agriculture  15.72700\n## 3     BWA        SSA Sub-saharan Africa       VA 1966        AGR Agriculture  17.68066\n## 4     BWA        SSA Sub-saharan Africa       VA 1967        AGR Agriculture  19.14591\n## 5     BWA        SSA Sub-saharan Africa       VA 1968        AGR Agriculture  21.09957\n## 6     BWA        SSA Sub-saharan Africa       VA 1969        AGR Agriculture  21.86221</pre>\n<p>Note how specifying the <code>labels</code> argument created a column that captures the sector descriptions, which would otherwise be lost in the reshaping process, and <code>na.rm = TRUE</code> removed missing values in the long frame. I note without demonstration that this operation has an exact reverse operation: <code>pivot(long_df, 1:5, \"Value\", \"Sector\", \"Description\", how = \"wider\")</code>.</p>\n<p>The second way to reshape the data is to create a wider frame with sector-variable columns:</p>\n<pre># Pivot Wider\npivot(GGDC10S, ids = 1:5, names = \"Variable\", how = \"wider\", na.rm = TRUE) |&gt; \n  namlab(N = TRUE, Ndistinct = TRUE)\n##      Variable    N Ndist                                                 Label\n## 1     Country 3376    36                                               Country\n## 2  Regioncode 3376     6                                           Region code\n## 3      Region 3376     6                                                Region\n## 4    Variable 3376     2                                              Variable\n## 5        Year 3376    67                                                  Year\n## 6      AGR_VA 1702  1700                                          Agriculture \n## 7     AGR_EMP 1674  1669                                          Agriculture \n## 8      MIN_VA 1702  1641                                                Mining\n## 9     MIN_EMP 1674  1622                                                Mining\n## 10     MAN_VA 1702  1702                                         Manufacturing\n## 11    MAN_EMP 1674  1672                                         Manufacturing\n## 12      PU_VA 1702  1665                                             Utilities\n## 13     PU_EMP 1674  1615                                             Utilities\n## 14     CON_VA 1702  1693                                          Construction\n## 15    CON_EMP 1674  1668                                          Construction\n## 16     WRT_VA 1702  1695                         Trade, restaurants and hotels\n## 17    WRT_EMP 1674  1670                         Trade, restaurants and hotels\n## 18     TRA_VA 1702  1694                  Transport, storage and communication\n## 19    TRA_EMP 1674  1662                  Transport, storage and communication\n## 20    FIRE_VA 1702  1696 Finance, insurance, real estate and business services\n## 21   FIRE_EMP 1674  1674 Finance, insurance, real estate and business services\n## 22     GOV_VA 1702  1698                                   Government services\n## 23    GOV_EMP 1674  1666                                   Government services\n## 24     OTH_VA 1702  1695               Community, social and personal services\n## 25    OTH_EMP 1674  1671               Community, social and personal services\n## 26     SUM_VA 1702  1702                               Summation of sector GDP\n## 27    SUM_EMP 1674  1674                               Summation of sector GDP</pre>\n<p>Note how the variable labels were copied to each of the two variables created for each sector. It is also possible to pass argument <code>transpose = c(\"columns\", \"names\")</code> to change the order of columns and/or naming of the casted columns. Wide pivots where multiple columns are cast do not have a well-defined reverse operation. It may nevertheless be very useful to analyze individual sectors.</p>\n<p>The third useful way to reshape this data for analysis is to recast it such that each variable goes into a separate column and the sectors are stacked in one column:</p>\n<pre># Pivot Recast\nrecast_df = pivot(GGDC10S, values = 6:16, \n      names = list(from = \"Variable\", to = \"Sectorcode\"),\n      labels = list(to = \"Sector\"), how = \"recast\", na.rm = TRUE)\nhead(recast_df)\n##   Country Regioncode             Region Year Sectorcode       Sector       VA      EMP\n## 1     BWA        SSA Sub-saharan Africa 1964        AGR Agriculture  16.30154 152.1179\n## 2     BWA        SSA Sub-saharan Africa 1965        AGR Agriculture  15.72700 153.2971\n## 3     BWA        SSA Sub-saharan Africa 1966        AGR Agriculture  17.68066 153.8867\n## 4     BWA        SSA Sub-saharan Africa 1967        AGR Agriculture  19.14591 155.0659\n## 5     BWA        SSA Sub-saharan Africa 1968        AGR Agriculture  21.09957 156.2451\n## 6     BWA        SSA Sub-saharan Africa 1969        AGR Agriculture  21.86221 157.4243</pre>\n<p>This is useful, for example, if we wanted to run a regression with sector-fixed effects. The code to reverse this pivot is</p>\n<pre># Reverse Pivot Recast\npivot(recast_df, values = c(\"VA\", \"EMP\"), \n      names = list(from = \"Sectorcode\", to = \"Variable\"),\n      labels = list(from = \"Sector\"), how = \"recast\") |&gt; head(3)\n##   Country Regioncode             Region Year Variable      AGR\n## 1     BWA        SSA Sub-saharan Africa 1964       VA 16.30154\n## 2     BWA        SSA Sub-saharan Africa 1965       VA 15.72700\n## 3     BWA        SSA Sub-saharan Africa 1966       VA 17.68066</pre>\n<p>This showcased just some of the functionality of <code>pivot()</code>, more extensive examples are available in the <a href=\"https://sebkrantz.github.io/collapse/reference/pivot.html\" rel=\"nofollow\" target=\"_blank\">documentation</a> (<code>?pivot</code>). But the above is enough to demonstrate this unified API’s power and flexibility; it is also blazing fast.</p>\n</div>\n<div class=\"section level1\" id=\"global-configurability-and-interactive-namespace-masking\">\n<h1>Global Configurability and Interactive Namespace Masking</h1>\n<p>The third major feature of <em>collapse</em> 2.0 is its extensive <a href=\"https://sebkrantz.github.io/collapse/reference/collapse-options.html\" rel=\"nofollow\" target=\"_blank\">global configurability</a> via the <code>set_collapse()</code> function, which includes the default behavior for missing values (<code>na.rm</code> arguments in all statistical functions and algorithms), sorted grouping (<code>sort</code>), multithreading and algorithmic optimizations (<code>nthreads</code>, <code>stable.algo</code>), presentational settings (<code>stub</code>, <code>digits</code>, <code>verbose</code>), and, surpassing all else, the package namespace itself (<code>mask</code>, <code>remove</code>).</p>\n<p>Why should the namespace, in particular, be modifiable? The main reason is that <em>collapse</em> provides many enhanced and performance improved equivalents to functions present in base R and <em>dplyr</em>, such as the <a href=\"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html\" rel=\"nofollow\" target=\"_blank\"><em>Fast Statistical Functions</em></a>, fast <a href=\"https://sebkrantz.github.io/collapse/reference/fast-grouping-ordering.html\" rel=\"nofollow\" target=\"_blank\"><em>Grouping and Ordering</em></a> functions and algorithms, <a href=\"https://sebkrantz.github.io/collapse/reference/fast-data-manipulation.html\" rel=\"nofollow\" target=\"_blank\"><em>Data Manipulation</em></a> and <a href=\"https://sebkrantz.github.io/collapse/reference/time-series-panel-series.html\" rel=\"nofollow\" target=\"_blank\"><em>Time Series</em></a> functions.</p>\n<p><em>collapse</em> is intentionally fully compatible with the base R and <em>dplyr</em> namespaces by adding f-prefixes to these performance-improved functions where conflicts exist. Since v1.7.0, there exists a global option <code>\"collapse_mask\"</code> which can be set before the package is loaded to export non-prefixed versions of these functions, but this was somewhat tedious and best done with an <code>.Rprofile</code> file. <em>collapse</em> 2.0 also adds this option to <code>set_collapse()</code> and makes it fully interactive; that is, it can be set and changed at any point within the active session.</p>\n<p>Concretely, what does this mean? Base R and <em>dplyr</em> are relatively slow compared to what can be achieved with group-level vectorization, SIMD instructions, and efficient algorithms, especially as data grows. To provide an example, I generate some large vectors and run some benchmarks for basic operations:</p>\n<pre>ul &lt;- outer(letters, letters, paste0)\nl &lt;- sample(ul, 1e7, replace = TRUE)\nm &lt;- sample(outer(month.abb, month.abb, paste0), 1e7, replace = TRUE)\nx &lt;- na_insert(rnorm(1e7), prop = 0.05)\ndata &lt;- data.frame(l, m, x)\n\nlibrary(microbenchmark)\nmicrobenchmark(\n  unique(l),\n  table(l, m),\n  sum(x, na.rm = TRUE),\n  median(x, na.rm = TRUE),\n  mean(x, na.rm = TRUE),\ntimes = 10)\n## Warning in microbenchmark(unique(l), table(l, m), sum(x, na.rm = TRUE), : less accurate nanosecond times to avoid potential integer overflows\n## Unit: milliseconds\n##                     expr       min        lq      mean    median        uq       max neval\n##                unique(l)  82.07856  85.14671  91.08940  86.02663  87.65242 124.87604    10\n##              table(l, m) 506.91215 546.24042 554.55146 549.27130 565.78729 640.67941    10\n##     sum(x, na.rm = TRUE)  15.39316  15.40485  15.58453  15.45362  15.54109  16.57486    10\n##  median(x, na.rm = TRUE) 155.55667 157.19572 164.38405 160.75044 165.29642 196.26446    10\n##    mean(x, na.rm = TRUE)  53.79520  54.12406  60.39059  55.94202  57.72792  97.40887    10\n\nlibrary(dplyr)\nmicrobenchmark(\n  dplyr = data |&gt;\n    subset(l %in% ul[1:500]) |&gt;\n    group_by(l, m) |&gt;\n    summarize(mean_x = mean(x, na.rm = TRUE), \n              median_x = median(x, na.rm = TRUE)), \ntimes = 10)\n## Unit: seconds\n##   expr      min       lq     mean   median       uq      max neval\n##  dplyr 2.159383 2.193034 2.230608 2.226005 2.250952 2.370895    10</pre>\n<p>The beauty of namespace masking is that we can turn parts or all of this code into <em>collapse</em> code by simply invoking the <code>mask</code> option to <code>set_collapse()</code>. The most comprehensive setting is <code>mask = \"all\"</code>. It is a secure option because invoking it instantly exports these functions in the <em>collapse</em> namespace and re-attaches the namespace to make sure it is at the top of the search path:</p>\n<pre>set_collapse(mask = \"all\")\n# This is all collapse code now + no need to set na.rm = TRUE (default in collapse)\n# We could use set_collapse(na.rm = FALSE) to bring collapse in-line with base R\nmicrobenchmark(\n  unique(l),\n  table(l, m),\n  sum(x),\n  median(x),\n  mean(x),\ntimes = 10)\n## Unit: milliseconds\n##         expr       min        lq       mean     median         uq        max neval\n##    unique(l) 14.215561 15.289105  15.516647  15.564871  15.786722  16.411152    10\n##  table(l, m) 94.875968 97.539410 107.189014 108.895508 115.038948 118.004806    10\n##       sum(x)  1.968984  1.998955   2.053936   2.037946   2.128843   2.152500    10\n##    median(x) 90.736772 91.784609  93.486921  92.288725  94.812787  99.236277    10\n##      mean(x)  2.384314  2.389931   2.454666   2.419861   2.456023   2.680621    10\n\nmicrobenchmark(\n  collapse = data |&gt;\n    subset(l %in% ul[1:500]) |&gt;\n    group_by(l, m) |&gt;\n    summarize(mean_x = mean(x), \n              median_x = median(x)), \ntimes = 10)\n## Unit: milliseconds\n##      expr      min       lq     mean   median       uq      max neval\n##  collapse 344.2272 351.6525 363.8762 358.4373 373.9692 402.6943    10\n\n# Reset the masking \n# set_collapse(mask = NULL)</pre>\n<p>Evidently, the <em>collapse</em> code runs much faster. The 5-10x speedups shown here are quite normal. Higher speedups can be experienced for grouped operations as the number of groups grows large and repetition in R becomes very costly. As indicated before, masking in <em>collapse</em> 2.0 is fully interactive and reversible: invoking <code>set_collapse(mask = NULL)</code> and running the same code again will execute it again with base R and <em>dplyr</em>.</p>\n<p>So, in summary, <em>collapse</em> 2.0 provides fast R, in R, in a very simple and broadly accessible way. There are many other advantages to using <em>collapse</em>, e.g., given that its <a href=\"https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html\" rel=\"nofollow\" target=\"_blank\"><em>Fast Statistical Functions</em></a> are S3 generic and support grouped and weighted aggregations and transformations out of the box, this saves many unnecessary calls to <code>apply()</code>, <code>lapply()</code> or <code>summarise()</code>, etc. (in addition to many unnecessary specifications of <code>na.rm = TRUE</code>) e.g.:</p>\n<pre># S3 generic statistical functions save a lot of syntax\nmean(mtcars)                 # = sapply(mtcars, mean)\n##        mpg        cyl       disp         hp       drat         wt       qsec         vs         am       gear       carb \n##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250   3.687500   2.812500\nmean(mtcars, w = runif(32))  # = sapply(mtcars, weighted.mean, w = runif(32))\n##         mpg         cyl        disp          hp        drat          wt        qsec          vs          am        gear        carb \n##  20.9009889   5.9682791 221.9436951 137.7951392   3.6740174   3.1670541  18.0195300   0.4878638   0.4240404   3.7486737   2.6293563\nmean(mtcars$mpg, mtcars$cyl) # = tapply(mtcars$mpg, mtcars$cyl, mean)\n##        4        6        8 \n## 26.66364 19.74286 15.10000\nmean(mtcars, TRA = \"-\") |&gt;   # = sweep(mtcars, 2, sapply(mtcars, mean))\n  head()\n##                         mpg     cyl        disp       hp       drat       wt     qsec      vs       am    gear    carb\n## Mazda RX4          0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.59725 -1.38875 -0.4375  0.59375  0.3125  1.1875\n## Mazda RX4 Wag      0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.34225 -0.82875 -0.4375  0.59375  0.3125  1.1875\n## Datsun 710         2.709375 -2.1875 -122.721875 -53.6875  0.2534375 -0.89725  0.76125  0.5625  0.59375  0.3125 -1.8125\n## Hornet 4 Drive     1.309375 -0.1875   27.278125 -36.6875 -0.5165625 -0.00225  1.59125  0.5625 -0.40625 -0.6875 -1.8125\n## Hornet Sportabout -1.390625  1.8125  129.278125  28.3125 -0.4465625  0.22275 -0.82875 -0.4375 -0.40625 -0.6875 -0.8125\n## Valiant           -1.990625 -0.1875   -5.721875 -41.6875 -0.8365625  0.24275  2.37125  0.5625 -0.40625 -0.6875 -1.8125\nmtcars |&gt; group_by(cyl, vs, am) |&gt; \n  mean() # = summarize(across(everything(), mean))\n##   cyl vs am      mpg     disp        hp     drat       wt     qsec     gear     carb\n## 1   4  0  1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 2.000000\n## 2   4  1  0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 1.666667\n## 3   4  1  1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 1.428571\n## 4   6  0  1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667\n## 5   6  1  0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000\n## 6   8  0  0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333\n## 7   8  0  1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000</pre>\n</div>\n<div class=\"section level1\" id=\"concluding-reflections\">\n<h1>Concluding Reflections</h1>\n<p>It has been a remarkable 3.5-year-long journey leading up to the development of <em>collapse</em> 2.0, and a tremendous feat of time, energy, and determination. I could probably have published 2 academic papers instead, but would still be writing horrible code like most economists, be trying to do complex things in econometrics, etc., using other mainstream data science libraries not really designed for that, or, worst case, just be stuck to commercial software. I’m happy I came out as an open-source developer and that I’ve been accompanied on this path by other great people from my profession.</p>\n<p>I also never regretted choosing R as a primary language. I find it unique in its simplicity and parsimony, the ability to work with different objects like vectors, matrices, and data frames in a fluent way, and to do rather complex things like matching with a single function call.</p>\n<p>On the other hand, R for me always lacked speed and the ability to do advanced statistical and data manipulation operations with ease, as I was used to coming from commercial environments (STATA, Mathematica).</p>\n<p><em>collapse</em> is my determined attempt to bring statistical complexity, parsimony, speed, and joy to statistics and data manipulation in R, and I believe it is the most encompassing attempt out there and preserves the fundamental character of the language.</p>\n<p>Notably, the <em>collapse</em> approach is not limited to a certain object (like e.g. <em>data.table</em>, which remains a great idea and implementation), and does not rely on data structures and syntax that are somewhat alien/external to the language and do not integrate with many of its first-order features (e.g. <em>arrow</em>, <em>polars</em>, <em>duckdb</em>). It is also arguably more successful than alternative ways to implement or compile the language (<a href=\"https://github.com/oracle/fastr\" rel=\"nofollow\" target=\"_blank\"><em>FastR</em> / <em>Graal VM</em></a>), because the fundamental performance problem in R is algorithmic efficiency and the lack of low-level vectorization for repetitive tasks<a class=\"footnote-ref\" href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fn2\" id=\"fnref2\" rel=\"nofollow\" target=\"_blank\"><sup>2</sup></a>.\n<!--\nConcretely, I think collapse 2.0, with its broad set of functions and algorithms ranging from basic arithmetic and programming to basic and advanced statistics, unique values, matching, ordering, and broad data manipulation, together with the namespace masking options and global configurability, is probably the most encompassing attempt ever made to fundamentally speed up and enrich the R language itself.\n--></p>\n<p>By reimplementing core parts of the language using efficient algorithms and providing rich and flexible vectorizations for many statistical operations across columns and groups in a class-agnostic way supporting nearly all frequently used data structures, <em>collapse</em> solves the fundamental performance problem in a way that integrates seamlessly with the core of the language. It also adds much-needed statistical complexity, particularly for weighted statistics, time series, and panel data. In short, it provides advanced and fast R, inside GNU R.</p>\n<p>It is not, and will never be, the absolute best that can be done in performance terms. The data formats used by the best-performing systems (such as the <em>arrow</em> columnar format underlying <em>polars</em>) are designed at the memory level to optimally use computer resources (SIMD etc.), with database applications in mind, and the people doing this did not study economics. But it is not yet clear that such architectures are very suitable for languages meant to do broad and linear-algebra heavy statistical computing tasks, and R just celebrated its 30th birthday this year. So, given the constraints imposed by a 30-year-old C-based language and API, frameworks like <em>collapse</em> and <em>data.table</em> are pushing the boundaries very far<a class=\"footnote-ref\" href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fn3\" id=\"fnref3\" rel=\"nofollow\" target=\"_blank\"><sup>3</sup></a>.</p>\n<p>Let me stop here; <em>collapse</em> 2.0 is out. It changed my R life, and I hope it will change yours.</p>\n</div>\n<div class=\"footnotes footnotes-end-of-document\">\n<hr>\n<ol>\n<li id=\"fn1\"><p>Some initial benchmarks were shared on <a href=\"https://twitter.com/collapse_R\" rel=\"nofollow\" target=\"_blank\">Twitter</a>, and <em>collapse</em> is about to enter the <a href=\"https://duckdb.org/2023/04/14/h2oai.html#results\" rel=\"nofollow\" target=\"_blank\">DuckDB database like ops benchmark</a>. <code>fmatch()</code> is also nearly an order of magnitude faster than <code>match()</code> for atomic vectors.<a class=\"footnote-back\" href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fnref1\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn2\"><p>See e.g. benchmarks for <a href=\"https://github.com/brodieG/r2c\" rel=\"nofollow\" target=\"_blank\"><em>r2c</em></a>.<a class=\"footnote-back\" href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fnref2\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n<li id=\"fn3\"><p>Actually, it is extremely impressive how well <em>data.table</em> still performs compared to modern libraries based on optimized memory models. The <a href=\"https://duckdblabs.github.io/db-benchmark/\" rel=\"nofollow\" target=\"_blank\">benchmarks</a> also show that in high-cardinality settings (many groups relative to the data size), optimized memory models don’t pay off that much, indicating that there is always a tradeoff between the complexity of statistical operations and the possibility of vectorization/bulk processing on modern hardware.<a class=\"footnote-back\" href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/#fnref3\" rel=\"nofollow\" target=\"_blank\">↩︎</a></p></li>\n</ol>\n</hr></div>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 3.8.9-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://sebkrantz.github.io/Rblog/2023/10/15/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/\"> R, Econometrics, High Performance</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\r\n\r\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\r\n</div> </div>\n</article>",
      "main_text": "Releasing collapse 2.0: Blazing Fast Joins, Reshaping, and Enhanced R\nPosted on\nOctober 14, 2023\nby\nR, Econometrics, High Performance\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nR, Econometrics, High Performance\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nI’m excited to announce the release of\ncollapse\n2.0, adding blazing fast joins, pivots, flexible namespace and many other features, including a brand new\nwebsite\n, an updated\ncheat sheet\n, and a new\nvignette\naimed at\ntidyverse\nusers.\nIn the 3.5 years after the first release of\ncollapse\n1.0 to CRAN in March 2020, the package has seen 10 major updates, and become a remarkable piece of statistical software that is robust, stable, lightweight, fast, statistically advanced, comprehensive, flexible, class-agnostic, verbose and well-documented. It is profoundly able to deal with rich (multi-level, irregular, weighted, nested, labelled, and missing) scientific data, and can enhance the workflow of every R user.\nThe addition of rich, fast, and verbose joins and pivots in this release, together with secure interactive namespace masking and extensive global configurability, should enable many R users to use it as a workhorse package for data manipulation and statistical computing tasks.\nIn this post, I briefly introduce the core new features of this release and end with some reflections on why I created the package and think that its approach towards speeding up and enriching R is more encompassing than others.\nFast, Class-Agnostic, and Verbose Table Joins\nJoins in\ncollapse\nhas been a much-requested feature. Still, I was long hesitant to take them on because they are complex, high-risk, operations, and I was unsure on how to go about them or provide an implementation that would satisfy my own ambitious demands to their performance, generality and verbosity.\nI am glad that, following repeated requests, I have overcome these hesitations and designed an implementation – inspired by\npolars\n– that I am very satisfied with.\ncollapse\n’s join function is simply called\njoin()\n, and provides 6 types of joins (left, inner, full, right, semi and anti), controlled by a\nhow\nargument – the default being a left join. It also provides two separate join algorithms: a vectorized hash join (the default,\nsort = FALSE\n) and a sort-merge-join (\nsort = TRUE\n). The join-column argument is called\non\n, and, if left empty, selects columns present in both datasets. An example with generated data follows:\nlibrary(collapse)\ndf1 <- data.frame(\n  id1 = c(1, 1, 2, 3),\n  id2 = c(\"a\", \"b\", \"b\", \"c\"),\n  name = c(\"John\", \"Jane\", \"Bob\", \"Carl\"),\n  age = c(35, 28, 42, 50)\n)\ndf2 <- data.frame(\n  id1 = c(1, 2, 3, 3),\n  id2 = c(\"a\", \"b\", \"c\", \"e\"),\n  salary = c(60000, 55000, 70000, 80000),\n  dept = c(\"IT\", \"Marketing\", \"Sales\", \"IT\")\n)\n\n# Different types of joins\nfor (i in c(\"left\",\"inner\",\"right\",\"full\",\"semi\",\"anti\"))\n    join(df1, df2, how = i) |> print()\n## left join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   1   b Jane  28     NA      <NA>\n## 3   2   b  Bob  42  55000 Marketing\n## 4   3   c Carl  50  70000     Sales\n## inner join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   2   b  Bob  42  55000 Marketing\n## 3   3   c Carl  50  70000     Sales\n## right join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   2   b  Bob  42  55000 Marketing\n## 3   3   c Carl  50  70000     Sales\n## 4   3   e <NA>  NA  80000        IT\n## full join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   1   b Jane  28     NA      <NA>\n## 3   2   b  Bob  42  55000 Marketing\n## 4   3   c Carl  50  70000     Sales\n## 5   3   e <NA>  NA  80000        IT\n## semi join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age\n## 1   1   a John  35\n## 2   2   b  Bob  42\n## 3   3   c Carl  50\n## anti join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age\n## 1   1   b Jane  28\nNotice how, by default (\nverbose = 1\n), a compact summary of the operation is printed, indicating the type of join, the datasets and columns, and the number and percentage of records from each dataset matched in the join operation.\njoin()\nalso preserves the attributes of the first argument (\nx\n) and the order of columns and rows (default\nkeep.col.order = TRUE\n,\nsort = FALSE\n) in it. We can thus think of a join operation as adding columns to a data frame-like object (\nx\n) from another similar object (\ny\n) .\nThere are several additional options to increase verbosity and assimilate the join operation:\n# verbose = 2 also shows classes, allowing you to detect implicit conversions (inside fmatch())\njoin(df1, df2, how = \"left\", verbose = 2)\n## left join: df1[id1:numeric, id2:character] 3/4 (75%) <m:m> df2[id1:numeric, id2:character] 3/4 (75%)\n##   id1 id2 name age salary      dept\n## 1   1   a John  35  60000        IT\n## 2   1   b Jane  28     NA      <NA>\n## 3   2   b  Bob  42  55000 Marketing\n## 4   3   c Carl  50  70000     Sales\n\n# Adding join column: useful especially for full join\njoin(df1, df2, how = \"full\", column = TRUE)\n## full join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 name age salary      dept   .join\n## 1   1   a John  35  60000        IT matched\n## 2   1   b Jane  28     NA      <NA>     df1\n## 3   2   b  Bob  42  55000 Marketing matched\n## 4   3   c Carl  50  70000     Sales matched\n## 5   3   e <NA>  NA  80000        IT     df2\n\n# Custom column + rearranging\njoin(df1, df2, how = \"full\", column = list(\"join\", c(\"x\", \"y\", \"x_y\")), \n     keep.col.order = FALSE)\n## full join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n##   id1 id2 join name age salary      dept\n## 1   1   a  x_y John  35  60000        IT\n## 2   1   b    x Jane  28     NA      <NA>\n## 3   2   b  x_y  Bob  42  55000 Marketing\n## 4   3   c  x_y Carl  50  70000     Sales\n## 5   3   e    y <NA>  NA  80000        IT\n\n# Attaching match attribute\nstr(join(df1, df2, attr = TRUE))\n## left join: df1[id1, id2] 3/4 (75%) <m:m> df2[id1, id2] 3/4 (75%)\n## 'data.frame':\t4 obs. of  6 variables:\n##  $ id1   : num  1 1 2 3\n##  $ id2   : chr  \"a\" \"b\" \"b\" \"c\"\n##  $ name  : chr  \"John\" \"Jane\" \"Bob\" \"Carl\"\n##  $ age   : num  35 28 42 50\n##  $ salary: num  60000 NA 55000 70000\n##  $ dept  : chr  \"IT\" NA \"Marketing\" \"Sales\"\n##  - attr(*, \"join.match\")=List of 3\n##   ..$ call   : language join(x = df1, y = df2, attr = TRUE)\n##   ..$ on.cols:List of 2\n##   .. ..$ x: chr [1:2] \"id1\" \"id2\"\n##   .. ..$ y: chr [1:2] \"id1\" \"id2\"\n##   ..$ match  : 'qG' int [1:4] 1 NA 2 3\n##   .. ..- attr(*, \"N.nomatch\")= int 1\n##   .. ..- attr(*, \"N.groups\")= int 4\n##   .. ..- attr(*, \"N.distinct\")= int 3\nFinally, it is possible to validate the join operation to be either one of\n\"m:m\"\n(default, no checks),\n\"1:m\"\n,\n\"m:1\"\nor\n\"1:1\"\n. For example:\njoin(df1, rowbind(df2, df2), validate = \"1:1\")\n## Error in join(df1, rowbind(df2, df2), validate = \"1:1\"): Join is not 1:1: df1 (x) is unique on the join columns; rowbind (y) is not unique on the join columns\nAnother check being automatically executed inside the workhorse function\nfmatch()\n(if\nsort = FALSE\n) is for overidentified join conditions, i.e., if the records are more than identified by the join columns. For example if we added\n\"name\"\nand\n\"dept\"\nto the join condition, this would issue a warning as the match is already identified by\n\"id1\"\nand\n\"id2\"\n:\njoin(df1, df2, on = c(\"id1\", \"id2\", \"name\" = \"dept\"), how = \"left\")\n## Warning in fmatch(x[ixon], y[iyon], nomatch = NA_integer_, count = count, : Overidentified match/join: the first 2 of 3 columns uniquely match the records. With overid > 0, fmatch() continues to\n## match columns. Consider removing columns or setting overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning.\n## left join: df1[id1, id2, name] 0/4 (0%) <m:m> df2[id1, id2, dept] 0/4 (0%)\n##   id1 id2 name age salary\n## 1   1   a John  35     NA\n## 2   1   b Jane  28     NA\n## 3   2   b  Bob  42     NA\n## 4   3   c Carl  50     NA\nThe warning can be silenced by passing\noverid = 2\nto\njoin()\n. To see better where this may be useful, consider the following example using\nfmatch()\n.\ndf1 <- data.frame(\n  id1 = c(1, 1, 2, 3),\n  id2 = c(\"a\", \"b\", \"b\", \"c\"),\n  name = c(\"John\", \"Bob\", \"Jane\", \"Carl\")\n)\ndf2 <- data.frame(\n  id1 = c(1, 2, 3, 3),\n  id2 = c(\"a\", \"b\", \"c\", \"e\"),\n  name = c(\"John\", \"Janne\", \"Carl\", \"Lynne\")\n)\n\n# This gives an overidentification warning: columns 1:2 identify the data\nfmatch(df1, df2)\n## Warning in fmatch(df1, df2): Overidentified match/join: the first 2 of 3 columns uniquely match the records. With overid > 0, fmatch() continues to match columns. Consider removing columns or setting\n## overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning.\n## [1]  1 NA NA  3\n# This just runs through without warning\nfmatch(df1, df2, overid = 2)\n## [1]  1 NA NA  3\n# This terminates computation after first 2 columns\nfmatch(df1, df2, overid = 0)\n## [1]  1 NA  2  3\nfmatch(df1[1:2], df2[1:2])  # Same thing!\n## [1]  1 NA  2  3\n# -> note that here we get an additional match based on the unique ids,\n# which we didn't get before because \"Jane\" != \"Janne\"\nSo, in summary, the implementation of joins on\ncollapse\nas provided by the\njoin()\nfunction is not only blazing fast\n1\nand class-agnostic but also allows you to verify all aspects of this high-risk operation.\nAdvanced Pivots\nThe second big addition in\ncollapse\n2.0 is\npivot()\n, which provides advanced data reshaping capabilities in a single parsimonious API. Notably, it supports longer-, wider-, and recast-pivoting functionality and can accommodate variable labels in the reshaping process.\nFortunately,\ncollapse\nsupplies a perfect test dataset to illustrate these capabilities: the 2014\nGroningen Growth and Development Centre 10-Sector Database\n, which provides sectoral employment and value-added series for 10 broad sectors in 43 countries:\nhead(GGDC10S)\n##   Country Regioncode             Region Variable Year      AGR      MIN       MAN        PU       CON      WRT      TRA     FIRE      GOV      OTH      SUM\n## 1     BWA        SSA Sub-saharan Africa       VA 1960       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 2     BWA        SSA Sub-saharan Africa       VA 1961       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 3     BWA        SSA Sub-saharan Africa       VA 1962       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 4     BWA        SSA Sub-saharan Africa       VA 1963       NA       NA        NA        NA        NA       NA       NA       NA       NA       NA       NA\n## 5     BWA        SSA Sub-saharan Africa       VA 1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229\n## 6     BWA        SSA Sub-saharan Africa       VA 1965 15.72700 2.495768 1.0181992 0.1350976 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710\n\nnamlab(GGDC10S, N = TRUE, Ndistinct = TRUE)\n##      Variable    N Ndist                                                 Label\n## 1     Country 5027    43                                               Country\n## 2  Regioncode 5027     6                                           Region code\n## 3      Region 5027     6                                                Region\n## 4    Variable 5027     2                                              Variable\n## 5        Year 5027    67                                                  Year\n## 6         AGR 4364  4353                                          Agriculture \n## 7         MIN 4355  4224                                                Mining\n## 8         MAN 4355  4353                                         Manufacturing\n## 9          PU 4354  4237                                             Utilities\n## 10        CON 4355  4339                                          Construction\n## 11        WRT 4355  4344                         Trade, restaurants and hotels\n## 12        TRA 4355  4334                  Transport, storage and communication\n## 13       FIRE 4355  4349 Finance, insurance, real estate and business services\n## 14        GOV 3482  3470                                   Government services\n## 15        OTH 4248  4238               Community, social and personal services\n## 16        SUM 4364  4364                               Summation of sector GDP\nEvidently, the data is supplied in a format where two variables, employment and value-added, are stacked in each sector column. The data is also labeled, with descriptions attached as\n\"label\"\nattributes (retrievable using\nvlabels()\nor, together with names, using\nnamlab()\n).\nThere are 3 different ways to reshape this data to make it easier to analyze. The first is to simply melt it into a long frame, e.g. for plotting with\nggplot2\n:\n# Pivot Longer\npivot(GGDC10S, ids = 1:5, \n      names = list(variable = \"Sectorcode\", value = \"Value\"), \n      labels = \"Sector\", how = \"longer\", na.rm = TRUE) |> head()\n##   Country Regioncode             Region Variable Year Sectorcode       Sector    Value\n## 1     BWA        SSA Sub-saharan Africa       VA 1964        AGR Agriculture  16.30154\n## 2     BWA        SSA Sub-saharan Africa       VA 1965        AGR Agriculture  15.72700\n## 3     BWA        SSA Sub-saharan Africa       VA 1966        AGR Agriculture  17.68066\n## 4     BWA        SSA Sub-saharan Africa       VA 1967        AGR Agriculture  19.14591\n## 5     BWA        SSA Sub-saharan Africa       VA 1968        AGR Agriculture  21.09957\n## 6     BWA        SSA Sub-saharan Africa       VA 1969        AGR Agriculture  21.86221\nNote how specifying the\nlabels\nargument created a column that captures the sector descriptions, which would otherwise be lost in the reshaping process, and\nna.rm = TRUE\nremoved missing values in the long frame. I note without demonstration that this operation has an exact reverse operation:\npivot(long_df, 1:5, \"Value\", \"Sector\", \"Description\", how = \"wider\")\n.\nThe second way to reshape the data is to create a wider frame with sector-variable columns:\n# Pivot Wider\npivot(GGDC10S, ids = 1:5, names = \"Variable\", how = \"wider\", na.rm = TRUE) |> \n  namlab(N = TRUE, Ndistinct = TRUE)\n##      Variable    N Ndist                                                 Label\n## 1     Country 3376    36                                               Country\n## 2  Regioncode 3376     6                                           Region code\n## 3      Region 3376     6                                                Region\n## 4    Variable 3376     2                                              Variable\n## 5        Year 3376    67                                                  Year\n## 6      AGR_VA 1702  1700                                          Agriculture \n## 7     AGR_EMP 1674  1669                                          Agriculture \n## 8      MIN_VA 1702  1641                                                Mining\n## 9     MIN_EMP 1674  1622                                                Mining\n## 10     MAN_VA 1702  1702                                         Manufacturing\n## 11    MAN_EMP 1674  1672                                         Manufacturing\n## 12      PU_VA 1702  1665                                             Utilities\n## 13     PU_EMP 1674  1615                                             Utilities\n## 14     CON_VA 1702  1693                                          Construction\n## 15    CON_EMP 1674  1668                                          Construction\n## 16     WRT_VA 1702  1695                         Trade, restaurants and hotels\n## 17    WRT_EMP 1674  1670                         Trade, restaurants and hotels\n## 18     TRA_VA 1702  1694                  Transport, storage and communication\n## 19    TRA_EMP 1674  1662                  Transport, storage and communication\n## 20    FIRE_VA 1702  1696 Finance, insurance, real estate and business services\n## 21   FIRE_EMP 1674  1674 Finance, insurance, real estate and business services\n## 22     GOV_VA 1702  1698                                   Government services\n## 23    GOV_EMP 1674  1666                                   Government services\n## 24     OTH_VA 1702  1695               Community, social and personal services\n## 25    OTH_EMP 1674  1671               Community, social and personal services\n## 26     SUM_VA 1702  1702                               Summation of sector GDP\n## 27    SUM_EMP 1674  1674                               Summation of sector GDP\nNote how the variable labels were copied to each of the two variables created for each sector. It is also possible to pass argument\ntranspose = c(\"columns\", \"names\")\nto change the order of columns and/or naming of the casted columns. Wide pivots where multiple columns are cast do not have a well-defined reverse operation. It may nevertheless be very useful to analyze individual sectors.\nThe third useful way to reshape this data for analysis is to recast it such that each variable goes into a separate column and the sectors are stacked in one column:\n# Pivot Recast\nrecast_df = pivot(GGDC10S, values = 6:16, \n      names = list(from = \"Variable\", to = \"Sectorcode\"),\n      labels = list(to = \"Sector\"), how = \"recast\", na.rm = TRUE)\nhead(recast_df)\n##   Country Regioncode             Region Year Sectorcode       Sector       VA      EMP\n## 1     BWA        SSA Sub-saharan Africa 1964        AGR Agriculture  16.30154 152.1179\n## 2     BWA        SSA Sub-saharan Africa 1965        AGR Agriculture  15.72700 153.2971\n## 3     BWA        SSA Sub-saharan Africa 1966        AGR Agriculture  17.68066 153.8867\n## 4     BWA        SSA Sub-saharan Africa 1967        AGR Agriculture  19.14591 155.0659\n## 5     BWA        SSA Sub-saharan Africa 1968        AGR Agriculture  21.09957 156.2451\n## 6     BWA        SSA Sub-saharan Africa 1969        AGR Agriculture  21.86221 157.4243\nThis is useful, for example, if we wanted to run a regression with sector-fixed effects. The code to reverse this pivot is\n# Reverse Pivot Recast\npivot(recast_df, values = c(\"VA\", \"EMP\"), \n      names = list(from = \"Sectorcode\", to = \"Variable\"),\n      labels = list(from = \"Sector\"), how = \"recast\") |> head(3)\n##   Country Regioncode             Region Year Variable      AGR\n## 1     BWA        SSA Sub-saharan Africa 1964       VA 16.30154\n## 2     BWA        SSA Sub-saharan Africa 1965       VA 15.72700\n## 3     BWA        SSA Sub-saharan Africa 1966       VA 17.68066\nThis showcased just some of the functionality of\npivot()\n, more extensive examples are available in the\ndocumentation\n(\n?pivot\n). But the above is enough to demonstrate this unified API’s power and flexibility; it is also blazing fast.\nGlobal Configurability and Interactive Namespace Masking\nThe third major feature of\ncollapse\n2.0 is its extensive\nglobal configurability\nvia the\nset_collapse()\nfunction, which includes the default behavior for missing values (\nna.rm\narguments in all statistical functions and algorithms), sorted grouping (\nsort\n), multithreading and algorithmic optimizations (\nnthreads\n,\nstable.algo\n), presentational settings (\nstub\n,\ndigits\n,\nverbose\n), and, surpassing all else, the package namespace itself (\nmask\n,\nremove\n).\nWhy should the namespace, in particular, be modifiable? The main reason is that\ncollapse\nprovides many enhanced and performance improved equivalents to functions present in base R and\ndplyr\n, such as the\nFast Statistical Functions\n, fast\nGrouping and Ordering\nfunctions and algorithms,\nData Manipulation\nand\nTime Series\nfunctions.\ncollapse\nis intentionally fully compatible with the base R and\ndplyr\nnamespaces by adding f-prefixes to these performance-improved functions where conflicts exist. Since v1.7.0, there exists a global option\n\"collapse_mask\"\nwhich can be set before the package is loaded to export non-prefixed versions of these functions, but this was somewhat tedious and best done with an\n.Rprofile\nfile.\ncollapse\n2.0 also adds this option to\nset_collapse()\nand makes it fully interactive; that is, it can be set and changed at any point within the active session.\nConcretely, what does this mean? Base R and\ndplyr\nare relatively slow compared to what can be achieved with group-level vectorization, SIMD instructions, and efficient algorithms, especially as data grows. To provide an example, I generate some large vectors and run some benchmarks for basic operations:\nul <- outer(letters, letters, paste0)\nl <- sample(ul, 1e7, replace = TRUE)\nm <- sample(outer(month.abb, month.abb, paste0), 1e7, replace = TRUE)\nx <- na_insert(rnorm(1e7), prop = 0.05)\ndata <- data.frame(l, m, x)\n\nlibrary(microbenchmark)\nmicrobenchmark(\n  unique(l),\n  table(l, m),\n  sum(x, na.rm = TRUE),\n  median(x, na.rm = TRUE),\n  mean(x, na.rm = TRUE),\ntimes = 10)\n## Warning in microbenchmark(unique(l), table(l, m), sum(x, na.rm = TRUE), : less accurate nanosecond times to avoid potential integer overflows\n## Unit: milliseconds\n##                     expr       min        lq      mean    median        uq       max neval\n##                unique(l)  82.07856  85.14671  91.08940  86.02663  87.65242 124.87604    10\n##              table(l, m) 506.91215 546.24042 554.55146 549.27130 565.78729 640.67941    10\n##     sum(x, na.rm = TRUE)  15.39316  15.40485  15.58453  15.45362  15.54109  16.57486    10\n##  median(x, na.rm = TRUE) 155.55667 157.19572 164.38405 160.75044 165.29642 196.26446    10\n##    mean(x, na.rm = TRUE)  53.79520  54.12406  60.39059  55.94202  57.72792  97.40887    10\n\nlibrary(dplyr)\nmicrobenchmark(\n  dplyr = data |>\n    subset(l %in% ul[1:500]) |>\n    group_by(l, m) |>\n    summarize(mean_x = mean(x, na.rm = TRUE), \n              median_x = median(x, na.rm = TRUE)), \ntimes = 10)\n## Unit: seconds\n##   expr      min       lq     mean   median       uq      max neval\n##  dplyr 2.159383 2.193034 2.230608 2.226005 2.250952 2.370895    10\nThe beauty of namespace masking is that we can turn parts or all of this code into\ncollapse\ncode by simply invoking the\nmask\noption to\nset_collapse()\n. The most comprehensive setting is\nmask = \"all\"\n. It is a secure option because invoking it instantly exports these functions in the\ncollapse\nnamespace and re-attaches the namespace to make sure it is at the top of the search path:\nset_collapse(mask = \"all\")\n# This is all collapse code now + no need to set na.rm = TRUE (default in collapse)\n# We could use set_collapse(na.rm = FALSE) to bring collapse in-line with base R\nmicrobenchmark(\n  unique(l),\n  table(l, m),\n  sum(x),\n  median(x),\n  mean(x),\ntimes = 10)\n## Unit: milliseconds\n##         expr       min        lq       mean     median         uq        max neval\n##    unique(l) 14.215561 15.289105  15.516647  15.564871  15.786722  16.411152    10\n##  table(l, m) 94.875968 97.539410 107.189014 108.895508 115.038948 118.004806    10\n##       sum(x)  1.968984  1.998955   2.053936   2.037946   2.128843   2.152500    10\n##    median(x) 90.736772 91.784609  93.486921  92.288725  94.812787  99.236277    10\n##      mean(x)  2.384314  2.389931   2.454666   2.419861   2.456023   2.680621    10\n\nmicrobenchmark(\n  collapse = data |>\n    subset(l %in% ul[1:500]) |>\n    group_by(l, m) |>\n    summarize(mean_x = mean(x), \n              median_x = median(x)), \ntimes = 10)\n## Unit: milliseconds\n##      expr      min       lq     mean   median       uq      max neval\n##  collapse 344.2272 351.6525 363.8762 358.4373 373.9692 402.6943    10\n\n# Reset the masking \n# set_collapse(mask = NULL)\nEvidently, the\ncollapse\ncode runs much faster. The 5-10x speedups shown here are quite normal. Higher speedups can be experienced for grouped operations as the number of groups grows large and repetition in R becomes very costly. As indicated before, masking in\ncollapse\n2.0 is fully interactive and reversible: invoking\nset_collapse(mask = NULL)\nand running the same code again will execute it again with base R and\ndplyr\n.\nSo, in summary,\ncollapse\n2.0 provides fast R, in R, in a very simple and broadly accessible way. There are many other advantages to using\ncollapse\n, e.g., given that its\nFast Statistical Functions\nare S3 generic and support grouped and weighted aggregations and transformations out of the box, this saves many unnecessary calls to\napply()\n,\nlapply()\nor\nsummarise()\n, etc. (in addition to many unnecessary specifications of\nna.rm = TRUE\n) e.g.:\n# S3 generic statistical functions save a lot of syntax\nmean(mtcars)                 # = sapply(mtcars, mean)\n##        mpg        cyl       disp         hp       drat         wt       qsec         vs         am       gear       carb \n##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250   3.687500   2.812500\nmean(mtcars, w = runif(32))  # = sapply(mtcars, weighted.mean, w = runif(32))\n##         mpg         cyl        disp          hp        drat          wt        qsec          vs          am        gear        carb \n##  20.9009889   5.9682791 221.9436951 137.7951392   3.6740174   3.1670541  18.0195300   0.4878638   0.4240404   3.7486737   2.6293563\nmean(mtcars$mpg, mtcars$cyl) # = tapply(mtcars$mpg, mtcars$cyl, mean)\n##        4        6        8 \n## 26.66364 19.74286 15.10000\nmean(mtcars, TRA = \"-\") |>   # = sweep(mtcars, 2, sapply(mtcars, mean))\n  head()\n##                         mpg     cyl        disp       hp       drat       wt     qsec      vs       am    gear    carb\n## Mazda RX4          0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.59725 -1.38875 -0.4375  0.59375  0.3125  1.1875\n## Mazda RX4 Wag      0.909375 -0.1875  -70.721875 -36.6875  0.3034375 -0.34225 -0.82875 -0.4375  0.59375  0.3125  1.1875\n## Datsun 710         2.709375 -2.1875 -122.721875 -53.6875  0.2534375 -0.89725  0.76125  0.5625  0.59375  0.3125 -1.8125\n## Hornet 4 Drive     1.309375 -0.1875   27.278125 -36.6875 -0.5165625 -0.00225  1.59125  0.5625 -0.40625 -0.6875 -1.8125\n## Hornet Sportabout -1.390625  1.8125  129.278125  28.3125 -0.4465625  0.22275 -0.82875 -0.4375 -0.40625 -0.6875 -0.8125\n## Valiant           -1.990625 -0.1875   -5.721875 -41.6875 -0.8365625  0.24275  2.37125  0.5625 -0.40625 -0.6875 -1.8125\nmtcars |> group_by(cyl, vs, am) |> \n  mean() # = summarize(across(everything(), mean))\n##   cyl vs am      mpg     disp        hp     drat       wt     qsec     gear     carb\n## 1   4  0  1 26.00000 120.3000  91.00000 4.430000 2.140000 16.70000 5.000000 2.000000\n## 2   4  1  0 22.90000 135.8667  84.66667 3.770000 2.935000 20.97000 3.666667 1.666667\n## 3   4  1  1 28.37143  89.8000  80.57143 4.148571 2.028286 18.70000 4.142857 1.428571\n## 4   6  0  1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667\n## 5   6  1  0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000\n## 6   8  0  0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333\n## 7   8  0  1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000\nConcluding Reflections\nIt has been a remarkable 3.5-year-long journey leading up to the development of\ncollapse\n2.0, and a tremendous feat of time, energy, and determination. I could probably have published 2 academic papers instead, but would still be writing horrible code like most economists, be trying to do complex things in econometrics, etc., using other mainstream data science libraries not really designed for that, or, worst case, just be stuck to commercial software. I’m happy I came out as an open-source developer and that I’ve been accompanied on this path by other great people from my profession.\nI also never regretted choosing R as a primary language. I find it unique in its simplicity and parsimony, the ability to work with different objects like vectors, matrices, and data frames in a fluent way, and to do rather complex things like matching with a single function call.\nOn the other hand, R for me always lacked speed and the ability to do advanced statistical and data manipulation operations with ease, as I was used to coming from commercial environments (STATA, Mathematica).\ncollapse\nis my determined attempt to bring statistical complexity, parsimony, speed, and joy to statistics and data manipulation in R, and I believe it is the most encompassing attempt out there and preserves the fundamental character of the language.\nNotably, the\ncollapse\napproach is not limited to a certain object (like e.g.\ndata.table\n, which remains a great idea and implementation), and does not rely on data structures and syntax that are somewhat alien/external to the language and do not integrate with many of its first-order features (e.g.\narrow\n,\npolars\n,\nduckdb\n). It is also arguably more successful than alternative ways to implement or compile the language (\nFastR\n/\nGraal VM\n), because the fundamental performance problem in R is algorithmic efficiency and the lack of low-level vectorization for repetitive tasks\n2\n.\nBy reimplementing core parts of the language using efficient algorithms and providing rich and flexible vectorizations for many statistical operations across columns and groups in a class-agnostic way supporting nearly all frequently used data structures,\ncollapse\nsolves the fundamental performance problem in a way that integrates seamlessly with the core of the language. It also adds much-needed statistical complexity, particularly for weighted statistics, time series, and panel data. In short, it provides advanced and fast R, inside GNU R.\nIt is not, and will never be, the absolute best that can be done in performance terms. The data formats used by the best-performing systems (such as the\narrow\ncolumnar format underlying\npolars\n) are designed at the memory level to optimally use computer resources (SIMD etc.), with database applications in mind, and the people doing this did not study economics. But it is not yet clear that such architectures are very suitable for languages meant to do broad and linear-algebra heavy statistical computing tasks, and R just celebrated its 30th birthday this year. So, given the constraints imposed by a 30-year-old C-based language and API, frameworks like\ncollapse\nand\ndata.table\nare pushing the boundaries very far\n3\n.\nLet me stop here;\ncollapse\n2.0 is out. It changed my R life, and I hope it will change yours.\nSome initial benchmarks were shared on\nTwitter\n, and\ncollapse\nis about to enter the\nDuckDB database like ops benchmark\n.\nfmatch()\nis also nearly an order of magnitude faster than\nmatch()\nfor atomic vectors.\n↩︎\nSee e.g. benchmarks for\nr2c\n.\n↩︎\nActually, it is extremely impressive how well\ndata.table\nstill performs compared to modern libraries based on optimized memory models. The\nbenchmarks\nalso show that in high-cardinality settings (many groups relative to the data size), optimized memory models don’t pay off that much, indicating that there is always a tradeoff between the complexity of statistical operations and the possibility of vectorization/bulk processing on modern hardware.\n↩︎\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nR, Econometrics, High Performance\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "I’m excited to announce the release of collapse 2.0, adding blazing fast joins, pivots, flexible namespace and many other features, including a brand new website, an updated cheat sheet, and a new vignette aimed at tidyverse users. In the 3.5 years after the first release of collapse 1.0 to CRAN in March 2020, the package has seen 10 major updates, and become a remarkable piece of statistical software that is robust, stable, lightweight, fast, statistically advanced, comprehensive, flexible, class-agnostic, verbose and well-documented. It is profoundly able to deal with rich (multi-level, irregular, weighted, nested, labelled, and missing) scientific data, and can enhance the workflow of every R user. The addition of rich, fast, and verbose joins and pivots in this release, together with secure interactive namespace masking and extensive global configurability, should enable many R users to use it as a workhorse package for data manipulation and statistical computing tasks. In this post, I briefly introduce the core new features of this release and end with some reflections on why I created the package and think that its approach towards speeding up and enriching R is more encompassing than others. Fast, Class-Agnostic, and Verbose Table Joins Joins in collapse has been a much-requested feature. Still, I was long hesitant to take them on because they are complex, high-risk, operations, and I was unsure on how to go about them or provide an implementation that would satisfy my own ambitious demands to their performance, generality and verbosity. I am glad that, following repeated requests, I have overcome these hesitations and designed an implementation - inspired by polars - that I am very satisfied with. collapse’s join function is simply called join(), and provides 6 types of joins (left, inner, full, right, semi and anti), controlled by a how argument - the default being a left join. It also provides two separate join algorithms: a vectorized hash join (the default, sort = FALSE) and a sort-merge-join (sort = TRUE). The join-column argument is called on, and, if left empty, selects columns present in both datasets. An example with generated data follows: library(collapse) df1 0, fmatch() continues to ## match columns. Consider removing columns or setting overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning. ## left join: df1[id1, id2, name] 0/4 (0%) df2[id1, id2, dept] 0/4 (0%) ## id1 id2 name age salary ## 1 1 a John 35 NA ## 2 1 b Jane 28 NA ## 3 2 b Bob 42 NA ## 4 3 c Carl 50 NA The warning can be silenced by passing overid = 2 to join(). To see better where this may be useful, consider the following example using fmatch(). df1 note that here we get an additional match based on the unique ids, # which we didn't get before because \"Jane\" != \"Janne\" So, in summary, the implementation of joins on collapse as provided by the join() function is not only blazing fast1 and class-agnostic but also allows you to verify all aspects of this high-risk operation. Advanced Pivots The second big addition in collapse 2.0 is pivot(), which provides advanced data reshaping capabilities in a single parsimonious API. Notably, it supports longer-, wider-, and recast-pivoting functionality and can accommodate variable labels in the reshaping process. Fortunately, collapse supplies a perfect test dataset to illustrate these capabilities: the 2014 Groningen Growth and Development Centre 10-Sector Database, which provides sectoral employment and value-added series for 10 broad sectors in 43 countries: head(GGDC10S) ## Country Regioncode Region Variable Year AGR MIN MAN PU CON WRT TRA FIRE GOV OTH SUM ## 1 BWA SSA Sub-saharan Africa VA 1960 NA NA NA NA NA NA NA NA NA NA NA ## 2 BWA SSA Sub-saharan Africa VA 1961 NA NA NA NA NA NA NA NA NA NA NA ## 3 BWA SSA Sub-saharan Africa VA 1962 NA NA NA NA NA NA NA NA NA NA NA ## 4 BWA SSA Sub-saharan Africa VA 1963 NA NA NA NA NA NA NA NA NA NA NA ## 5 BWA SSA Sub-saharan Africa VA 1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229 ## 6 BWA SSA Sub-saharan Africa VA 1965 15.72700 2.495768 1.0181992 0.1350976 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710 namlab(GGDC10S, N = TRUE, Ndistinct = TRUE) ## Variable N Ndist Label ## 1 Country 5027 43 Country ## 2 Regioncode 5027 6 Region code ## 3 Region 5027 6 Region ## 4 Variable 5027 2 Variable ## 5 Year 5027 67 Year ## 6 AGR 4364 4353 Agriculture ## 7 MIN 4355 4224 Mining ## 8 MAN 4355 4353 Manufacturing ## 9 PU 4354 4237 Utilities ## 10 CON 4355 4339 Construction ## 11 WRT 4355 4344 Trade, restaurants and hotels ## 12 TRA 4355 4334 Transport, storage and communication ## 13 FIRE 4355 4349 Finance, insurance, real estate and business services ## 14 GOV 3482 3470 Government services ## 15 OTH 4248 4238 Community, social and personal services ## 16 SUM 4364 4364 Summation of sector GDP Evidently, the data is supplied in a format where two variables, employment and value-added, are stacked in each sector column. The data is also labeled, with descriptions attached as \"label\" attributes (retrievable using vlabels() or, together with names, using namlab()). There are 3 different ways to reshape this data to make it easier to analyze. The first is to simply melt it into a long frame, e.g. for plotting with ggplot2: # Pivot Longer pivot(GGDC10S, ids = 1:5, names = list(variable = \"Sectorcode\", value = \"Value\"), labels = \"Sector\", how = \"longer\", na.rm = TRUE) |> head() ## Country Regioncode Region Variable Year Sectorcode Sector Value ## 1 BWA SSA Sub-saharan Africa VA 1964 AGR Agriculture 16.30154 ## 2 BWA SSA Sub-saharan Africa VA 1965 AGR Agriculture 15.72700 ## 3 BWA SSA Sub-saharan Africa VA 1966 AGR Agriculture 17.68066 ## 4 BWA SSA Sub-saharan Africa VA 1967 AGR Agriculture 19.14591 ## 5 BWA SSA Sub-saharan Africa VA 1968 AGR Agriculture 21.09957 ## 6 BWA SSA Sub-saharan Africa VA 1969 AGR Agriculture 21.86221 Note how specifying the labels argument created a column that captures the sector descriptions, which would otherwise be lost in the reshaping process, and na.rm = TRUE removed missing values in the long frame. I note without demonstration that this operation has an exact reverse operation: pivot(long_df, 1:5, \"Value\", \"Sector\", \"Description\", how = \"wider\"). The second way to reshape the data is to create a wider frame with sector-variable columns: # Pivot Wider pivot(GGDC10S, ids = 1:5, names = \"Variable\", how = \"wider\", na.rm = TRUE) |> namlab(N = TRUE, Ndistinct = TRUE) ## Variable N Ndist Label ## 1 Country 3376 36 Country ## 2 Regioncode 3376 6 Region code ## 3 Region 3376 6 Region ## 4 Variable 3376 2 Variable ## 5 Year 3376 67 Year ## 6 AGR_VA 1702 1700 Agriculture ## 7 AGR_EMP 1674 1669 Agriculture ## 8 MIN_VA 1702 1641 Mining ## 9 MIN_EMP 1674 1622 Mining ## 10 MAN_VA 1702 1702 Manufacturing ## 11 MAN_EMP 1674 1672 Manufacturing ## 12 PU_VA 1702 1665 Utilities ## 13 PU_EMP 1674 1615 Utilities ## 14 CON_VA 1702 1693 Construction ## 15 CON_EMP 1674 1668 Construction ## 16 WRT_VA 1702 1695 Trade, restaurants and hotels ## 17 WRT_EMP 1674 1670 Trade, restaurants and hotels ## 18 TRA_VA 1702 1694 Transport, storage and communication ## 19 TRA_EMP 1674 1662 Transport, storage and communication ## 20 FIRE_VA 1702 1696 Finance, insurance, real estate and business services ## 21 FIRE_EMP 1674 1674 Finance, insurance, real estate and business services ## 22 GOV_VA 1702 1698 Government services ## 23 GOV_EMP 1674 1666 Government services ## 24 OTH_VA 1702 1695 Community, social and personal services ## 25 OTH_EMP 1674 1671 Community, social and personal services ## 26 SUM_VA 1702 1702 Summation of sector GDP ## 27 SUM_EMP 1674 1674 Summation of sector GDP Note how the variable labels were copied to each of the two variables created for each sector. It is also possible to pass argument transpose = c(\"columns\", \"names\") to change the order of columns and/or naming of the casted columns. Wide pivots where multiple columns are cast do not have a well-defined reverse operation. It may nevertheless be very useful to analyze individual sectors. The third useful way to reshape this data for analysis is to recast it such that each variable goes into a separate column and the sectors are stacked in one column: # Pivot Recast recast_df = pivot(GGDC10S, values = 6:16, names = list(from = \"Variable\", to = \"Sectorcode\"), labels = list(to = \"Sector\"), how = \"recast\", na.rm = TRUE) head(recast_df) ## Country Regioncode Region Year Sectorcode Sector VA EMP ## 1 BWA SSA Sub-saharan Africa 1964 AGR Agriculture 16.30154 152.1179 ## 2 BWA SSA Sub-saharan Africa 1965 AGR Agriculture 15.72700 153.2971 ## 3 BWA SSA Sub-saharan Africa 1966 AGR Agriculture 17.68066 153.8867 ## 4 BWA SSA Sub-saharan Africa 1967 AGR Agriculture 19.14591 155.0659 ## 5 BWA SSA Sub-saharan Africa 1968 AGR Agriculture 21.09957 156.2451 ## 6 BWA SSA Sub-saharan Africa 1969 AGR Agriculture 21.86221 157.4243 This is useful, for example, if we wanted to run a regression with sector-fixed effects. The code to reverse this pivot is # Reverse Pivot Recast pivot(recast_df, values = c(\"VA\", \"EMP\"), names = list(from = \"Sectorcode\", to = \"Variable\"), labels = list(from = \"Sector\"), how = \"recast\") |> head(3) ## Country Regioncode Region Year Variable AGR ## 1 BWA SSA Sub-saharan Africa 1964 VA 16.30154 ## 2 BWA SSA Sub-saharan Africa 1965 VA 15.72700 ## 3 BWA SSA Sub-saharan Africa 1966 VA 17.68066 This showcased just some of the functionality of pivot(), more extensive examples are available in the documentation (?pivot). But the above is enough to demonstrate this unified API’s power and flexibility; it is also blazing fast. Global Configurability and Interactive Namespace Masking The third major feature of collapse 2.0 is its extensive global configurability via the set_collapse() function, which includes the default behavior for missing values (na.rm arguments in all statistical functions and algorithms), sorted grouping (sort), multithreading and algorithmic optimizations (nthreads, stable.algo), presentational settings (stub, digits, verbose), and, surpassing all else, the package namespace itself (mask, remove). Why should the namespace, in particular, be modifiable? The main reason is that collapse provides many enhanced and performance improved equivalents to functions present in base R and dplyr, such as the Fast Statistical Functions, fast Grouping and Ordering functions and algorithms, Data Manipulation and Time Series functions. collapse is intentionally fully compatible with the base R and dplyr namespaces by adding f-prefixes to these performance-improved functions where conflicts exist. Since v1.7.0, there exists a global option \"collapse_mask\" which can be set before the package is loaded to export non-prefixed versions of these functions, but this was somewhat tedious and best done with an .Rprofile file. collapse 2.0 also adds this option to set_collapse() and makes it fully interactive; that is, it can be set and changed at any point within the active session. Concretely, what does this mean? Base R and dplyr are relatively slow compared to what can be achieved with group-level vectorization, SIMD instructions, and efficient algorithms, especially as data grows. To provide an example, I generate some large vectors and run some benchmarks for basic operations: ul group_by(l, m) |> summarize(mean_x = mean(x), median_x = median(x)), times = 10) ## Unit: milliseconds ## expr min lq mean median uq max neval ## collapse 344.2272 351.6525 363.8762 358.4373 373.9692 402.6943 10 # Reset the masking # set_collapse(mask = NULL) Evidently, the collapse code runs much faster. The 5-10x speedups shown here are quite normal. Higher speedups can be experienced for grouped operations as the number of groups grows large and repetition in R becomes very costly. As indicated before, masking in collapse 2.0 is fully interactive and reversible: invoking set_collapse(mask = NULL) and running the same code again will execute it again with base R and dplyr. So, in summary, collapse 2.0 provides fast R, in R, in a very simple and broadly accessible way. There are many other advantages to using collapse, e.g., given that its Fast Statistical Functions are S3 generic and support grouped and weighted aggregations and transformations out of the box, this saves many unnecessary calls to apply(), lapply() or summarise(), etc. (in addition to many unnecessary specifications of na.rm = TRUE) e.g.: # S3 generic statistical functions save a lot of syntax mean(mtcars) # = sapply(mtcars, mean) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 0.437500 0.406250 3.687500 2.812500 mean(mtcars, w = runif(32)) # = sapply(mtcars, weighted.mean, w = runif(32)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.9009889 5.9682791 221.9436951 137.7951392 3.6740174 3.1670541 18.0195300 0.4878638 0.4240404 3.7486737 2.6293563 mean(mtcars$mpg, mtcars$cyl) # = tapply(mtcars$mpg, mtcars$cyl, mean) ## 4 6 8 ## 26.66364 19.74286 15.10000 mean(mtcars, TRA = \"-\") |> # = sweep(mtcars, 2, sapply(mtcars, mean)) head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 0.909375 -0.1875 -70.721875 -36.6875 0.3034375 -0.59725 -1.38875 -0.4375 0.59375 0.3125 1.1875 ## Mazda RX4 Wag 0.909375 -0.1875 -70.721875 -36.6875 0.3034375 -0.34225 -0.82875 -0.4375 0.59375 0.3125 1.1875 ## Datsun 710 2.709375 -2.1875 -122.721875 -53.6875 0.2534375 -0.89725 0.76125 0.5625 0.59375 0.3125 -1.8125 ## Hornet 4 Drive 1.309375 -0.1875 27.278125 -36.6875 -0.5165625 -0.00225 1.59125 0.5625 -0.40625 -0.6875 -1.8125 ## Hornet Sportabout -1.390625 1.8125 129.278125 28.3125 -0.4465625 0.22275 -0.82875 -0.4375 -0.40625 -0.6875 -0.8125 ## Valiant -1.990625 -0.1875 -5.721875 -41.6875 -0.8365625 0.24275 2.37125 0.5625 -0.40625 -0.6875 -1.8125 mtcars |> group_by(cyl, vs, am) |> mean() # = summarize(across(everything(), mean)) ## cyl vs am mpg disp hp drat wt qsec gear carb ## 1 4 0 1 26.00000 120.3000 91.00000 4.430000 2.140000 16.70000 5.000000 2.000000 ## 2 4 1 0 22.90000 135.8667 84.66667 3.770000 2.935000 20.97000 3.666667 1.666667 ## 3 4 1 1 28.37143 89.8000 80.57143 4.148571 2.028286 18.70000 4.142857 1.428571 ## 4 6 0 1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667 ## 5 6 1 0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000 ## 6 8 0 0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333 ## 7 8 0 1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000 Concluding Reflections It has been a remarkable 3.5-year-long journey leading up to the development of collapse 2.0, and a tremendous feat of time, energy, and determination. I could probably have published 2 academic papers instead, but would still be writing horrible code like most economists, be trying to do complex things in econometrics, etc., using other mainstream data science libraries not really designed for that, or, worst case, just be stuck to commercial software. I’m happy I came out as an open-source developer and that I’ve been accompanied on this path by other great people from my profession. I also never regretted choosing R as a primary language. I find it unique in its simplicity and parsimony, the ability to work with different objects like vectors, matrices, and data frames in a fluent way, and to do rather complex things like matching with a single function call. On the other hand, R for me always lacked speed and the ability to do advanced statistical and data manipulation operations with ease, as I was used to coming from commercial environments (STATA, Mathematica). collapse is my determined attempt to bring statistical complexity, parsimony, speed, and joy to statistics and data manipulation in R, and I believe it is the most encompassing attempt out there and preserves the fundamental character of the language. Notably, the collapse approach is not limited to a certain object (like e.g. data.table, which remains a great idea and implementation), and does not rely on data structures and syntax that are somewhat alien/external to the language and do not integrate with many of its first-order features (e.g. arrow, polars, duckdb). It is also arguably more successful than alternative ways to implement or compile the language (FastR / Graal VM), because the fundamental performance problem in R is algorithmic efficiency and the lack of low-level vectorization for repetitive tasks2. By reimplementing core parts of the language using efficient algorithms and providing rich and flexible vectorizations for many statistical operations across columns and groups in a class-agnostic way supporting nearly all frequently used data structures, collapse solves the fundamental performance problem in a way that integrates seamlessly with the core of the language. It also adds much-needed statistical complexity, particularly for weighted statistics, time series, and panel data. In short, it provides advanced and fast R, inside GNU R. It is not, and will never be, the absolute best that can be done in performance terms. The data formats used by the best-performing systems (such as the arrow columnar format underlying polars) are designed at the memory level to optimally use computer resources (SIMD etc.), with database applications in mind, and the people doing this did not study economics. But it is not yet clear that such architectures are very suitable for languages meant to do broad and linear-algebra heavy statistical computing tasks, and R just celebrated its 30th birthday this year. So, given the constraints imposed by a 30-year-old C-based language and API, frameworks like collapse and data.table are pushing the boundaries very far3. Let me stop here; collapse 2.0 is out. It changed my R life, and I hope it will change yours. Some initial benchmarks were shared on Twitter, and collapse is about to enter the DuckDB database like ops benchmark. fmatch() is also nearly an order of magnitude faster than match() for atomic vectors.↩︎ See e.g. benchmarks for r2c.↩︎ Actually, it is extremely impressive how well data.table still performs compared to modern libraries based on optimized memory models. The benchmarks also show that in high-cardinality settings (many groups relative to the data size), optimized memory models don’t pay off that much, indicating that there is always a tradeoff between the complexity of statistical operations and the possibility of vectorization/bulk processing on modern hardware.↩︎",
      "meta_keywords": null,
      "og_description": "I’m excited to announce the release of collapse 2.0, adding blazing fast joins, pivots, flexible namespace and many other features, including a brand new website, an updated cheat sheet, and a new vignette aimed at tidyverse users. In the 3.5 years after the first release of collapse 1.0 to CRAN in March 2020, the package has seen 10 major updates, and become a remarkable piece of statistical software that is robust, stable, lightweight, fast, statistically advanced, comprehensive, flexible, class-agnostic, verbose and well-documented. It is profoundly able to deal with rich (multi-level, irregular, weighted, nested, labelled, and missing) scientific data, and can enhance the workflow of every R user. The addition of rich, fast, and verbose joins and pivots in this release, together with secure interactive namespace masking and extensive global configurability, should enable many R users to use it as a workhorse package for data manipulation and statistical computing tasks. In this post, I briefly introduce the core new features of this release and end with some reflections on why I created the package and think that its approach towards speeding up and enriching R is more encompassing than others. Fast, Class-Agnostic, and Verbose Table Joins Joins in collapse has been a much-requested feature. Still, I was long hesitant to take them on because they are complex, high-risk, operations, and I was unsure on how to go about them or provide an implementation that would satisfy my own ambitious demands to their performance, generality and verbosity. I am glad that, following repeated requests, I have overcome these hesitations and designed an implementation - inspired by polars - that I am very satisfied with. collapse’s join function is simply called join(), and provides 6 types of joins (left, inner, full, right, semi and anti), controlled by a how argument - the default being a left join. It also provides two separate join algorithms: a vectorized hash join (the default, sort = FALSE) and a sort-merge-join (sort = TRUE). The join-column argument is called on, and, if left empty, selects columns present in both datasets. An example with generated data follows: library(collapse) df1 0, fmatch() continues to ## match columns. Consider removing columns or setting overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning. ## left join: df1[id1, id2, name] 0/4 (0%) df2[id1, id2, dept] 0/4 (0%) ## id1 id2 name age salary ## 1 1 a John 35 NA ## 2 1 b Jane 28 NA ## 3 2 b Bob 42 NA ## 4 3 c Carl 50 NA The warning can be silenced by passing overid = 2 to join(). To see better where this may be useful, consider the following example using fmatch(). df1 note that here we get an additional match based on the unique ids, # which we didn't get before because \"Jane\" != \"Janne\" So, in summary, the implementation of joins on collapse as provided by the join() function is not only blazing fast1 and class-agnostic but also allows you to verify all aspects of this high-risk operation. Advanced Pivots The second big addition in collapse 2.0 is pivot(), which provides advanced data reshaping capabilities in a single parsimonious API. Notably, it supports longer-, wider-, and recast-pivoting functionality and can accommodate variable labels in the reshaping process. Fortunately, collapse supplies a perfect test dataset to illustrate these capabilities: the 2014 Groningen Growth and Development Centre 10-Sector Database, which provides sectoral employment and value-added series for 10 broad sectors in 43 countries: head(GGDC10S) ## Country Regioncode Region Variable Year AGR MIN MAN PU CON WRT TRA FIRE GOV OTH SUM ## 1 BWA SSA Sub-saharan Africa VA 1960 NA NA NA NA NA NA NA NA NA NA NA ## 2 BWA SSA Sub-saharan Africa VA 1961 NA NA NA NA NA NA NA NA NA NA NA ## 3 BWA SSA Sub-saharan Africa VA 1962 NA NA NA NA NA NA NA NA NA NA NA ## 4 BWA SSA Sub-saharan Africa VA 1963 NA NA NA NA NA NA NA NA NA NA NA ## 5 BWA SSA Sub-saharan Africa VA 1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229 ## 6 BWA SSA Sub-saharan Africa VA 1965 15.72700 2.495768 1.0181992 0.1350976 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710 namlab(GGDC10S, N = TRUE, Ndistinct = TRUE) ## Variable N Ndist Label ## 1 Country 5027 43 Country ## 2 Regioncode 5027 6 Region code ## 3 Region 5027 6 Region ## 4 Variable 5027 2 Variable ## 5 Year 5027 67 Year ## 6 AGR 4364 4353 Agriculture ## 7 MIN 4355 4224 Mining ## 8 MAN 4355 4353 Manufacturing ## 9 PU 4354 4237 Utilities ## 10 CON 4355 4339 Construction ## 11 WRT 4355 4344 Trade, restaurants and hotels ## 12 TRA 4355 4334 Transport, storage and communication ## 13 FIRE 4355 4349 Finance, insurance, real estate and business services ## 14 GOV 3482 3470 Government services ## 15 OTH 4248 4238 Community, social and personal services ## 16 SUM 4364 4364 Summation of sector GDP Evidently, the data is supplied in a format where two variables, employment and value-added, are stacked in each sector column. The data is also labeled, with descriptions attached as \"label\" attributes (retrievable using vlabels() or, together with names, using namlab()). There are 3 different ways to reshape this data to make it easier to analyze. The first is to simply melt it into a long frame, e.g. for plotting with ggplot2: # Pivot Longer pivot(GGDC10S, ids = 1:5, names = list(variable = \"Sectorcode\", value = \"Value\"), labels = \"Sector\", how = \"longer\", na.rm = TRUE) |> head() ## Country Regioncode Region Variable Year Sectorcode Sector Value ## 1 BWA SSA Sub-saharan Africa VA 1964 AGR Agriculture 16.30154 ## 2 BWA SSA Sub-saharan Africa VA 1965 AGR Agriculture 15.72700 ## 3 BWA SSA Sub-saharan Africa VA 1966 AGR Agriculture 17.68066 ## 4 BWA SSA Sub-saharan Africa VA 1967 AGR Agriculture 19.14591 ## 5 BWA SSA Sub-saharan Africa VA 1968 AGR Agriculture 21.09957 ## 6 BWA SSA Sub-saharan Africa VA 1969 AGR Agriculture 21.86221 Note how specifying the labels argument created a column that captures the sector descriptions, which would otherwise be lost in the reshaping process, and na.rm = TRUE removed missing values in the long frame. I note without demonstration that this operation has an exact reverse operation: pivot(long_df, 1:5, \"Value\", \"Sector\", \"Description\", how = \"wider\"). The second way to reshape the data is to create a wider frame with sector-variable columns: # Pivot Wider pivot(GGDC10S, ids = 1:5, names = \"Variable\", how = \"wider\", na.rm = TRUE) |> namlab(N = TRUE, Ndistinct = TRUE) ## Variable N Ndist Label ## 1 Country 3376 36 Country ## 2 Regioncode 3376 6 Region code ## 3 Region 3376 6 Region ## 4 Variable 3376 2 Variable ## 5 Year 3376 67 Year ## 6 AGR_VA 1702 1700 Agriculture ## 7 AGR_EMP 1674 1669 Agriculture ## 8 MIN_VA 1702 1641 Mining ## 9 MIN_EMP 1674 1622 Mining ## 10 MAN_VA 1702 1702 Manufacturing ## 11 MAN_EMP 1674 1672 Manufacturing ## 12 PU_VA 1702 1665 Utilities ## 13 PU_EMP 1674 1615 Utilities ## 14 CON_VA 1702 1693 Construction ## 15 CON_EMP 1674 1668 Construction ## 16 WRT_VA 1702 1695 Trade, restaurants and hotels ## 17 WRT_EMP 1674 1670 Trade, restaurants and hotels ## 18 TRA_VA 1702 1694 Transport, storage and communication ## 19 TRA_EMP 1674 1662 Transport, storage and communication ## 20 FIRE_VA 1702 1696 Finance, insurance, real estate and business services ## 21 FIRE_EMP 1674 1674 Finance, insurance, real estate and business services ## 22 GOV_VA 1702 1698 Government services ## 23 GOV_EMP 1674 1666 Government services ## 24 OTH_VA 1702 1695 Community, social and personal services ## 25 OTH_EMP 1674 1671 Community, social and personal services ## 26 SUM_VA 1702 1702 Summation of sector GDP ## 27 SUM_EMP 1674 1674 Summation of sector GDP Note how the variable labels were copied to each of the two variables created for each sector. It is also possible to pass argument transpose = c(\"columns\", \"names\") to change the order of columns and/or naming of the casted columns. Wide pivots where multiple columns are cast do not have a well-defined reverse operation. It may nevertheless be very useful to analyze individual sectors. The third useful way to reshape this data for analysis is to recast it such that each variable goes into a separate column and the sectors are stacked in one column: # Pivot Recast recast_df = pivot(GGDC10S, values = 6:16, names = list(from = \"Variable\", to = \"Sectorcode\"), labels = list(to = \"Sector\"), how = \"recast\", na.rm = TRUE) head(recast_df) ## Country Regioncode Region Year Sectorcode Sector VA EMP ## 1 BWA SSA Sub-saharan Africa 1964 AGR Agriculture 16.30154 152.1179 ## 2 BWA SSA Sub-saharan Africa 1965 AGR Agriculture 15.72700 153.2971 ## 3 BWA SSA Sub-saharan Africa 1966 AGR Agriculture 17.68066 153.8867 ## 4 BWA SSA Sub-saharan Africa 1967 AGR Agriculture 19.14591 155.0659 ## 5 BWA SSA Sub-saharan Africa 1968 AGR Agriculture 21.09957 156.2451 ## 6 BWA SSA Sub-saharan Africa 1969 AGR Agriculture 21.86221 157.4243 This is useful, for example, if we wanted to run a regression with sector-fixed effects. The code to reverse this pivot is # Reverse Pivot Recast pivot(recast_df, values = c(\"VA\", \"EMP\"), names = list(from = \"Sectorcode\", to = \"Variable\"), labels = list(from = \"Sector\"), how = \"recast\") |> head(3) ## Country Regioncode Region Year Variable AGR ## 1 BWA SSA Sub-saharan Africa 1964 VA 16.30154 ## 2 BWA SSA Sub-saharan Africa 1965 VA 15.72700 ## 3 BWA SSA Sub-saharan Africa 1966 VA 17.68066 This showcased just some of the functionality of pivot(), more extensive examples are available in the documentation (?pivot). But the above is enough to demonstrate this unified API’s power and flexibility; it is also blazing fast. Global Configurability and Interactive Namespace Masking The third major feature of collapse 2.0 is its extensive global configurability via the set_collapse() function, which includes the default behavior for missing values (na.rm arguments in all statistical functions and algorithms), sorted grouping (sort), multithreading and algorithmic optimizations (nthreads, stable.algo), presentational settings (stub, digits, verbose), and, surpassing all else, the package namespace itself (mask, remove). Why should the namespace, in particular, be modifiable? The main reason is that collapse provides many enhanced and performance improved equivalents to functions present in base R and dplyr, such as the Fast Statistical Functions, fast Grouping and Ordering functions and algorithms, Data Manipulation and Time Series functions. collapse is intentionally fully compatible with the base R and dplyr namespaces by adding f-prefixes to these performance-improved functions where conflicts exist. Since v1.7.0, there exists a global option \"collapse_mask\" which can be set before the package is loaded to export non-prefixed versions of these functions, but this was somewhat tedious and best done with an .Rprofile file. collapse 2.0 also adds this option to set_collapse() and makes it fully interactive; that is, it can be set and changed at any point within the active session. Concretely, what does this mean? Base R and dplyr are relatively slow compared to what can be achieved with group-level vectorization, SIMD instructions, and efficient algorithms, especially as data grows. To provide an example, I generate some large vectors and run some benchmarks for basic operations: ul group_by(l, m) |> summarize(mean_x = mean(x), median_x = median(x)), times = 10) ## Unit: milliseconds ## expr min lq mean median uq max neval ## collapse 344.2272 351.6525 363.8762 358.4373 373.9692 402.6943 10 # Reset the masking # set_collapse(mask = NULL) Evidently, the collapse code runs much faster. The 5-10x speedups shown here are quite normal. Higher speedups can be experienced for grouped operations as the number of groups grows large and repetition in R becomes very costly. As indicated before, masking in collapse 2.0 is fully interactive and reversible: invoking set_collapse(mask = NULL) and running the same code again will execute it again with base R and dplyr. So, in summary, collapse 2.0 provides fast R, in R, in a very simple and broadly accessible way. There are many other advantages to using collapse, e.g., given that its Fast Statistical Functions are S3 generic and support grouped and weighted aggregations and transformations out of the box, this saves many unnecessary calls to apply(), lapply() or summarise(), etc. (in addition to many unnecessary specifications of na.rm = TRUE) e.g.: # S3 generic statistical functions save a lot of syntax mean(mtcars) # = sapply(mtcars, mean) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 0.437500 0.406250 3.687500 2.812500 mean(mtcars, w = runif(32)) # = sapply(mtcars, weighted.mean, w = runif(32)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.9009889 5.9682791 221.9436951 137.7951392 3.6740174 3.1670541 18.0195300 0.4878638 0.4240404 3.7486737 2.6293563 mean(mtcars$mpg, mtcars$cyl) # = tapply(mtcars$mpg, mtcars$cyl, mean) ## 4 6 8 ## 26.66364 19.74286 15.10000 mean(mtcars, TRA = \"-\") |> # = sweep(mtcars, 2, sapply(mtcars, mean)) head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 0.909375 -0.1875 -70.721875 -36.6875 0.3034375 -0.59725 -1.38875 -0.4375 0.59375 0.3125 1.1875 ## Mazda RX4 Wag 0.909375 -0.1875 -70.721875 -36.6875 0.3034375 -0.34225 -0.82875 -0.4375 0.59375 0.3125 1.1875 ## Datsun 710 2.709375 -2.1875 -122.721875 -53.6875 0.2534375 -0.89725 0.76125 0.5625 0.59375 0.3125 -1.8125 ## Hornet 4 Drive 1.309375 -0.1875 27.278125 -36.6875 -0.5165625 -0.00225 1.59125 0.5625 -0.40625 -0.6875 -1.8125 ## Hornet Sportabout -1.390625 1.8125 129.278125 28.3125 -0.4465625 0.22275 -0.82875 -0.4375 -0.40625 -0.6875 -0.8125 ## Valiant -1.990625 -0.1875 -5.721875 -41.6875 -0.8365625 0.24275 2.37125 0.5625 -0.40625 -0.6875 -1.8125 mtcars |> group_by(cyl, vs, am) |> mean() # = summarize(across(everything(), mean)) ## cyl vs am mpg disp hp drat wt qsec gear carb ## 1 4 0 1 26.00000 120.3000 91.00000 4.430000 2.140000 16.70000 5.000000 2.000000 ## 2 4 1 0 22.90000 135.8667 84.66667 3.770000 2.935000 20.97000 3.666667 1.666667 ## 3 4 1 1 28.37143 89.8000 80.57143 4.148571 2.028286 18.70000 4.142857 1.428571 ## 4 6 0 1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667 ## 5 6 1 0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000 ## 6 8 0 0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333 ## 7 8 0 1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000 Concluding Reflections It has been a remarkable 3.5-year-long journey leading up to the development of collapse 2.0, and a tremendous feat of time, energy, and determination. I could probably have published 2 academic papers instead, but would still be writing horrible code like most economists, be trying to do complex things in econometrics, etc., using other mainstream data science libraries not really designed for that, or, worst case, just be stuck to commercial software. I’m happy I came out as an open-source developer and that I’ve been accompanied on this path by other great people from my profession. I also never regretted choosing R as a primary language. I find it unique in its simplicity and parsimony, the ability to work with different objects like vectors, matrices, and data frames in a fluent way, and to do rather complex things like matching with a single function call. On the other hand, R for me always lacked speed and the ability to do advanced statistical and data manipulation operations with ease, as I was used to coming from commercial environments (STATA, Mathematica). collapse is my determined attempt to bring statistical complexity, parsimony, speed, and joy to statistics and data manipulation in R, and I believe it is the most encompassing attempt out there and preserves the fundamental character of the language. Notably, the collapse approach is not limited to a certain object (like e.g. data.table, which remains a great idea and implementation), and does not rely on data structures and syntax that are somewhat alien/external to the language and do not integrate with many of its first-order features (e.g. arrow, polars, duckdb). It is also arguably more successful than alternative ways to implement or compile the language (FastR / Graal VM), because the fundamental performance problem in R is algorithmic efficiency and the lack of low-level vectorization for repetitive tasks2. By reimplementing core parts of the language using efficient algorithms and providing rich and flexible vectorizations for many statistical operations across columns and groups in a class-agnostic way supporting nearly all frequently used data structures, collapse solves the fundamental performance problem in a way that integrates seamlessly with the core of the language. It also adds much-needed statistical complexity, particularly for weighted statistics, time series, and panel data. In short, it provides advanced and fast R, inside GNU R. It is not, and will never be, the absolute best that can be done in performance terms. The data formats used by the best-performing systems (such as the arrow columnar format underlying polars) are designed at the memory level to optimally use computer resources (SIMD etc.), with database applications in mind, and the people doing this did not study economics. But it is not yet clear that such architectures are very suitable for languages meant to do broad and linear-algebra heavy statistical computing tasks, and R just celebrated its 30th birthday this year. So, given the constraints imposed by a 30-year-old C-based language and API, frameworks like collapse and data.table are pushing the boundaries very far3. Let me stop here; collapse 2.0 is out. It changed my R life, and I hope it will change yours. Some initial benchmarks were shared on Twitter, and collapse is about to enter the DuckDB database like ops benchmark. fmatch() is also nearly an order of magnitude faster than match() for atomic vectors.↩︎ See e.g. benchmarks for r2c.↩︎ Actually, it is extremely impressive how well data.table still performs compared to modern libraries based on optimized memory models. The benchmarks also show that in high-cardinality settings (many groups relative to the data size), optimized memory models don’t pay off that much, indicating that there is always a tradeoff between the complexity of statistical operations and the possibility of vectorization/bulk processing on modern hardware.↩︎",
      "og_image": "https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png",
      "og_title": "Releasing collapse 2.0: Blazing Fast Joins, Reshaping, and Enhanced R | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 24.4,
      "sitemap_lastmod": "2023-10-15T00:00:00+00:00",
      "twitter_description": "I’m excited to announce the release of collapse 2.0, adding blazing fast joins, pivots, flexible namespace and many other features, including a brand new website, an updated cheat sheet, and a new vignette aimed at tidyverse users. In the 3.5 years after the first release of collapse 1.0 to CRAN in March 2020, the package has seen 10 major updates, and become a remarkable piece of statistical software that is robust, stable, lightweight, fast, statistically advanced, comprehensive, flexible, class-agnostic, verbose and well-documented. It is profoundly able to deal with rich (multi-level, irregular, weighted, nested, labelled, and missing) scientific data, and can enhance the workflow of every R user. The addition of rich, fast, and verbose joins and pivots in this release, together with secure interactive namespace masking and extensive global configurability, should enable many R users to use it as a workhorse package for data manipulation and statistical computing tasks. In this post, I briefly introduce the core new features of this release and end with some reflections on why I created the package and think that its approach towards speeding up and enriching R is more encompassing than others. Fast, Class-Agnostic, and Verbose Table Joins Joins in collapse has been a much-requested feature. Still, I was long hesitant to take them on because they are complex, high-risk, operations, and I was unsure on how to go about them or provide an implementation that would satisfy my own ambitious demands to their performance, generality and verbosity. I am glad that, following repeated requests, I have overcome these hesitations and designed an implementation - inspired by polars - that I am very satisfied with. collapse’s join function is simply called join(), and provides 6 types of joins (left, inner, full, right, semi and anti), controlled by a how argument - the default being a left join. It also provides two separate join algorithms: a vectorized hash join (the default, sort = FALSE) and a sort-merge-join (sort = TRUE). The join-column argument is called on, and, if left empty, selects columns present in both datasets. An example with generated data follows: library(collapse) df1 0, fmatch() continues to ## match columns. Consider removing columns or setting overid = 0 to terminate the algorithm after 2 columns (the results may differ, see ?fmatch). Alternatively set overid = 2 to silence this warning. ## left join: df1[id1, id2, name] 0/4 (0%) df2[id1, id2, dept] 0/4 (0%) ## id1 id2 name age salary ## 1 1 a John 35 NA ## 2 1 b Jane 28 NA ## 3 2 b Bob 42 NA ## 4 3 c Carl 50 NA The warning can be silenced by passing overid = 2 to join(). To see better where this may be useful, consider the following example using fmatch(). df1 note that here we get an additional match based on the unique ids, # which we didn't get before because \"Jane\" != \"Janne\" So, in summary, the implementation of joins on collapse as provided by the join() function is not only blazing fast1 and class-agnostic but also allows you to verify all aspects of this high-risk operation. Advanced Pivots The second big addition in collapse 2.0 is pivot(), which provides advanced data reshaping capabilities in a single parsimonious API. Notably, it supports longer-, wider-, and recast-pivoting functionality and can accommodate variable labels in the reshaping process. Fortunately, collapse supplies a perfect test dataset to illustrate these capabilities: the 2014 Groningen Growth and Development Centre 10-Sector Database, which provides sectoral employment and value-added series for 10 broad sectors in 43 countries: head(GGDC10S) ## Country Regioncode Region Variable Year AGR MIN MAN PU CON WRT TRA FIRE GOV OTH SUM ## 1 BWA SSA Sub-saharan Africa VA 1960 NA NA NA NA NA NA NA NA NA NA NA ## 2 BWA SSA Sub-saharan Africa VA 1961 NA NA NA NA NA NA NA NA NA NA NA ## 3 BWA SSA Sub-saharan Africa VA 1962 NA NA NA NA NA NA NA NA NA NA NA ## 4 BWA SSA Sub-saharan Africa VA 1963 NA NA NA NA NA NA NA NA NA NA NA ## 5 BWA SSA Sub-saharan Africa VA 1964 16.30154 3.494075 0.7365696 0.1043936 0.6600454 6.243732 1.658928 1.119194 4.822485 2.341328 37.48229 ## 6 BWA SSA Sub-saharan Africa VA 1965 15.72700 2.495768 1.0181992 0.1350976 1.3462312 7.064825 1.939007 1.246789 5.695848 2.678338 39.34710 namlab(GGDC10S, N = TRUE, Ndistinct = TRUE) ## Variable N Ndist Label ## 1 Country 5027 43 Country ## 2 Regioncode 5027 6 Region code ## 3 Region 5027 6 Region ## 4 Variable 5027 2 Variable ## 5 Year 5027 67 Year ## 6 AGR 4364 4353 Agriculture ## 7 MIN 4355 4224 Mining ## 8 MAN 4355 4353 Manufacturing ## 9 PU 4354 4237 Utilities ## 10 CON 4355 4339 Construction ## 11 WRT 4355 4344 Trade, restaurants and hotels ## 12 TRA 4355 4334 Transport, storage and communication ## 13 FIRE 4355 4349 Finance, insurance, real estate and business services ## 14 GOV 3482 3470 Government services ## 15 OTH 4248 4238 Community, social and personal services ## 16 SUM 4364 4364 Summation of sector GDP Evidently, the data is supplied in a format where two variables, employment and value-added, are stacked in each sector column. The data is also labeled, with descriptions attached as \"label\" attributes (retrievable using vlabels() or, together with names, using namlab()). There are 3 different ways to reshape this data to make it easier to analyze. The first is to simply melt it into a long frame, e.g. for plotting with ggplot2: # Pivot Longer pivot(GGDC10S, ids = 1:5, names = list(variable = \"Sectorcode\", value = \"Value\"), labels = \"Sector\", how = \"longer\", na.rm = TRUE) |> head() ## Country Regioncode Region Variable Year Sectorcode Sector Value ## 1 BWA SSA Sub-saharan Africa VA 1964 AGR Agriculture 16.30154 ## 2 BWA SSA Sub-saharan Africa VA 1965 AGR Agriculture 15.72700 ## 3 BWA SSA Sub-saharan Africa VA 1966 AGR Agriculture 17.68066 ## 4 BWA SSA Sub-saharan Africa VA 1967 AGR Agriculture 19.14591 ## 5 BWA SSA Sub-saharan Africa VA 1968 AGR Agriculture 21.09957 ## 6 BWA SSA Sub-saharan Africa VA 1969 AGR Agriculture 21.86221 Note how specifying the labels argument created a column that captures the sector descriptions, which would otherwise be lost in the reshaping process, and na.rm = TRUE removed missing values in the long frame. I note without demonstration that this operation has an exact reverse operation: pivot(long_df, 1:5, \"Value\", \"Sector\", \"Description\", how = \"wider\"). The second way to reshape the data is to create a wider frame with sector-variable columns: # Pivot Wider pivot(GGDC10S, ids = 1:5, names = \"Variable\", how = \"wider\", na.rm = TRUE) |> namlab(N = TRUE, Ndistinct = TRUE) ## Variable N Ndist Label ## 1 Country 3376 36 Country ## 2 Regioncode 3376 6 Region code ## 3 Region 3376 6 Region ## 4 Variable 3376 2 Variable ## 5 Year 3376 67 Year ## 6 AGR_VA 1702 1700 Agriculture ## 7 AGR_EMP 1674 1669 Agriculture ## 8 MIN_VA 1702 1641 Mining ## 9 MIN_EMP 1674 1622 Mining ## 10 MAN_VA 1702 1702 Manufacturing ## 11 MAN_EMP 1674 1672 Manufacturing ## 12 PU_VA 1702 1665 Utilities ## 13 PU_EMP 1674 1615 Utilities ## 14 CON_VA 1702 1693 Construction ## 15 CON_EMP 1674 1668 Construction ## 16 WRT_VA 1702 1695 Trade, restaurants and hotels ## 17 WRT_EMP 1674 1670 Trade, restaurants and hotels ## 18 TRA_VA 1702 1694 Transport, storage and communication ## 19 TRA_EMP 1674 1662 Transport, storage and communication ## 20 FIRE_VA 1702 1696 Finance, insurance, real estate and business services ## 21 FIRE_EMP 1674 1674 Finance, insurance, real estate and business services ## 22 GOV_VA 1702 1698 Government services ## 23 GOV_EMP 1674 1666 Government services ## 24 OTH_VA 1702 1695 Community, social and personal services ## 25 OTH_EMP 1674 1671 Community, social and personal services ## 26 SUM_VA 1702 1702 Summation of sector GDP ## 27 SUM_EMP 1674 1674 Summation of sector GDP Note how the variable labels were copied to each of the two variables created for each sector. It is also possible to pass argument transpose = c(\"columns\", \"names\") to change the order of columns and/or naming of the casted columns. Wide pivots where multiple columns are cast do not have a well-defined reverse operation. It may nevertheless be very useful to analyze individual sectors. The third useful way to reshape this data for analysis is to recast it such that each variable goes into a separate column and the sectors are stacked in one column: # Pivot Recast recast_df = pivot(GGDC10S, values = 6:16, names = list(from = \"Variable\", to = \"Sectorcode\"), labels = list(to = \"Sector\"), how = \"recast\", na.rm = TRUE) head(recast_df) ## Country Regioncode Region Year Sectorcode Sector VA EMP ## 1 BWA SSA Sub-saharan Africa 1964 AGR Agriculture 16.30154 152.1179 ## 2 BWA SSA Sub-saharan Africa 1965 AGR Agriculture 15.72700 153.2971 ## 3 BWA SSA Sub-saharan Africa 1966 AGR Agriculture 17.68066 153.8867 ## 4 BWA SSA Sub-saharan Africa 1967 AGR Agriculture 19.14591 155.0659 ## 5 BWA SSA Sub-saharan Africa 1968 AGR Agriculture 21.09957 156.2451 ## 6 BWA SSA Sub-saharan Africa 1969 AGR Agriculture 21.86221 157.4243 This is useful, for example, if we wanted to run a regression with sector-fixed effects. The code to reverse this pivot is # Reverse Pivot Recast pivot(recast_df, values = c(\"VA\", \"EMP\"), names = list(from = \"Sectorcode\", to = \"Variable\"), labels = list(from = \"Sector\"), how = \"recast\") |> head(3) ## Country Regioncode Region Year Variable AGR ## 1 BWA SSA Sub-saharan Africa 1964 VA 16.30154 ## 2 BWA SSA Sub-saharan Africa 1965 VA 15.72700 ## 3 BWA SSA Sub-saharan Africa 1966 VA 17.68066 This showcased just some of the functionality of pivot(), more extensive examples are available in the documentation (?pivot). But the above is enough to demonstrate this unified API’s power and flexibility; it is also blazing fast. Global Configurability and Interactive Namespace Masking The third major feature of collapse 2.0 is its extensive global configurability via the set_collapse() function, which includes the default behavior for missing values (na.rm arguments in all statistical functions and algorithms), sorted grouping (sort), multithreading and algorithmic optimizations (nthreads, stable.algo), presentational settings (stub, digits, verbose), and, surpassing all else, the package namespace itself (mask, remove). Why should the namespace, in particular, be modifiable? The main reason is that collapse provides many enhanced and performance improved equivalents to functions present in base R and dplyr, such as the Fast Statistical Functions, fast Grouping and Ordering functions and algorithms, Data Manipulation and Time Series functions. collapse is intentionally fully compatible with the base R and dplyr namespaces by adding f-prefixes to these performance-improved functions where conflicts exist. Since v1.7.0, there exists a global option \"collapse_mask\" which can be set before the package is loaded to export non-prefixed versions of these functions, but this was somewhat tedious and best done with an .Rprofile file. collapse 2.0 also adds this option to set_collapse() and makes it fully interactive; that is, it can be set and changed at any point within the active session. Concretely, what does this mean? Base R and dplyr are relatively slow compared to what can be achieved with group-level vectorization, SIMD instructions, and efficient algorithms, especially as data grows. To provide an example, I generate some large vectors and run some benchmarks for basic operations: ul group_by(l, m) |> summarize(mean_x = mean(x), median_x = median(x)), times = 10) ## Unit: milliseconds ## expr min lq mean median uq max neval ## collapse 344.2272 351.6525 363.8762 358.4373 373.9692 402.6943 10 # Reset the masking # set_collapse(mask = NULL) Evidently, the collapse code runs much faster. The 5-10x speedups shown here are quite normal. Higher speedups can be experienced for grouped operations as the number of groups grows large and repetition in R becomes very costly. As indicated before, masking in collapse 2.0 is fully interactive and reversible: invoking set_collapse(mask = NULL) and running the same code again will execute it again with base R and dplyr. So, in summary, collapse 2.0 provides fast R, in R, in a very simple and broadly accessible way. There are many other advantages to using collapse, e.g., given that its Fast Statistical Functions are S3 generic and support grouped and weighted aggregations and transformations out of the box, this saves many unnecessary calls to apply(), lapply() or summarise(), etc. (in addition to many unnecessary specifications of na.rm = TRUE) e.g.: # S3 generic statistical functions save a lot of syntax mean(mtcars) # = sapply(mtcars, mean) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 0.437500 0.406250 3.687500 2.812500 mean(mtcars, w = runif(32)) # = sapply(mtcars, weighted.mean, w = runif(32)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.9009889 5.9682791 221.9436951 137.7951392 3.6740174 3.1670541 18.0195300 0.4878638 0.4240404 3.7486737 2.6293563 mean(mtcars$mpg, mtcars$cyl) # = tapply(mtcars$mpg, mtcars$cyl, mean) ## 4 6 8 ## 26.66364 19.74286 15.10000 mean(mtcars, TRA = \"-\") |> # = sweep(mtcars, 2, sapply(mtcars, mean)) head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 0.909375 -0.1875 -70.721875 -36.6875 0.3034375 -0.59725 -1.38875 -0.4375 0.59375 0.3125 1.1875 ## Mazda RX4 Wag 0.909375 -0.1875 -70.721875 -36.6875 0.3034375 -0.34225 -0.82875 -0.4375 0.59375 0.3125 1.1875 ## Datsun 710 2.709375 -2.1875 -122.721875 -53.6875 0.2534375 -0.89725 0.76125 0.5625 0.59375 0.3125 -1.8125 ## Hornet 4 Drive 1.309375 -0.1875 27.278125 -36.6875 -0.5165625 -0.00225 1.59125 0.5625 -0.40625 -0.6875 -1.8125 ## Hornet Sportabout -1.390625 1.8125 129.278125 28.3125 -0.4465625 0.22275 -0.82875 -0.4375 -0.40625 -0.6875 -0.8125 ## Valiant -1.990625 -0.1875 -5.721875 -41.6875 -0.8365625 0.24275 2.37125 0.5625 -0.40625 -0.6875 -1.8125 mtcars |> group_by(cyl, vs, am) |> mean() # = summarize(across(everything(), mean)) ## cyl vs am mpg disp hp drat wt qsec gear carb ## 1 4 0 1 26.00000 120.3000 91.00000 4.430000 2.140000 16.70000 5.000000 2.000000 ## 2 4 1 0 22.90000 135.8667 84.66667 3.770000 2.935000 20.97000 3.666667 1.666667 ## 3 4 1 1 28.37143 89.8000 80.57143 4.148571 2.028286 18.70000 4.142857 1.428571 ## 4 6 0 1 20.56667 155.0000 131.66667 3.806667 2.755000 16.32667 4.333333 4.666667 ## 5 6 1 0 19.12500 204.5500 115.25000 3.420000 3.388750 19.21500 3.500000 2.500000 ## 6 8 0 0 15.05000 357.6167 194.16667 3.120833 4.104083 17.14250 3.000000 3.083333 ## 7 8 0 1 15.40000 326.0000 299.50000 3.880000 3.370000 14.55000 5.000000 6.000000 Concluding Reflections It has been a remarkable 3.5-year-long journey leading up to the development of collapse 2.0, and a tremendous feat of time, energy, and determination. I could probably have published 2 academic papers instead, but would still be writing horrible code like most economists, be trying to do complex things in econometrics, etc., using other mainstream data science libraries not really designed for that, or, worst case, just be stuck to commercial software. I’m happy I came out as an open-source developer and that I’ve been accompanied on this path by other great people from my profession. I also never regretted choosing R as a primary language. I find it unique in its simplicity and parsimony, the ability to work with different objects like vectors, matrices, and data frames in a fluent way, and to do rather complex things like matching with a single function call. On the other hand, R for me always lacked speed and the ability to do advanced statistical and data manipulation operations with ease, as I was used to coming from commercial environments (STATA, Mathematica). collapse is my determined attempt to bring statistical complexity, parsimony, speed, and joy to statistics and data manipulation in R, and I believe it is the most encompassing attempt out there and preserves the fundamental character of the language. Notably, the collapse approach is not limited to a certain object (like e.g. data.table, which remains a great idea and implementation), and does not rely on data structures and syntax that are somewhat alien/external to the language and do not integrate with many of its first-order features (e.g. arrow, polars, duckdb). It is also arguably more successful than alternative ways to implement or compile the language (FastR / Graal VM), because the fundamental performance problem in R is algorithmic efficiency and the lack of low-level vectorization for repetitive tasks2. By reimplementing core parts of the language using efficient algorithms and providing rich and flexible vectorizations for many statistical operations across columns and groups in a class-agnostic way supporting nearly all frequently used data structures, collapse solves the fundamental performance problem in a way that integrates seamlessly with the core of the language. It also adds much-needed statistical complexity, particularly for weighted statistics, time series, and panel data. In short, it provides advanced and fast R, inside GNU R. It is not, and will never be, the absolute best that can be done in performance terms. The data formats used by the best-performing systems (such as the arrow columnar format underlying polars) are designed at the memory level to optimally use computer resources (SIMD etc.), with database applications in mind, and the people doing this did not study economics. But it is not yet clear that such architectures are very suitable for languages meant to do broad and linear-algebra heavy statistical computing tasks, and R just celebrated its 30th birthday this year. So, given the constraints imposed by a 30-year-old C-based language and API, frameworks like collapse and data.table are pushing the boundaries very far3. Let me stop here; collapse 2.0 is out. It changed my R life, and I hope it will change yours. Some initial benchmarks were shared on Twitter, and collapse is about to enter the DuckDB database like ops benchmark. fmatch() is also nearly an order of magnitude faster than match() for atomic vectors.↩︎ See e.g. benchmarks for r2c.↩︎ Actually, it is extremely impressive how well data.table still performs compared to modern libraries based on optimized memory models. The benchmarks also show that in high-cardinality settings (many groups relative to the data size), optimized memory models don’t pay off that much, indicating that there is always a tradeoff between the complexity of statistical operations and the possibility of vectorization/bulk processing on modern hardware.↩︎",
      "twitter_title": "Releasing collapse 2.0: Blazing Fast Joins, Reshaping, and Enhanced R | R-bloggers",
      "url": "https://www.r-bloggers.com/2023/10/releasing-collapse-2-0-blazing-fast-joins-reshaping-and-enhanced-r/",
      "word_count": 4877
    }
  }
}