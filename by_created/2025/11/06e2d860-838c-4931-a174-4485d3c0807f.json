{
  "uuid": "06e2d860-838c-4931-a174-4485d3c0807f",
  "created_at": "2025-11-22 19:58:26",
  "raw_json": {
    "article_author": null,
    "article_headline": null,
    "article_modified": null,
    "article_published": null,
    "article_section": null,
    "article_tags": null,
    "canonical_url": "https://www.r-bloggers.com/2025/06/understanding-basis-spline-b-spline-by-working-through-cox-deboor-algorithm/",
    "crawled_at": "2025-11-22T10:47:53.696295",
    "external_links": [
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/",
        "text": "r on Everyday Is A School Day"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#motivations",
        "text": null
      },
      {
        "href": "https://www.youtube.com/watch?v=FdnMWdICdRs&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus",
        "text": "Statistical Rethinking 2023 videos"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#objective",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#spline",
        "text": "What is Basis Spline?"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#coxdeboor",
        "text": "What is Cox deBoor Recursion Formula?"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#code",
        "text": "Let‚Äôs code"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#visualize",
        "text": "Let‚Äôs visualize"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#opportunity",
        "text": "Opportunities for improvement"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#lessons",
        "text": "Lessons Learnt"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#spline",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#to-understand-this-we-need-to-know-the-available-parameters",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#coxdeboor",
        "text": null
      },
      {
        "href": "https://en.wikipedia.org/wiki/De_Boor%27s_algorithm",
        "text": "Cox deBoor Algorithm"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#ok-thats-a-lot-of-letters-and-subscripts-im-really-confused-",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#code",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#visualize",
        "text": null
      },
      {
        "href": "https://www.youtube.com/watch?v=F0N4b7K_iYQ&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=4",
        "text": "Richard McElreath‚Äôs Statistical Rethinking"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#opportunity",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/#lesson",
        "text": null
      },
      {
        "href": "https://www.kenkoonwong.com/blog/",
        "text": "comment or visit my other blogs"
      },
      {
        "href": "https://bsky.app/profile/kenkoonwong.bsky.social",
        "text": "BlueSky"
      },
      {
        "href": "https://twitter.com/kenkoonwong/",
        "text": "twitter"
      },
      {
        "href": "https://github.com/kenkoonwong/",
        "text": "GitHub"
      },
      {
        "href": "https://med-mastodon.com/@kenkoonwong",
        "text": "Mastodon"
      },
      {
        "href": "https://www.kenkoonwong.com/contact/",
        "text": "contact me"
      },
      {
        "href": "https://www.kenkoonwong.com/blog/bspline/",
        "text": "r on Everyday Is A School Day"
      },
      {
        "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
        "text": "daily e-mail updates"
      },
      {
        "href": "https://www.r-project.org/",
        "text": "R"
      },
      {
        "href": "https://www.r-users.com/",
        "text": "Click here if you're looking to post or find an R/data-science job"
      },
      {
        "href": "http://r-posts.com/",
        "text": "here"
      }
    ],
    "h1_title": "R-bloggers",
    "html_title": "Understanding Basis Spline (B-spline) By Working Through Cox-deBoor Algorithm | R-bloggers",
    "images": [
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-1-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-2-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i0.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-7-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-8-1.png?w=450&ssl=1"
      },
      {
        "alt": null,
        "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
        "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
      },
      {
        "alt": null,
        "base64": null,
        "src": "https://i1.wp.com/www.kenkoonwong.com/blog/bspline/mcmc_plot.png?w=578&ssl=1"
      }
    ],
    "internal_links": [
      {
        "href": "https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/",
        "text": "r on Everyday Is A School Day"
      },
      {
        "href": "https://www.r-bloggers.com/category/r-bloggers/",
        "text": "R bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers"
      },
      {
        "href": "https://www.r-bloggers.com/contact-us/",
        "text": "here"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      },
      {
        "href": "https://www.r-bloggers.com/",
        "text": "R-bloggers.com"
      },
      {
        "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
        "text": "learning R"
      },
      {
        "href": "https://www.r-bloggers.com/add-your-blog/",
        "text": "click here"
      }
    ],
    "lang": "en-US",
    "main_html": "<article class=\"post-392838 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Understanding Basis Spline (B-spline) By Working Through Cox-deBoor Algorithm</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">June 2, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/r-on-everyday-is-a-school-day/\">r on Everyday Is A School Day</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.kenkoonwong.com/blog/bspline/\"> r on Everyday Is A School Day</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47--><blockquote>\n<p>I finally understood B-splines by working through the Cox-deBoor algorithm step-by-step, discovering they‚Äôre just weighted combinations of basis functions that make non-linear regression linear. What surprised me is going through Bayesian statistics really helped me understand the engine behind the model! Will try this again in the future!</p>\n</blockquote>\n<h2 id=\"motivations\">Motivations\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#motivations\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>I‚Äôve always been curious and amazed by spline as smoothing function. The first time I heard of that was with <code>mgcv</code> package on generalized additve model. The second time I heard of that was Richard McElreath‚Äôs \n<a href=\"https://www.youtube.com/watch?v=FdnMWdICdRs&amp;list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus\" rel=\"nofollow\" target=\"_blank\">Statistical Rethinking 2023 videos</a>, more to come on that. His book cover, that‚Äôs the product of basis spline, and of course his talent to convert that into wonders. Ther first time I heard him stating that he prefers basis spline smoothing function over polynomial was what caught my attention and never really understood the basics behind it, until recently. It‚Äôs been mentioned here and there over the year, I‚Äôve also tried to read up a bit on it and it just did not stick and I was not able to understand it. Hence, the motivation is to get the intuition behind, and nothing better than to actually look at the underlying formula that makes up the function and then code it! I also found that learning this in bayesian way is very helpful as well, as you have to write all the formula yourself, what to estimate, what to include etc. These 2 combinations, simulation and code in bayesian have been very helpful for me to understand the concept better. So, what are we waiting for, let‚Äôs get some basics in smoothening this spline out!</p>\n<h2 id=\"objective\">Objective\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#objective\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/bspline/#spline\" rel=\"nofollow\" target=\"_blank\">What is Basis Spline?</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/bspline/#coxdeboor\" rel=\"nofollow\" target=\"_blank\">What is Cox deBoor Recursion Formula?</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/bspline/#code\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs code</a>\n<ul>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/bspline/#visualize\" rel=\"nofollow\" target=\"_blank\">Let‚Äôs visualize</a></li>\n</ul>\n</li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/bspline/#opportunity\" rel=\"nofollow\" target=\"_blank\">Opportunities for improvement</a></li>\n<li>\n<a href=\"https://www.kenkoonwong.com/blog/bspline/#lessons\" rel=\"nofollow\" target=\"_blank\">Lessons Learnt</a></li>\n</ul>\n<h2 id=\"spline\">What is Basis Spline?\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#spline\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>Basis spline, or B-spline, is a piecewise polynomial function that is used for smoothing data. It is defined by a set of control points and a degree, which determines the polynomial degree of the segments between the control points. The B-spline is constructed using a set of basis functions, which are defined over a set of knots. The B-spline is a linear combination of these basis functions, where the coefficients are the weights assigned to each basis function.</p>\n<p>Wow that‚Äôs a lot of words! Let‚Äôs take a look and see when we usually use this function and expand there. Let‚Äôs write some simulation and then use <code>mgcv</code> package and model a <code>gam</code> and look at the output.</p>\n<pre>library(tidyverse)\nlibrary(mgcv)\n\nset.seed(1)\nn &lt;- 1000\nx &lt;- runif(n,-5,5)\ny &lt;- sin(x) + rnorm(n, 0, 0.5)\ndf &lt;- tibble(x,y)\n\ndf |&gt;\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha = 0.2) +\n  theme_bw()\n</pre><img data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-1-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-1-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Alright, we have a dataset with <code>x</code> and <code>y</code> values, where <code>y</code> is a noisy sine wave. Now, let‚Äôs fit a generalized additive model (GAM) using the <code>mgcv</code> package to see how the spline smooths the data.</p>\n<pre>gam_model &lt;- gam(y ~ s(x, bs = \"cr\"), data = df)\n\ndf |&gt;\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = predict(gam_model)), color = \"blue\", size = 1) +\n  theme_bw()\n</pre><img data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-2-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-2-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Look at that nice blue curve! That‚Äôs our gam model. Looks like it fits a really good non-linear relationship of <code>x</code> and <code>y</code>. Now let‚Äôs look at the summary output</p>\n<pre>summary(gam_model)\n\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x, bs = \"cr\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -0.04663    0.01623  -2.874  0.00415 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##        edf Ref.df   F p-value    \n## s(x) 8.545   8.94 211  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.654   Deviance explained = 65.7%\n## GCV = 0.26587  Scale est. = 0.26334   n = 1000\n</pre><p>The summary output shows us the estimated coefficients for the spline basis functions, the effective degrees of freedom, and the significance of the smooth term. The <code>s(x, bs = \"cr\")</code> indicates that we are using a cubic regression spline (CR) for the smooth term.</p>\n<p>Something to pay attention to is the effective degree of freedom (<code>edf</code>). It measures the complexity of the wiggliness of the smooth function. edf = 1 means it‚Äôs linear. the higher the number the higher the wiggliness is. We may on another post look more closely at what these numbers mean, how the check the models etc, but right now I‚Äôm more interested in how basis spline is constructed and how it works under the hood. <code>gam</code> has a more sophisticated penalty for the wiggliness of the spline, but for now let‚Äôs just focus on the basics of how the spline is constructed using linear regression.</p>\n<p>$$\ny(x) = \\sum_{j=1}^{m} c_j B_{j,k}(x) + \\varepsilon\n$$\nWhere:</p>\n<p><code>\\(c_j\\)</code> is the coefficient vector (something we want to estimate). <br/>\n<code>\\(B_{j,k}\\)</code> is the basis function evaluated at <code>\\(x_i\\)</code> (the design matrix)</p>\n<p>If we were to write it in a longer form, it would look like</p>\n<p>$$\ny(x) = c_1 B_{1,k}(x) + c_2 B_{2,k}(x) + ‚Ä¶ + c_m B_{m,k}(x) + \\varepsilon\n$$</p>\n<p>Instead of fitting <code>x</code> directly to <code>y</code>, we are fitting the basis functions <code>\\(B_{j,k}(x)\\)</code> to <code>y</code>. The coefficients <code>\\(c_j\\)</code> are the weights assigned to each basis function. The basis functions are defined over a set of knots, which are the points where the piecewise polynomial segments meet. The degree of the polynomial is determined by the parameter <code>k</code>.</p>\n<p>How we construct the basis functions? Well‚Ä¶ in comes the Cox deBoor algorithm!</p>\n<h4 id=\"to-understand-this-we-need-to-know-the-available-parameters\">To Understand This, We Need To Know The Available Parameters\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#to-understand-this-we-need-to-know-the-available-parameters\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<ul>\n<li><code>number of basis functions</code>: this is called <code>df</code> in <code>spline::bs()</code></li>\n<li><code>degree</code>: this is the degree of the polynomial segments, which is usually set to 3 for cubic splines.</li>\n<li><code>knots</code>: Through out this blog, we will use <code>t</code> for this. This is the set of knots that define the piecewise polynomial segments. The knots are the points where the polynomial segments meet. The number of knots is determined by the <code>df</code> parameter, and the knots are usually evenly spaced within the range of the data.</li>\n</ul>\n<h2 id=\"coxdeboor\">What is Cox deBoor Recursion Formula?\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#coxdeboor\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<p>The Cox deBoor algorithm is a recursive method for constructing B-spline basis functions. It is based on the idea of piecewise polynomial interpolation, where the polynomial segments are defined over a set of knots. The algorithm allows us to compute the basis functions efficiently without having to evaluate them directly.</p>\n<p>The Cox deBoor algorithm is defined as follows:\n$$\nB_{i,0}(x) =\n\\begin{cases}\n1 &amp; \\text{if } t_i \\leq x &lt; t_{i+1} \\\\\n0 &amp; \\text{otherwise}\n\\end{cases}\n$$\n$$\nB_{i,k}(x) = \\frac{x - t_i}{t_{i+k} - x_i} \\cdot  B_{i,k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} \\cdot B_{i+1,k-1}(x)\n$$</p>\n<p>Where:</p>\n<ul>\n<li><code>i</code> is the index of the knot,</li>\n<li><code>k</code> is the degree of the polynomial (we‚Äôre going to use 3 for cubic spline).</li>\n<li><code>\\(t_i\\)</code> and <code>\\(t_{i+k}\\)</code> are the knots, which are the points where the piecewise polynomial segments meet.</li>\n<li><code>\\(B_{i,0}(x)\\)</code> is the zeroth degree basis function, which is a piecewise constant function defined over the knots.</li>\n<li><code>\\(B_{i,k}(x)\\)</code> is the k-th degree basis function, which is defined recursively using the previous degree basis functions.\nThe algorithm starts with the zeroth degree basis functions, which are defined as piecewise constant functions over the knots. Then, it recursively computes the higher degree basis functions using the previous degree basis functions. The recursion continues until the desired degree is reached.</li>\n</ul>\n<p>Dive deeper \n<a href=\"https://en.wikipedia.org/wiki/De_Boor%27s_algorithm\" rel=\"nofollow\" target=\"_blank\">Cox deBoor Algorithm</a></p>\n<h4 id=\"ok-thats-a-lot-of-letters-and-subscripts-im-really-confused-\">OK, That‚Äôs a lot of letters and subscripts, I‚Äôm REALLY confused! üòµ‚Äçüí´\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#ok-thats-a-lot-of-letters-and-subscripts-im-really-confused-\" rel=\"nofollow\" target=\"_blank\"></a>\n</h4>\n<p>Let‚Äôs to the calculation and see how this works.</p>\n<p>Let‚Äôs assume out <code>t</code> aka <code>knots</code> for degree 3: <code>[-3, -2, -1, 0, 1, 2, 3, 4, 5, 6]</code>. And take note that first first knot starts as <code>0</code>.</p>\n<ol>\n<li>\n<p>Let‚Äôs calculate our zeroth degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{i,0}(x) =\n\\begin{cases}\n1 &amp; \\text{if } t_i \\leq x &lt; t_{i+1} \\\\\n0 &amp; \\text{otherwise}\n\\end{cases} \\\\\nB_{0,0}(x=1.5) =\n\\begin{cases}\n1 &amp; \\text{if } t_0 \\leq x &lt; t_{0+1} \\\\\n0 &amp; \\text{otherwise}\n\\end{cases} \\\\\n= 0 \\text{ , given } t_0 = -3 \\text{ and } t_1 = -2 \\\\\nB_{1,0}(x=1.5) = 0 \\\\\nB_{2,0}(x=1.5) = 0 \\\\\nB_{3,0}(x=1.5) = 0 \\\\\nB_{4,0}(x=1.5) = 1 \\\\\nB_{5,0}(x=1.5) = 0 \\\\\nB_{6,0}(x=1.5) = 0 \\\\\nB_{7,0}(x=1.5) = 0 \\\\\nB_{8,0}(x=1.5) = 0 \\\\\nB_{9,0}(x=1.5) = 0\n$$</p>\n</li>\n<li>\n<p>Now, let‚Äôs calculate the first degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{0,1}(x=1.5) = \\frac{x - t_0}{t_{1} - t_0} \\cdot B_{0,0}(x) + \\frac{t_{2} - x}{t_{2} - t_1} \\cdot B_{1,0}(x) \\\\\n= \\frac{1.5 - (-3)}{-2 - (-3)} \\cdot 0 + \\frac{-1 - 1.5}{-1 - (-2)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{1,1}(x=1.5) = \\frac{x - t_1}{t_{2} - t_1} \\cdot B_{1,0}(x) + \\frac{t_{3} - x}{t_{3} - t_2} \\cdot B_{2,0}(x) \\\\\n= \\frac{1.5 - (-2)}{-1 - (-2)} \\cdot 0 + \\frac{0 - 1.5}{0 - (-1)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{2,1}(x=1.5) = \\frac{x - t_2}{t_{3} - t_2} \\cdot B_{2,0}(x) + \\frac{t_{4} - x}{t_{4} - t_3} \\cdot B_{3,0}(x) \\\\\n= \\frac{1.5 - (-1)}{0 - (-1)} \\cdot 0 + \\frac{1 - 1.5}{1 - 0} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{3,1}(x=1.5) = \\frac{x - t_3}{t_{4} - t_3} \\cdot B_{3,0}(x) + \\frac{t_{5} - x}{t_{5} - t_4} \\cdot B_{4,0}(x) \\\\\n= \\frac{1.5 - 0}{1 - 0} \\cdot 0 + \\frac{2 - 1.5}{2 - 1} \\cdot 1 \\\\\n= 0 + \\frac{0.5}{1} \\cdot 1 \\\\\n= 0.5 \\\\\nB_{4,1}(x=1.5) = \\frac{x - t_4}{t_{5} - t_4} \\cdot B_{4,0}(x) + \\frac{t_{6} - x}{t_{6} - t_5} \\cdot B_{5,0}(x) \\\\\n= \\frac{1.5 - 1}{2 - 1} \\cdot 1 + \\frac{3 - 1.5}{3 - 2} \\cdot 0 \\\\\n= \\frac{0.5}{1} \\cdot 1 + 0 \\\\\n= 0.5 \\\\\nB_{5,1}(x=1.5) = \\frac{x - t_5}{t_{6} - t_5} \\cdot B_{5,0}(x) + \\frac{t_{7} - x}{t_{7} - t_6} \\cdot B_{6,0}(x) \\\\\n= \\frac{1.5 - 2}{3 - 2} \\cdot 0 + \\frac{4 - 1.5}{4 - 3} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{6,1}(x=1.5) = \\frac{x - t_6}{t_{7} - t_6} \\cdot B_{6,0}(x) + \\frac{t_{8} - x}{t_{8} - t_7} \\cdot B_{7,0}(x) \\\\\n= \\frac{1.5 - 3}{4 - 3} \\cdot 0 + \\frac{5 - 1.5}{5 - 4} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{7,1}(x=1.5) = \\frac{x - t_7}{t_{8} - t_7} \\cdot B_{7,0}(x) + \\frac{t_{9} - x}{t_{9} - t_8} \\cdot B_{8,0}(x) \\\\\n= \\frac{1.5 - 4}{5 - 4} \\cdot 0 + \\frac{6 - 1.5}{6 - 5} \\cdot 0 \\\\\n= 0 + 0 = 0\n$$</p>\n</li>\n<li>\n<p>Now, let‚Äôs calculate the second degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{0,2}(x=1.5) = \\frac{x - t_0}{t_{2} - t_0} \\cdot B_{0,1}(x) + \\frac{t_{3} - x}{t_{3} - t_1} \\cdot B_{1,1}(x) \\\\\n= \\frac{1.5 - (-3)}{-1 - (-3)} \\cdot 0 + \\frac{0 - 1.5}{0 - (-2)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{1,2}(x=1.5) = \\frac{x - t_1}{t_{3} - t_1} \\cdot B_{1,1}(x) + \\frac{t_{4} - x}{t_{4} - t_2} \\cdot B_{2,1}(x) \\\\\n= \\frac{1.5 - (-2)}{0 - (-2)} \\cdot 0 + \\frac{1 - 1.5}{1 - (-1)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{2,2}(x=1.5) = \\frac{x - t_2}{t_{4} - t_2} \\cdot B_{2,1}(x) + \\frac{t_{5} - x}{t_{5} - t_3} \\cdot B_{3,1}(x) \\\\\n= \\frac{1.5 - (-1)}{1 - (-1)} \\cdot 0 + \\frac{2 - 1.5}{2 - 0} \\cdot 0.5 \\\\\n= 0 + \\frac{0.5}{2} \\cdot 0.5 \\\\\n= 0.25 \\cdot 0.5 = 0.125 \\\\\nB_{3,2}(x=1.5) = \\frac{x - t_3}{t_{5} - t_3} \\cdot B_{3,1}(x) + \\frac{t_{6} - x}{t_{6} - t_4} \\cdot B_{4,1}(x) \\\\\n= \\frac{1.5 - 0}{2 - 0} \\cdot 0.5 + \\frac{3 - 1.5}{3 - 1} \\cdot 0.5 \\\\\n= \\frac{1.5}{2} \\cdot 0.5 + \\frac{1.5}{2} \\cdot 0.5 \\\\\n= 0.75 \\cdot 0.5 + 0.75 \\cdot 0.5 \\\\\n= 0.375 + 0.375 = 0.75 \\\\\nB_{4,2}(x=1.5) = \\frac{x - t_4}{t_{6} - t_4} \\cdot B_{4,1}(x) + \\frac{t_{7} - x}{t_{7} - t_5} \\cdot B_{5,1}(x) \\\\\n= \\frac{1.5 - 1}{3 - 1} \\cdot 0.5 + \\frac{4 - 1.5}{4 - 2} \\cdot 0 \\\\\n= \\frac{0.5}{2} \\cdot 0.5 + 0 \\\\\n= 0.25 \\cdot 0.5 = 0.125 \\\\\nB_{5,2}(x=1.5) = \\frac{x - t_5}{t_{7} - t_5} \\cdot B_{5,1}(x) + \\frac{t_{8} - x}{t_{8} - t_6} \\cdot B_{6,1}(x) \\\\\n= \\frac{1.5 - 2}{4 - 2} \\cdot 0 + \\frac{5 - 1.5}{5 - 3} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{6,2}(x=1.5) = \\frac{x - t_6}{t_{8} - t_6} \\cdot B_{6,1}(x) + \\frac{t_{9} - x}{t_{9} - t_7} \\cdot B_{7,1}(x) \\\\\n= \\frac{1.5 - 3}{5 - 3} \\cdot 0 + \\frac{6 - 1.5}{6 - 4} \\cdot 0 \\\\\n= 0 + 0 = 0\n$$\nAlright, still with me?</p>\n</li>\n<li>\n<p>Now, let‚Äôs calculate the third degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{0,3}(x=1.5) = \\frac{x - t_0}{t_{3} - t_0} \\cdot B_{0,2}(x) + \\frac{t_{4} - x}{t_{4} - t_1} \\cdot B_{1,2}(x) \\\\\n= \\frac{1.5 - (-3)}{0 - (-3)} \\cdot 0 + \\frac{1 - 1.5}{1 - (-2)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{1,3}(x=1.5) = \\frac{x - t_1}{t_{4} - t_1} \\cdot B_{1,2}(x) + \\frac{t_{5} - x}{t_{5} - t_2} \\cdot B_{2,2}(x) \\\\\n= \\frac{1.5 - (-2)}{1 - (-2)} \\cdot 0 + \\frac{2 - 1.5}{2 - (-1)} \\cdot 0.125 \\\\\n= 0 + \\frac{0.5}{3} \\cdot 0.125 \\\\\n= \\frac{1}{6} \\cdot 0.125 \\\\\n= 0.02083 \\\\\nB_{2,3}(x=1.5) = \\frac{x - t_2}{t_{5} - t_2} \\cdot B_{2,2}(x) + \\frac{t_{6} - x}{t_{6} - t_3} \\cdot B_{3,2}(x) \\\\\n= \\frac{1.5 - (-1)}{2 - (-1)} \\cdot 0.125 + \\frac{3 - 1.5}{3 - 0} \\cdot 0.75 \\\\\n= \\frac{2.5}{3} \\cdot 0.125 + \\frac{1.5}{3} \\cdot 0.75 \\\\\n= \\frac{5}{6} \\cdot 0.125 + \\frac{1}{2} \\cdot 0.75 \\\\\n= 0.104166\\overline{6} + 0.375 \\\\\n= 0.4792 \\\\\nB_{3,3}(x=1.5) = \\frac{x - t_3}{t_{6} - t_3} \\cdot B_{3,2}(x) + \\frac{t_{7} - x}{t_{7} - t_4} \\cdot B_{4,2}(x) \\\\\n= \\frac{1.5 - 0}{3 - 0} \\cdot 0.75 + \\frac{4 - 1.5}{4 - 1} \\cdot 0.125 \\\\\n= \\frac{1.5}{3} \\cdot 0.75 + \\frac{2.5}{3} \\cdot 0.125 \\\\\n= \\frac{1}{2} \\cdot 0.75 + \\frac{5}{6} \\cdot 0.125 \\\\\n= 0.375 + 0.104166\\overline{6} \\\\\n= 0.479166\\overline{6} \\\\\nB_{4,3}(x=1.5) = \\frac{x - t_4}{t_{7} - t_4} \\cdot B_{4,2}(x) + \\frac{t_{8} - x}{t_{8} - t_5} \\cdot B_{5,2}(x) \\\\\n= \\frac{1.5 - 1}{4 - 1} \\cdot 0.125 + \\frac{5 - 1.5}{5 - 2} \\cdot 0 \\\\\n= \\frac{0.5}{3} \\cdot 0.125 + 0 \\\\\n= \\frac{1}{6} \\cdot 0.125 \\\\\n= 0.0208\\overline{3} \\\\\nB_{5,3}(x=1.5) = \\frac{x - t_5}{t_{8} - t_5} \\cdot B_{5,2}(x) + \\frac{t_{9} - x}{t_{9} - t_6} \\cdot B_{6,2}(x) \\\\\n= \\frac{1.5 - 2}{5 - 2} \\cdot 0 + \\frac{6 - 1.5}{6 - 3} \\cdot 0 \\\\\n= 0 + 0\n= 0\n$$\nAlright, let‚Äôs look at <code>spliness:bs()</code> function in R and see if we get the same result</p>\n</li>\n</ol>\n<pre>library(splines)\nx_i &lt;- 1.5\nbasis_matrix &lt;- splines::bs(x_i, \n                           knots = c(-2, -1, 0, 1, 2, 3, 4, 5),  # interior knots\n                           Boundary.knots = c(-3, 6),            # your boundary knots\n                           degree = 3)\n\nbasis_matrix\n\n##      1 2 3          4         5         6          7 8 9 10 11\n## [1,] 0 0 0 0.02083333 0.4791667 0.4791667 0.02083333 0 0  0  0\n## attr(,\"degree\")\n## [1] 3\n## attr(,\"knots\")\n## [1] -2 -1  0  1  2  3  4  5\n## attr(,\"Boundary.knots\")\n## [1] -3  6\n## attr(,\"intercept\")\n## [1] FALSE\n## attr(,\"class\")\n## [1] \"bs\"     \"basis\"  \"matrix\"\n</pre><p>Awesome!!! We have very similar numbers !!! But why are there so many columns? And they‚Äôre filled with zeros, reminds me almost like padding. Apparently this is mainly to ensure no issues when <code>x</code> is very close to <code>boundary knots</code> is my understanding. And the way they add these padding depends on <code>degrees + 1</code> if you use <code>degree = 3</code>, add <code>1</code> to it and that would be how many repeats of the numbers of your boundary knots.</p>\n<p>Wow, that was really cool! Can‚Äôt believe we calculated all those, with many many trial and error of course on pen and paper. Let‚Äôs use <code>splines::bs()</code> and create basis matrix for our original dataset!</p>\n<h2 id=\"code\">Let‚Äôs code\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#code\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<pre>library(splines)\n\nbasis_matrix &lt;- bs(x = df$x, df = 10, degree = 3, intercept = T)\n\nbasis_matrix |&gt; head(10)\n\n##              1           2           3         4           5          6\n##  [1,] 0.000000 0.001710709 0.254088352 0.6432639 0.100937064 0.00000000\n##  [2,] 0.000000 0.000000000 0.008978124 0.3970003 0.559853971 0.03416757\n##  [3,] 0.000000 0.000000000 0.000000000 0.0000000 0.146190239 0.65598485\n##  [4,] 0.000000 0.000000000 0.000000000 0.0000000 0.000000000 0.00000000\n##  [5,] 0.000000 0.061617580 0.518423931 0.4100735 0.009885001 0.00000000\n##  [6,] 0.000000 0.000000000 0.000000000 0.0000000 0.000000000 0.00000000\n##  [7,] 0.000000 0.000000000 0.000000000 0.0000000 0.000000000 0.00000000\n##  [8,] 0.000000 0.000000000 0.000000000 0.0000000 0.008305368 0.38042994\n##  [9,] 0.000000 0.000000000 0.000000000 0.0000000 0.032566024 0.51323693\n## [10,] 0.200424 0.595242195 0.192030599 0.0123032 0.000000000 0.00000000\n##                 7            8         9         10\n##  [1,] 0.000000000 0.0000000000 0.0000000 0.00000000\n##  [2,] 0.000000000 0.0000000000 0.0000000 0.00000000\n##  [3,] 0.197526778 0.0002981355 0.0000000 0.00000000\n##  [4,] 0.044205251 0.3880099033 0.5304629 0.03732193\n##  [5,] 0.000000000 0.0000000000 0.0000000 0.00000000\n##  [6,] 0.059974278 0.4418574606 0.4799942 0.01817410\n##  [7,] 0.009664164 0.1778373908 0.5976815 0.21481698\n##  [8,] 0.556522931 0.0547417617 0.0000000 0.00000000\n##  [9,] 0.435168766 0.0190282776 0.0000000 0.00000000\n## [10,] 0.000000000 0.0000000000 0.0000000 0.00000000\n\nattr(basis_matrix, \"knots\")\n\n## [1] -3.5289455 -2.0677071 -0.7635657  0.5407687  2.1571925  3.6217023\n\nattr(basis_matrix, \"Boundary.knots\")\n\n## [1] -4.986853  4.999306\n</pre><p>Alright, we have our basis matrix! The <code>bs()</code> function returns a matrix where each column corresponds to a basis function evaluated at the <code>x</code> values. The <code>df</code> parameter specifies the number of basis functions, and the <code>degree</code> parameter specifies the degree of the polynomial segments. The <code>intercept = T</code> argument adds an intercept term to the model.</p>\n<p>Now, let‚Äôs fit a linear regression model using the basis matrix as the design matrix. This is similar to how we would fit a GAM model, but we will use a linear regression model for simplicity.</p>\n<pre>lm_model &lt;- lm(y ~ basis_matrix, data = df)\n\nsummary(lm_model)\n\n## \n## Call:\n## lm(formula = y ~ basis_matrix, data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.60301 -0.35186 -0.00505  0.35426  1.69358 \n## \n## Coefficients: (1 not defined because of singularities)\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)     -0.8762     0.1327  -6.602 6.62e-11 ***\n## basis_matrix1    1.7943     0.1974   9.090  &lt; 2e-16 ***\n## basis_matrix2    1.8904     0.1968   9.605  &lt; 2e-16 ***\n## basis_matrix3    1.3809     0.1933   7.143 1.77e-12 ***\n## basis_matrix4   -0.2858     0.1688  -1.694   0.0907 .  \n## basis_matrix5   -0.0927     0.1671  -0.555   0.5793    \n## basis_matrix6    1.9814     0.1622  12.214  &lt; 2e-16 ***\n## basis_matrix7    1.7176     0.1867   9.201  &lt; 2e-16 ***\n## basis_matrix8    0.6788     0.1588   4.275 2.09e-05 ***\n## basis_matrix9   -0.5299     0.2389  -2.219   0.0267 *  \n## basis_matrix10       NA         NA      NA       NA    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5136 on 990 degrees of freedom\n## Multiple R-squared:  0.6565,\tAdjusted R-squared:  0.6534 \n## F-statistic: 210.3 on 9 and 990 DF,  p-value: &lt; 2.2e-16\n</pre><p>Alright, we have our linear regression model fitted using the basis matrix.</p>\n<pre>df |&gt;\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = predict(lm_model)), color = \"blue\", size = 1) +\n  theme_bw()\n</pre><img data-lazy-src=\"https://i0.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-7-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-7-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Look at that beauty!!! The blue curve is the fitted model using the basis spline. It captures the non-linear relationship between <code>x</code> and <code>y</code> very well, just like the GAM model we fitted earlier. The coefficients of the basis functions represent the weights assigned to each basis function, which determine the shape of the spline.</p>\n<h3 id=\"visualize\">Let‚Äôs look at all the splines!\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#visualize\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h3>\n<pre>tibble(x = x, y = y) |&gt;\n  bind_cols(basis_matrix) |&gt;\n  mutate(\n    intercept = lm_model$coefficients[1],  # Add this line!\n    `1` = `1` * lm_model$coefficients[2],\n    `2` = `2` * lm_model$coefficients[3],\n    `3` = `3` * lm_model$coefficients[4],\n    `4` = `4` * lm_model$coefficients[5],\n    `5` = `5` * lm_model$coefficients[6],\n    `6` = `6` * lm_model$coefficients[7],\n    `7` = `7` * lm_model$coefficients[8],\n    `8` = `8` * lm_model$coefficients[9],\n    `9` = `9` * lm_model$coefficients[10]\n  ) |&gt; \n  mutate(total = intercept + `1` + `2` + `3` + `4` + `5` + `6` + `7` + `8` + `9`) |&gt; \n  ggplot() +\n  geom_point(aes(x=x, y=y), alpha = 0.1) +\n  geom_line(aes(x=x, y=total), color = \"black\", size = 1.5, alpha = 0.8) +\n  geom_hline(aes(yintercept = intercept), color = \"red\") +\n  geom_line(aes(x, `1`), color = \"blue\") +\n  geom_line(aes(x, `2`), color = \"green\") +\n  geom_line(aes(x, `3`), color = \"purple\") +\n  geom_line(aes(x, `4`), color = \"orange\") +\n  geom_line(aes(x, `5`), color = \"brown\") +\n  geom_line(aes(x, `6`), color = \"pink\") +\n  geom_line(aes(x, `7`), color = \"cyan\") +\n  geom_line(aes(x, `8`), color = \"magenta\") +\n  geom_line(aes(x, `9`), color = \"yellow\") +\n  geom_line(aes(x, `10`), color = \"darkgreen\") +\n  labs(title = \"B-spline, black = predicted values (sum of all splines + intercept)\",\n       subtitle = \"y = sin(x) + rnorm(n, sd=0.5)\",\n       y = \"y\") +\n  theme_minimal()\n</pre><img data-lazy-src=\"https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-8-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img data-recalc-dims=\"1\" src=\"https://i2.wp.com/www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-8-1.png?w=450&amp;ssl=1\"/></noscript>\n<p>Take note that colors other than <code>black</code> are intercept and also the splines. The <code>black</code> line is our predicted values, which is the sum of all the splines and the intercept. Notice I didn‚Äôt use <code>predict</code> at all to form the <code>black</code> line, we basically summed ALL the weighted coefficients on x and that is our prediction! üôå Even though this is not a straight line, it‚Äôs still linear in nature! This is fascinating!</p>\n<p>The first time time I heard about how we can use splines as predictors in linear regression was on \n<a href=\"https://www.youtube.com/watch?v=F0N4b7K_iYQ&amp;list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&amp;index=4\" rel=\"nofollow\" target=\"_blank\">Richard McElreath‚Äôs Statistical Rethinking</a> with bayesian stats. At 52:15 he discussed why don‚Äôt use polynomial and how b-spline is not as bad. 59:21 is when he talks about bspline. Highly recommend watching it!</p>\n<pre>library(cmdstanr)\n\nstan_code &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;             \n  int&lt;lower=0&gt; K;              \n  matrix[N, K] B;              \n  vector[N] y;                \n}\n\nparameters {\n  vector[K] beta;               \n  real&lt;lower=0&gt; sigma;         \n}\n\ntransformed parameters {\n  vector[N] mu;                \n  mu = B * beta;               \n}\n\nmodel {\n  // Priors\n  beta ~ normal(0, 2);        \n  sigma ~ exponential(1);      \n  \n  // Likelihood\n  y ~ normal(mu, sigma);       \n}\n\n\"\n\n# Prepare data for Stan\nstan_data &lt;- list(\n  N = n,\n  K = 10,\n  B = basis_matrix,\n  y = y\n)\n\n# Write stan file\nmod &lt;- write_stan_file(stan_code)\nmodel &lt;- cmdstan_model(mod)\n\n# Fit the model\nfit &lt;- model$sample(\n  data = stan_data,\n  chains = 4, \n  iter_sampling = 2000,\n  iter_warmup = 1000,\n  seed = 1,\n  parallel_chains = 4\n)\n\n# Posterior\ndraws &lt;- fit$draws(variables = c(\"beta\", \"mu\"), format = \"df\", inc_warmup = F)\n\n# data wrangling, select only mu\ndf_draw &lt;- draws |&gt;\n  mutate(iter = row_number()) |&gt;\n  filter(iter %in% sample(1:8000, 1000, replace=F)) |&gt; #sample 1000 iters\n  select(iter, contains(\"mu\")) |&gt;\n  pivot_longer(cols = contains(\"mu\"), names_to = \"mu\", values_to = \"value\") |&gt;\n  mutate(idx = str_extract(mu, \"\\\\d+\")) |&gt;\n  left_join(df |&gt; mutate(idx = row_number() |&gt; as.character()), by = \"idx\") \n  \n# plot\nplot &lt;- ggplot() +\n  geom_line(data=df_draw, aes(x = x, y = value, group=iter), alpha = 0.009, color=\"blue\") +\n  geom_point(data=df,aes(x=x,y=y),alpha=0.2) +\n  theme_bw()\n\nplot\n</pre><p><img alt=\"\" data-lazy-src=\"https://i1.wp.com/www.kenkoonwong.com/blog/bspline/mcmc_plot.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.kenkoonwong.com/blog/bspline/mcmc_plot.png?w=578&amp;ssl=1\"/></noscript>\nWhat really connected my intuition was when I saw Richard‚Äôs math formula when constructing a linear model! check out the book Statistical Rethinking page 117.</p>\n<p>$$\n\\begin{gather}\ny_i \\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i = \\beta_0 + \\sum_{i=1}^{m} \\beta_i B_{i,k}(x_i) \\\\\n\\beta_i \\sim N(0, 2) \\\\\n\\sigma \\sim \\text{Exponential}(1) \\\\\n\\end{gather}\n$$\nThis is exactly what we did in the code above! We constructed a linear model with basis splines as predictors, and we can see how the coefficients of the basis functions determine the shape of the spline.</p>\n<h2 id=\"opportunity\">Opportunities for improvement\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#opportunity\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li><code>gam</code> has a more sophisticated penalty for the wiggliness of the spline, which is not captured in this simple linear regression model.</li>\n<li>more to read up on <code>gam</code> on model diagnostics, multivariate splines, and how to interpret the results.</li>\n<li>how to calculate edf</li>\n<li><code>splines::bs()</code> function also has interesting boundary knot behavior</li>\n<li>there exists other types of splines, such as natural splines and regression splines, which have different properties and applications.</li>\n</ul>\n<h2 id=\"lesson\">Lessons Learnt\n  <a href=\"https://www.kenkoonwong.com/blog/bspline/#lesson\" rel=\"nofollow\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"anchor-symbol\" height=\"26\" viewbox=\"0 0 22 22\" width=\"26\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M0 0h24v24H0z\" fill=\"currentColor\"></path>\n<path d=\"M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z\"></path>\n</svg></a>\n</h2>\n<ul>\n<li>B-splines are a powerful tool for modeling non-linear relationships in regression analysis.</li>\n<li>The Cox deBoor algorithm provides an efficient way to compute B-spline basis functions recursively.</li>\n<li>In the future, if I don‚Äôt understand certain thing, look at bayesian stats, they often have to construct model from the bottom up, this will help me understand the engine behind</li>\n<li>The <code>splines::bs()</code> function in R allows us to easily create B-spline basis matrices for regression analysis.</li>\n</ul>\n<p>If you like this article:</p>\n<ul>\n<li>please feel free to send me a \n<a href=\"https://www.kenkoonwong.com/blog/\" rel=\"nofollow\" target=\"_blank\">comment or visit my other blogs</a></li>\n<li>please feel free to follow me on \n<a href=\"https://bsky.app/profile/kenkoonwong.bsky.social\" rel=\"nofollow\" target=\"_blank\">BlueSky</a>, \n<a href=\"https://twitter.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">twitter</a>, \n<a href=\"https://github.com/kenkoonwong/\" rel=\"nofollow\" target=\"_blank\">GitHub</a> or \n<a href=\"https://med-mastodon.com/@kenkoonwong\" rel=\"nofollow\" target=\"_blank\">Mastodon</a></li>\n<li>if you would like collaborate please feel free to \n<a href=\"https://www.kenkoonwong.com/contact/\" rel=\"nofollow\" target=\"_blank\">contact me</a></li>\n</ul>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.kenkoonwong.com/blog/bspline/\"> r on Everyday Is A School Day</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
    "main_text": "Understanding Basis Spline (B-spline) By Working Through Cox-deBoor Algorithm\nPosted on\nJune 2, 2025\nby\nr on Everyday Is A School Day\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nr on Everyday Is A School Day\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nI finally understood B-splines by working through the Cox-deBoor algorithm step-by-step, discovering they‚Äôre just weighted combinations of basis functions that make non-linear regression linear. What surprised me is going through Bayesian statistics really helped me understand the engine behind the model! Will try this again in the future!\nMotivations\nI‚Äôve always been curious and amazed by spline as smoothing function. The first time I heard of that was with\nmgcv\npackage on generalized additve model. The second time I heard of that was Richard McElreath‚Äôs\nStatistical Rethinking 2023 videos\n, more to come on that. His book cover, that‚Äôs the product of basis spline, and of course his talent to convert that into wonders. Ther first time I heard him stating that he prefers basis spline smoothing function over polynomial was what caught my attention and never really understood the basics behind it, until recently. It‚Äôs been mentioned here and there over the year, I‚Äôve also tried to read up a bit on it and it just did not stick and I was not able to understand it. Hence, the motivation is to get the intuition behind, and nothing better than to actually look at the underlying formula that makes up the function and then code it! I also found that learning this in bayesian way is very helpful as well, as you have to write all the formula yourself, what to estimate, what to include etc. These 2 combinations, simulation and code in bayesian have been very helpful for me to understand the concept better. So, what are we waiting for, let‚Äôs get some basics in smoothening this spline out!\nObjective\nWhat is Basis Spline?\nWhat is Cox deBoor Recursion Formula?\nLet‚Äôs code\nLet‚Äôs visualize\nOpportunities for improvement\nLessons Learnt\nWhat is Basis Spline?\nBasis spline, or B-spline, is a piecewise polynomial function that is used for smoothing data. It is defined by a set of control points and a degree, which determines the polynomial degree of the segments between the control points. The B-spline is constructed using a set of basis functions, which are defined over a set of knots. The B-spline is a linear combination of these basis functions, where the coefficients are the weights assigned to each basis function.\nWow that‚Äôs a lot of words! Let‚Äôs take a look and see when we usually use this function and expand there. Let‚Äôs write some simulation and then use\nmgcv\npackage and model a\ngam\nand look at the output.\nlibrary(tidyverse)\nlibrary(mgcv)\n\nset.seed(1)\nn <- 1000\nx <- runif(n,-5,5)\ny <- sin(x) + rnorm(n, 0, 0.5)\ndf <- tibble(x,y)\n\ndf |>\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha = 0.2) +\n  theme_bw()\nAlright, we have a dataset with\nx\nand\ny\nvalues, where\ny\nis a noisy sine wave. Now, let‚Äôs fit a generalized additive model (GAM) using the\nmgcv\npackage to see how the spline smooths the data.\ngam_model <- gam(y ~ s(x, bs = \"cr\"), data = df)\n\ndf |>\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = predict(gam_model)), color = \"blue\", size = 1) +\n  theme_bw()\nLook at that nice blue curve! That‚Äôs our gam model. Looks like it fits a really good non-linear relationship of\nx\nand\ny\n. Now let‚Äôs look at the summary output\nsummary(gam_model)\n\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x, bs = \"cr\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) -0.04663    0.01623  -2.874  0.00415 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##        edf Ref.df   F p-value    \n## s(x) 8.545   8.94 211  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.654   Deviance explained = 65.7%\n## GCV = 0.26587  Scale est. = 0.26334   n = 1000\nThe summary output shows us the estimated coefficients for the spline basis functions, the effective degrees of freedom, and the significance of the smooth term. The\ns(x, bs = \"cr\")\nindicates that we are using a cubic regression spline (CR) for the smooth term.\nSomething to pay attention to is the effective degree of freedom (\nedf\n). It measures the complexity of the wiggliness of the smooth function. edf = 1 means it‚Äôs linear. the higher the number the higher the wiggliness is. We may on another post look more closely at what these numbers mean, how the check the models etc, but right now I‚Äôm more interested in how basis spline is constructed and how it works under the hood.\ngam\nhas a more sophisticated penalty for the wiggliness of the spline, but for now let‚Äôs just focus on the basics of how the spline is constructed using linear regression.\n$$\ny(x) = \\sum_{j=1}^{m} c_j B_{j,k}(x) + \\varepsilon\n$$\nWhere:\n\\(c_j\\)\nis the coefficient vector (something we want to estimate).\n\\(B_{j,k}\\)\nis the basis function evaluated at\n\\(x_i\\)\n(the design matrix)\nIf we were to write it in a longer form, it would look like\n$$\ny(x) = c_1 B_{1,k}(x) + c_2 B_{2,k}(x) + ‚Ä¶ + c_m B_{m,k}(x) + \\varepsilon\n$$\nInstead of fitting\nx\ndirectly to\ny\n, we are fitting the basis functions\n\\(B_{j,k}(x)\\)\nto\ny\n. The coefficients\n\\(c_j\\)\nare the weights assigned to each basis function. The basis functions are defined over a set of knots, which are the points where the piecewise polynomial segments meet. The degree of the polynomial is determined by the parameter\nk\n.\nHow we construct the basis functions? Well‚Ä¶ in comes the Cox deBoor algorithm!\nTo Understand This, We Need To Know The Available Parameters\nnumber of basis functions\n: this is called\ndf\nin\nspline::bs()\ndegree\n: this is the degree of the polynomial segments, which is usually set to 3 for cubic splines.\nknots\n: Through out this blog, we will use\nt\nfor this. This is the set of knots that define the piecewise polynomial segments. The knots are the points where the polynomial segments meet. The number of knots is determined by the\ndf\nparameter, and the knots are usually evenly spaced within the range of the data.\nWhat is Cox deBoor Recursion Formula?\nThe Cox deBoor algorithm is a recursive method for constructing B-spline basis functions. It is based on the idea of piecewise polynomial interpolation, where the polynomial segments are defined over a set of knots. The algorithm allows us to compute the basis functions efficiently without having to evaluate them directly.\nThe Cox deBoor algorithm is defined as follows:\n$$\nB_{i,0}(x) =\n\\begin{cases}\n1 & \\text{if } t_i \\leq x < t_{i+1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n$$\nB_{i,k}(x) = \\frac{x - t_i}{t_{i+k} - x_i} \\cdot  B_{i,k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} \\cdot B_{i+1,k-1}(x)\n$$\nWhere:\ni\nis the index of the knot,\nk\nis the degree of the polynomial (we‚Äôre going to use 3 for cubic spline).\n\\(t_i\\)\nand\n\\(t_{i+k}\\)\nare the knots, which are the points where the piecewise polynomial segments meet.\n\\(B_{i,0}(x)\\)\nis the zeroth degree basis function, which is a piecewise constant function defined over the knots.\n\\(B_{i,k}(x)\\)\nis the k-th degree basis function, which is defined recursively using the previous degree basis functions.\nThe algorithm starts with the zeroth degree basis functions, which are defined as piecewise constant functions over the knots. Then, it recursively computes the higher degree basis functions using the previous degree basis functions. The recursion continues until the desired degree is reached.\nDive deeper\nCox deBoor Algorithm\nOK, That‚Äôs a lot of letters and subscripts, I‚Äôm REALLY confused! üòµ‚Äçüí´\nLet‚Äôs to the calculation and see how this works.\nLet‚Äôs assume out\nt\naka\nknots\nfor degree 3:\n[-3, -2, -1, 0, 1, 2, 3, 4, 5, 6]\n. And take note that first first knot starts as\n0\n.\nLet‚Äôs calculate our zeroth degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{i,0}(x) =\n\\begin{cases}\n1 & \\text{if } t_i \\leq x < t_{i+1} \\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\nB_{0,0}(x=1.5) =\n\\begin{cases}\n1 & \\text{if } t_0 \\leq x < t_{0+1} \\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\n= 0 \\text{ , given } t_0 = -3 \\text{ and } t_1 = -2 \\\\\nB_{1,0}(x=1.5) = 0 \\\\\nB_{2,0}(x=1.5) = 0 \\\\\nB_{3,0}(x=1.5) = 0 \\\\\nB_{4,0}(x=1.5) = 1 \\\\\nB_{5,0}(x=1.5) = 0 \\\\\nB_{6,0}(x=1.5) = 0 \\\\\nB_{7,0}(x=1.5) = 0 \\\\\nB_{8,0}(x=1.5) = 0 \\\\\nB_{9,0}(x=1.5) = 0\n$$\nNow, let‚Äôs calculate the first degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{0,1}(x=1.5) = \\frac{x - t_0}{t_{1} - t_0} \\cdot B_{0,0}(x) + \\frac{t_{2} - x}{t_{2} - t_1} \\cdot B_{1,0}(x) \\\\\n= \\frac{1.5 - (-3)}{-2 - (-3)} \\cdot 0 + \\frac{-1 - 1.5}{-1 - (-2)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{1,1}(x=1.5) = \\frac{x - t_1}{t_{2} - t_1} \\cdot B_{1,0}(x) + \\frac{t_{3} - x}{t_{3} - t_2} \\cdot B_{2,0}(x) \\\\\n= \\frac{1.5 - (-2)}{-1 - (-2)} \\cdot 0 + \\frac{0 - 1.5}{0 - (-1)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{2,1}(x=1.5) = \\frac{x - t_2}{t_{3} - t_2} \\cdot B_{2,0}(x) + \\frac{t_{4} - x}{t_{4} - t_3} \\cdot B_{3,0}(x) \\\\\n= \\frac{1.5 - (-1)}{0 - (-1)} \\cdot 0 + \\frac{1 - 1.5}{1 - 0} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{3,1}(x=1.5) = \\frac{x - t_3}{t_{4} - t_3} \\cdot B_{3,0}(x) + \\frac{t_{5} - x}{t_{5} - t_4} \\cdot B_{4,0}(x) \\\\\n= \\frac{1.5 - 0}{1 - 0} \\cdot 0 + \\frac{2 - 1.5}{2 - 1} \\cdot 1 \\\\\n= 0 + \\frac{0.5}{1} \\cdot 1 \\\\\n= 0.5 \\\\\nB_{4,1}(x=1.5) = \\frac{x - t_4}{t_{5} - t_4} \\cdot B_{4,0}(x) + \\frac{t_{6} - x}{t_{6} - t_5} \\cdot B_{5,0}(x) \\\\\n= \\frac{1.5 - 1}{2 - 1} \\cdot 1 + \\frac{3 - 1.5}{3 - 2} \\cdot 0 \\\\\n= \\frac{0.5}{1} \\cdot 1 + 0 \\\\\n= 0.5 \\\\\nB_{5,1}(x=1.5) = \\frac{x - t_5}{t_{6} - t_5} \\cdot B_{5,0}(x) + \\frac{t_{7} - x}{t_{7} - t_6} \\cdot B_{6,0}(x) \\\\\n= \\frac{1.5 - 2}{3 - 2} \\cdot 0 + \\frac{4 - 1.5}{4 - 3} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{6,1}(x=1.5) = \\frac{x - t_6}{t_{7} - t_6} \\cdot B_{6,0}(x) + \\frac{t_{8} - x}{t_{8} - t_7} \\cdot B_{7,0}(x) \\\\\n= \\frac{1.5 - 3}{4 - 3} \\cdot 0 + \\frac{5 - 1.5}{5 - 4} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{7,1}(x=1.5) = \\frac{x - t_7}{t_{8} - t_7} \\cdot B_{7,0}(x) + \\frac{t_{9} - x}{t_{9} - t_8} \\cdot B_{8,0}(x) \\\\\n= \\frac{1.5 - 4}{5 - 4} \\cdot 0 + \\frac{6 - 1.5}{6 - 5} \\cdot 0 \\\\\n= 0 + 0 = 0\n$$\nNow, let‚Äôs calculate the second degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{0,2}(x=1.5) = \\frac{x - t_0}{t_{2} - t_0} \\cdot B_{0,1}(x) + \\frac{t_{3} - x}{t_{3} - t_1} \\cdot B_{1,1}(x) \\\\\n= \\frac{1.5 - (-3)}{-1 - (-3)} \\cdot 0 + \\frac{0 - 1.5}{0 - (-2)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{1,2}(x=1.5) = \\frac{x - t_1}{t_{3} - t_1} \\cdot B_{1,1}(x) + \\frac{t_{4} - x}{t_{4} - t_2} \\cdot B_{2,1}(x) \\\\\n= \\frac{1.5 - (-2)}{0 - (-2)} \\cdot 0 + \\frac{1 - 1.5}{1 - (-1)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{2,2}(x=1.5) = \\frac{x - t_2}{t_{4} - t_2} \\cdot B_{2,1}(x) + \\frac{t_{5} - x}{t_{5} - t_3} \\cdot B_{3,1}(x) \\\\\n= \\frac{1.5 - (-1)}{1 - (-1)} \\cdot 0 + \\frac{2 - 1.5}{2 - 0} \\cdot 0.5 \\\\\n= 0 + \\frac{0.5}{2} \\cdot 0.5 \\\\\n= 0.25 \\cdot 0.5 = 0.125 \\\\\nB_{3,2}(x=1.5) = \\frac{x - t_3}{t_{5} - t_3} \\cdot B_{3,1}(x) + \\frac{t_{6} - x}{t_{6} - t_4} \\cdot B_{4,1}(x) \\\\\n= \\frac{1.5 - 0}{2 - 0} \\cdot 0.5 + \\frac{3 - 1.5}{3 - 1} \\cdot 0.5 \\\\\n= \\frac{1.5}{2} \\cdot 0.5 + \\frac{1.5}{2} \\cdot 0.5 \\\\\n= 0.75 \\cdot 0.5 + 0.75 \\cdot 0.5 \\\\\n= 0.375 + 0.375 = 0.75 \\\\\nB_{4,2}(x=1.5) = \\frac{x - t_4}{t_{6} - t_4} \\cdot B_{4,1}(x) + \\frac{t_{7} - x}{t_{7} - t_5} \\cdot B_{5,1}(x) \\\\\n= \\frac{1.5 - 1}{3 - 1} \\cdot 0.5 + \\frac{4 - 1.5}{4 - 2} \\cdot 0 \\\\\n= \\frac{0.5}{2} \\cdot 0.5 + 0 \\\\\n= 0.25 \\cdot 0.5 = 0.125 \\\\\nB_{5,2}(x=1.5) = \\frac{x - t_5}{t_{7} - t_5} \\cdot B_{5,1}(x) + \\frac{t_{8} - x}{t_{8} - t_6} \\cdot B_{6,1}(x) \\\\\n= \\frac{1.5 - 2}{4 - 2} \\cdot 0 + \\frac{5 - 1.5}{5 - 3} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{6,2}(x=1.5) = \\frac{x - t_6}{t_{8} - t_6} \\cdot B_{6,1}(x) + \\frac{t_{9} - x}{t_{9} - t_7} \\cdot B_{7,1}(x) \\\\\n= \\frac{1.5 - 3}{5 - 3} \\cdot 0 + \\frac{6 - 1.5}{6 - 4} \\cdot 0 \\\\\n= 0 + 0 = 0\n$$\nAlright, still with me?\nNow, let‚Äôs calculate the third degree basis function at x = 1.5\n$$\nt_0 = -3, t_1 = -2, t_2 = -1, t_3 = 0, t_4 = 1, t_5 = 2, t_6 = 3, t_7 = 4, t_8 = 5, t_9 = 6 \\\\\nB_{0,3}(x=1.5) = \\frac{x - t_0}{t_{3} - t_0} \\cdot B_{0,2}(x) + \\frac{t_{4} - x}{t_{4} - t_1} \\cdot B_{1,2}(x) \\\\\n= \\frac{1.5 - (-3)}{0 - (-3)} \\cdot 0 + \\frac{1 - 1.5}{1 - (-2)} \\cdot 0 \\\\\n= 0 + 0 = 0 \\\\\nB_{1,3}(x=1.5) = \\frac{x - t_1}{t_{4} - t_1} \\cdot B_{1,2}(x) + \\frac{t_{5} - x}{t_{5} - t_2} \\cdot B_{2,2}(x) \\\\\n= \\frac{1.5 - (-2)}{1 - (-2)} \\cdot 0 + \\frac{2 - 1.5}{2 - (-1)} \\cdot 0.125 \\\\\n= 0 + \\frac{0.5}{3} \\cdot 0.125 \\\\\n= \\frac{1}{6} \\cdot 0.125 \\\\\n= 0.02083 \\\\\nB_{2,3}(x=1.5) = \\frac{x - t_2}{t_{5} - t_2} \\cdot B_{2,2}(x) + \\frac{t_{6} - x}{t_{6} - t_3} \\cdot B_{3,2}(x) \\\\\n= \\frac{1.5 - (-1)}{2 - (-1)} \\cdot 0.125 + \\frac{3 - 1.5}{3 - 0} \\cdot 0.75 \\\\\n= \\frac{2.5}{3} \\cdot 0.125 + \\frac{1.5}{3} \\cdot 0.75 \\\\\n= \\frac{5}{6} \\cdot 0.125 + \\frac{1}{2} \\cdot 0.75 \\\\\n= 0.104166\\overline{6} + 0.375 \\\\\n= 0.4792 \\\\\nB_{3,3}(x=1.5) = \\frac{x - t_3}{t_{6} - t_3} \\cdot B_{3,2}(x) + \\frac{t_{7} - x}{t_{7} - t_4} \\cdot B_{4,2}(x) \\\\\n= \\frac{1.5 - 0}{3 - 0} \\cdot 0.75 + \\frac{4 - 1.5}{4 - 1} \\cdot 0.125 \\\\\n= \\frac{1.5}{3} \\cdot 0.75 + \\frac{2.5}{3} \\cdot 0.125 \\\\\n= \\frac{1}{2} \\cdot 0.75 + \\frac{5}{6} \\cdot 0.125 \\\\\n= 0.375 + 0.104166\\overline{6} \\\\\n= 0.479166\\overline{6} \\\\\nB_{4,3}(x=1.5) = \\frac{x - t_4}{t_{7} - t_4} \\cdot B_{4,2}(x) + \\frac{t_{8} - x}{t_{8} - t_5} \\cdot B_{5,2}(x) \\\\\n= \\frac{1.5 - 1}{4 - 1} \\cdot 0.125 + \\frac{5 - 1.5}{5 - 2} \\cdot 0 \\\\\n= \\frac{0.5}{3} \\cdot 0.125 + 0 \\\\\n= \\frac{1}{6} \\cdot 0.125 \\\\\n= 0.0208\\overline{3} \\\\\nB_{5,3}(x=1.5) = \\frac{x - t_5}{t_{8} - t_5} \\cdot B_{5,2}(x) + \\frac{t_{9} - x}{t_{9} - t_6} \\cdot B_{6,2}(x) \\\\\n= \\frac{1.5 - 2}{5 - 2} \\cdot 0 + \\frac{6 - 1.5}{6 - 3} \\cdot 0 \\\\\n= 0 + 0\n= 0\n$$\nAlright, let‚Äôs look at\nspliness:bs()\nfunction in R and see if we get the same result\nlibrary(splines)\nx_i <- 1.5\nbasis_matrix <- splines::bs(x_i, \n                           knots = c(-2, -1, 0, 1, 2, 3, 4, 5),  # interior knots\n                           Boundary.knots = c(-3, 6),            # your boundary knots\n                           degree = 3)\n\nbasis_matrix\n\n##      1 2 3          4         5         6          7 8 9 10 11\n## [1,] 0 0 0 0.02083333 0.4791667 0.4791667 0.02083333 0 0  0  0\n## attr(,\"degree\")\n## [1] 3\n## attr(,\"knots\")\n## [1] -2 -1  0  1  2  3  4  5\n## attr(,\"Boundary.knots\")\n## [1] -3  6\n## attr(,\"intercept\")\n## [1] FALSE\n## attr(,\"class\")\n## [1] \"bs\"     \"basis\"  \"matrix\"\nAwesome!!! We have very similar numbers !!! But why are there so many columns? And they‚Äôre filled with zeros, reminds me almost like padding. Apparently this is mainly to ensure no issues when\nx\nis very close to\nboundary knots\nis my understanding. And the way they add these padding depends on\ndegrees + 1\nif you use\ndegree = 3\n, add\n1\nto it and that would be how many repeats of the numbers of your boundary knots.\nWow, that was really cool! Can‚Äôt believe we calculated all those, with many many trial and error of course on pen and paper. Let‚Äôs use\nsplines::bs()\nand create basis matrix for our original dataset!\nLet‚Äôs code\nlibrary(splines)\n\nbasis_matrix <- bs(x = df$x, df = 10, degree = 3, intercept = T)\n\nbasis_matrix |> head(10)\n\n##              1           2           3         4           5          6\n##  [1,] 0.000000 0.001710709 0.254088352 0.6432639 0.100937064 0.00000000\n##  [2,] 0.000000 0.000000000 0.008978124 0.3970003 0.559853971 0.03416757\n##  [3,] 0.000000 0.000000000 0.000000000 0.0000000 0.146190239 0.65598485\n##  [4,] 0.000000 0.000000000 0.000000000 0.0000000 0.000000000 0.00000000\n##  [5,] 0.000000 0.061617580 0.518423931 0.4100735 0.009885001 0.00000000\n##  [6,] 0.000000 0.000000000 0.000000000 0.0000000 0.000000000 0.00000000\n##  [7,] 0.000000 0.000000000 0.000000000 0.0000000 0.000000000 0.00000000\n##  [8,] 0.000000 0.000000000 0.000000000 0.0000000 0.008305368 0.38042994\n##  [9,] 0.000000 0.000000000 0.000000000 0.0000000 0.032566024 0.51323693\n## [10,] 0.200424 0.595242195 0.192030599 0.0123032 0.000000000 0.00000000\n##                 7            8         9         10\n##  [1,] 0.000000000 0.0000000000 0.0000000 0.00000000\n##  [2,] 0.000000000 0.0000000000 0.0000000 0.00000000\n##  [3,] 0.197526778 0.0002981355 0.0000000 0.00000000\n##  [4,] 0.044205251 0.3880099033 0.5304629 0.03732193\n##  [5,] 0.000000000 0.0000000000 0.0000000 0.00000000\n##  [6,] 0.059974278 0.4418574606 0.4799942 0.01817410\n##  [7,] 0.009664164 0.1778373908 0.5976815 0.21481698\n##  [8,] 0.556522931 0.0547417617 0.0000000 0.00000000\n##  [9,] 0.435168766 0.0190282776 0.0000000 0.00000000\n## [10,] 0.000000000 0.0000000000 0.0000000 0.00000000\n\nattr(basis_matrix, \"knots\")\n\n## [1] -3.5289455 -2.0677071 -0.7635657  0.5407687  2.1571925  3.6217023\n\nattr(basis_matrix, \"Boundary.knots\")\n\n## [1] -4.986853  4.999306\nAlright, we have our basis matrix! The\nbs()\nfunction returns a matrix where each column corresponds to a basis function evaluated at the\nx\nvalues. The\ndf\nparameter specifies the number of basis functions, and the\ndegree\nparameter specifies the degree of the polynomial segments. The\nintercept = T\nargument adds an intercept term to the model.\nNow, let‚Äôs fit a linear regression model using the basis matrix as the design matrix. This is similar to how we would fit a GAM model, but we will use a linear regression model for simplicity.\nlm_model <- lm(y ~ basis_matrix, data = df)\n\nsummary(lm_model)\n\n## \n## Call:\n## lm(formula = y ~ basis_matrix, data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.60301 -0.35186 -0.00505  0.35426  1.69358 \n## \n## Coefficients: (1 not defined because of singularities)\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     -0.8762     0.1327  -6.602 6.62e-11 ***\n## basis_matrix1    1.7943     0.1974   9.090  < 2e-16 ***\n## basis_matrix2    1.8904     0.1968   9.605  < 2e-16 ***\n## basis_matrix3    1.3809     0.1933   7.143 1.77e-12 ***\n## basis_matrix4   -0.2858     0.1688  -1.694   0.0907 .  \n## basis_matrix5   -0.0927     0.1671  -0.555   0.5793    \n## basis_matrix6    1.9814     0.1622  12.214  < 2e-16 ***\n## basis_matrix7    1.7176     0.1867   9.201  < 2e-16 ***\n## basis_matrix8    0.6788     0.1588   4.275 2.09e-05 ***\n## basis_matrix9   -0.5299     0.2389  -2.219   0.0267 *  \n## basis_matrix10       NA         NA      NA       NA    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5136 on 990 degrees of freedom\n## Multiple R-squared:  0.6565,\tAdjusted R-squared:  0.6534 \n## F-statistic: 210.3 on 9 and 990 DF,  p-value: < 2.2e-16\nAlright, we have our linear regression model fitted using the basis matrix.\ndf |>\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = predict(lm_model)), color = \"blue\", size = 1) +\n  theme_bw()\nLook at that beauty!!! The blue curve is the fitted model using the basis spline. It captures the non-linear relationship between\nx\nand\ny\nvery well, just like the GAM model we fitted earlier. The coefficients of the basis functions represent the weights assigned to each basis function, which determine the shape of the spline.\nLet‚Äôs look at all the splines!\ntibble(x = x, y = y) |>\n  bind_cols(basis_matrix) |>\n  mutate(\n    intercept = lm_model$coefficients[1],  # Add this line!\n    `1` = `1` * lm_model$coefficients[2],\n    `2` = `2` * lm_model$coefficients[3],\n    `3` = `3` * lm_model$coefficients[4],\n    `4` = `4` * lm_model$coefficients[5],\n    `5` = `5` * lm_model$coefficients[6],\n    `6` = `6` * lm_model$coefficients[7],\n    `7` = `7` * lm_model$coefficients[8],\n    `8` = `8` * lm_model$coefficients[9],\n    `9` = `9` * lm_model$coefficients[10]\n  ) |> \n  mutate(total = intercept + `1` + `2` + `3` + `4` + `5` + `6` + `7` + `8` + `9`) |> \n  ggplot() +\n  geom_point(aes(x=x, y=y), alpha = 0.1) +\n  geom_line(aes(x=x, y=total), color = \"black\", size = 1.5, alpha = 0.8) +\n  geom_hline(aes(yintercept = intercept), color = \"red\") +\n  geom_line(aes(x, `1`), color = \"blue\") +\n  geom_line(aes(x, `2`), color = \"green\") +\n  geom_line(aes(x, `3`), color = \"purple\") +\n  geom_line(aes(x, `4`), color = \"orange\") +\n  geom_line(aes(x, `5`), color = \"brown\") +\n  geom_line(aes(x, `6`), color = \"pink\") +\n  geom_line(aes(x, `7`), color = \"cyan\") +\n  geom_line(aes(x, `8`), color = \"magenta\") +\n  geom_line(aes(x, `9`), color = \"yellow\") +\n  geom_line(aes(x, `10`), color = \"darkgreen\") +\n  labs(title = \"B-spline, black = predicted values (sum of all splines + intercept)\",\n       subtitle = \"y = sin(x) + rnorm(n, sd=0.5)\",\n       y = \"y\") +\n  theme_minimal()\nTake note that colors other than\nblack\nare intercept and also the splines. The\nblack\nline is our predicted values, which is the sum of all the splines and the intercept. Notice I didn‚Äôt use\npredict\nat all to form the\nblack\nline, we basically summed ALL the weighted coefficients on x and that is our prediction! üôå Even though this is not a straight line, it‚Äôs still linear in nature! This is fascinating!\nThe first time time I heard about how we can use splines as predictors in linear regression was on\nRichard McElreath‚Äôs Statistical Rethinking\nwith bayesian stats. At 52:15 he discussed why don‚Äôt use polynomial and how b-spline is not as bad. 59:21 is when he talks about bspline. Highly recommend watching it!\nlibrary(cmdstanr)\n\nstan_code <- \"\ndata {\n  int<lower=0> N;             \n  int<lower=0> K;              \n  matrix[N, K] B;              \n  vector[N] y;                \n}\n\nparameters {\n  vector[K] beta;               \n  real<lower=0> sigma;         \n}\n\ntransformed parameters {\n  vector[N] mu;                \n  mu = B * beta;               \n}\n\nmodel {\n  // Priors\n  beta ~ normal(0, 2);        \n  sigma ~ exponential(1);      \n  \n  // Likelihood\n  y ~ normal(mu, sigma);       \n}\n\n\"\n\n# Prepare data for Stan\nstan_data <- list(\n  N = n,\n  K = 10,\n  B = basis_matrix,\n  y = y\n)\n\n# Write stan file\nmod <- write_stan_file(stan_code)\nmodel <- cmdstan_model(mod)\n\n# Fit the model\nfit <- model$sample(\n  data = stan_data,\n  chains = 4, \n  iter_sampling = 2000,\n  iter_warmup = 1000,\n  seed = 1,\n  parallel_chains = 4\n)\n\n# Posterior\ndraws <- fit$draws(variables = c(\"beta\", \"mu\"), format = \"df\", inc_warmup = F)\n\n# data wrangling, select only mu\ndf_draw <- draws |>\n  mutate(iter = row_number()) |>\n  filter(iter %in% sample(1:8000, 1000, replace=F)) |> #sample 1000 iters\n  select(iter, contains(\"mu\")) |>\n  pivot_longer(cols = contains(\"mu\"), names_to = \"mu\", values_to = \"value\") |>\n  mutate(idx = str_extract(mu, \"\\\\d+\")) |>\n  left_join(df |> mutate(idx = row_number() |> as.character()), by = \"idx\") \n  \n# plot\nplot <- ggplot() +\n  geom_line(data=df_draw, aes(x = x, y = value, group=iter), alpha = 0.009, color=\"blue\") +\n  geom_point(data=df,aes(x=x,y=y),alpha=0.2) +\n  theme_bw()\n\nplot\nWhat really connected my intuition was when I saw Richard‚Äôs math formula when constructing a linear model! check out the book Statistical Rethinking page 117.\n$$\n\\begin{gather}\ny_i \\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i = \\beta_0 + \\sum_{i=1}^{m} \\beta_i B_{i,k}(x_i) \\\\\n\\beta_i \\sim N(0, 2) \\\\\n\\sigma \\sim \\text{Exponential}(1) \\\\\n\\end{gather}\n$$\nThis is exactly what we did in the code above! We constructed a linear model with basis splines as predictors, and we can see how the coefficients of the basis functions determine the shape of the spline.\nOpportunities for improvement\ngam\nhas a more sophisticated penalty for the wiggliness of the spline, which is not captured in this simple linear regression model.\nmore to read up on\ngam\non model diagnostics, multivariate splines, and how to interpret the results.\nhow to calculate edf\nsplines::bs()\nfunction also has interesting boundary knot behavior\nthere exists other types of splines, such as natural splines and regression splines, which have different properties and applications.\nLessons Learnt\nB-splines are a powerful tool for modeling non-linear relationships in regression analysis.\nThe Cox deBoor algorithm provides an efficient way to compute B-spline basis functions recursively.\nIn the future, if I don‚Äôt understand certain thing, look at bayesian stats, they often have to construct model from the bottom up, this will help me understand the engine behind\nThe\nsplines::bs()\nfunction in R allows us to easily create B-spline basis matrices for regression analysis.\nIf you like this article:\nplease feel free to send me a\ncomment or visit my other blogs\nplease feel free to follow me on\nBlueSky\n,\ntwitter\n,\nGitHub\nor\nMastodon\nif you would like collaborate please feel free to\ncontact me\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nr on Everyday Is A School Day\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
    "meta_description": "I finally understood B-splines by working through the Cox-deBoor algorithm step-by-step, discovering they‚Äôre just weighted combinations of basis functions that make non-linear regression linear. What surprised me is going through Bayesian statis...",
    "meta_keywords": null,
    "og_description": "I finally understood B-splines by working through the Cox-deBoor algorithm step-by-step, discovering they‚Äôre just weighted combinations of basis functions that make non-linear regression linear. What surprised me is going through Bayesian statis...",
    "og_image": "https://www.kenkoonwong.com/blog/bspline/index_files/figure-html/unnamed-chunk-1-1.png",
    "og_title": "Understanding Basis Spline (B-spline) By Working Through Cox-deBoor Algorithm | R-bloggers",
    "raw_jsonld_article": null,
    "reading_time_min": 23,
    "sitemap_lastmod": null,
    "twitter_description": "I finally understood B-splines by working through the Cox-deBoor algorithm step-by-step, discovering they‚Äôre just weighted combinations of basis functions that make non-linear regression linear. What surprised me is going through Bayesian statis...",
    "twitter_title": "Understanding Basis Spline (B-spline) By Working Through Cox-deBoor Algorithm | R-bloggers",
    "url": "https://www.r-bloggers.com/2025/06/understanding-basis-spline-b-spline-by-working-through-cox-deboor-algorithm/",
    "word_count": 4602
  }
}