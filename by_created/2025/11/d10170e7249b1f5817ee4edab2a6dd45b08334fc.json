{
  "id": "d10170e7249b1f5817ee4edab2a6dd45b08334fc",
  "url": "https://www.r-bloggers.com/2025/06/ultra-processed-food-an-ai-polling-simulation/",
  "created_at_utc": "2025-11-22T19:58:16Z",
  "data": null,
  "raw_original": {
    "uuid": "91a94368-baf6-4d59-8fce-8ba975573f3a",
    "created_at": "2025-11-22 19:58:16",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/06/ultra-processed-food-an-ai-polling-simulation/",
      "crawled_at": "2025-11-22T10:46:49.901366",
      "external_links": [
        {
          "href": "https://cbowdon.github.io/posts/food/",
          "text": "Chris Bowdon"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.cambridge.org/core/journals/political-analysis/article/abs/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49",
          "text": "simulate public opinion polls with AI"
        },
        {
          "href": "https://journofinder.com/blog/uk-newspaper-circulation-figures",
          "text": "JournoFinder"
        },
        {
          "href": "https://datacove.co.uk/",
          "text": "Datacove"
        },
        {
          "href": "https://www.polecat.com/",
          "text": "Polecat"
        },
        {
          "href": "https://ellmer.tidyverse.org/",
          "text": "ellmer"
        },
        {
          "href": "https://cbowdon.github.io/posts/food/",
          "text": "Chris Bowdon"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "Ultra-processed food: an AI polling simulation | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-responses-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/framerusercontent.com/images/1BjmhUMPE6dsrYLtK1qkJV8p5k.png?w=578&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-article-counts-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-sarima-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-source-dist-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-source-count-context-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIBAMAAAA2IaO4AAAAKlBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADmU0mKAAAADXRSTlMARIkiZlTdqxDvmc12Bf+SEwAAAAlwSFlzAAAOxAAADsQBlSsOGwAAACpJREFUCB1jYFQ0cmRgNVNQZ4iQbBBmaNBiAIKtQMxxA0iwbgASPAcYGAByEwXc4G6iEwAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?r"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAAKBAMAAACgUqiRAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAVklEQVQIHWNgVHY2TWAAgYpMA8YFYJaANQNTAJjFcJaBVQHCmsHAX8DALMzAwP6BQY6DIZc5gYH5AcNZZob8MAYGJgOGXBEGtpsQpQxsAaxQFouFGQMAfpwML48m1e8AAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?n_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIBAMAAAA2IaO4AAAAKlBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADmU0mKAAAADXRSTlMAZiJUic0yEN3vq0R2VHD1mQAAAAlwSFlzAAAOxAAADsQBlSsOGwAAADBJREFUCB1jYGAUFGJgMGRwZGBwTShgYNC4acDAwKBygYHVgGECAxcD9wEG7s2GDABqzQZa3aknjAAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAPBAMAAADpFNeEAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVM0iq7tEEO9mmTJ23YmxseOzAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABZElEQVQoFYWPPUvDUBSGn7Y26acfkwhO+geyOQkuWlGROigUihQHcREzCGIpteBQxCWLSGqRrlUE/4A0oIsg2uLUreDkJjiI6ODNTZpKK/XCzXnve5733hzwTdJ/9QCz/XnoBnb0fxLdQMj4K7DXMbuBSLLT66jspae7geiH1/ot8nPtUw9wBNlmOYF6Zd7p5M1bS6LhVDshgN1mOe1/XS3XheebgFiLC3gkUvEl8LtDKcu6jEigwv4mKVrCGa9pBDPiwBpB40akNcmJq76kkICBz+CQYVBm4hYhPWdBUX2gAVGXVxZkUgJDmkADBbuzQazO9Mi5RqAx9qK+uTSE046UQM1MCkdslCKBd0oodcIVtkVBlWBuxeEd4MT++wFLWPFrWKeKUiBisGW/Oipssov2VywHWEJzpGPyTNAirpEWg0cztjnldtxybNcDz8t9mlXUJ7NkMHh/5tkdkfvWxQjzp/AD/2NUDATh5zcAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?%5Clambda_%7BBBC%7D%20%3E%20%5Clambda_%7BFT%7D"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAGEAAAAMBAMAAACAQs4UAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABNUlEQVQoFXWPv0vDQBTHv0muSVpbFdpZglUEf9CDUJHqENSxBRERQYWuIkIGB926ShdBB8faLh3t4tzJRTq7+g8IBUFX33tXq6B5cJ/37nuf5BIgobYT8uS4+pR0ZhW3yvX/DmsHJv0jXJxoqwX1HM5HAtTChUBcpyNtJByGi8FbpbMETK/D3gFWkO0LrD1MavN2/ziiYSQsQ601MjpHyQCpAOgiowWngB1TzGV9MI1AF+aRji0K7jBFxq2aNegBLntU/pE8aoRPdi859oaY8eH18i8CNeRQymlLM4L7XqEdLzivGDhw+mgK/AdAibl5L+1baPEf7HJka5wVkNW4EnjXQIHz6j6TygipwKP5URLBRIw2BF24DY7OJR8jXafx1yerUrihBcgVV8faz6BKc7Qp30TAFw51QClDdxwrAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?n_%7BBBC%7D%20%3E%20n_%7BFT%7D"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAAKBAMAAACgUqiRAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAVklEQVQIHWNgVHY2TWAAgYpMA8YFYJaANQNTAJjFcJaBVQHCmsHAX8DALMzAwP6BQY6DIZc5gYH5AcNZZob8MAYGJgOGXBEGtpsQpQxsAaxQFouFGQMAfpwML48m1e8AAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?n_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAAKBAMAAACgUqiRAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAVklEQVQIHWNgVHY2TWAAgYpMA8YFYJaANQNTAJjFcJaBVQHCmsHAX8DALMzAwP6BQY6DIZc5gYH5AcNZZob8MAYGJgOGXBEGtpsQpQxsAaxQFouFGQMAfpwML48m1e8AAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?n_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAMBAMAAACZySCyAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHYiZpkQRO+Jq7vdMs3/9KjfAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAYklEQVQIHWNgEFJ2CWAAAjaxLwwhIAY7ewPDLBCDgWkBw1Iwg1uAoZHB0YGBgT+B/QDvBF8GhvsJfBd4f11gYFB1VGZguNnAwLAFqPIWwwIGhs9ARkrJBQbeD0AGCNT9hNAAAKkU7SLirCUAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?p_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPBAMAAADJ+Ih5AAAALVBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAOrOgAAAADnRSTlMAMrsiRO92q2ZUEM2JmfLO1I4AAAAJcEhZcwAADsQAAA7EAZUrDhsAAABjSURBVAgdY2BgYBB6DCRAgCUAQjOwFUAZTA1QBp8BmMGWORUioMFQDmZwKDCUgRmsDQyBYIadAeMDBo4ZDAxyDGwKDjIcAgysDKsPNMgVAs2/OfHGAsajEBMYC9ghDK6WRgYAL/cP8Lj6/isAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?d_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAAATBAMAAABo/p7UAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAB40lEQVQ4EY2SP4gTURDGf5vd7G6yiYl/OkGCNpYrQREsbuFKBdfGwmotBC1y2ULwFPFAOVAE/+AhanWFlSDaWhnESjjIgQiiYrTQTiIqZ2HhzL4kuxExvmLm+7753sybTeB/j9OaOHM0kcbgza+3D5IxyXIZrM9GOTRVmCLVGGvDKFdN2i3pmIFWaPJfYiPE+Wb0RZMGkq5HOTboz9hNcL8WxWxGZTRovViZwnNQWykq1VRYbdVIy7Lrrvm9iWHFKJtcSaT4iuM9aLfTilbd78YjZPFUaK0ahvWkL40iZffaO1/CM846zdtYYT0MMs+tLNKA5gFKsWEsnJExdSXeT432gMZJb0j9k8MFFawvGqESwRrllmI5z3WBqiJ7qFE+w+Gk3sN7epMFFbZ0dUsIIrhDI8XeqrQHL04fUVQeaFxKmaOSyPj3HBTux0GLPaE0FGnIDp+OLVVEKd1tCSDoa+zCR7b7fBCjWLlG6bHb35fdkx3WbLpH1ZefpabiILFj3tnEXGRThL+Ct+H+SOCcDAjpbMN9nd8RVH+UvdZbb6PVy5tDSk1pAw/p3JDfZeR2Y/nX/vt4samfkOncH3md/fLoGWfknb+UMO4x44Ypn89dtSjHM5GffarMNvtxxW7phAj6Dcq1V6UnDkekAAAAAElFTkSuQmCC",
          "src": "https://latex.codecogs.com/png.latex?%0An_s%20%5Csim%20Pois(%5Clambda_s)%0A"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAAATBAMAAACAUum6AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAMrsiRO92q2ZUEM2Jmd0gkiEXAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACdElEQVQ4EZWTPWgUQRTHf/txl93LJVkEkZBArhK1OlBRFLmzECxELxAhCMIVYmK3WhgUhBX8iMaPVTB+BGQLiwTBi41tziJFIMoRIcSQwsbGKpJCDAZ8M3tHTvYg5rHs/Of33pudmfcWtm2ZcquUdAx3rCWcnzfmF4oJGoO2ljxT09guJbymj/0TTiccAi60gvBC47Sf8KYC+AMHEg4B060gtGtsBglvIY/1O0E1cJPRmmdDNXTl9aT5dRvMXDPY1GYyWjttH9KXnm0GNtQIb3ZDd43x+6OXwR3+yB1RU889Umo6ederh7oXDz+MZQn2MNLA36sSF6nZ+ZcPvpAJ+63jgzyFRQ4qNRG6OTrlGncGImKbqgQdgZarODmu1nHP/iuQVRNLuuRM3rZWXecbQzjTdHWLeoSU6SiEWsRZxRNk5XRi55CK9ceUZTpymGriyH67ymRrdFTlUxJT8bQyy/SIf4W2sopTNkRbTYsfFPKGbE43h6CFI9cUlzUpBKTkdoqsSQyPleqnkued+AdUve4FomAvqVDnl+gjnSv2Op7QQApxtiyCTnnPRry2KeBOF/vggyhrlTFb3b2ICduo3lShrEtH6/ySuN5Wgz6pVbNVPJwBmHcYk1MGnZ7ji5KrXXRoj9T9rTjGL8+6DsY671H5Vg7765OlyJhtXonszMqnGxH0vmIYYy6y5kYRJd14KiQtT6BcvSVjUE6yNJ5H5TuBXsPwW/+v/3ygMbH8WO3Cx5WWDmRvKt+MNM/cio8eB2313hcHTJ701Frqp9H5h7bKa+U/tglDpHZ1W26I7Yy2rFC3EHumGmvdCQ3+/6OcKmkW/AVTPokmjb9LLAAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?%0Ad_s%20%5Csim%20Binom(n_s,%20p_s)%0A"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAMBAMAAACZySCyAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHYiZpkQRO+Jq7vdMs3/9KjfAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAYklEQVQIHWNgEFJ2CWAAAjaxLwwhIAY7ewPDLBCDgWkBw1Iwg1uAoZHB0YGBgT+B/QDvBF8GhvsJfBd4f11gYFB1VGZguNnAwLAFqPIWwwIGhs9ARkrJBQbeD0AGCNT9hNAAAKkU7SLirCUAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?p_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPBAMAAADJ+Ih5AAAALVBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAOrOgAAAADnRSTlMAMrsiRO92q2ZUEM2JmfLO1I4AAAAJcEhZcwAADsQAAA7EAZUrDhsAAABjSURBVAgdY2BgYBB6DCRAgCUAQjOwFUAZTA1QBp8BmMGWORUioMFQDmZwKDCUgRmsDQyBYIadAeMDBo4ZDAxyDGwKDjIcAgysDKsPNMgVAs2/OfHGAsajEBMYC9ghDK6WRgYAL/cP8Lj6/isAAAAASUVORK5CYII=",
          "src": "https://latex.codecogs.com/png.latex?d_s"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAHsAAAATBAMAAABCTm97AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAMrsiRO92q2ZUEM2Jmd0gkiEXAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACLElEQVQ4EY1TPWgUQRT+9vbndte9vUUEkRzsYaUiuPgDooRLkaggxEQQNNUVIWq3sVDsRrA4C8lZGIyFbGkQzQVBEAvXwiKiSbCQaGVj7UEC6oHgezMXbjcBL6/45nvf+97MvGEX2GG49W3GrrJ7fVtl+e+n/UFOLQLOy5yC5yo1xvIyZYUYTnfTUVWcpOVG3ucImVtxXqasLOD+VupxtbRouZAovoms0VFCLlkIA2h/soI8ydxiXJKOcpQ1Sj4DlCayaoE9pWpWAs5Sal1/mBc5oylHAjjX3mHfKtBoRCarWocRztTJ+5KweAC3JCf5R0qQcHaxMfUB+IITbnMcjvCEzyouS5xfECXBrAzYVdyUIjBwjA71ONE3GO0Wynv0NrxFF6dZcN4wYmgEXszETEDPMc6U4hsPV2BmtxmptBB4q9B/XsEAC7MhvwBwFUUaCfAT1CKtDfsRZyR9PnWbWbHFWIswAzOgy6zgBeVG7NdxTwAHYTa5hWYPYVWHKjaZQAXrUp0I/JQxBF7hiYHX5ONXOg/ru5beAToYBbeQaOJZKsJptvei1mTuB3aMjzZinMGuBMYE9HXtF30RHSyCW47Sjb4+WEu0971WYt4ROYK+1AAqjzE8J2A1aTfgMCpjsNZmI3ALlTm0mH6I/4ceq/peugt/pbLlkNLcuzRQn+han54LwF8pt2xu2adTlgd7JnpSGaWkS3awGPI12Wi8TZW//5WVTyLdeEuQ8g+3xGvENmCjVQAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?%0Ad_s%20%5Csim%20Pois(%5Clambda_s%20p_s)%0A"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": "data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIBAMAAAA2IaO4AAAAKlBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADmU0mKAAAADXRSTlMARIkiZlTdqxDvmc12Bf+SEwAAAAlwSFlzAAAOxAAADsQBlSsOGwAAACpJREFUCB1jYFQ0cmRgNVNQZ4iQbBBmaNBiAIKtQMxxA0iwbgASPAcYGAByEwXc4G6iEwAAAABJRU5ErkJggg==",
          "src": "https://latex.codecogs.com/png.latex?r"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-growth-in-readers-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-actual-gender-split-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-sum-distibutions-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-response-distribution-1.png?w=450&ssl=1"
        },
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i1.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-media-sim-over-time-1.png?w=450&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/chris-bowdon/",
          "text": "Chris Bowdon"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-393479 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">Ultra-processed food: an AI polling simulation</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">June 22, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/chris-bowdon/\">Chris Bowdon</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://cbowdon.github.io/posts/food/\"> Chris Bowdon</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>There‚Äôs a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I‚Äôd never heard until this year. I‚Äôm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been running a tracker survey containing this question since mid-2023, so we can find out.</p>\n<p>An idea that‚Äôs generating a lot of excitement recently is the suggestion that you can <a href=\"https://www.cambridge.org/core/journals/political-analysis/article/abs/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49\" rel=\"nofollow\" target=\"_blank\">simulate public opinion polls with AI</a>. The concept is very simple: you calibrate multiple LLM assistants to represent your different demographics and then sample their responses to questions. This is quicker and cheaper than polling real people, <em>if</em> you can make it reliable.</p>\n<p>That sounds incredibly fun! In this post I‚Äôll explore the FSA tracker survey data, and then build an AI simulation model.</p>\n<section class=\"level1\" id=\"the-survey-data\">\n<h1>The survey data</h1>\n<p>First we need to scrape the survey data. While it‚Äôs lovely that the FSA publishes all the monthly survey results (which were run by YouGov) it is annoying that they don‚Äôt publish compiled statistics, so we need to get our tibbles dirty.</p>\n<p>As usual, if you like coding with R you can see the gory details under the folds.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>library(tidyverse)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(knitr)\n\nif (!str_ends(getwd(), \"posts/food\")) {\n  setwd(\"posts/food\")\n}\n\ndownload_files &lt;- function() {\n  url &lt;- \"https://data.food.gov.uk/catalog/datasets/0bfd916a-4e01-4cb8-ba16-763f0b36b50c\"\n\n  html_content &lt;- request(url) |&gt;\n    req_perform() |&gt;\n    resp_body_string()\n\n  # Assuming html_content is already defined\n  doc &lt;- read_html(html_content)\n  xlsx_links &lt;- doc %&gt;% html_nodes(\"a[href$='.xlsx']\") %&gt;% html_attr(\"href\")\n  for (link in xlsx_links) {\n    file_url &lt;- link\n    file_name &lt;- basename(file_url)\n    destfile &lt;- sprintf(\"data/%s\", file_name)\n    if (!file.exists(destfile)) {\n      download.file(url = file_url, destfile = destfile)\n    }\n  }\n}\n\ndownload_files()</pre>\n</details>\n</div>\n<p>(Is there an easy way to read tracker survey spreadsheets into R? They have a common format, but I never found anything that did the job and had to roll my own. Anyway, eventually we end up with a single dataset.)</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>library(tidyverse)\nlibrary(readxl)\nlibrary(knitr)\n\nextract_month_year &lt;- function(file) {\n  decoded_filename &lt;- URLdecode(file)\n\n  month_year_regex &lt;- \"^.*(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s?(\\\\d{2}|\\\\d{4}).*.xlsx$\"\n\n  if (grepl(month_year_regex, decoded_filename)) {\n    extracted_month_year &lt;- regmatches(\n      decoded_filename,\n      regexec(month_year_regex, decoded_filename)\n    )\n\n    # Extract month and year\n    month &lt;- extracted_month_year[[1]][[2]]\n    year &lt;- extracted_month_year[[1]][[3]]\n\n    if (str_length(year) == 2) {\n      year &lt;- paste0(\"20\", year)\n    }\n\n    return(list(filename = file, month = month, year = year))\n  } else {\n    print(file)\n    return(NULL)\n  }\n}\n\nextract_question &lt;- function(file, question_prefix) {\n  fmy &lt;- extract_month_year(file)\n\n  data &lt;- read_excel(\n    file,\n    sheet = \"Percents\",\n    col_names = FALSE\n  )\n  headers &lt;- as.vector(data[5, ], mode = \"character\")\n  headers[1] &lt;- \"QCategory\"\n  headers &lt;- zoo::na.locf(headers)\n  subheaders &lt;- as.vector(data[6, ], mode = \"character\")\n  subheaders &lt;- zoo::na.fill(subheaders, \"\")\n  combined_headers &lt;- map2(\n    headers,\n    subheaders,\n    \\(x, y) str_c(x, y, sep = \":: \")\n  )\n  combined_headers[1] &lt;- \"QCategory\"\n  combined_headers[2] &lt;- \"Total\"\n\n  colnames(data) &lt;- combined_headers\n\n  has_question &lt;- any(str_starts(pull(data, QCategory), question_prefix))\n\n  if (!is.na(has_question)) {\n    data |&gt;\n      filter(!is.na(QCategory)) |&gt;\n      slice(\n        which(str_starts(QCategory, question_prefix)):n()\n      ) |&gt;\n      head(10) |&gt; # take the cats\n      tail(-1) |&gt; # drop the header\n      mutate(\n        filename = fmy$filename,\n        monthname = fmy$month,\n        year = as.integer(fmy$year),\n        month = match(\n          monthname,\n          c(\n            \"January\",\n            \"February\",\n            \"March\",\n            \"April\",\n            \"May\",\n            \"June\",\n            \"July\",\n            \"August\",\n            \"September\",\n            \"October\",\n            \"November\",\n            \"December\"\n          )\n        )\n      )\n  }\n}\n\nquestion_df &lt;- function(question_prefix) {\n  df_filename &lt;- sprintf(\"data/%s.rds\", question_prefix)\n  if (file.exists(df_filename)) {\n    df &lt;- read_rds(df_filename)\n  } else {\n    xlsx_files &lt;- list.files(\n      path = \"data\",\n      pattern = \"*.xlsx\",\n      full.names = TRUE\n    )\n    df &lt;- xlsx_files |&gt;\n      sort(decreasing = TRUE) |&gt;\n      lapply(\\(f) extract_question(f, question_prefix)) |&gt;\n      keep(\\(x) !is.null(x)) |&gt;\n      bind_rows() |&gt;\n      mutate(Question = question_prefix)\n\n    df |&gt; write_rds(df_filename)\n  }\n  df\n}\n\ndf &lt;- question_df(\"Q12_14\")\n\ntidy_df &lt;- df |&gt;\n  filter(QCategory != \"Unweighted base\" &amp; QCategory != \"Base: All\") |&gt;\n  pivot_longer(\n    c(Total, contains(\"::\")),\n    names_to = \"Demographic\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(\n    Period = as.Date(sprintf(\"%04d-%02d-01\", year, month)),\n    Response = QCategory,\n    Value = as.numeric(Value)\n  ) |&gt;\n  select(Question, Demographic, Period, Response, Value) |&gt;\n  arrange(Question, Period, Response, Demographic)\n\ntidy_df |&gt; head() |&gt; kable()</pre>\n</details>\n<div class=\"cell quarto-float quarto-figure quarto-figure-center anchored\" id=\"tbl-monthly-spreadsheets\">\n<figure class=\"quarto-float quarto-float-tbl figure\">\n<figcaption class=\"quarto-float-caption-top quarto-float-caption quarto-float-tbl\" id=\"tbl-monthly-spreadsheets-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nTable¬†1: Example of the survey data.\n</figcaption>\n<div aria-describedby=\"tbl-monthly-spreadsheets-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<div class=\"cell-output-display\">\n<table class=\"do-not-create-environment cell caption-top table table-sm table-striped small\">\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Question</th>\n<th style=\"text-align: left;\">Demographic</th>\n<th style=\"text-align: left;\">Period</th>\n<th style=\"text-align: left;\">Response</th>\n<th style=\"text-align: right;\">Value</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Q12_14</td>\n<td style=\"text-align: left;\">Age:: 16-24</td>\n<td style=\"text-align: left;\">2023-08-01</td>\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.0384</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Q12_14</td>\n<td style=\"text-align: left;\">Age:: 25-34</td>\n<td style=\"text-align: left;\">2023-08-01</td>\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.0518</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Q12_14</td>\n<td style=\"text-align: left;\">Age:: 35-44</td>\n<td style=\"text-align: left;\">2023-08-01</td>\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.0156</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Q12_14</td>\n<td style=\"text-align: left;\">Age:: 45-54</td>\n<td style=\"text-align: left;\">2023-08-01</td>\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.0142</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Q12_14</td>\n<td style=\"text-align: left;\">Age:: 55-74</td>\n<td style=\"text-align: left;\">2023-08-01</td>\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.0161</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Q12_14</td>\n<td style=\"text-align: left;\">Age:: 75+</td>\n<td style=\"text-align: left;\">2023-08-01</td>\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.0204</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<p>Finally we have ‚ú®tidy‚ú® data and we can visualise it.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>library(patchwork)\n\nRESPONSE_CATS &lt;- c(\n  \"Highly concerned\",\n  \"Somewhat concerned\",\n  \"Not very concerned\",\n  \"Not concerned at all\",\n  \"Don't know\"\n)\n\nnet_concern &lt;- tidy_df |&gt;\n  filter(Demographic == \"Total\" &amp; str_starts(Response, \"Net: \"))\n\nbreakdown &lt;- tidy_df |&gt;\n  filter(Demographic == \"Total\" &amp; !str_starts(Response, \"Net: \")) |&gt;\n  mutate(\n    Response = if_else(Response %in% RESPONSE_CATS, Response, \"Don't know\"),\n    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE)\n  ) |&gt;\n  # Re-normalise after removing the IDK cat\n  group_by(Question, Demographic, Period) |&gt;\n  mutate(Value = Value / sum(Value)) |&gt;\n  ungroup()\n\nplot_net_concern &lt;- ggplot(net_concern) +\n  aes(x = Period, y = Value) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(title = \"Net concern\", x = NULL, y = \"Proportion of respondents\")\n\nplot_breakdown &lt;- ggplot(breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(title = \"Response breakdown\", x = NULL, y = \"Proportion of respondents\")\n\nplot_net_concern + plot_breakdown</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-responses\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-responses-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-responses-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-responses-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-responses-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†1: Tracker for ‚ÄúAt the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?‚Äù Left is net concern, right is all responses.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>The number of people with concerns is evidently slowly increasing. A linear regression fits net concern very well, so it‚Äôs reasonable to expect continued growth at the same rate (pending any intervention like policy changes). Presumably the line bends as we approach the inevitable core of stubborn sods who will never admit to being concerned about anything, but the data we have doesn‚Äôt support that kind of model.</p>\n<p>The breakdown by response type is interesting. We have to watch our step, given the noisiness, but it does seem to show a migration towards being <em>highly</em> concerned.</p>\n<p>Note that I have merged the two categories ‚ÄúI don‚Äôt know enough to comment‚Äù and ‚ÄúDon‚Äôt know‚Äù in the original data for two reasons: first, there‚Äôs a technical limit of 5 categories coming later, and second there is very little difference between these two.</p>\n</section>\n<section class=\"level1\" id=\"the-media-landscape\">\n<h1>The media landscape</h1>\n<p>Besides calibrating the simulated demographics, the major challenge is ensuring that the models are exposed to the same information. If interested in current events, this information is most likely not in the training data. This means you need to ensure the AI has been exposed to the same media.</p>\n<p>Now, here‚Äôs an interesting observation: a surprisingly small number of publications account for a large proportion of UK readership. See this data from <a href=\"https://journofinder.com/blog/uk-newspaper-circulation-figures\" rel=\"nofollow\" target=\"_blank\">JournoFinder</a>.</p>\n<p><img class=\"img-fluid\" data-lazy-src=\"https://i0.wp.com/framerusercontent.com/images/1BjmhUMPE6dsrYLtK1qkJV8p5k.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/framerusercontent.com/images/1BjmhUMPE6dsrYLtK1qkJV8p5k.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>The UK population is approximately 70 million people. Given that the Guardian and the Daily Mail target very different demographics (left and right wing) we wouldn‚Äôt expect much overlap between their readership, i.e.¬†their combined monthly online readership alone probably covers half the news-reading British population. Note also that BBC News isn‚Äôt included in that table, but has a similar level of traffic, albeit with higher overlap.</p>\n<p>This suggests that if we‚Äôre interested in understanding how the media shapes public perception in the UK, we can do well by analysing the output of just a handful of sources. This is particularly true in the modern media landscape, in which:</p>\n<ul>\n<li>Live television is a dying medium. (I was at a brilliant R meetup hosted by <a href=\"https://datacove.co.uk/\" rel=\"nofollow\" target=\"_blank\">Datacove</a> recently when the session discussion turned to media channels. No one present had watched live TV in the last 24 hours, and only around 1 in 10 in the last month.)</li>\n<li>Radio has had its time, has had its power (though yet to have its finest hour üé∏).</li>\n<li>Social media is increasingly popular, but news content on social media is dominated by reposts of articles from traditional journalists.</li>\n</ul>\n<p>So let‚Äôs create a corpus of news articles about ultra-processed food from the top 10 sources in the table above, plus the BBC. They have a combined readership of 64.8 million monthly readers in the UK (plus the BBC‚Äôs unknown-but-high readership); even with high overlap, this clearly represents the majority of the UK population. It‚Äôs enough to give us a solid understanding of what‚Äôs going on.</p>\n<p>I happen to work at <a href=\"https://www.polecat.com/\" rel=\"nofollow\" target=\"_blank\">Polecat</a> who have licensed access to this data, so I can analyse it very easily. (Can‚Äôt share the raw data here though. üôä)</p>\n<p>Since November 2023 there have been over 2.5 million articles from those sources, but only around 2000 mentioned ultra-processed food, and &lt;500 seem to be focused on it, as opposed to mentioning it alongside other food or public health issues. Though all mentions would help with awareness, focus articles have a stronger effect on informing opinion.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>source(\"articles.R\")\n\nupf_articles &lt;- load_upf_articles()\ndomain_counts &lt;- load_domain_counts()\n\nggplot(\n  upf_articles |&gt;\n    mutate(\n      mention_type = if_else(is_focus, \"Focus article\", \"Mentions UPFs\")\n    )\n) +\n  aes(x = period, group = is_focus) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~mention_type) +\n  labs(\n    title = \"Articles mentioning ultra-processed food from top UK sources, by month.\",\n    x = NULL,\n    y = \"Article count\"\n  )</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-article-counts\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-article-counts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-article-counts-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-article-counts-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-article-counts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†2: Counts of articles mentioning ultra-processed food from the top UK online news sources, by month.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>N.B. The Sun and the Times have further licence restrictions, so aren‚Äôt included here.</p>\n<p>Although the number of articles focused on UPFs is not showing any trend, the number of articles mentioning UPFs is increasing over time. The fluctuation is most likely seasonal, and the trend is apparently upwards. There‚Äôs <em>just</em> about enough data to fix a SARIMA model. (We ought to have two years, we have eighteen months but the cycles seem to be sub-year.)</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>mention_counts &lt;- ts(\n  (group_by(upf_articles, period) |&gt; count())$n,\n  start = c(2024, 1),\n  deltat = 1 / 12\n)\nafit &lt;- forecast::auto.arima(mention_counts, d = 1, D = 1)\nplot(forecast::forecast(afit, h = 5, level = 90))</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-sarima\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-sarima-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-sarima-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-sarima-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-sarima-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†3: A (very uncertain) SARIMA fit of the mention counts.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>It‚Äôs tempting to look at the upward trend in mentions and the upward trend in net concern, and call it a day. Media mentions go up, net concern goes up, quod erat demonstratum! But this is a bit flimsy.</p>\n<section class=\"level2\" id=\"why-cant-we-just-count-mentions-and-be-done-with-it\">\n<h2 class=\"anchored\" data-anchor-id=\"why-cant-we-just-count-mentions-and-be-done-with-it\">Why can‚Äôt we just count mentions and be done with it?</h2>\n<ol type=\"1\">\n<li>Even considering that we‚Äôve already reasoned about UK audiences and scoped our search to selected top publications, this is still a poor model of readership. Every mention in every article counts equally, even though we can see that the Guardian has <strong>over ten times</strong> the number of readers the Financial Times does.</li>\n<li>This model assumes that every mention of UPFs contributes an equal amount of concern. Some of them might be positive, or cause different levels of concern.</li>\n<li>Net concern has increased by &lt;5%, whereas the mentions have doubled. We have reason to be skeptical that mentions count is the whole story.</li>\n</ol>\n<p>We need to take a more nuanced approach.</p>\n</section>\n</section>\n<section class=\"level1\" id=\"a-better-readership-model\">\n<h1>A better readership model</h1>\n<p>Let‚Äôs start with a look at how the articles are distributed amongst the sources.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>ggplot(\n  upf_articles |&gt; group_by(domain, period) |&gt; count()\n) +\n  aes(x = period, y = n, group = domain, colour = domain) +\n  facet_wrap(~domain, nrow = 2) +\n  geom_point(show.legend = F) +\n  geom_line(linewidth = 1, show.legend = F)</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-source-dist\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-source-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-source-dist-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-source-dist-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-source-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†4: Articles focusing on ultra-processed food by source, since Jan 2024.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>Perhaps unsurprisingly, the Daily Mail has by far the most. Who‚Äôd have thought? They are also showing the most obvious increase in attention to the topic. (Incidentally, the Daily Mail also has the most articles about cancer, bone disease, and early death. They must employ a lot of medical professionals.) To be completely fair to the Daily Mail though, we should note it is a smaller proportion of their total output, which is enormous.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>ggplot(\n  group_by(domain_counts, domain) |&gt; summarise(n = sum(n_articles))\n) +\n  aes(\n    y = fct_reorder(domain, n),\n    x = n,\n    group = domain,\n    colour = domain,\n    fill = domain\n  ) +\n  geom_col(linewidth = 1, show.legend = F)</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-source-count-context\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-source-count-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-source-count-context-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-source-count-context-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-source-count-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†5: Total number of articles output by top sources, since Jan 2024.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<section class=\"level2\" id=\"modelling\">\n<h2 class=\"anchored\" data-anchor-id=\"modelling\">Modelling</h2>\n<p>We now have enough information to build a Monte Carlo model of media exposure. We‚Äôll keep it as simple as we can without compromising too much on representativeness.</p>\n<blockquote class=\"blockquote\">\n<p>Everything should be as simple as possible, but no simpler. (Einstein?)</p>\n</blockquote>\n<p>Let‚Äôs start by assuming the UK population is exposed to articles from each source in proportion to that source‚Äôs estimated readership. We‚Äôre going to get a bit maths now.</p>\n<p>Let every reader <img data-lazy-src=\"https://latex.codecogs.com/png.latex?r\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?r\"/></noscript> sample a number of articles <img data-lazy-src=\"https://latex.codecogs.com/png.latex?n_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?n_s\"/></noscript> from each source <img data-lazy-src=\"https://latex.codecogs.com/png.latex?s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?s\"/></noscript> according to a Poisson distribution - i.e.¬†a natural discrete distribution of counts. The Poisson distribution is parameterised according to relative readership levels, so for example <img data-lazy-src=\"https://latex.codecogs.com/png.latex?%5Clambda_%7BBBC%7D%20%3E%20%5Clambda_%7BFT%7D\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%5Clambda_%7BBBC%7D%20%3E%20%5Clambda_%7BFT%7D\"/></noscript> and on average <img data-lazy-src=\"https://latex.codecogs.com/png.latex?n_%7BBBC%7D%20%3E%20n_%7BFT%7D\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?n_%7BBBC%7D%20%3E%20n_%7BFT%7D\"/></noscript> but with lots of randomisation, so some of our simulated readers will draw more from the Mail, some more from the Metro, etc.</p>\n<p>The reader then draws <img data-lazy-src=\"https://latex.codecogs.com/png.latex?n_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?n_s\"/></noscript> articles at random from each source. (This is a simplification of course.) To represent the fact that only a small proportion of articles from each source in each period are about UPFs, we draw the <img data-lazy-src=\"https://latex.codecogs.com/png.latex?n_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?n_s\"/></noscript> observations from a binomial distribution parameterised by <img data-lazy-src=\"https://latex.codecogs.com/png.latex?p_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?p_s\"/></noscript>, the relative frequency of UPF articles for the source. The count of UPF-related articles we get back is denoted <img data-lazy-src=\"https://latex.codecogs.com/png.latex?d_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?d_s\"/></noscript>.</p>\n<p><img data-lazy-src=\"https://latex.codecogs.com/png.latex?%0An_s%20%5Csim%20Pois(%5Clambda_s)%0A\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%0An_s%20%5Csim%20Pois(%5Clambda_s)%0A\"/></noscript> <img data-lazy-src=\"https://latex.codecogs.com/png.latex?%0Ad_s%20%5Csim%20Binom(n_s,%20p_s)%0A\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%0Ad_s%20%5Csim%20Binom(n_s,%20p_s)%0A\"/></noscript></p>\n<p>Since <img data-lazy-src=\"https://latex.codecogs.com/png.latex?p_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?p_s\"/></noscript> is very small, this will be very inefficient and we will mostly draw zeros. But maths comes to the rescue, because the unconditional distribution of <img data-lazy-src=\"https://latex.codecogs.com/png.latex?d_s\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?d_s\"/></noscript> can be given by:</p>\n<p><img data-lazy-src=\"https://latex.codecogs.com/png.latex?%0Ad_s%20%5Csim%20Pois(%5Clambda_s%20p_s)%0A\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?%0Ad_s%20%5Csim%20Pois(%5Clambda_s%20p_s)%0A\"/></noscript></p>\n<p>This is much faster to compute. We need to do so for each of our simulated readers <img data-lazy-src=\"https://latex.codecogs.com/png.latex?r\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img src=\"https://latex.codecogs.com/png.latex?r\"/></noscript> to get a count of relevant articles they will read from each source. We then materialise these articles by sampling from the actual UPF articles from the soure in that time period, generate the LLM assessment for the articles, and take the mean assessment.</p>\n<p>This is quite a complicated model, but it‚Äôs also leaving A LOT out. We‚Äôre ignoring the demographics of each source‚Äôs readership, for example, and we‚Äôre making the incorrect assumption that all articles are equally likely to be read. It would be better to have a multinomial distribution across the articles rather than our simple binomial. Ideally that would be informed by accurate article-level readership statistics, though those are difficult if not impossible to get for all sources. We‚Äôre also ignoring incomplete reads or misreads.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>readership &lt;- tibble(\n  domain = factor(DOMAINS, levels = DOMAINS, ordered = TRUE),\n  monthly_readers = 1000000 *\n    c(59, 19.7, 12.3, 8.3, 5.9, 5.7, 4.6, 2.6, 2.5, 1.3) # BBC estimated from OFCOM survey\n) |&gt;\n  merge(\n    domain_counts |&gt;\n      group_by(domain, period) |&gt;\n      summarise(n_total_articles = sum(n_articles)),\n    all.x = TRUE\n  ) |&gt;\n  merge(\n    upf_articles |&gt; group_by(domain, period) |&gt; summarise(n_upf_articles = n()),\n    all.x = TRUE\n  ) |&gt;\n  mutate(\n    n_upf_articles = coalesce(n_upf_articles, 0),\n    prob_upf = n_upf_articles / n_total_articles,\n    # Number of articles in the time period, calibrated such\n    # that the expected number of BBC articles is approx 1/day.\n    # This is an educated guess, not much research to back it I'm afraid. Pew found in 2015 that\n    # \"An overwhelming majority of both long-form readers (72%) and short-form readers (79%)\n    # view just one article on a given site over the course of a month on their cellphone.\"\n    # - https://www.pewresearch.org/journalism/2016/05/05/long-form-reading-shows-signs-of-life-in-our-mobile-news-world/\n    # That was specific to the heady earlier days of the mobile web though.\n    # I found various industry-backed studies suggesting it could be higher, such as this NewsWorks study that\n    # found young people reading 6/day, however this is really suspicious and they have an\n    # obvious incentive to inflate the numbers.\n    # https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/young-people-news/\n    lambda = 30.4 * monthly_readers / max(monthly_readers),\n  )</pre>\n</details>\n</div>\n<p>I‚Äôve had to make an educated guess on the expected number of articles read per month - details are in the code above.</p>\n<p>We can now build the model using R‚Äôs statistical functions and simulate a number of readers. Then we can examine how many articles on UPFs the readers are likely to read each month.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>set.seed(42)\n\nsample_article_counts &lt;- function(month_readership, n_sim_readers) {\n  result &lt;- matrix(\n    ncol = length(DOMAINS),\n    nrow = n_sim_readers,\n    dimnames = list(1:n_sim_readers, DOMAINS)\n  )\n  for (i in 1:nrow(month_readership)) {\n    source &lt;- as.list(month_readership[i, ])\n    n &lt;- rpois(n_sim_readers, source$lambda * source$prob_upf)\n    result[, i] &lt;- n\n  }\n  result\n}\n\nPERIODS &lt;- seq.Date(\n  from = as.Date(\"2024-01-01\"),\n  to = as.Date(\"2025-05-01\"),\n  by = \"1 month\"\n)\n\nN_SIM_READERS = 10000\n\nsim_counts &lt;- PERIODS |&gt;\n  map(\n    function(p) {\n      sample_article_counts(\n        month_readership = filter(readership, period == p),\n        n_sim_readers = N_SIM_READERS\n      )\n    }\n  )</pre>\n</details>\n</div>\n<div class=\"cell anchored\" data-tbl-caption=\"Distribution of average number of UPF articles read per month\" id=\"tbl-article-reads-monthly\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>monthly_sim_count_totals &lt;- sim_counts |&gt; map(~ table(rowSums(.)))\n\nmonthly_sim_count_totals |&gt;\n  map(as.data.frame) |&gt;\n  bind_rows() |&gt;\n  rename(n_upf_articles_read_in_month = Var1) |&gt;\n  group_by(n_upf_articles_read_in_month) |&gt;\n  summarise(percent_of_readers = 100 * mean(Freq) / N_SIM_READERS) |&gt;\n  kable(digits = 1)</pre>\n</details>\n<div class=\"cell-output-display\">\n<table class=\"caption-top table table-sm table-striped small\">\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">n_upf_articles_read_in_month</th>\n<th style=\"text-align: right;\">percent_of_readers</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">0</td>\n<td style=\"text-align: right;\">96.3</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: right;\">3.6</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: right;\">0.1</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">3</td>\n<td style=\"text-align: right;\">0.0</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<p>This suggests that around 4% of news readers will read an article on UPFs every month. That seems quite reasonable. The number has grown from around 2% to around 6% over the last year and a half. This is more realistic than our original model, which simply noted a doubling in the number of articles. Our more sophisticated model says the number of articles read has probably tripled, but that they only influence a very small proportion of readers.</p>\n<p>A little reminder: we have modelled just the top 10 UK news sources, which have orders of magnitude more eyeballs than all other online news sources. We would expect the contribution from local news, blogs, trade pubs, etc. to be negligible.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>upf_readership_over_time &lt;- map2(\n  PERIODS,\n  monthly_sim_count_totals,\n  function(p, t) {\n    tibble(\n      period = p,\n      pc_sim_readers_reading_at_least_one_article = 100 *\n        (N_SIM_READERS - t[[1]]) /\n        N_SIM_READERS\n    )\n  }\n) |&gt;\n  bind_rows()\n\nggplot(upf_readership_over_time) +\n  aes(x = period, y = pc_sim_readers_reading_at_least_one_article) +\n  geom_line() +\n  scale_y_continuous(\n    limits = c(0, 20),\n    breaks = seq(0, 20, 10),\n    minor_breaks = seq(0, 20, 5)\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent of simulated readers\",\n  )</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-growth-in-readers\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-growth-in-readers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-growth-in-readers-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-growth-in-readers-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-growth-in-readers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†6: Change in percent of simulated readers who read at least one UPF article in a month over time.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>By examining the model parameters, we can see that the sources that contribute most are the Guardian, the Telegraph, and the BBC.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>#|\nreadership |&gt;\n  group_by(domain) |&gt;\n  summarise(\n    lambda = mean(lambda),\n    prob_upf = mean(prob_upf),\n    monthly_readers = mean(monthly_readers),\n    monthly_total_articles = mean(n_total_articles),\n    monthly_upf_articles = mean(n_upf_articles),\n    exp_monthly_upf_articles = mean(lambda * prob_upf)\n  ) |&gt;\n  mutate(\n    exp_monthly_upf_articles = exp_monthly_upf_articles /\n      sum(exp_monthly_upf_articles)\n  ) |&gt;\n  arrange(desc(exp_monthly_upf_articles)) |&gt;\n  kable(\n    digits = c(0, 1, 4, 0, 0, 0, 2),\n    format.args = list(decimal.mark = \".\", big.mark = \",\")\n  )</pre>\n</details>\n<div class=\"cell quarto-float quarto-figure quarto-figure-center anchored\" id=\"tbl-domain-params\">\n<figure class=\"quarto-float quarto-float-tbl figure\">\n<figcaption class=\"quarto-float-caption-top quarto-float-caption quarto-float-tbl\" id=\"tbl-domain-params-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nTable¬†2: Mean monthly model parameters for each domain.\n</figcaption>\n<div aria-describedby=\"tbl-domain-params-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<div class=\"cell-output-display\">\n<table class=\"do-not-create-environment cell caption-top table table-sm table-striped small\">\n<colgroup>\n<col style=\"width: 15%\"/>\n<col style=\"width: 5%\"/>\n<col style=\"width: 7%\"/>\n<col style=\"width: 13%\"/>\n<col style=\"width: 19%\"/>\n<col style=\"width: 17%\"/>\n<col style=\"width: 21%\"/>\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">domain</th>\n<th style=\"text-align: right;\">lambda</th>\n<th style=\"text-align: right;\">prob_upf</th>\n<th style=\"text-align: right;\">monthly_readers</th>\n<th style=\"text-align: right;\">monthly_total_articles</th>\n<th style=\"text-align: right;\">monthly_upf_articles</th>\n<th style=\"text-align: right;\">exp_monthly_upf_articles</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">theguardian.com</td>\n<td style=\"text-align: right;\">10.2</td>\n<td style=\"text-align: right;\">0.0012</td>\n<td style=\"text-align: right;\">19,700,000</td>\n<td style=\"text-align: right;\">7,665</td>\n<td style=\"text-align: right;\">10</td>\n<td style=\"text-align: right;\">0.33</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">telegraph.co.uk</td>\n<td style=\"text-align: right;\">2.4</td>\n<td style=\"text-align: right;\">0.0033</td>\n<td style=\"text-align: right;\">4,600,000</td>\n<td style=\"text-align: right;\">5,167</td>\n<td style=\"text-align: right;\">17</td>\n<td style=\"text-align: right;\">0.20</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">bbc.co.uk</td>\n<td style=\"text-align: right;\">30.4</td>\n<td style=\"text-align: right;\">0.0001</td>\n<td style=\"text-align: right;\">59,000,000</td>\n<td style=\"text-align: right;\">17,938</td>\n<td style=\"text-align: right;\">2</td>\n<td style=\"text-align: right;\">0.11</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">dailymail.co.uk</td>\n<td style=\"text-align: right;\">6.3</td>\n<td style=\"text-align: right;\">0.0006</td>\n<td style=\"text-align: right;\">12,300,000</td>\n<td style=\"text-align: right;\">52,210</td>\n<td style=\"text-align: right;\">33</td>\n<td style=\"text-align: right;\">0.11</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">independent.co.uk</td>\n<td style=\"text-align: right;\">4.3</td>\n<td style=\"text-align: right;\">0.0006</td>\n<td style=\"text-align: right;\">8,300,000</td>\n<td style=\"text-align: right;\">14,587</td>\n<td style=\"text-align: right;\">9</td>\n<td style=\"text-align: right;\">0.07</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">express.co.uk</td>\n<td style=\"text-align: right;\">3.0</td>\n<td style=\"text-align: right;\">0.0008</td>\n<td style=\"text-align: right;\">5,900,000</td>\n<td style=\"text-align: right;\">12,922</td>\n<td style=\"text-align: right;\">11</td>\n<td style=\"text-align: right;\">0.07</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">mirror.co.uk</td>\n<td style=\"text-align: right;\">2.9</td>\n<td style=\"text-align: right;\">0.0008</td>\n<td style=\"text-align: right;\">5,700,000</td>\n<td style=\"text-align: right;\">13,484</td>\n<td style=\"text-align: right;\">10</td>\n<td style=\"text-align: right;\">0.06</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">standard.co.uk</td>\n<td style=\"text-align: right;\">1.3</td>\n<td style=\"text-align: right;\">0.0006</td>\n<td style=\"text-align: right;\">2,600,000</td>\n<td style=\"text-align: right;\">6,563</td>\n<td style=\"text-align: right;\">4</td>\n<td style=\"text-align: right;\">0.02</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">ft.com</td>\n<td style=\"text-align: right;\">0.7</td>\n<td style=\"text-align: right;\">0.0009</td>\n<td style=\"text-align: right;\">1,300,000</td>\n<td style=\"text-align: right;\">12,870</td>\n<td style=\"text-align: right;\">11</td>\n<td style=\"text-align: right;\">0.02</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">metro.co.uk</td>\n<td style=\"text-align: right;\">1.3</td>\n<td style=\"text-align: right;\">0.0004</td>\n<td style=\"text-align: right;\">2,500,000</td>\n<td style=\"text-align: right;\">6,682</td>\n<td style=\"text-align: right;\">3</td>\n<td style=\"text-align: right;\">0.01</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<p>Again this differs significantly from our simpler model, which would have pinned it all on the Daily Mail, as that source has the largest absolute number of UPF articles. The Telegraph is initially surprising given their comparatively low readership, but they have a very high probability of producing UPF articles.</p>\n<p>With a more realistic model of readership achieved, we can move on to the AI polling.</p>\n</section>\n</section>\n<section class=\"level1\" id=\"building-the-ais-world-view\">\n<h1>Building the AI‚Äôs world view</h1>\n<p>The next step is to sample specific articles for each reader given their counts. In our model, these are the articles that will influence opinions, i.e.¬†those which inform the LLM survey respondents.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>sample_index &lt;- expand.grid(\n  period = 1:length(PERIODS),\n  reader = 1:N_SIM_READERS,\n  domain = 1:length(DOMAINS)\n)\n\nsampled_articles &lt;- 1:nrow(sample_index) |&gt;\n  map(\n    function(i) {\n      p &lt;- sample_index[i, 1]\n      r &lt;- sample_index[i, 2]\n      s &lt;- sample_index[i, 3]\n\n      n &lt;- sim_counts[[p]][r, s]\n\n      if (n &gt; 0) {\n        upf_articles |&gt;\n          filter(period == PERIODS[p] &amp; domain == DOMAINS[s]) |&gt;\n          slice_sample(n = n, replace = FALSE) |&gt;\n          mutate(period = PERIODS[p], reader = r, n = n)\n      }\n    },\n    .progress = F\n  ) |&gt;\n  bind_rows()\n\nprint(\n  sprintf(\n    \"Percent of readers who have read at least one UPF article in the total period: %.1f%%\",\n    100 * n_distinct(sampled_articles$reader) / N_SIM_READERS\n  )\n)</pre>\n</details>\n<div class=\"cell-output cell-output-stdout\">\n<pre>[1] \"Percent of readers who have read at least one UPF article in the total period: 46.8%\"</pre>\n</div>\n</div>\n<p>Aggregating this sample this tells us that 50% of our (simulated) reading population have read at least one article about UPFs in the last year and a half.</p>\n<p>Finally we‚Äôre ready to see what effect these articles have on the readers. Break out the AI!</p>\n</section>\n<section class=\"level1\" id=\"asking-the-ai-questions\">\n<h1>Asking the AI questions</h1>\n<p>Now we get to move on from that awful CPU-based statistic model (boo, dull) to an awesome GPU-based statistic model (wow, sexy).</p>\n<p>The approach we take is to force the LLM into picking a category via structured outputs, and then reviewing the probabilities that it would have picked each category. R afficionados, we are using <a href=\"https://ellmer.tidyverse.org/\" rel=\"nofollow\" target=\"_blank\">ellmer</a>, which is more or less the de-facto AI package for R.</p>\n<p>To avoid sharing the licensed news data with a third-party, the model is a local Qwen3 model running with a patched version of mlx-omni-server. Qwen3‚Äôs reasoning has been disabled because that changes the conditioning of the model and we just want its immediate ‚Äúsystem 1‚Äù response to the survey questions. Working with mlx-omni-server is also awesome because when you‚Äôre confused about some of the finer points of logits and sampling you can just LOOK at the code and see what‚Äôs happening.</p>\n<p>Most of the AI code is folded, but it‚Äôs useful to see the prompts.</p>\n<div class=\"cell\">\n<pre># This is the system prompt, which initiates every conversation by giving the AI a persona and instructions.\nSYSTEM_PROMPT &lt;- \"You are a member of the British public, with your own life experiences and opinions.\n\nThis is your identity: \n&lt;identity&gt;\n{{simulated_identity}}\n&lt;/identity&gt;\n\nYou are being asked for your views as part of a survey. Respond to each question from the pollster.\"\n\n# This is the question precisely as asked in the YouGov survey.\nUPF_QUESTION &lt;- \"At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?\"\n\n# This is how we wrap the question with relevant articles (and a date, for context).\nQUESTION_ARTICLE_WRAPPER &lt;- \"Answer the question, recalling that you have seen the following articles that may or may not have influenced your opinion.\n\n&lt;articles&gt;\n{{headlines}}\n&lt;/articles&gt;\n\n(Today's date is {{date_today}}.)\n\nQUESTION: {{question}}\"</pre>\n</div>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>library(ellmer)\n\ncreate_respondent &lt;- function(\n  simulated_identity = \"Average person\"\n) {\n  chat_openai(\n    base_url = \"http://0.0.0.0:10240/v1/\",\n    # Gemma is good\n    #model = \"mlx-community/gemma-3-1b-it-4bit-DWQ\",\n    #model = \"mlx-community/gemma-3-27b-it-4bit-DWQ\",\n    #model = \"mlx-community/gemma-3-27b-it-qat-8bit\",\n    # Qwen is great\n    #model = \"mlx-community/Qwen3-0.6B-4bit-DWQ-053125\",\n    #model = \"mlx-community/Qwen3-1.7B-4bit-DWQ-053125\",\n    model = \"mlx-community/Qwen3-4B-4bit-DWQ-053125\",\n    #model = \"mlx-community/Qwen3-14B-4bit-DWQ-053125\",\n    # Uncensored?\n    #model = \"mlx-community/Josiefied-Qwen3-0.6B-abliterated-v1-4bit\",\n    # GPT for debugging, do not use with data\n    #model = \"gpt-4.1-nano\",\n    api_key = \"n/a\",\n    system_prompt = interpolate(SYSTEM_PROMPT),\n    params = params(\n      # Single output (all the local server supports)\n      n = 1,\n      # Get max available log probs\n      log_probs = TRUE,\n      top_logprobs = 5,\n      # Keep the full distribution for sampling\n      top_p = 1,\n      # Do not adjust the distribution, so no cascading randomness\n      temperature = 0\n    ),\n    api_args = list(\n      # Ensure we disable reasoning from Qwen\n      enable_thinking = FALSE\n    )\n  )\n}\n\nextract_answer_dist &lt;- function(respondent, choices) {\n  rc &lt;- tibble(\n    token = LETTERS[1:length(choices)],\n    Response = factor(choices, levels = choices, ordered = TRUE)\n  )\n\n  token &lt;- respondent$last_turn()@json$choices[[1]]$logprobs$content |&gt;\n    keep(~ str_detect(.x$token, paste(rc$token, collapse = \"|\"))) |&gt;\n    first()\n\n  if (is.null(token)) {\n    stop(\n      respondent$last_turn()@json$choices[[1]]\n    )\n  }\n\n  token$top_logprobs |&gt;\n    bind_rows() |&gt;\n    select(token, logprob) |&gt;\n    unique() |&gt;\n    merge(rc) |&gt;\n    mutate(Probability = exp(logprob)) |&gt;\n    select(Response, Probability, logprob) |&gt;\n    arrange(Response)\n}\n\nquestion_with_articles &lt;- function(question, articles, date_today = today()) {\n  headlines &lt;- articles |&gt;\n    map(function(a) {\n      with(\n        a,\n        {\n          excerpts &lt;- paste(\n            map(highlights, ~ sprintf(\"&gt; %s\", .)),\n            collapse = \"\\n\"\n          )\n          sprintf(\n            \"HEADLINE: %s (%s, %s)\\nEXCERPTS:\\n%s\\n\\n\",\n            title,\n            domain,\n            publish_date,\n            excerpts\n          )\n        }\n      )\n    })\n\n  interpolate(QUESTION_ARTICLE_WRAPPER)\n}\n\n\nmultiple_choice_question &lt;- function(question, choices) {\n  paste(\n    c(\n      question,\n      \"\",\n      imap(choices, ~ sprintf(\"%s) %s\", LETTERS[.y], .x)),\n      \"\",\n      \"Respond with the letter of your chosen answer.\"\n    ),\n    collapse = \"\\n\"\n  )\n}\n\nask &lt;- function(respondent, question, choices) {\n  mcq &lt;- multiple_choice_question(question, choices)\n  tryCatch(\n    {\n      respondent$chat_structured(\n        mcq,\n        type = type_enum(\n          description = \"Answer letter\",\n          values = LETTERS[1:length(choices)]\n        )\n      )\n      extract_answer_dist(respondent, choices)\n    },\n    error = function(e) {\n      print(respondent$last_turn()@json)\n      stop(e)\n    }\n  )\n}</pre>\n</details>\n</div>\n<p>We can now get the probability that the LLM would have selected each response in this conversation, given its identity.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>default_response &lt;- ask(\n  create_respondent(\"Average person\"),\n  UPF_QUESTION,\n  RESPONSE_CATS\n)\ndefault_response |&gt; kable(digits = 3)</pre>\n</details>\n<div class=\"cell quarto-float quarto-figure quarto-figure-center anchored\" id=\"tbl-default-response\">\n<figure class=\"quarto-float quarto-float-tbl figure\">\n<figcaption class=\"quarto-float-caption-top quarto-float-caption quarto-float-tbl\" id=\"tbl-default-response-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nTable¬†3: The response distribution for the UPF survey question with a default ‚ÄúAverage person‚Äù identity.\n</figcaption>\n<div aria-describedby=\"tbl-default-response-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<div class=\"cell-output-display\">\n<table class=\"do-not-create-environment cell caption-top table table-sm table-striped small\">\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Response</th>\n<th style=\"text-align: right;\">Probability</th>\n<th style=\"text-align: right;\">logprob</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Highly concerned</td>\n<td style=\"text-align: right;\">0.076</td>\n<td style=\"text-align: right;\">-2.574</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Somewhat concerned</td>\n<td style=\"text-align: right;\">0.439</td>\n<td style=\"text-align: right;\">-0.824</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Not very concerned</td>\n<td style=\"text-align: right;\">0.342</td>\n<td style=\"text-align: right;\">-1.074</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Not concerned at all</td>\n<td style=\"text-align: right;\">0.076</td>\n<td style=\"text-align: right;\">-2.574</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.067</td>\n<td style=\"text-align: right;\">-2.699</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<p>Note that I‚Äôve removed the response category ‚ÄúI don‚Äôt know enough to comment‚Äù for technical reasons and its similarity to ‚ÄúDon‚Äôt know‚Äù.</p>\n<p>Although we have fixed the temperature to zero to avoid cascading randomness from token sampling, there is still some non-determinism at the hardware level. It almost never affects the most likely token and logits.</p>\n<p>We can now present the AI with an example article, to understand the article‚Äôs influence on its position.</p>\n<div class=\"cell\">\n<pre>fake_articles &lt;- list(\n  list(\n    title = \"Ultra-processed food linked to potential bowel cancer risk, scientists say\",\n    domain = \"dailymail.co.uk\",\n    publish_date = as.Date(\"2025-05-01\"),\n    highlights = c(\n      \"Ultra-processed foods increase the cancer risk by 3%.\",\n      \"Maybe we should be concerned about ultra-processed foods.\"\n    )\n  )\n)\n\nask(\n  create_respondent(\"Average person\"),\n  question_with_articles(UPF_QUESTION, fake_articles),\n  RESPONSE_CATS\n) |&gt;\n  kable(digits = 3)</pre>\n<div class=\"cell quarto-float quarto-figure quarto-figure-center anchored\" id=\"tbl-llm-simulation\">\n<figure class=\"quarto-float quarto-float-tbl figure\">\n<figcaption class=\"quarto-float-caption-top quarto-float-caption quarto-float-tbl\" id=\"tbl-llm-simulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nTable¬†4: Example of simulated AI polling response to a fictional alarming headline.\n</figcaption>\n<div aria-describedby=\"tbl-llm-simulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<div class=\"cell-output-display\">\n<table class=\"do-not-create-environment cell caption-top table table-sm table-striped small\">\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Response</th>\n<th style=\"text-align: right;\">Probability</th>\n<th style=\"text-align: right;\">logprob</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Highly concerned</td>\n<td style=\"text-align: right;\">0.022</td>\n<td style=\"text-align: right;\">-3.819</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Somewhat concerned</td>\n<td style=\"text-align: right;\">0.727</td>\n<td style=\"text-align: right;\">-0.319</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Not very concerned</td>\n<td style=\"text-align: right;\">0.162</td>\n<td style=\"text-align: right;\">-1.819</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Not concerned at all</td>\n<td style=\"text-align: right;\">0.036</td>\n<td style=\"text-align: right;\">-3.319</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.053</td>\n<td style=\"text-align: right;\">-2.944</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<p>We can also condition the model on a different identity to get a different response distribution.</p>\n<div class=\"cell\">\n<pre>ask(\n  create_respondent(\"Someone very concerned about their health\"),\n  question_with_articles(UPF_QUESTION, fake_articles),\n  RESPONSE_CATS\n) |&gt;\n  kable(digits = 3)</pre>\n<div class=\"cell quarto-float quarto-figure quarto-figure-center anchored\" id=\"tbl-concerned\">\n<figure class=\"quarto-float quarto-float-tbl figure\">\n<figcaption class=\"quarto-float-caption-top quarto-float-caption quarto-float-tbl\" id=\"tbl-concerned-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nTable¬†5: Response distribution of LLM again, this time with the identity of someone very concerned about their health.\n</figcaption>\n<div aria-describedby=\"tbl-concerned-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<div class=\"cell-output-display\">\n<table class=\"do-not-create-environment cell caption-top table table-sm table-striped small\">\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Response</th>\n<th style=\"text-align: right;\">Probability</th>\n<th style=\"text-align: right;\">logprob</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Highly concerned</td>\n<td style=\"text-align: right;\">0.418</td>\n<td style=\"text-align: right;\">-0.871</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Somewhat concerned</td>\n<td style=\"text-align: right;\">0.537</td>\n<td style=\"text-align: right;\">-0.621</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Not very concerned</td>\n<td style=\"text-align: right;\">0.016</td>\n<td style=\"text-align: right;\">-4.121</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Not concerned at all</td>\n<td style=\"text-align: right;\">0.010</td>\n<td style=\"text-align: right;\">-4.621</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.018</td>\n<td style=\"text-align: right;\">-3.996</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<p>A basic model, but already quite interesting. Let‚Äôs test it for bias.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>gender_resp &lt;- c(\"Female\", \"Male\") |&gt;\n  map(\n    ~ ask(\n      create_respondent(.),\n      UPF_QUESTION,\n      RESPONSE_CATS\n    ) |&gt;\n      mutate(simulated_identity = .)\n  ) |&gt;\n  bind_rows()\n\n# gender_resp |&gt; mutate(Net = Response %in% c(\"Highly concerned\", \"Somewhat concerned\")) |&gt; group_by(simulated_identity, Net) |&gt; summarise(p = sum(Probability))\n\ngender_resp |&gt;\n  pivot_wider(\n    id_cols = Response,\n    names_from = simulated_identity,\n    values_from = Probability\n  ) |&gt;\n  kable(digits = 3)</pre>\n</details>\n<div class=\"cell quarto-float quarto-figure quarto-figure-center anchored\" id=\"tbl-gender-test\">\n<figure class=\"quarto-float quarto-float-tbl figure\">\n<figcaption class=\"quarto-float-caption-top quarto-float-caption quarto-float-tbl\" id=\"tbl-gender-test-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nTable¬†6: Response distribution of LLM for different genders.\n</figcaption>\n<div aria-describedby=\"tbl-gender-test-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<div class=\"cell-output-display\">\n<table class=\"do-not-create-environment cell caption-top table table-sm table-striped small\">\n<thead>\n<tr class=\"header\">\n<th style=\"text-align: left;\">Response</th>\n<th style=\"text-align: right;\">Female</th>\n<th style=\"text-align: right;\">Male</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Highly concerned</td>\n<td style=\"text-align: right;\">0.082</td>\n<td style=\"text-align: right;\">0.109</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Somewhat concerned</td>\n<td style=\"text-align: right;\">0.603</td>\n<td style=\"text-align: right;\">0.552</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Not very concerned</td>\n<td style=\"text-align: right;\">0.196</td>\n<td style=\"text-align: right;\">0.203</td>\n</tr>\n<tr class=\"even\">\n<td style=\"text-align: left;\">Not concerned at all</td>\n<td style=\"text-align: right;\">0.064</td>\n<td style=\"text-align: right;\">0.085</td>\n</tr>\n<tr class=\"odd\">\n<td style=\"text-align: left;\">Don‚Äôt know</td>\n<td style=\"text-align: right;\">0.056</td>\n<td style=\"text-align: right;\">0.051</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<p>There is a little bias there. Normally we would seek to avoid this, but that‚Äôs the point of AI simulated polling, to exploit the biases to model ‚Äútypical‚Äù responses. If we look at the actual responses by gender, we can see that there really is a gender difference i.e.¬†female respondents were more likely to be highly concerned compared to males.</p>\n<p>In my testing, larger models were more likely to demonstrate bias in the same direction as real respondents (i.e.¬†females having more net concern). Not all model families were suitable, for example Gemma 3 was more than 90% weighted towards ‚ÄúHighly concerned‚Äù.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>gender_breakdown &lt;- tidy_df |&gt;\n  filter(str_starts(Demographic, \"Gender\") &amp; !str_starts(Response, \"Net: \")) |&gt;\n  mutate(\n    Response = if_else(Response %in% RESPONSE_CATS, Response, \"Don't know\"),\n    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE),\n    Demographic = str_replace(Demographic, \"Gender:: \", \"\")\n  ) |&gt;\n  # Re-normalise after removing the IDK cat\n  group_by(Question, Demographic, Period) |&gt;\n  mutate(Value = Value / sum(Value)) |&gt;\n  ungroup()\n\nplot_gender_dist &lt;- ggplot(\n  gender_breakdown |&gt;\n    group_by(Demographic, Response) |&gt;\n    summarise(Value = mean(Value)) |&gt;\n    mutate(Value = if_else(Demographic == \"Male\", Value, -Value))\n) +\n  aes(x = Value, y = fct_rev(Response), fill = Demographic) +\n  scale_fill_manual(values = c(\"Male\" = \"orange\", \"Female\" = \"purple\")) +\n  geom_col() +\n  labs(\n    title = \"Gender response distribution\",\n    x = \"Mean proportion of respondents\",\n    y = NULL\n  )\n\nplot_gender_time &lt;- ggplot(gender_breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  facet_wrap(~Demographic) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(\n    title = \"Response breakdown over time\",\n    x = NULL,\n    y = \"Proportion of respondents\"\n  )\n\nplot_gender_dist + plot_gender_time + plot_layout(nrow = 2, ncol = 1)</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-actual-gender-split\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-actual-gender-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-actual-gender-split-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-actual-gender-split-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-actual-gender-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†7: Actual response distribution for different genders. Females are more likely to say they are ‚Äúhighly concerned‚Äù about ultra-processed foods.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>To make a useful gender comparison, we‚Äôd need to calibrate the LLM such that its predictions are consistent with known gender biases. The original paper from Argyle et al.¬†did this by conditioning the LLM on backstories - where we just have ‚Äúmale‚Äù or ‚Äúfemale‚Äù, they have a more comprehensive paragraph - and sampling from a set of backstories designed to be representative of the population.</p>\n</section>\n<section class=\"level1\" id=\"running-ai-over-the-dataset\">\n<h1>Running AI over the dataset</h1>\n<p>Let‚Äôs apply this to our articles now. We can use <code>furrr</code> to run every simulated survey respondent in parallel. We‚Äôll start with a single ‚Äúaverage person‚Äù identity.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>library(furrr)\n\nsamples_per_reader &lt;- sampled_articles |&gt;\n  group_by(period, reader) |&gt;\n  summarize(\n    sample_key = str_flatten_comma(sort(article_id)),\n    article_ids = list(sort(article_id)),\n    n_article = n_distinct(article_id),\n  )\n\nunique_samples &lt;- samples_per_reader |&gt;\n  distinct(period, sample_key, article_ids)\n\nsim_ident &lt;- \"Average person\"\n\nsimulate_reader &lt;- function(row) {\n  tryCatch(\n    {\n      arts &lt;- upf_articles |&gt; filter(article_id %in% row$article_ids)\n      dist &lt;- ask(\n        create_respondent(sim_ident),\n        question_with_articles(\n          UPF_QUESTION,\n          pmap(arts, list),\n          date_today = row$period\n        ),\n        RESPONSE_CATS\n      )\n      dist |&gt;\n        mutate(sample_key = row$sample_key, simulated_identity = sim_ident)\n    },\n    error = \\(e) print(e, row)\n  )\n}\n\nmap_simulated_readers &lt;- function(unique_samples, sim_ident, force = FALSE) {\n  datafile &lt;- sprintf(\"data/article-responses - %s.rds\", sim_ident)\n\n  if (!file.exists(datafile) || force) {\n    with(plan(multisession, workers = 4), {\n      tasks &lt;- future_map(\n        pmap(unique_samples, list),\n        simulate_reader,\n        .progress = TRUE\n      )\n    })\n\n    article_responses &lt;- bind_rows(tasks) |&gt; as_tibble()\n    article_responses |&gt; write_rds(datafile)\n  }\n  read_rds(datafile)\n}\n\narticle_responses &lt;- map_simulated_readers(\n  unique_samples,\n  sim_ident,\n  #force = TRUE\n)\n\nsim_results &lt;- samples_per_reader |&gt; merge(article_responses)</pre>\n</details>\n</div>\n<p>We have now asked every simulated reader the poll question every month. We have a probability distribution of possible answers for each, so we can up-sample. We also can assume that the readers we didn‚Äôt simulate saw nothing that would change their minds. This is ignoring word-of-mouth and other forms of media, so for a better model we will need to consider those.</p>\n<p>What we need to do now is combine the default probability distribution with the article-readers. We‚Äôll do that month by month, and we can sample from the default distribution for the non-readers and sample from the individual distributions for readers. That would be much the same as a weighted sum, which is the simpler option.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>period_sim_dists &lt;- matrix(nrow = length(PERIODS), ncol = length(RESPONSE_CATS))\n\nfor (i in seq_along(PERIODS)) {\n  period_sim_results &lt;- sim_results |&gt; filter(period == PERIODS[i])\n  n_non_readers &lt;- N_SIM_READERS - n_distinct(period_sim_results$reader)\n\n  non_reader_dist &lt;- n_non_readers * default_response$Probability\n  reader_dist &lt;- (period_sim_results |&gt;\n    group_by(Response) |&gt;\n    summarise(p = sum(Probability)))$p\n  total_dist &lt;- non_reader_dist + reader_dist\n  period_sim_dists[i, ] &lt;- total_dist / sum(total_dist)\n}\n\nrownames(period_sim_dists) &lt;- as.character(PERIODS)\ncolnames(period_sim_dists) &lt;- RESPONSE_CATS\n\nsim_breakdown &lt;- as_tibble(period_sim_dists) |&gt;\n  mutate(Period = PERIODS) |&gt;\n  pivot_longer(-c(Period), names_to = \"Response\", values_to = \"Value\") |&gt;\n  mutate(Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE))\n\nsim_net_concern &lt;- sim_breakdown |&gt;\n  mutate(\n    Concerned = Response %in% c(\"Highly concerned\", \"Somewhat concerned\")\n  ) |&gt;\n  group_by(Period, Concerned) |&gt;\n  summarise(Value = sum(Value)) |&gt;\n  filter(Concerned)\n\nplot_actual_breakdown &lt;- ggplot(breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point(show.legend = FALSE) +\n  stat_smooth(method = \"lm\", show.legend = FALSE) +\n  labs(\n    title = \"Actual response breakdown\",\n    x = NULL,\n    y = \"Proportion of respondents\"\n  )\n\nplot_sim_breakdown &lt;- ggplot(sim_breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(\n    title = \"Simulated response breakdown\",\n    x = NULL,\n    y = \"Proportion of simulated respondents\"\n  )\n\nplot_actual_breakdown + plot_sim_breakdown</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-sum-distibutions\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-sum-distibutions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-sum-distibutions-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-sum-distibutions-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-sum-distibutions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†8: Comparison of actual and simulated responses over time.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>This is encouraging! Remember the AI has <em>not</em> been calibrated on the actual survey data, only on our media model, and hasn‚Äôt had demographic conditioning at this point. Despite this, it predicts similar trends to reality: a small percentage point increase in high concern, with similar drops in ‚Äúsomewhat‚Äù and ‚Äúnot very‚Äù. The starting points for each trend are mostly wrong though, which points to the default response needing refining.</p>\n<section class=\"level2\" id=\"digging-deeper\">\n<h2 class=\"anchored\" data-anchor-id=\"digging-deeper\">Digging deeper</h2>\n<p>Let‚Äôs dig into our simulated responses a bit.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>ggplot(\n  article_responses |&gt;\n    group_by(Response) |&gt;\n    summarise(Probability = mean(Probability))\n) +\n  aes(y = forcats::fct_rev(Response), x = Probability, fill = Response) +\n  geom_col(show.legend = F) +\n  scale_x_continuous(limits = c(0, 1)) +\n  labs(x = \"Mean probability across all articles\", y = \"Response\")</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-response-distribution\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-response-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-response-distribution-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i2.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-response-distribution-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-response-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†9: Response distribution\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>Again, we haven‚Äôt calibrated the LLM on real human responses, but among the headlines there are plenty about cancer, bone disease, and early death. ‚ÄúHighly concerned‚Äù is not unrealistic.</p>\n<p>This suggests that we could be underweighting the influence of media in our model. Considering just the contribution of simulated respondents who were exposed to media shows how these respondents have influenced the overall trend.</p>\n<div class=\"cell\">\n<details class=\"code-fold\">\n<summary>Code</summary>\n<pre>ggplot(\n  sim_results |&gt;\n    rename(Period = period) |&gt;\n    group_by(Period, Response) |&gt;\n    summarise(Value = sum(Probability) / N_SIM_READERS)\n) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth() +\n  labs(x = NULL, y = \"Proportion of all respondents\")</pre>\n</details>\n<div class=\"cell-output-display\">\n<div class=\"quarto-float quarto-figure quarto-figure-center anchored\" id=\"fig-media-sim-over-time\">\n<figure class=\"quarto-float quarto-float-fig figure\">\n<div aria-describedby=\"fig-media-sim-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\n<img class=\"img-fluid figure-img\" data-lazy-src=\"https://i1.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-media-sim-over-time-1.png?w=450&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img class=\"img-fluid figure-img\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/cbowdon.github.io/posts/food/index_files/figure-html/fig-media-sim-over-time-1.png?w=450&amp;ssl=1\"/></noscript>\n</div>\n<figcaption class=\"quarto-float-caption-bottom quarto-float-caption quarto-float-fig\" id=\"fig-media-sim-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca\">\nFigure¬†10: Contribution of simulated readers to response trends.\n</figcaption>\n</figure>\n</div>\n</div>\n</div>\n<p>It‚Äôs interesting that the articles appear to have polarised the simulated readers, with both ‚ÄúHighly concerned‚Äù and ‚ÄúNot concerned at all‚Äù growing. The rise in ‚ÄúDon‚Äôt know‚Äù might be confusion from contradictory articles, or might be due to articles that only mentioned ultra-processed foods in passing.</p>\n</section>\n</section>\n<section class=\"level1\" id=\"where-next\">\n<h1>Where next?</h1>\n<p>We could make some big improvements:</p>\n<ul>\n<li>Better results would come from refining our media model. It seems likely that we‚Äôre underestimating the media influence.</li>\n<li>So far we‚Äôve only simulated thousands of ‚ÄúAverage person‚Äù identities. Some basic demographic splits - the ‚Äúsilicon sampling‚Äù approach - would be expected to improve realism. That said, UPFs are not a highly polarised issue so the improvement would most likely be modest here.</li>\n</ul>\n<p>However, this little project could easily turn into a months-long labour. Time to stop, go outside, touch grass.</p>\n</section>\n<section class=\"level1\" id=\"wrapping-up\">\n<h1>Wrapping up</h1>\n<p>We‚Äôve covered a lot of ground here:</p>\n<ol type=\"1\">\n<li>Collated and visualised the FSA‚Äôs tracker survey for concern about UPFs.</li>\n<li>Built a probabilistic media readership model that reflects real UK news consumption.</li>\n<li>Built an LLM-based survey respondent simulator.</li>\n<li>Combined the Monte Carlo media model and the LLM respondents to simulate how British people exposed to news media over the last 18 months would respond to the UPF question, i.e.¬†simulated the FSA‚Äôs tracker survey with AI.</li>\n</ol>\n<p>We were following the ideas and approach outlined by Argyle et al, but with a much more niche survey question (British public opinion on UPFs vs US voting preference) and with a media model to capture the information that would truly inform opinion.</p>\n<p>The simulated survey showed some promising correlation with the actual survey. It really is exciting considering the relative effort compared to polling a thousand of people every month. I can‚Äôt wait to see how this develops.</p>\n</section>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://cbowdon.github.io/posts/food/\"> Chris Bowdon</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "Ultra-processed food: an AI polling simulation\nPosted on\nJune 22, 2025\nby\nChris Bowdon\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nChris Bowdon\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nThere‚Äôs a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I‚Äôd never heard until this year. I‚Äôm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been running a tracker survey containing this question since mid-2023, so we can find out.\nAn idea that‚Äôs generating a lot of excitement recently is the suggestion that you can\nsimulate public opinion polls with AI\n. The concept is very simple: you calibrate multiple LLM assistants to represent your different demographics and then sample their responses to questions. This is quicker and cheaper than polling real people,\nif\nyou can make it reliable.\nThat sounds incredibly fun! In this post I‚Äôll explore the FSA tracker survey data, and then build an AI simulation model.\nThe survey data\nFirst we need to scrape the survey data. While it‚Äôs lovely that the FSA publishes all the monthly survey results (which were run by YouGov) it is annoying that they don‚Äôt publish compiled statistics, so we need to get our tibbles dirty.\nAs usual, if you like coding with R you can see the gory details under the folds.\nCode\nlibrary(tidyverse)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(knitr)\n\nif (!str_ends(getwd(), \"posts/food\")) {\n  setwd(\"posts/food\")\n}\n\ndownload_files <- function() {\n  url <- \"https://data.food.gov.uk/catalog/datasets/0bfd916a-4e01-4cb8-ba16-763f0b36b50c\"\n\n  html_content <- request(url) |>\n    req_perform() |>\n    resp_body_string()\n\n  # Assuming html_content is already defined\n  doc <- read_html(html_content)\n  xlsx_links <- doc %>% html_nodes(\"a[href$='.xlsx']\") %>% html_attr(\"href\")\n  for (link in xlsx_links) {\n    file_url <- link\n    file_name <- basename(file_url)\n    destfile <- sprintf(\"data/%s\", file_name)\n    if (!file.exists(destfile)) {\n      download.file(url = file_url, destfile = destfile)\n    }\n  }\n}\n\ndownload_files()\n(Is there an easy way to read tracker survey spreadsheets into R? They have a common format, but I never found anything that did the job and had to roll my own. Anyway, eventually we end up with a single dataset.)\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(knitr)\n\nextract_month_year <- function(file) {\n  decoded_filename <- URLdecode(file)\n\n  month_year_regex <- \"^.*(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s?(\\\\d{2}|\\\\d{4}).*.xlsx$\"\n\n  if (grepl(month_year_regex, decoded_filename)) {\n    extracted_month_year <- regmatches(\n      decoded_filename,\n      regexec(month_year_regex, decoded_filename)\n    )\n\n    # Extract month and year\n    month <- extracted_month_year[[1]][[2]]\n    year <- extracted_month_year[[1]][[3]]\n\n    if (str_length(year) == 2) {\n      year <- paste0(\"20\", year)\n    }\n\n    return(list(filename = file, month = month, year = year))\n  } else {\n    print(file)\n    return(NULL)\n  }\n}\n\nextract_question <- function(file, question_prefix) {\n  fmy <- extract_month_year(file)\n\n  data <- read_excel(\n    file,\n    sheet = \"Percents\",\n    col_names = FALSE\n  )\n  headers <- as.vector(data[5, ], mode = \"character\")\n  headers[1] <- \"QCategory\"\n  headers <- zoo::na.locf(headers)\n  subheaders <- as.vector(data[6, ], mode = \"character\")\n  subheaders <- zoo::na.fill(subheaders, \"\")\n  combined_headers <- map2(\n    headers,\n    subheaders,\n    \\(x, y) str_c(x, y, sep = \":: \")\n  )\n  combined_headers[1] <- \"QCategory\"\n  combined_headers[2] <- \"Total\"\n\n  colnames(data) <- combined_headers\n\n  has_question <- any(str_starts(pull(data, QCategory), question_prefix))\n\n  if (!is.na(has_question)) {\n    data |>\n      filter(!is.na(QCategory)) |>\n      slice(\n        which(str_starts(QCategory, question_prefix)):n()\n      ) |>\n      head(10) |> # take the cats\n      tail(-1) |> # drop the header\n      mutate(\n        filename = fmy$filename,\n        monthname = fmy$month,\n        year = as.integer(fmy$year),\n        month = match(\n          monthname,\n          c(\n            \"January\",\n            \"February\",\n            \"March\",\n            \"April\",\n            \"May\",\n            \"June\",\n            \"July\",\n            \"August\",\n            \"September\",\n            \"October\",\n            \"November\",\n            \"December\"\n          )\n        )\n      )\n  }\n}\n\nquestion_df <- function(question_prefix) {\n  df_filename <- sprintf(\"data/%s.rds\", question_prefix)\n  if (file.exists(df_filename)) {\n    df <- read_rds(df_filename)\n  } else {\n    xlsx_files <- list.files(\n      path = \"data\",\n      pattern = \"*.xlsx\",\n      full.names = TRUE\n    )\n    df <- xlsx_files |>\n      sort(decreasing = TRUE) |>\n      lapply(\\(f) extract_question(f, question_prefix)) |>\n      keep(\\(x) !is.null(x)) |>\n      bind_rows() |>\n      mutate(Question = question_prefix)\n\n    df |> write_rds(df_filename)\n  }\n  df\n}\n\ndf <- question_df(\"Q12_14\")\n\ntidy_df <- df |>\n  filter(QCategory != \"Unweighted base\" & QCategory != \"Base: All\") |>\n  pivot_longer(\n    c(Total, contains(\"::\")),\n    names_to = \"Demographic\",\n    values_to = \"Value\"\n  ) |>\n  mutate(\n    Period = as.Date(sprintf(\"%04d-%02d-01\", year, month)),\n    Response = QCategory,\n    Value = as.numeric(Value)\n  ) |>\n  select(Question, Demographic, Period, Response, Value) |>\n  arrange(Question, Period, Response, Demographic)\n\ntidy_df |> head() |> kable()\nTable¬†1: Example of the survey data.\nQuestion\nDemographic\nPeriod\nResponse\nValue\nQ12_14\nAge:: 16-24\n2023-08-01\nDon‚Äôt know\n0.0384\nQ12_14\nAge:: 25-34\n2023-08-01\nDon‚Äôt know\n0.0518\nQ12_14\nAge:: 35-44\n2023-08-01\nDon‚Äôt know\n0.0156\nQ12_14\nAge:: 45-54\n2023-08-01\nDon‚Äôt know\n0.0142\nQ12_14\nAge:: 55-74\n2023-08-01\nDon‚Äôt know\n0.0161\nQ12_14\nAge:: 75+\n2023-08-01\nDon‚Äôt know\n0.0204\nFinally we have ‚ú®tidy‚ú® data and we can visualise it.\nCode\nlibrary(patchwork)\n\nRESPONSE_CATS <- c(\n  \"Highly concerned\",\n  \"Somewhat concerned\",\n  \"Not very concerned\",\n  \"Not concerned at all\",\n  \"Don't know\"\n)\n\nnet_concern <- tidy_df |>\n  filter(Demographic == \"Total\" & str_starts(Response, \"Net: \"))\n\nbreakdown <- tidy_df |>\n  filter(Demographic == \"Total\" & !str_starts(Response, \"Net: \")) |>\n  mutate(\n    Response = if_else(Response %in% RESPONSE_CATS, Response, \"Don't know\"),\n    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE)\n  ) |>\n  # Re-normalise after removing the IDK cat\n  group_by(Question, Demographic, Period) |>\n  mutate(Value = Value / sum(Value)) |>\n  ungroup()\n\nplot_net_concern <- ggplot(net_concern) +\n  aes(x = Period, y = Value) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(title = \"Net concern\", x = NULL, y = \"Proportion of respondents\")\n\nplot_breakdown <- ggplot(breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(title = \"Response breakdown\", x = NULL, y = \"Proportion of respondents\")\n\nplot_net_concern + plot_breakdown\nFigure¬†1: Tracker for ‚ÄúAt the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?‚Äù Left is net concern, right is all responses.\nThe number of people with concerns is evidently slowly increasing. A linear regression fits net concern very well, so it‚Äôs reasonable to expect continued growth at the same rate (pending any intervention like policy changes). Presumably the line bends as we approach the inevitable core of stubborn sods who will never admit to being concerned about anything, but the data we have doesn‚Äôt support that kind of model.\nThe breakdown by response type is interesting. We have to watch our step, given the noisiness, but it does seem to show a migration towards being\nhighly\nconcerned.\nNote that I have merged the two categories ‚ÄúI don‚Äôt know enough to comment‚Äù and ‚ÄúDon‚Äôt know‚Äù in the original data for two reasons: first, there‚Äôs a technical limit of 5 categories coming later, and second there is very little difference between these two.\nThe media landscape\nBesides calibrating the simulated demographics, the major challenge is ensuring that the models are exposed to the same information. If interested in current events, this information is most likely not in the training data. This means you need to ensure the AI has been exposed to the same media.\nNow, here‚Äôs an interesting observation: a surprisingly small number of publications account for a large proportion of UK readership. See this data from\nJournoFinder\n.\nThe UK population is approximately 70 million people. Given that the Guardian and the Daily Mail target very different demographics (left and right wing) we wouldn‚Äôt expect much overlap between their readership, i.e.¬†their combined monthly online readership alone probably covers half the news-reading British population. Note also that BBC News isn‚Äôt included in that table, but has a similar level of traffic, albeit with higher overlap.\nThis suggests that if we‚Äôre interested in understanding how the media shapes public perception in the UK, we can do well by analysing the output of just a handful of sources. This is particularly true in the modern media landscape, in which:\nLive television is a dying medium. (I was at a brilliant R meetup hosted by\nDatacove\nrecently when the session discussion turned to media channels. No one present had watched live TV in the last 24 hours, and only around 1 in 10 in the last month.)\nRadio has had its time, has had its power (though yet to have its finest hour üé∏).\nSocial media is increasingly popular, but news content on social media is dominated by reposts of articles from traditional journalists.\nSo let‚Äôs create a corpus of news articles about ultra-processed food from the top 10 sources in the table above, plus the BBC. They have a combined readership of 64.8 million monthly readers in the UK (plus the BBC‚Äôs unknown-but-high readership); even with high overlap, this clearly represents the majority of the UK population. It‚Äôs enough to give us a solid understanding of what‚Äôs going on.\nI happen to work at\nPolecat\nwho have licensed access to this data, so I can analyse it very easily. (Can‚Äôt share the raw data here though. üôä)\nSince November 2023 there have been over 2.5 million articles from those sources, but only around 2000 mentioned ultra-processed food, and <500 seem to be focused on it, as opposed to mentioning it alongside other food or public health issues. Though all mentions would help with awareness, focus articles have a stronger effect on informing opinion.\nCode\nsource(\"articles.R\")\n\nupf_articles <- load_upf_articles()\ndomain_counts <- load_domain_counts()\n\nggplot(\n  upf_articles |>\n    mutate(\n      mention_type = if_else(is_focus, \"Focus article\", \"Mentions UPFs\")\n    )\n) +\n  aes(x = period, group = is_focus) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~mention_type) +\n  labs(\n    title = \"Articles mentioning ultra-processed food from top UK sources, by month.\",\n    x = NULL,\n    y = \"Article count\"\n  )\nFigure¬†2: Counts of articles mentioning ultra-processed food from the top UK online news sources, by month.\nN.B. The Sun and the Times have further licence restrictions, so aren‚Äôt included here.\nAlthough the number of articles focused on UPFs is not showing any trend, the number of articles mentioning UPFs is increasing over time. The fluctuation is most likely seasonal, and the trend is apparently upwards. There‚Äôs\njust\nabout enough data to fix a SARIMA model. (We ought to have two years, we have eighteen months but the cycles seem to be sub-year.)\nCode\nmention_counts <- ts(\n  (group_by(upf_articles, period) |> count())$n,\n  start = c(2024, 1),\n  deltat = 1 / 12\n)\nafit <- forecast::auto.arima(mention_counts, d = 1, D = 1)\nplot(forecast::forecast(afit, h = 5, level = 90))\nFigure¬†3: A (very uncertain) SARIMA fit of the mention counts.\nIt‚Äôs tempting to look at the upward trend in mentions and the upward trend in net concern, and call it a day. Media mentions go up, net concern goes up, quod erat demonstratum! But this is a bit flimsy.\nWhy can‚Äôt we just count mentions and be done with it?\nEven considering that we‚Äôve already reasoned about UK audiences and scoped our search to selected top publications, this is still a poor model of readership. Every mention in every article counts equally, even though we can see that the Guardian has\nover ten times\nthe number of readers the Financial Times does.\nThis model assumes that every mention of UPFs contributes an equal amount of concern. Some of them might be positive, or cause different levels of concern.\nNet concern has increased by <5%, whereas the mentions have doubled. We have reason to be skeptical that mentions count is the whole story.\nWe need to take a more nuanced approach.\nA better readership model\nLet‚Äôs start with a look at how the articles are distributed amongst the sources.\nCode\nggplot(\n  upf_articles |> group_by(domain, period) |> count()\n) +\n  aes(x = period, y = n, group = domain, colour = domain) +\n  facet_wrap(~domain, nrow = 2) +\n  geom_point(show.legend = F) +\n  geom_line(linewidth = 1, show.legend = F)\nFigure¬†4: Articles focusing on ultra-processed food by source, since Jan 2024.\nPerhaps unsurprisingly, the Daily Mail has by far the most. Who‚Äôd have thought? They are also showing the most obvious increase in attention to the topic. (Incidentally, the Daily Mail also has the most articles about cancer, bone disease, and early death. They must employ a lot of medical professionals.) To be completely fair to the Daily Mail though, we should note it is a smaller proportion of their total output, which is enormous.\nCode\nggplot(\n  group_by(domain_counts, domain) |> summarise(n = sum(n_articles))\n) +\n  aes(\n    y = fct_reorder(domain, n),\n    x = n,\n    group = domain,\n    colour = domain,\n    fill = domain\n  ) +\n  geom_col(linewidth = 1, show.legend = F)\nFigure¬†5: Total number of articles output by top sources, since Jan 2024.\nModelling\nWe now have enough information to build a Monte Carlo model of media exposure. We‚Äôll keep it as simple as we can without compromising too much on representativeness.\nEverything should be as simple as possible, but no simpler. (Einstein?)\nLet‚Äôs start by assuming the UK population is exposed to articles from each source in proportion to that source‚Äôs estimated readership. We‚Äôre going to get a bit maths now.\nLet every reader\nsample a number of articles\nfrom each source\naccording to a Poisson distribution - i.e.¬†a natural discrete distribution of counts. The Poisson distribution is parameterised according to relative readership levels, so for example\nand on average\nbut with lots of randomisation, so some of our simulated readers will draw more from the Mail, some more from the Metro, etc.\nThe reader then draws\narticles at random from each source. (This is a simplification of course.) To represent the fact that only a small proportion of articles from each source in each period are about UPFs, we draw the\nobservations from a binomial distribution parameterised by\n, the relative frequency of UPF articles for the source. The count of UPF-related articles we get back is denoted\n.\nSince\nis very small, this will be very inefficient and we will mostly draw zeros. But maths comes to the rescue, because the unconditional distribution of\ncan be given by:\nThis is much faster to compute. We need to do so for each of our simulated readers\nto get a count of relevant articles they will read from each source. We then materialise these articles by sampling from the actual UPF articles from the soure in that time period, generate the LLM assessment for the articles, and take the mean assessment.\nThis is quite a complicated model, but it‚Äôs also leaving A LOT out. We‚Äôre ignoring the demographics of each source‚Äôs readership, for example, and we‚Äôre making the incorrect assumption that all articles are equally likely to be read. It would be better to have a multinomial distribution across the articles rather than our simple binomial. Ideally that would be informed by accurate article-level readership statistics, though those are difficult if not impossible to get for all sources. We‚Äôre also ignoring incomplete reads or misreads.\nCode\nreadership <- tibble(\n  domain = factor(DOMAINS, levels = DOMAINS, ordered = TRUE),\n  monthly_readers = 1000000 *\n    c(59, 19.7, 12.3, 8.3, 5.9, 5.7, 4.6, 2.6, 2.5, 1.3) # BBC estimated from OFCOM survey\n) |>\n  merge(\n    domain_counts |>\n      group_by(domain, period) |>\n      summarise(n_total_articles = sum(n_articles)),\n    all.x = TRUE\n  ) |>\n  merge(\n    upf_articles |> group_by(domain, period) |> summarise(n_upf_articles = n()),\n    all.x = TRUE\n  ) |>\n  mutate(\n    n_upf_articles = coalesce(n_upf_articles, 0),\n    prob_upf = n_upf_articles / n_total_articles,\n    # Number of articles in the time period, calibrated such\n    # that the expected number of BBC articles is approx 1/day.\n    # This is an educated guess, not much research to back it I'm afraid. Pew found in 2015 that\n    # \"An overwhelming majority of both long-form readers (72%) and short-form readers (79%)\n    # view just one article on a given site over the course of a month on their cellphone.\"\n    # - https://www.pewresearch.org/journalism/2016/05/05/long-form-reading-shows-signs-of-life-in-our-mobile-news-world/\n    # That was specific to the heady earlier days of the mobile web though.\n    # I found various industry-backed studies suggesting it could be higher, such as this NewsWorks study that\n    # found young people reading 6/day, however this is really suspicious and they have an\n    # obvious incentive to inflate the numbers.\n    # https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/young-people-news/\n    lambda = 30.4 * monthly_readers / max(monthly_readers),\n  )\nI‚Äôve had to make an educated guess on the expected number of articles read per month - details are in the code above.\nWe can now build the model using R‚Äôs statistical functions and simulate a number of readers. Then we can examine how many articles on UPFs the readers are likely to read each month.\nCode\nset.seed(42)\n\nsample_article_counts <- function(month_readership, n_sim_readers) {\n  result <- matrix(\n    ncol = length(DOMAINS),\n    nrow = n_sim_readers,\n    dimnames = list(1:n_sim_readers, DOMAINS)\n  )\n  for (i in 1:nrow(month_readership)) {\n    source <- as.list(month_readership[i, ])\n    n <- rpois(n_sim_readers, source$lambda * source$prob_upf)\n    result[, i] <- n\n  }\n  result\n}\n\nPERIODS <- seq.Date(\n  from = as.Date(\"2024-01-01\"),\n  to = as.Date(\"2025-05-01\"),\n  by = \"1 month\"\n)\n\nN_SIM_READERS = 10000\n\nsim_counts <- PERIODS |>\n  map(\n    function(p) {\n      sample_article_counts(\n        month_readership = filter(readership, period == p),\n        n_sim_readers = N_SIM_READERS\n      )\n    }\n  )\nCode\nmonthly_sim_count_totals <- sim_counts |> map(~ table(rowSums(.)))\n\nmonthly_sim_count_totals |>\n  map(as.data.frame) |>\n  bind_rows() |>\n  rename(n_upf_articles_read_in_month = Var1) |>\n  group_by(n_upf_articles_read_in_month) |>\n  summarise(percent_of_readers = 100 * mean(Freq) / N_SIM_READERS) |>\n  kable(digits = 1)\nn_upf_articles_read_in_month\npercent_of_readers\n0\n96.3\n1\n3.6\n2\n0.1\n3\n0.0\nThis suggests that around 4% of news readers will read an article on UPFs every month. That seems quite reasonable. The number has grown from around 2% to around 6% over the last year and a half. This is more realistic than our original model, which simply noted a doubling in the number of articles. Our more sophisticated model says the number of articles read has probably tripled, but that they only influence a very small proportion of readers.\nA little reminder: we have modelled just the top 10 UK news sources, which have orders of magnitude more eyeballs than all other online news sources. We would expect the contribution from local news, blogs, trade pubs, etc. to be negligible.\nCode\nupf_readership_over_time <- map2(\n  PERIODS,\n  monthly_sim_count_totals,\n  function(p, t) {\n    tibble(\n      period = p,\n      pc_sim_readers_reading_at_least_one_article = 100 *\n        (N_SIM_READERS - t[[1]]) /\n        N_SIM_READERS\n    )\n  }\n) |>\n  bind_rows()\n\nggplot(upf_readership_over_time) +\n  aes(x = period, y = pc_sim_readers_reading_at_least_one_article) +\n  geom_line() +\n  scale_y_continuous(\n    limits = c(0, 20),\n    breaks = seq(0, 20, 10),\n    minor_breaks = seq(0, 20, 5)\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent of simulated readers\",\n  )\nFigure¬†6: Change in percent of simulated readers who read at least one UPF article in a month over time.\nBy examining the model parameters, we can see that the sources that contribute most are the Guardian, the Telegraph, and the BBC.\nCode\n#|\nreadership |>\n  group_by(domain) |>\n  summarise(\n    lambda = mean(lambda),\n    prob_upf = mean(prob_upf),\n    monthly_readers = mean(monthly_readers),\n    monthly_total_articles = mean(n_total_articles),\n    monthly_upf_articles = mean(n_upf_articles),\n    exp_monthly_upf_articles = mean(lambda * prob_upf)\n  ) |>\n  mutate(\n    exp_monthly_upf_articles = exp_monthly_upf_articles /\n      sum(exp_monthly_upf_articles)\n  ) |>\n  arrange(desc(exp_monthly_upf_articles)) |>\n  kable(\n    digits = c(0, 1, 4, 0, 0, 0, 2),\n    format.args = list(decimal.mark = \".\", big.mark = \",\")\n  )\nTable¬†2: Mean monthly model parameters for each domain.\ndomain\nlambda\nprob_upf\nmonthly_readers\nmonthly_total_articles\nmonthly_upf_articles\nexp_monthly_upf_articles\ntheguardian.com\n10.2\n0.0012\n19,700,000\n7,665\n10\n0.33\ntelegraph.co.uk\n2.4\n0.0033\n4,600,000\n5,167\n17\n0.20\nbbc.co.uk\n30.4\n0.0001\n59,000,000\n17,938\n2\n0.11\ndailymail.co.uk\n6.3\n0.0006\n12,300,000\n52,210\n33\n0.11\nindependent.co.uk\n4.3\n0.0006\n8,300,000\n14,587\n9\n0.07\nexpress.co.uk\n3.0\n0.0008\n5,900,000\n12,922\n11\n0.07\nmirror.co.uk\n2.9\n0.0008\n5,700,000\n13,484\n10\n0.06\nstandard.co.uk\n1.3\n0.0006\n2,600,000\n6,563\n4\n0.02\nft.com\n0.7\n0.0009\n1,300,000\n12,870\n11\n0.02\nmetro.co.uk\n1.3\n0.0004\n2,500,000\n6,682\n3\n0.01\nAgain this differs significantly from our simpler model, which would have pinned it all on the Daily Mail, as that source has the largest absolute number of UPF articles. The Telegraph is initially surprising given their comparatively low readership, but they have a very high probability of producing UPF articles.\nWith a more realistic model of readership achieved, we can move on to the AI polling.\nBuilding the AI‚Äôs world view\nThe next step is to sample specific articles for each reader given their counts. In our model, these are the articles that will influence opinions, i.e.¬†those which inform the LLM survey respondents.\nCode\nsample_index <- expand.grid(\n  period = 1:length(PERIODS),\n  reader = 1:N_SIM_READERS,\n  domain = 1:length(DOMAINS)\n)\n\nsampled_articles <- 1:nrow(sample_index) |>\n  map(\n    function(i) {\n      p <- sample_index[i, 1]\n      r <- sample_index[i, 2]\n      s <- sample_index[i, 3]\n\n      n <- sim_counts[[p]][r, s]\n\n      if (n > 0) {\n        upf_articles |>\n          filter(period == PERIODS[p] & domain == DOMAINS[s]) |>\n          slice_sample(n = n, replace = FALSE) |>\n          mutate(period = PERIODS[p], reader = r, n = n)\n      }\n    },\n    .progress = F\n  ) |>\n  bind_rows()\n\nprint(\n  sprintf(\n    \"Percent of readers who have read at least one UPF article in the total period: %.1f%%\",\n    100 * n_distinct(sampled_articles$reader) / N_SIM_READERS\n  )\n)\n[1] \"Percent of readers who have read at least one UPF article in the total period: 46.8%\"\nAggregating this sample this tells us that 50% of our (simulated) reading population have read at least one article about UPFs in the last year and a half.\nFinally we‚Äôre ready to see what effect these articles have on the readers. Break out the AI!\nAsking the AI questions\nNow we get to move on from that awful CPU-based statistic model (boo, dull) to an awesome GPU-based statistic model (wow, sexy).\nThe approach we take is to force the LLM into picking a category via structured outputs, and then reviewing the probabilities that it would have picked each category. R afficionados, we are using\nellmer\n, which is more or less the de-facto AI package for R.\nTo avoid sharing the licensed news data with a third-party, the model is a local Qwen3 model running with a patched version of mlx-omni-server. Qwen3‚Äôs reasoning has been disabled because that changes the conditioning of the model and we just want its immediate ‚Äúsystem 1‚Äù response to the survey questions. Working with mlx-omni-server is also awesome because when you‚Äôre confused about some of the finer points of logits and sampling you can just LOOK at the code and see what‚Äôs happening.\nMost of the AI code is folded, but it‚Äôs useful to see the prompts.\n# This is the system prompt, which initiates every conversation by giving the AI a persona and instructions.\nSYSTEM_PROMPT <- \"You are a member of the British public, with your own life experiences and opinions.\n\nThis is your identity: \n<identity>\n{{simulated_identity}}\n</identity>\n\nYou are being asked for your views as part of a survey. Respond to each question from the pollster.\"\n\n# This is the question precisely as asked in the YouGov survey.\nUPF_QUESTION <- \"At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?\"\n\n# This is how we wrap the question with relevant articles (and a date, for context).\nQUESTION_ARTICLE_WRAPPER <- \"Answer the question, recalling that you have seen the following articles that may or may not have influenced your opinion.\n\n<articles>\n{{headlines}}\n</articles>\n\n(Today's date is {{date_today}}.)\n\nQUESTION: {{question}}\"\nCode\nlibrary(ellmer)\n\ncreate_respondent <- function(\n  simulated_identity = \"Average person\"\n) {\n  chat_openai(\n    base_url = \"http://0.0.0.0:10240/v1/\",\n    # Gemma is good\n    #model = \"mlx-community/gemma-3-1b-it-4bit-DWQ\",\n    #model = \"mlx-community/gemma-3-27b-it-4bit-DWQ\",\n    #model = \"mlx-community/gemma-3-27b-it-qat-8bit\",\n    # Qwen is great\n    #model = \"mlx-community/Qwen3-0.6B-4bit-DWQ-053125\",\n    #model = \"mlx-community/Qwen3-1.7B-4bit-DWQ-053125\",\n    model = \"mlx-community/Qwen3-4B-4bit-DWQ-053125\",\n    #model = \"mlx-community/Qwen3-14B-4bit-DWQ-053125\",\n    # Uncensored?\n    #model = \"mlx-community/Josiefied-Qwen3-0.6B-abliterated-v1-4bit\",\n    # GPT for debugging, do not use with data\n    #model = \"gpt-4.1-nano\",\n    api_key = \"n/a\",\n    system_prompt = interpolate(SYSTEM_PROMPT),\n    params = params(\n      # Single output (all the local server supports)\n      n = 1,\n      # Get max available log probs\n      log_probs = TRUE,\n      top_logprobs = 5,\n      # Keep the full distribution for sampling\n      top_p = 1,\n      # Do not adjust the distribution, so no cascading randomness\n      temperature = 0\n    ),\n    api_args = list(\n      # Ensure we disable reasoning from Qwen\n      enable_thinking = FALSE\n    )\n  )\n}\n\nextract_answer_dist <- function(respondent, choices) {\n  rc <- tibble(\n    token = LETTERS[1:length(choices)],\n    Response = factor(choices, levels = choices, ordered = TRUE)\n  )\n\n  token <- respondent$last_turn()@json$choices[[1]]$logprobs$content |>\n    keep(~ str_detect(.x$token, paste(rc$token, collapse = \"|\"))) |>\n    first()\n\n  if (is.null(token)) {\n    stop(\n      respondent$last_turn()@json$choices[[1]]\n    )\n  }\n\n  token$top_logprobs |>\n    bind_rows() |>\n    select(token, logprob) |>\n    unique() |>\n    merge(rc) |>\n    mutate(Probability = exp(logprob)) |>\n    select(Response, Probability, logprob) |>\n    arrange(Response)\n}\n\nquestion_with_articles <- function(question, articles, date_today = today()) {\n  headlines <- articles |>\n    map(function(a) {\n      with(\n        a,\n        {\n          excerpts <- paste(\n            map(highlights, ~ sprintf(\"> %s\", .)),\n            collapse = \"\\n\"\n          )\n          sprintf(\n            \"HEADLINE: %s (%s, %s)\\nEXCERPTS:\\n%s\\n\\n\",\n            title,\n            domain,\n            publish_date,\n            excerpts\n          )\n        }\n      )\n    })\n\n  interpolate(QUESTION_ARTICLE_WRAPPER)\n}\n\nmultiple_choice_question <- function(question, choices) {\n  paste(\n    c(\n      question,\n      \"\",\n      imap(choices, ~ sprintf(\"%s) %s\", LETTERS[.y], .x)),\n      \"\",\n      \"Respond with the letter of your chosen answer.\"\n    ),\n    collapse = \"\\n\"\n  )\n}\n\nask <- function(respondent, question, choices) {\n  mcq <- multiple_choice_question(question, choices)\n  tryCatch(\n    {\n      respondent$chat_structured(\n        mcq,\n        type = type_enum(\n          description = \"Answer letter\",\n          values = LETTERS[1:length(choices)]\n        )\n      )\n      extract_answer_dist(respondent, choices)\n    },\n    error = function(e) {\n      print(respondent$last_turn()@json)\n      stop(e)\n    }\n  )\n}\nWe can now get the probability that the LLM would have selected each response in this conversation, given its identity.\nCode\ndefault_response <- ask(\n  create_respondent(\"Average person\"),\n  UPF_QUESTION,\n  RESPONSE_CATS\n)\ndefault_response |> kable(digits = 3)\nTable¬†3: The response distribution for the UPF survey question with a default ‚ÄúAverage person‚Äù identity.\nResponse\nProbability\nlogprob\nHighly concerned\n0.076\n-2.574\nSomewhat concerned\n0.439\n-0.824\nNot very concerned\n0.342\n-1.074\nNot concerned at all\n0.076\n-2.574\nDon‚Äôt know\n0.067\n-2.699\nNote that I‚Äôve removed the response category ‚ÄúI don‚Äôt know enough to comment‚Äù for technical reasons and its similarity to ‚ÄúDon‚Äôt know‚Äù.\nAlthough we have fixed the temperature to zero to avoid cascading randomness from token sampling, there is still some non-determinism at the hardware level. It almost never affects the most likely token and logits.\nWe can now present the AI with an example article, to understand the article‚Äôs influence on its position.\nfake_articles <- list(\n  list(\n    title = \"Ultra-processed food linked to potential bowel cancer risk, scientists say\",\n    domain = \"dailymail.co.uk\",\n    publish_date = as.Date(\"2025-05-01\"),\n    highlights = c(\n      \"Ultra-processed foods increase the cancer risk by 3%.\",\n      \"Maybe we should be concerned about ultra-processed foods.\"\n    )\n  )\n)\n\nask(\n  create_respondent(\"Average person\"),\n  question_with_articles(UPF_QUESTION, fake_articles),\n  RESPONSE_CATS\n) |>\n  kable(digits = 3)\nTable¬†4: Example of simulated AI polling response to a fictional alarming headline.\nResponse\nProbability\nlogprob\nHighly concerned\n0.022\n-3.819\nSomewhat concerned\n0.727\n-0.319\nNot very concerned\n0.162\n-1.819\nNot concerned at all\n0.036\n-3.319\nDon‚Äôt know\n0.053\n-2.944\nWe can also condition the model on a different identity to get a different response distribution.\nask(\n  create_respondent(\"Someone very concerned about their health\"),\n  question_with_articles(UPF_QUESTION, fake_articles),\n  RESPONSE_CATS\n) |>\n  kable(digits = 3)\nTable¬†5: Response distribution of LLM again, this time with the identity of someone very concerned about their health.\nResponse\nProbability\nlogprob\nHighly concerned\n0.418\n-0.871\nSomewhat concerned\n0.537\n-0.621\nNot very concerned\n0.016\n-4.121\nNot concerned at all\n0.010\n-4.621\nDon‚Äôt know\n0.018\n-3.996\nA basic model, but already quite interesting. Let‚Äôs test it for bias.\nCode\ngender_resp <- c(\"Female\", \"Male\") |>\n  map(\n    ~ ask(\n      create_respondent(.),\n      UPF_QUESTION,\n      RESPONSE_CATS\n    ) |>\n      mutate(simulated_identity = .)\n  ) |>\n  bind_rows()\n\n# gender_resp |> mutate(Net = Response %in% c(\"Highly concerned\", \"Somewhat concerned\")) |> group_by(simulated_identity, Net) |> summarise(p = sum(Probability))\n\ngender_resp |>\n  pivot_wider(\n    id_cols = Response,\n    names_from = simulated_identity,\n    values_from = Probability\n  ) |>\n  kable(digits = 3)\nTable¬†6: Response distribution of LLM for different genders.\nResponse\nFemale\nMale\nHighly concerned\n0.082\n0.109\nSomewhat concerned\n0.603\n0.552\nNot very concerned\n0.196\n0.203\nNot concerned at all\n0.064\n0.085\nDon‚Äôt know\n0.056\n0.051\nThere is a little bias there. Normally we would seek to avoid this, but that‚Äôs the point of AI simulated polling, to exploit the biases to model ‚Äútypical‚Äù responses. If we look at the actual responses by gender, we can see that there really is a gender difference i.e.¬†female respondents were more likely to be highly concerned compared to males.\nIn my testing, larger models were more likely to demonstrate bias in the same direction as real respondents (i.e.¬†females having more net concern). Not all model families were suitable, for example Gemma 3 was more than 90% weighted towards ‚ÄúHighly concerned‚Äù.\nCode\ngender_breakdown <- tidy_df |>\n  filter(str_starts(Demographic, \"Gender\") & !str_starts(Response, \"Net: \")) |>\n  mutate(\n    Response = if_else(Response %in% RESPONSE_CATS, Response, \"Don't know\"),\n    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE),\n    Demographic = str_replace(Demographic, \"Gender:: \", \"\")\n  ) |>\n  # Re-normalise after removing the IDK cat\n  group_by(Question, Demographic, Period) |>\n  mutate(Value = Value / sum(Value)) |>\n  ungroup()\n\nplot_gender_dist <- ggplot(\n  gender_breakdown |>\n    group_by(Demographic, Response) |>\n    summarise(Value = mean(Value)) |>\n    mutate(Value = if_else(Demographic == \"Male\", Value, -Value))\n) +\n  aes(x = Value, y = fct_rev(Response), fill = Demographic) +\n  scale_fill_manual(values = c(\"Male\" = \"orange\", \"Female\" = \"purple\")) +\n  geom_col() +\n  labs(\n    title = \"Gender response distribution\",\n    x = \"Mean proportion of respondents\",\n    y = NULL\n  )\n\nplot_gender_time <- ggplot(gender_breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  facet_wrap(~Demographic) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(\n    title = \"Response breakdown over time\",\n    x = NULL,\n    y = \"Proportion of respondents\"\n  )\n\nplot_gender_dist + plot_gender_time + plot_layout(nrow = 2, ncol = 1)\nFigure¬†7: Actual response distribution for different genders. Females are more likely to say they are ‚Äúhighly concerned‚Äù about ultra-processed foods.\nTo make a useful gender comparison, we‚Äôd need to calibrate the LLM such that its predictions are consistent with known gender biases. The original paper from Argyle et al.¬†did this by conditioning the LLM on backstories - where we just have ‚Äúmale‚Äù or ‚Äúfemale‚Äù, they have a more comprehensive paragraph - and sampling from a set of backstories designed to be representative of the population.\nRunning AI over the dataset\nLet‚Äôs apply this to our articles now. We can use\nfurrr\nto run every simulated survey respondent in parallel. We‚Äôll start with a single ‚Äúaverage person‚Äù identity.\nCode\nlibrary(furrr)\n\nsamples_per_reader <- sampled_articles |>\n  group_by(period, reader) |>\n  summarize(\n    sample_key = str_flatten_comma(sort(article_id)),\n    article_ids = list(sort(article_id)),\n    n_article = n_distinct(article_id),\n  )\n\nunique_samples <- samples_per_reader |>\n  distinct(period, sample_key, article_ids)\n\nsim_ident <- \"Average person\"\n\nsimulate_reader <- function(row) {\n  tryCatch(\n    {\n      arts <- upf_articles |> filter(article_id %in% row$article_ids)\n      dist <- ask(\n        create_respondent(sim_ident),\n        question_with_articles(\n          UPF_QUESTION,\n          pmap(arts, list),\n          date_today = row$period\n        ),\n        RESPONSE_CATS\n      )\n      dist |>\n        mutate(sample_key = row$sample_key, simulated_identity = sim_ident)\n    },\n    error = \\(e) print(e, row)\n  )\n}\n\nmap_simulated_readers <- function(unique_samples, sim_ident, force = FALSE) {\n  datafile <- sprintf(\"data/article-responses - %s.rds\", sim_ident)\n\n  if (!file.exists(datafile) || force) {\n    with(plan(multisession, workers = 4), {\n      tasks <- future_map(\n        pmap(unique_samples, list),\n        simulate_reader,\n        .progress = TRUE\n      )\n    })\n\n    article_responses <- bind_rows(tasks) |> as_tibble()\n    article_responses |> write_rds(datafile)\n  }\n  read_rds(datafile)\n}\n\narticle_responses <- map_simulated_readers(\n  unique_samples,\n  sim_ident,\n  #force = TRUE\n)\n\nsim_results <- samples_per_reader |> merge(article_responses)\nWe have now asked every simulated reader the poll question every month. We have a probability distribution of possible answers for each, so we can up-sample. We also can assume that the readers we didn‚Äôt simulate saw nothing that would change their minds. This is ignoring word-of-mouth and other forms of media, so for a better model we will need to consider those.\nWhat we need to do now is combine the default probability distribution with the article-readers. We‚Äôll do that month by month, and we can sample from the default distribution for the non-readers and sample from the individual distributions for readers. That would be much the same as a weighted sum, which is the simpler option.\nCode\nperiod_sim_dists <- matrix(nrow = length(PERIODS), ncol = length(RESPONSE_CATS))\n\nfor (i in seq_along(PERIODS)) {\n  period_sim_results <- sim_results |> filter(period == PERIODS[i])\n  n_non_readers <- N_SIM_READERS - n_distinct(period_sim_results$reader)\n\n  non_reader_dist <- n_non_readers * default_response$Probability\n  reader_dist <- (period_sim_results |>\n    group_by(Response) |>\n    summarise(p = sum(Probability)))$p\n  total_dist <- non_reader_dist + reader_dist\n  period_sim_dists[i, ] <- total_dist / sum(total_dist)\n}\n\nrownames(period_sim_dists) <- as.character(PERIODS)\ncolnames(period_sim_dists) <- RESPONSE_CATS\n\nsim_breakdown <- as_tibble(period_sim_dists) |>\n  mutate(Period = PERIODS) |>\n  pivot_longer(-c(Period), names_to = \"Response\", values_to = \"Value\") |>\n  mutate(Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE))\n\nsim_net_concern <- sim_breakdown |>\n  mutate(\n    Concerned = Response %in% c(\"Highly concerned\", \"Somewhat concerned\")\n  ) |>\n  group_by(Period, Concerned) |>\n  summarise(Value = sum(Value)) |>\n  filter(Concerned)\n\nplot_actual_breakdown <- ggplot(breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point(show.legend = FALSE) +\n  stat_smooth(method = \"lm\", show.legend = FALSE) +\n  labs(\n    title = \"Actual response breakdown\",\n    x = NULL,\n    y = \"Proportion of respondents\"\n  )\n\nplot_sim_breakdown <- ggplot(sim_breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(\n    title = \"Simulated response breakdown\",\n    x = NULL,\n    y = \"Proportion of simulated respondents\"\n  )\n\nplot_actual_breakdown + plot_sim_breakdown\nFigure¬†8: Comparison of actual and simulated responses over time.\nThis is encouraging! Remember the AI has\nnot\nbeen calibrated on the actual survey data, only on our media model, and hasn‚Äôt had demographic conditioning at this point. Despite this, it predicts similar trends to reality: a small percentage point increase in high concern, with similar drops in ‚Äúsomewhat‚Äù and ‚Äúnot very‚Äù. The starting points for each trend are mostly wrong though, which points to the default response needing refining.\nDigging deeper\nLet‚Äôs dig into our simulated responses a bit.\nCode\nggplot(\n  article_responses |>\n    group_by(Response) |>\n    summarise(Probability = mean(Probability))\n) +\n  aes(y = forcats::fct_rev(Response), x = Probability, fill = Response) +\n  geom_col(show.legend = F) +\n  scale_x_continuous(limits = c(0, 1)) +\n  labs(x = \"Mean probability across all articles\", y = \"Response\")\nFigure¬†9: Response distribution\nAgain, we haven‚Äôt calibrated the LLM on real human responses, but among the headlines there are plenty about cancer, bone disease, and early death. ‚ÄúHighly concerned‚Äù is not unrealistic.\nThis suggests that we could be underweighting the influence of media in our model. Considering just the contribution of simulated respondents who were exposed to media shows how these respondents have influenced the overall trend.\nCode\nggplot(\n  sim_results |>\n    rename(Period = period) |>\n    group_by(Period, Response) |>\n    summarise(Value = sum(Probability) / N_SIM_READERS)\n) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth() +\n  labs(x = NULL, y = \"Proportion of all respondents\")\nFigure¬†10: Contribution of simulated readers to response trends.\nIt‚Äôs interesting that the articles appear to have polarised the simulated readers, with both ‚ÄúHighly concerned‚Äù and ‚ÄúNot concerned at all‚Äù growing. The rise in ‚ÄúDon‚Äôt know‚Äù might be confusion from contradictory articles, or might be due to articles that only mentioned ultra-processed foods in passing.\nWhere next?\nWe could make some big improvements:\nBetter results would come from refining our media model. It seems likely that we‚Äôre underestimating the media influence.\nSo far we‚Äôve only simulated thousands of ‚ÄúAverage person‚Äù identities. Some basic demographic splits - the ‚Äúsilicon sampling‚Äù approach - would be expected to improve realism. That said, UPFs are not a highly polarised issue so the improvement would most likely be modest here.\nHowever, this little project could easily turn into a months-long labour. Time to stop, go outside, touch grass.\nWrapping up\nWe‚Äôve covered a lot of ground here:\nCollated and visualised the FSA‚Äôs tracker survey for concern about UPFs.\nBuilt a probabilistic media readership model that reflects real UK news consumption.\nBuilt an LLM-based survey respondent simulator.\nCombined the Monte Carlo media model and the LLM respondents to simulate how British people exposed to news media over the last 18 months would respond to the UPF question, i.e.¬†simulated the FSA‚Äôs tracker survey with AI.\nWe were following the ideas and approach outlined by Argyle et al, but with a much more niche survey question (British public opinion on UPFs vs US voting preference) and with a media model to capture the information that would truly inform opinion.\nThe simulated survey showed some promising correlation with the actual survey. It really is exciting considering the relative effort compared to polling a thousand of people every month. I can‚Äôt wait to see how this develops.\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nChris Bowdon\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "There‚Äôs a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I‚Äôd never heard until this year. I‚Äôm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been ru...",
      "meta_keywords": null,
      "og_description": "There‚Äôs a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I‚Äôd never heard until this year. I‚Äôm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been ru...",
      "og_image": "https://cbowdon.github.io/posts/food/index_files/figure-html/fig-responses-1.png",
      "og_title": "Ultra-processed food: an AI polling simulation | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 29.9,
      "sitemap_lastmod": null,
      "twitter_description": "There‚Äôs a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I‚Äôd never heard until this year. I‚Äôm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been ru...",
      "twitter_title": "Ultra-processed food: an AI polling simulation | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/06/ultra-processed-food-an-ai-polling-simulation/",
      "word_count": 5973
    }
  }
}