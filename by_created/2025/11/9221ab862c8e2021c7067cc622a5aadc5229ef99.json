{
  "id": "9221ab862c8e2021c7067cc622a5aadc5229ef99",
  "url": "https://www.r-bloggers.com/2025/10/whats-new-for-python-in-2025/",
  "created_at_utc": "2025-11-22T19:57:28Z",
  "data": null,
  "raw_original": {
    "uuid": "d1d7b4c4-3ae5-4447-8e49-501957e4cabc",
    "created_at": "2025-11-22 19:57:28",
    "raw_json": {
      "article_author": null,
      "article_headline": null,
      "article_modified": null,
      "article_published": null,
      "article_section": null,
      "article_tags": null,
      "canonical_url": "https://www.r-bloggers.com/2025/10/whats-new-for-python-in-2025/",
      "crawled_at": "2025-11-22T10:41:47.841647",
      "external_links": [
        {
          "href": "https://www.jumpingrivers.com/blog/whats-new-py314/",
          "text": "The Jumping Rivers Blog"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        },
        {
          "href": "https://www.jumpingrivers.com/blog/whats-new-py314/",
          "text": null
        },
        {
          "href": "https://docs.python.org/3.14/whatsnew/3.14.html",
          "text": "Python 3.14 release notes"
        },
        {
          "href": "https://jupyter.org/",
          "text": "Jupyter"
        },
        {
          "href": "https://docs.astral.sh/uv/",
          "text": "uv"
        },
        {
          "href": "https://python-poetry.org/",
          "text": "poetry"
        },
        {
          "href": "https://www.jumpingrivers.com/blog/python-package/",
          "text": "blog series on Poetry"
        },
        {
          "href": "https://github.com/pyenv/pyenv",
          "text": "pyenv"
        },
        {
          "href": "https://docs.astral.sh/uv/getting-started/installation/",
          "text": "instructions for installinguv"
        },
        {
          "href": "https://jupyterlab.readthedocs.io/en/stable/",
          "text": "JupyterLab"
        },
        {
          "href": "https://py-free-threading.github.io/",
          "text": "“Python Free-Threading Guide”"
        },
        {
          "href": "https://www.nltk.org/",
          "text": "nltk"
        },
        {
          "href": "https://datascience.blog.wzb.eu/2017/06/19/speeding-up-nltk-with-parallel-processing/",
          "text": "“WZB Data Science Blog”"
        },
        {
          "href": "https://www.gutenberg.org/",
          "text": "Project Gutenberg"
        },
        {
          "href": "https://docs.python.org/3/library/timeit.html",
          "text": "timeit"
        },
        {
          "href": "https://htop.dev/",
          "text": "htoptool"
        },
        {
          "href": "https://docs.python.org/3.14/whatsnew/3.14.html",
          "text": "Python 3.14 release notes"
        },
        {
          "href": "https://posit.co/blog/positron-product-announcement-aug-2025",
          "text": "Positron IDE"
        },
        {
          "href": "https://pandas.pydata.org/docs/dev/whatsnew/v3.0.0.html",
          "text": "Pandas 3.0"
        },
        {
          "href": "https://plotly.com/blog/chart-smarter-not-harder-universal-dataframe-support/",
          "text": "Plotly write-up"
        },
        {
          "href": "https://www.linkedin.com/company/jumping-rivers-ltd",
          "text": "LinkedIn"
        },
        {
          "href": "https://bsky.app/profile/jumpingrivers.com",
          "text": "Bluesky"
        },
        {
          "href": "https://www.jumpingrivers.com/blog/whats-new-py314/",
          "text": "original post"
        },
        {
          "href": "https://www.jumpingrivers.com/blog/whats-new-py314/",
          "text": "The Jumping Rivers Blog"
        },
        {
          "href": "https://feedburner.google.com/fb/a/mailverify?uri=RBloggers",
          "text": "daily e-mail updates"
        },
        {
          "href": "https://www.r-project.org/",
          "text": "R"
        },
        {
          "href": "https://www.r-users.com/",
          "text": "Click here if you're looking to post or find an R/data-science job"
        },
        {
          "href": "http://r-posts.com/",
          "text": "here"
        }
      ],
      "h1_title": "R-bloggers",
      "html_title": "What’s new for Python in 2025? | R-bloggers",
      "images": [
        {
          "alt": null,
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": null,
          "base64": null,
          "src": "https://i0.wp.com/www.jumpingrivers.com/blog/whats-new-py314/featured.png?w=400&ssl=1"
        },
        {
          "alt": "Visual demonstration that 16 processors were busy",
          "base64": "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
          "src": "https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif"
        },
        {
          "alt": "Visual demonstration that 16 processors were busy",
          "base64": null,
          "src": "https://i1.wp.com/www.jumpingrivers.com/blog/whats-new-py314/busy-processors.png?w=578&ssl=1"
        }
      ],
      "internal_links": [
        {
          "href": "https://www.r-bloggers.com/author/the-jumping-rivers-blog/",
          "text": "The Jumping Rivers Blog"
        },
        {
          "href": "https://www.r-bloggers.com/category/r-bloggers/",
          "text": "R bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers"
        },
        {
          "href": "https://www.r-bloggers.com/contact-us/",
          "text": "here"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        },
        {
          "href": "https://www.r-bloggers.com/",
          "text": "R-bloggers.com"
        },
        {
          "href": "https://www.r-bloggers.com/how-to-learn-r-2/",
          "text": "learning R"
        },
        {
          "href": "https://www.r-bloggers.com/add-your-blog/",
          "text": "click here"
        }
      ],
      "lang": "en-US",
      "main_html": "<article class=\"post-396142 post type-post status-publish format-standard hentry category-r-bloggers\">\n<header class=\"post-header\">\n<h1 class=\"entry-title\">What’s new for Python in 2025?</h1>\n<p class=\"meta post-meta\">Posted on <span class=\"updated\">October 16, 2025</span>  by <span class=\"vcard author\"><a class=\"fn\" href=\"https://www.r-bloggers.com/author/the-jumping-rivers-blog/\">The Jumping Rivers Blog</a></span>  in <a href=\"https://www.r-bloggers.com/category/r-bloggers/\" rel=\"category tag\">R bloggers</a> | 0 Comments</p>\n</header>\n<div class=\"entry clearfix\">\n<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.jumpingrivers.com/blog/whats-new-py314/\"> The Jumping Rivers Blog</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<p>\n<a href=\"https://www.jumpingrivers.com/blog/whats-new-py314/\">\n<img class=\"image-center\" data-lazy-src=\"https://i0.wp.com/www.jumpingrivers.com/blog/whats-new-py314/featured.png?w=400&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\" style=\"width:400px\"/><noscript><img class=\"image-center\" data-recalc-dims=\"1\" src=\"https://i0.wp.com/www.jumpingrivers.com/blog/whats-new-py314/featured.png?w=400&amp;ssl=1\" style=\"width:400px\"/></noscript>\n</a>\n</p>\n<p>Python 3.14 was released on 7th October 2025. Here we summarise some\nof the more interesting changes and some trends in Python development and data-science\nover the past year. We will highlight the following:</p>\n<ul>\n<li>the colourful Python command-line interface;</li>\n<li>project-management tool <code>uv</code>;</li>\n<li>free-threading;</li>\n<li>and a brief summary of other developments.</li>\n</ul>\n<p>The <a href=\"https://docs.python.org/3.14/whatsnew/3.14.html\" rel=\"nofollow\" target=\"_blank\">Python 3.14 release notes</a>\nalso describe the changes to base Python.</p>\n<h2 id=\"colourful-repl\">Colourful REPL</h2>\n<p>At Jumping Rivers we have taught a lot of people to program in Python.\nThroughout a programming career you get used to making, and learning\nfrom, mistakes. The most common mistakes made in introductory\nprogramming lessons may still trip you up in 10 years time: unmatched\nparentheses, typos, missing quote symbols, unimported dependencies.</p>\n<p>Our Python training courses are presented using\n<a href=\"https://jupyter.org/\" rel=\"nofollow\" target=\"_blank\">Jupyter</a>. Jupyter\nnotebooks have syntax highlighting that makes it easy to identify an\nunfinished string, or a mis-spelled keyword.</p>\n<p>But, most Python learners don’t use Jupyter (or other high-level\nprogramming tools) on day one – they experiment with Python at the\ncommand line. You can type “python” into your shell/terminal window and\nstart programming into the “REPL” (read-evaluate-print loop).</p>\n<p>Any effort to make the REPL easier to work with will be beneficial to\nbeginning programmers. So the introduction of syntax highlighting in the\nPython 3.14 REPL is really beneficial.</p>\n\n<h2 id=\"uv-and-package-development\"><code>uv</code> and package development</h2>\n<p>One of the big trends in Python development within 2025, is the rise of\nthe project management tool\n<a href=\"https://docs.astral.sh/uv/\" rel=\"nofollow\" target=\"_blank\"><code>uv</code></a>. This is a Rust-based command-line tool\nand can be used to initialise a package / project structure, to specify\nthe development and runtime environment of a project, and to publish a\npackage to PyPI.</p>\n<p>At Jumping Rivers, we have used <code>poetry</code> for many of the jobs that <code>uv</code>\nexcels at. Python is used for the data preparation tasks for\ndiffify.com, and we use\n<a href=\"https://python-poetry.org/\" rel=\"nofollow\" target=\"_blank\"><code>poetry</code></a> to ensure that our developers each use\nprecisely the same package versions when working on that project (See our current\n<a href=\"https://www.jumpingrivers.com/blog/python-package/\" rel=\"nofollow\" target=\"_blank\">blog series on Poetry</a>). But,\n<code>poetry</code> doesn’t prevent developers using different versions of Python.\nFor that, we need a second tool like\n<a href=\"https://github.com/pyenv/pyenv\" rel=\"nofollow\" target=\"_blank\"><code>pyenv</code></a> (which allows switching\nbetween different Python versions) or for each developer to have the\nsame Python version installed on their machine.</p>\n<p><code>uv</code> goes a step further than <code>poetry</code> and allows us to pin Python\nversions for a project. Let’s use <code>uv</code> to install Python 3.14, so that\nwe can test out features in the new release.</p>\n<p>First follow the\n<a href=\"https://docs.astral.sh/uv/getting-started/installation/\" rel=\"nofollow\" target=\"_blank\">instructions for installing <code>uv</code></a>.</p>\n<p>Then at the command line, we will use <code>uv</code> to create a new project where\nwe’ll use Python 3.14.</p>\n<pre># [bash]\ncd ~/temp\nmkdir blog-py3.14\ncd blog-py3.14\n\n# Which versions of Python 3.14 are available via uv?\nuv python list | grep 3.14\n# cpython-3.14.0rc2-linux-x86_64-gnu &lt;download available&gt;\n# cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu &lt;download available&gt;\n</pre><p>You’ll see something similar regardless of the operating system that you\nuse. That lists two versions of Python 3.14 – one with an optional\nsystem called “Free Threading” (see later). We’ll install both versions\nof Python:</p>\n<pre>uv python install cpython-3.14.0rc2-linux-x86_64-gnu\nuv python install cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu\n</pre><p>Users of <code>pyenv</code> will be able to install Python 3.14 in a similar\nmanner.</p>\n<p>We can select between the two different Python versions at the command\nline. First using the version that does not have free threading:</p>\n<pre>uv run --python=3.14 python\n# Python 3.14.0rc2 (main, Aug 18 2025, 19:19:22) [Clang 20.1.4 ] on linux\n# ...\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys._is_gil_enabled()\n# True\n</pre><p>Then using the version with free threading (note the <code>t</code> suffix)</p>\n<pre>uv run --python=3.14t python\n# ...\n# Python 3.14.0rc2 free-threading build (main, Aug 18 2025, 19:19:12) [Clang 20.1.4 ] on linux\n# ...\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys._is_gil_enabled()\n# False\n</pre><h3 id=\"project-creation-and-management-with-uv\">Project creation and management with <code>uv</code></h3>\n<p><code>uv</code> is capable of much more than allowing us to switch between\ndifferent versions of Python. The following commands initialise a Python\nproject with <code>uv</code>:</p>\n<pre># From ~/temp/blog-py3.14\n\n# Indicate the default python version for the project\nuv python pin 3.14\n\n# Initialise a project in the current directory\nuv init .\n\n# Check the Python version\nuv run python --version\n# Python 3.14.0rc2\n</pre><p>This adds some files for project metadata (pyproject.toml, README.md)\nand version control:</p>\n<pre>tree -a -L 1\n# .\n# ├── .git\n# ├── .gitignore\n# ├── main.py\n# ├── pyproject.toml\n# ├── .python-version\n# ├── README.md\n# ├── uv.lock\n# └── .venv\n#\n# 2 directories, 6 files\n</pre><p>Now we can add package dependencies using <code>uv add &lt;packageName&gt;</code> and\nother standard project-management tasks. But one thing I wanted to\nhighlight is that <code>uv</code> allows us to start a Jupyter notebook, using the\nproject’s Python interpreter, without either adding <code>jupyter</code> as a\ndependency or explicitly defining a kernel for <code>jupyter</code>:</p>\n<pre>uv run --with jupyter jupyter lab\n</pre><p>Creating a new notebook using the default Python 3 kernel in the\n<a href=\"https://jupyterlab.readthedocs.io/en/stable/\" rel=\"nofollow\" target=\"_blank\">JupyterLab</a> session that\nstarts, should ensure you are using the currently active Python 3.14\nenvironment.</p>\n<h2 id=\"threading\">Threading</h2>\n<p>Python 3.13 introduced an experimental feature, ‘Free-threading’, that\nis now officially supported as of 3.14.</p>\n<p>First though, what is a ’thread’? When a program runs on your computer,\nthere are lots of different tasks going on. Some of those tasks could\nrun independently of each other. You, as the programmer, may need to\nexplain to the computer which tasks can run independently. A thread is a\nway of cordoning-off one of those tasks; it’s a way of telling the\ncomputer that your software is running on, that <em>this task here</em> can run\nseparately from <em>those tasks there</em>, and the logic for running\n<em>this task</em> too. (Basically).</p>\n<p>Python has allowed developers to define threads for a while. If you have\na few tasks that are largely independent of each other, each of these\ntasks can run in a separate thread. Threads can access the same memory\nspace, meaning that they can access and modify shared variables in a Python\nsession. In general, this also means that a computation in one thread\ncould update a value that is used by another thread, or that two\ndifferent threads could make conflicting updates to the same variable.\nThis freedom can lead to bugs. The CPython interpreter was originally\nwritten with a locking mechanism (the Global Interpreter Lock, GIL) that\nprevented different threads from running at the same time (even when\nmultiple processors were available) and limited the reach of these bugs.</p>\n<p>Traditionally, you would have used threads for “non-CPU-bound tasks” in\nPython. These are the kinds of tasks that would be unaffected by having\nmore, or faster, processors available to the Python instance: network\ntraffic, file access, waiting for user input. For CPU-bound tasks, like\ncalculations and data-processing, you could use Python’s\n‘multiprocessing’ library (although some libraries like ‘numpy’ have\ntheir own low-level mechanisms for splitting work across cores). This\nstarts multiple Python instances, each doing a portion of the\nprocessing, and allows a workload to be partitioned across multiple\nprocessors.</p>\n<p>The main other differences between threading and multiprocessing in\nPython are in memory and data management. With threading, you have one\nPython instance, with each thread having access to the same memory\nspace. With multiprocessing, you have multiple Python instances that\nwork independently: the instances do not share memory, so to partition a\nworkload using multiprocessing, Python has to send copies of (subsets\nof) your data to the new instances. This could mean that you need to\nstore two or more copies of a large dataset in memory when using\nmultiprocessing upon it.</p>\n<p>Simultaneous processing across threads that share memory-space is now\npossible using the free-threaded build of Python. Many third-party\npackages have been rewritten to accommodate this new build and you can\nlearn more about free-threading and the progress of the changes in the\n<a href=\"https://py-free-threading.github.io/\" rel=\"nofollow\" target=\"_blank\">“Python Free-Threading Guide”</a>.</p>\n<p>As a simple-ish example, lets consider natural language processing.\nThere is a wonderful blog post about parallel processing with the\n<a href=\"https://www.nltk.org/\" rel=\"nofollow\" target=\"_blank\"><code>nltk</code></a> package on the\n<a href=\"https://datascience.blog.wzb.eu/2017/06/19/speeding-up-nltk-with-parallel-processing/\" rel=\"nofollow\" target=\"_blank\">“WZB Data Science Blog”</a>.\nWe will extend that example to use free-threading.</p>\n<p><code>ntlk</code> provides access to some of the\n<a href=\"https://www.gutenberg.org/\" rel=\"nofollow\" target=\"_blank\">Project Gutenberg</a> books, and we can\naccess this data as follows:</p>\n<pre># main.py\nimport nltk\n\ndef setup():\n nltk.download(\"gutenberg\")\n nltk.download(\"punkt_tab\")\n nltk.download('averaged_perceptron_tagger_eng')\n corpus = { f_id: nltk.corpus.gutenberg.raw(f_id)\n for f_id in nltk.corpus.gutenberg.fileids()\n }\n return corpus\n\ncorpus = setup()\n</pre><p>The key-value pairs in <code>corpus</code> are the abbreviated book-title and\ncontents for 18 books. For example:</p>\n<pre>corpus[\"austen-emma.txt\"]\n# [Emma by Jane Austen 1816]\n#\n# VOLUME I\n#\n# CHAPTER I\n#\n#\n# Emma Woodhouse, handsome, clever, and rich, with a comfortable home ...\n</pre><p>A standard part of a text-processing workflow is to tokenise and tag the\n“parts-of-speech” (POS) in a document. We can do this using two <code>nltk</code>\nfunctions:</p>\n<pre># main.py ... continued\ndef tokenise_and_pos_tag(doc):\n return nltk.pos_tag(nltk.word_tokenize(doc))\n</pre><p>A function to sequentially tokenise and POS-tag the contents of a corpus\nof books can be written:</p>\n<pre># main.py ... continued\ndef tokenise_seq(corpus):\n tokens = {\n f_id: tokenise_and_pos_tag(doc)\n for f_id, doc in corpus.items()\n }\n return tokens\n</pre><p>You need to install or build Python in a particular way to make use of\n“Free-threaded” Python. In the above, we installed Python “3.14t” using\n<code>uv</code>, so we can compare the speed of free-threaded and sequential,\nsingle-core, processing.</p>\n<p>We will use the\n<a href=\"https://docs.python.org/3/library/timeit.html\" rel=\"nofollow\" target=\"_blank\"><code>timeit</code></a> package to\nanalyse processing speed, from the command line.</p>\n<pre># Activate the threaded version of Python 3.14\nuv python pin 3.14t\n\n# Install the dependencies for our main.py script\nuv add timeit nltk\n\n# Time the `tokenise_seq()` function\n# -- but do not time any setup code...\nPYTHON_GIL=0 \\\n uv run python -m timeit \\\n --setup \"import main; corpus = main.setup()\" \\\n \"main.tokenise_seq(corpus)\"\n\n# [lots of output messages]\n# 1 loop, best of 5: 53.1 sec per loop\n</pre><p>After some initial steps where the <code>nltk</code> datasets were downloaded and the\n<code>corpus</code> object was created (neither of which were timed, because these\nsteps were part of the <code>timeit</code> <code>--setup</code> block), <code>tokenise_seq(corpus)</code> was\nrun multiple times and the fastest speed was around 53 seconds.</p>\n<p>A small note: we have used the environment variable <code>PYTHON_GIL=0</code> here.\nThis makes it explicit that we are using free-threading (turning off the\nGIL). This wouldn’t normally be necessary to take advantage of\nfree-threading (in Python “3.14t”), but was needed because one of the\ndependencies of <code>nltk</code> hasn’t\nbeen validated for the free-threaded build yet.</p>\n<p>To write a threaded-version of the same, we introduce two functions. The\nfirst is a helper that takes (filename, document-content) pairs and\nreturns (filename, processed-document) pairs:</p>\n<pre>def tupled_tokeniser(pair):\n file_id, doc = pair\n return file_id, tokenise_and_pos_tag(doc)\n</pre><p>The second function creates a Thread-pool, taking advantage of as many CPUs as there are available\non my machine (16, counted by <code>multiprocessing.cpu_count()</code>). Each document is processed as a\nseparate thread and we wait for all of the documents to be processed before returning results to the\ncaller:</p>\n<pre>import multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, wait\n# ...\ndef tokenise_threaded(corpus):\n with ThreadPoolExecutor(max_workers=mp.cpu_count()) as tpe:\n try:\n futures = [\n tpe.submit(tupled_tokeniser, pair)\n for pair in corpus.items()\n ]\n wait(futures)\n finally:\n # output is a list of (file-id, data) pairs\n tokens = [f.result() for f in futures]\n return tokens\n\n# Time the `tokenise_threaded()` function\n# -- but do not time any setup code...\nPYTHON_GIL=0 \\\n uv run python -m timeit \\\n --setup \"import main; corpus = main.setup()\" \\\n \"main.tokenise_threaded(corpus)\"\n# [lots of output messages]\n# 1 loop, best of 5: 32.5 sec per loop\n</pre><p>I could see that every core was used when processing the documents, using the\n<a href=\"https://htop.dev/\" rel=\"nofollow\" target=\"_blank\"><code>htop</code> tool</a> on Ubuntu. At points during the run, each of the 16 CPUs was at\nnear to 100% use (whereas only one or two CPUs were busy at any time during the sequential run):</p>\n<p><img alt=\"Visual demonstration that 16 processors were busy\" data-lazy-src=\"https://i1.wp.com/www.jumpingrivers.com/blog/whats-new-py314/busy-processors.png?w=578&amp;ssl=1\" data-recalc-dims=\"1\" src=\"https://www.r-bloggers.com/wp-content/plugins/jetpack/modules/lazy-images/images/1x1.trans.gif\"/><noscript><img alt=\"Visual demonstration that 16 processors were busy\" data-recalc-dims=\"1\" src=\"https://i1.wp.com/www.jumpingrivers.com/blog/whats-new-py314/busy-processors.png?w=578&amp;ssl=1\"/></noscript></p>\n<p>But, despite using 16x as many CPUs, the multithreaded version of the\nprocessing script was only about 40% faster. There was only 18 books in\nthe dataset and some disparity between the book lengths (the bible,\ncontaining millions of words was processed much slower than the others).\nMaybe the speed up would be greater with a larger or more balanced\ndataset.</p>\n<p>In the post on the WZB Data Science blog, there is a multiprocessing\nimplementation of the above. Running their multiprocessing code with 16\nCPUs gave a similar speed up to multithreading (minimum time 31.2 seconds).\nIndeed, if I was writing this code for a real project, multiprocessing would\nremain my choice, because the analysis for one book can proceed independently of\nthat for any other book and data volumes aren’t that big.</p>\n<h2 id=\"other-news\">Other News</h2>\n<p>Python 3.14 has also introduced some improvements to exception-handling, a new approach to\nstring templating and improvements to the use of concurrent interpreters.\nSee the\n<a href=\"https://docs.python.org/3.14/whatsnew/3.14.html\" rel=\"nofollow\" target=\"_blank\">Python 3.14 release notes</a> for further details.</p>\n<p>In the wider Python Data Science ecosystem, a few other developments have occurred or are due\nbefore the end of 2025:</p>\n<ul>\n<li>The first stable release of the\n<a href=\"https://posit.co/blog/positron-product-announcement-aug-2025\" rel=\"nofollow\" target=\"_blank\">Positron IDE</a> was made in August;</li>\n<li><a href=\"https://pandas.pydata.org/docs/dev/whatsnew/v3.0.0.html\" rel=\"nofollow\" target=\"_blank\">Pandas 3.0</a> is due before the end of the\nyear, and will introduce strings as a data-type, copy-on-write behaviour, and implicit access to\ncolumns in DataFrame-modification code;</li>\n<li>Tools that ingest DataFrames are becoming agnostic to DataFrame library through the Narwahls\nproject. See the\n<a href=\"https://plotly.com/blog/chart-smarter-not-harder-universal-dataframe-support/\" rel=\"nofollow\" target=\"_blank\">Plotly write-up</a>\non this subject.</li>\n</ul>\n<p>Python data science progresses at such a speed that we can only really scratch the surface here.\nHave we missed anything in the wider Python ecosystem (2025 edition) that will make a huge\ndifference to your data work? Let us know on\n<a href=\"https://www.linkedin.com/company/jumping-rivers-ltd\" rel=\"nofollow\" target=\"_blank\">LinkedIn</a> or\n<a href=\"https://bsky.app/profile/jumpingrivers.com\" rel=\"nofollow\" target=\"_blank\">Bluesky</a>.</p>\n<p>\nFor updates and revisions to this article, see the <a href=\"https://www.jumpingrivers.com/blog/whats-new-py314/\">original post</a>\n</p>\n<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n</div>\n<!-- Share buttons by mashshare.net - Version: 4.0.47-->\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.jumpingrivers.com/blog/whats-new-py314/\"> The Jumping Rivers Blog</a></strong>.</div>\n<hr/>\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a href=\"https://www.r-project.org/\" rel=\"nofollow\" title=\"The R Project for Statistical Computing\">R</a> news and tutorials about <a href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\" title=\"R tutorials\">learning R</a> and many other topics. <a href=\"https://www.r-users.com/\" rel=\"nofollow\" title=\"Data science jobs\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr/>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div> </div>\n</article>",
      "main_text": "What’s new for Python in 2025?\nPosted on\nOctober 16, 2025\nby\nThe Jumping Rivers Blog\nin\nR bloggers\n| 0 Comments\n[This article was first published on\nThe Jumping Rivers Blog\n, and kindly contributed to\nR-bloggers\n].  (You can report issue about the content on this page\nhere\n)\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.\nPython 3.14 was released on 7th October 2025. Here we summarise some\nof the more interesting changes and some trends in Python development and data-science\nover the past year. We will highlight the following:\nthe colourful Python command-line interface;\nproject-management tool\nuv\n;\nfree-threading;\nand a brief summary of other developments.\nThe\nPython 3.14 release notes\nalso describe the changes to base Python.\nColourful REPL\nAt Jumping Rivers we have taught a lot of people to program in Python.\nThroughout a programming career you get used to making, and learning\nfrom, mistakes. The most common mistakes made in introductory\nprogramming lessons may still trip you up in 10 years time: unmatched\nparentheses, typos, missing quote symbols, unimported dependencies.\nOur Python training courses are presented using\nJupyter\n. Jupyter\nnotebooks have syntax highlighting that makes it easy to identify an\nunfinished string, or a mis-spelled keyword.\nBut, most Python learners don’t use Jupyter (or other high-level\nprogramming tools) on day one – they experiment with Python at the\ncommand line. You can type “python” into your shell/terminal window and\nstart programming into the “REPL” (read-evaluate-print loop).\nAny effort to make the REPL easier to work with will be beneficial to\nbeginning programmers. So the introduction of syntax highlighting in the\nPython 3.14 REPL is really beneficial.\nuv\nand package development\nOne of the big trends in Python development within 2025, is the rise of\nthe project management tool\nuv\n. This is a Rust-based command-line tool\nand can be used to initialise a package / project structure, to specify\nthe development and runtime environment of a project, and to publish a\npackage to PyPI.\nAt Jumping Rivers, we have used\npoetry\nfor many of the jobs that\nuv\nexcels at. Python is used for the data preparation tasks for\ndiffify.com, and we use\npoetry\nto ensure that our developers each use\nprecisely the same package versions when working on that project (See our current\nblog series on Poetry\n). But,\npoetry\ndoesn’t prevent developers using different versions of Python.\nFor that, we need a second tool like\npyenv\n(which allows switching\nbetween different Python versions) or for each developer to have the\nsame Python version installed on their machine.\nuv\ngoes a step further than\npoetry\nand allows us to pin Python\nversions for a project. Let’s use\nuv\nto install Python 3.14, so that\nwe can test out features in the new release.\nFirst follow the\ninstructions for installing\nuv\n.\nThen at the command line, we will use\nuv\nto create a new project where\nwe’ll use Python 3.14.\n# [bash]\ncd ~/temp\nmkdir blog-py3.14\ncd blog-py3.14\n\n# Which versions of Python 3.14 are available via uv?\nuv python list | grep 3.14\n# cpython-3.14.0rc2-linux-x86_64-gnu <download available>\n# cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu <download available>\nYou’ll see something similar regardless of the operating system that you\nuse. That lists two versions of Python 3.14 – one with an optional\nsystem called “Free Threading” (see later). We’ll install both versions\nof Python:\nuv python install cpython-3.14.0rc2-linux-x86_64-gnu\nuv python install cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu\nUsers of\npyenv\nwill be able to install Python 3.14 in a similar\nmanner.\nWe can select between the two different Python versions at the command\nline. First using the version that does not have free threading:\nuv run --python=3.14 python\n# Python 3.14.0rc2 (main, Aug 18 2025, 19:19:22) [Clang 20.1.4 ] on linux\n# ...\n>>> import sys\n>>> sys._is_gil_enabled()\n# True\nThen using the version with free threading (note the\nt\nsuffix)\nuv run --python=3.14t python\n# ...\n# Python 3.14.0rc2 free-threading build (main, Aug 18 2025, 19:19:12) [Clang 20.1.4 ] on linux\n# ...\n>>> import sys\n>>> sys._is_gil_enabled()\n# False\nProject creation and management with\nuv\nuv\nis capable of much more than allowing us to switch between\ndifferent versions of Python. The following commands initialise a Python\nproject with\nuv\n:\n# From ~/temp/blog-py3.14\n\n# Indicate the default python version for the project\nuv python pin 3.14\n\n# Initialise a project in the current directory\nuv init .\n\n# Check the Python version\nuv run python --version\n# Python 3.14.0rc2\nThis adds some files for project metadata (pyproject.toml, README.md)\nand version control:\ntree -a -L 1\n# .\n# ├── .git\n# ├── .gitignore\n# ├── main.py\n# ├── pyproject.toml\n# ├── .python-version\n# ├── README.md\n# ├── uv.lock\n# └── .venv\n#\n# 2 directories, 6 files\nNow we can add package dependencies using\nuv add <packageName>\nand\nother standard project-management tasks. But one thing I wanted to\nhighlight is that\nuv\nallows us to start a Jupyter notebook, using the\nproject’s Python interpreter, without either adding\njupyter\nas a\ndependency or explicitly defining a kernel for\njupyter\n:\nuv run --with jupyter jupyter lab\nCreating a new notebook using the default Python 3 kernel in the\nJupyterLab\nsession that\nstarts, should ensure you are using the currently active Python 3.14\nenvironment.\nThreading\nPython 3.13 introduced an experimental feature, ‘Free-threading’, that\nis now officially supported as of 3.14.\nFirst though, what is a ’thread’? When a program runs on your computer,\nthere are lots of different tasks going on. Some of those tasks could\nrun independently of each other. You, as the programmer, may need to\nexplain to the computer which tasks can run independently. A thread is a\nway of cordoning-off one of those tasks; it’s a way of telling the\ncomputer that your software is running on, that\nthis task here\ncan run\nseparately from\nthose tasks there\n, and the logic for running\nthis task\ntoo. (Basically).\nPython has allowed developers to define threads for a while. If you have\na few tasks that are largely independent of each other, each of these\ntasks can run in a separate thread. Threads can access the same memory\nspace, meaning that they can access and modify shared variables in a Python\nsession. In general, this also means that a computation in one thread\ncould update a value that is used by another thread, or that two\ndifferent threads could make conflicting updates to the same variable.\nThis freedom can lead to bugs. The CPython interpreter was originally\nwritten with a locking mechanism (the Global Interpreter Lock, GIL) that\nprevented different threads from running at the same time (even when\nmultiple processors were available) and limited the reach of these bugs.\nTraditionally, you would have used threads for “non-CPU-bound tasks” in\nPython. These are the kinds of tasks that would be unaffected by having\nmore, or faster, processors available to the Python instance: network\ntraffic, file access, waiting for user input. For CPU-bound tasks, like\ncalculations and data-processing, you could use Python’s\n‘multiprocessing’ library (although some libraries like ‘numpy’ have\ntheir own low-level mechanisms for splitting work across cores). This\nstarts multiple Python instances, each doing a portion of the\nprocessing, and allows a workload to be partitioned across multiple\nprocessors.\nThe main other differences between threading and multiprocessing in\nPython are in memory and data management. With threading, you have one\nPython instance, with each thread having access to the same memory\nspace. With multiprocessing, you have multiple Python instances that\nwork independently: the instances do not share memory, so to partition a\nworkload using multiprocessing, Python has to send copies of (subsets\nof) your data to the new instances. This could mean that you need to\nstore two or more copies of a large dataset in memory when using\nmultiprocessing upon it.\nSimultaneous processing across threads that share memory-space is now\npossible using the free-threaded build of Python. Many third-party\npackages have been rewritten to accommodate this new build and you can\nlearn more about free-threading and the progress of the changes in the\n“Python Free-Threading Guide”\n.\nAs a simple-ish example, lets consider natural language processing.\nThere is a wonderful blog post about parallel processing with the\nnltk\npackage on the\n“WZB Data Science Blog”\n.\nWe will extend that example to use free-threading.\nntlk\nprovides access to some of the\nProject Gutenberg\nbooks, and we can\naccess this data as follows:\n# main.py\nimport nltk\n\ndef setup():\n nltk.download(\"gutenberg\")\n nltk.download(\"punkt_tab\")\n nltk.download('averaged_perceptron_tagger_eng')\n corpus = { f_id: nltk.corpus.gutenberg.raw(f_id)\n for f_id in nltk.corpus.gutenberg.fileids()\n }\n return corpus\n\ncorpus = setup()\nThe key-value pairs in\ncorpus\nare the abbreviated book-title and\ncontents for 18 books. For example:\ncorpus[\"austen-emma.txt\"]\n# [Emma by Jane Austen 1816]\n#\n# VOLUME I\n#\n# CHAPTER I\n#\n#\n# Emma Woodhouse, handsome, clever, and rich, with a comfortable home ...\nA standard part of a text-processing workflow is to tokenise and tag the\n“parts-of-speech” (POS) in a document. We can do this using two\nnltk\nfunctions:\n# main.py ... continued\ndef tokenise_and_pos_tag(doc):\n return nltk.pos_tag(nltk.word_tokenize(doc))\nA function to sequentially tokenise and POS-tag the contents of a corpus\nof books can be written:\n# main.py ... continued\ndef tokenise_seq(corpus):\n tokens = {\n f_id: tokenise_and_pos_tag(doc)\n for f_id, doc in corpus.items()\n }\n return tokens\nYou need to install or build Python in a particular way to make use of\n“Free-threaded” Python. In the above, we installed Python “3.14t” using\nuv\n, so we can compare the speed of free-threaded and sequential,\nsingle-core, processing.\nWe will use the\ntimeit\npackage to\nanalyse processing speed, from the command line.\n# Activate the threaded version of Python 3.14\nuv python pin 3.14t\n\n# Install the dependencies for our main.py script\nuv add timeit nltk\n\n# Time the `tokenise_seq()` function\n# -- but do not time any setup code...\nPYTHON_GIL=0 \\\n uv run python -m timeit \\\n --setup \"import main; corpus = main.setup()\" \\\n \"main.tokenise_seq(corpus)\"\n\n# [lots of output messages]\n# 1 loop, best of 5: 53.1 sec per loop\nAfter some initial steps where the\nnltk\ndatasets were downloaded and the\ncorpus\nobject was created (neither of which were timed, because these\nsteps were part of the\ntimeit\n--setup\nblock),\ntokenise_seq(corpus)\nwas\nrun multiple times and the fastest speed was around 53 seconds.\nA small note: we have used the environment variable\nPYTHON_GIL=0\nhere.\nThis makes it explicit that we are using free-threading (turning off the\nGIL). This wouldn’t normally be necessary to take advantage of\nfree-threading (in Python “3.14t”), but was needed because one of the\ndependencies of\nnltk\nhasn’t\nbeen validated for the free-threaded build yet.\nTo write a threaded-version of the same, we introduce two functions. The\nfirst is a helper that takes (filename, document-content) pairs and\nreturns (filename, processed-document) pairs:\ndef tupled_tokeniser(pair):\n file_id, doc = pair\n return file_id, tokenise_and_pos_tag(doc)\nThe second function creates a Thread-pool, taking advantage of as many CPUs as there are available\non my machine (16, counted by\nmultiprocessing.cpu_count()\n). Each document is processed as a\nseparate thread and we wait for all of the documents to be processed before returning results to the\ncaller:\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, wait\n# ...\ndef tokenise_threaded(corpus):\n with ThreadPoolExecutor(max_workers=mp.cpu_count()) as tpe:\n try:\n futures = [\n tpe.submit(tupled_tokeniser, pair)\n for pair in corpus.items()\n ]\n wait(futures)\n finally:\n # output is a list of (file-id, data) pairs\n tokens = [f.result() for f in futures]\n return tokens\n\n# Time the `tokenise_threaded()` function\n# -- but do not time any setup code...\nPYTHON_GIL=0 \\\n uv run python -m timeit \\\n --setup \"import main; corpus = main.setup()\" \\\n \"main.tokenise_threaded(corpus)\"\n# [lots of output messages]\n# 1 loop, best of 5: 32.5 sec per loop\nI could see that every core was used when processing the documents, using the\nhtop\ntool\non Ubuntu. At points during the run, each of the 16 CPUs was at\nnear to 100% use (whereas only one or two CPUs were busy at any time during the sequential run):\nBut, despite using 16x as many CPUs, the multithreaded version of the\nprocessing script was only about 40% faster. There was only 18 books in\nthe dataset and some disparity between the book lengths (the bible,\ncontaining millions of words was processed much slower than the others).\nMaybe the speed up would be greater with a larger or more balanced\ndataset.\nIn the post on the WZB Data Science blog, there is a multiprocessing\nimplementation of the above. Running their multiprocessing code with 16\nCPUs gave a similar speed up to multithreading (minimum time 31.2 seconds).\nIndeed, if I was writing this code for a real project, multiprocessing would\nremain my choice, because the analysis for one book can proceed independently of\nthat for any other book and data volumes aren’t that big.\nOther News\nPython 3.14 has also introduced some improvements to exception-handling, a new approach to\nstring templating and improvements to the use of concurrent interpreters.\nSee the\nPython 3.14 release notes\nfor further details.\nIn the wider Python Data Science ecosystem, a few other developments have occurred or are due\nbefore the end of 2025:\nThe first stable release of the\nPositron IDE\nwas made in August;\nPandas 3.0\nis due before the end of the\nyear, and will introduce strings as a data-type, copy-on-write behaviour, and implicit access to\ncolumns in DataFrame-modification code;\nTools that ingest DataFrames are becoming agnostic to DataFrame library through the Narwahls\nproject. See the\nPlotly write-up\non this subject.\nPython data science progresses at such a speed that we can only really scratch the surface here.\nHave we missed anything in the wider Python ecosystem (2025 edition) that will make a huge\ndifference to your data work? Let us know on\nLinkedIn\nor\nBluesky\n.\nFor updates and revisions to this article, see the\noriginal post\nRelated\nTo\nleave a comment\nfor the author, please follow the link and comment on their blog:\nThe Jumping Rivers Blog\n.\nR-bloggers.com\noffers\ndaily e-mail updates\nabout\nR\nnews and tutorials about\nlearning R\nand many other topics.\nClick here if you're looking to post or find an R/data-science job\n.\nWant to share your content on R-bloggers?\nclick here\nif you have a blog, or\nhere\nif you don't.",
      "meta_description": "Python 3.14 was released on 7th October 2025. Here we summarise some of the more interesting changes and some trends in Python development and data-science over the past year. We will highlight the following: the colourful Python command-line interface; project-management tool uv; free-threading; and a brief summary of other developments. The Python 3.14 release notes also describe the changes to base Python. Colourful REPL At Jumping Rivers we have taught a lot of people to program in Python. Throughout a programming career you get used to making, and learning from, mistakes. The most common mistakes made in introductory programming lessons may still trip you up in 10 years time: unmatched parentheses, typos, missing quote symbols, unimported dependencies. Our Python training courses are presented using Jupyter. Jupyter notebooks have syntax highlighting that makes it easy to identify an unfinished string, or a mis-spelled keyword. But, most Python learners don’t use Jupyter (or other high-level programming tools) on day one - they experiment with Python at the command line. You can type “python” into your shell/terminal window and start programming into the “REPL” (read-evaluate-print loop). Any effort to make the REPL easier to work with will be beneficial to beginning programmers. So the introduction of syntax highlighting in the Python 3.14 REPL is really beneficial. Whether you want to start from scratch, or improve your skills, Jumping Rivers has a training course for you. uv and package development One of the big trends in Python development within 2025, is the rise of the project management tool uv. This is a Rust-based command-line tool and can be used to initialise a package / project structure, to specify the development and runtime environment of a project, and to publish a package to PyPI. At Jumping Rivers, we have used poetry for many of the jobs that uv excels at. Python is used for the data preparation tasks for diffify.com, and we use poetry to ensure that our developers each use precisely the same package versions when working on that project (See our current blog series on Poetry). But, poetry doesn’t prevent developers using different versions of Python. For that, we need a second tool like pyenv (which allows switching between different Python versions) or for each developer to have the same Python version installed on their machine. uv goes a step further than poetry and allows us to pin Python versions for a project. Let’s use uv to install Python 3.14, so that we can test out features in the new release. First follow the instructions for installing uv. Then at the command line, we will use uv to create a new project where we’ll use Python 3.14. # [bash] cd ~/temp mkdir blog-py3.14 cd blog-py3.14 # Which versions of Python 3.14 are available via uv? uv python list | grep 3.14 # cpython-3.14.0rc2-linux-x86_64-gnu # cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu You’ll see something similar regardless of the operating system that you use. That lists two versions of Python 3.14 - one with an optional system called “Free Threading” (see later). We’ll install both versions of Python: uv python install cpython-3.14.0rc2-linux-x86_64-gnu uv python install cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu Users of pyenv will be able to install Python 3.14 in a similar manner. We can select between the two different Python versions at the command line. First using the version that does not have free threading: uv run --python=3.14 python # Python 3.14.0rc2 (main, Aug 18 2025, 19:19:22) [Clang 20.1.4 ] on linux # ... >>> import sys >>> sys._is_gil_enabled() # True Then using the version with free threading (note the t suffix) uv run --python=3.14t python # ... # Python 3.14.0rc2 free-threading build (main, Aug 18 2025, 19:19:12) [Clang 20.1.4 ] on linux # ... >>> import sys >>> sys._is_gil_enabled() # False Project creation and management with uv uv is capable of much more than allowing us to switch between different versions of Python. The following commands initialise a Python project with uv: # From ~/temp/blog-py3.14 # Indicate the default python version for the project uv python pin 3.14 # Initialise a project in the current directory uv init . # Check the Python version uv run python --version # Python 3.14.0rc2 This adds some files for project metadata (pyproject.toml, README.md) and version control: tree -a -L 1 # . # ├── .git # ├── .gitignore # ├── main.py # ├── pyproject.toml # ├── .python-version # ├── README.md # ├── uv.lock # └── .venv # # 2 directories, 6 files Now we can add package dependencies using uv add and other standard project-management tasks. But one thing I wanted to highlight is that uv allows us to start a Jupyter notebook, using the project’s Python interpreter, without either adding jupyter as a dependency or explicitly defining a kernel for jupyter: uv run --with jupyter jupyter lab Creating a new notebook using the default Python 3 kernel in the JupyterLab session that starts, should ensure you are using the currently active Python 3.14 environment. Threading Python 3.13 introduced an experimental feature, ‘Free-threading’, that is now officially supported as of 3.14. First though, what is a ’thread’? When a program runs on your computer, there are lots of different tasks going on. Some of those tasks could run independently of each other. You, as the programmer, may need to explain to the computer which tasks can run independently. A thread is a way of cordoning-off one of those tasks; it’s a way of telling the computer that your software is running on, that this task here can run separately from those tasks there, and the logic for running this task too. (Basically). Python has allowed developers to define threads for a while. If you have a few tasks that are largely independent of each other, each of these tasks can run in a separate thread. Threads can access the same memory space, meaning that they can access and modify shared variables in a Python session. In general, this also means that a computation in one thread could update a value that is used by another thread, or that two different threads could make conflicting updates to the same variable. This freedom can lead to bugs. The CPython interpreter was originally written with a locking mechanism (the Global Interpreter Lock, GIL) that prevented different threads from running at the same time (even when multiple processors were available) and limited the reach of these bugs. Traditionally, you would have used threads for “non-CPU-bound tasks” in Python. These are the kinds of tasks that would be unaffected by having more, or faster, processors available to the Python instance: network traffic, file access, waiting for user input. For CPU-bound tasks, like calculations and data-processing, you could use Python’s ‘multiprocessing’ library (although some libraries like ‘numpy’ have their own low-level mechanisms for splitting work across cores). This starts multiple Python instances, each doing a portion of the processing, and allows a workload to be partitioned across multiple processors. The main other differences between threading and multiprocessing in Python are in memory and data management. With threading, you have one Python instance, with each thread having access to the same memory space. With multiprocessing, you have multiple Python instances that work independently: the instances do not share memory, so to partition a workload using multiprocessing, Python has to send copies of (subsets of) your data to the new instances. This could mean that you need to store two or more copies of a large dataset in memory when using multiprocessing upon it. Simultaneous processing across threads that share memory-space is now possible using the free-threaded build of Python. Many third-party packages have been rewritten to accommodate this new build and you can learn more about free-threading and the progress of the changes in the “Python Free-Threading Guide”. As a simple-ish example, lets consider natural language processing. There is a wonderful blog post about parallel processing with the nltk package on the “WZB Data Science Blog”. We will extend that example to use free-threading. ntlk provides access to some of the Project Gutenberg books, and we can access this data as follows: # main.py import nltk def setup(): nltk.download(\"gutenberg\") nltk.download(\"punkt_tab\") nltk.download('averaged_perceptron_tagger_eng') corpus = { f_id: nltk.corpus.gutenberg.raw(f_id) for f_id in nltk.corpus.gutenberg.fileids() } return corpus corpus = setup() The key-value pairs in corpus are the abbreviated book-title and contents for 18 books. For example: corpus[\"austen-emma.txt\"] # [Emma by Jane Austen 1816] # # VOLUME I # # CHAPTER I # # # Emma Woodhouse, handsome, clever, and rich, with a comfortable home ... A standard part of a text-processing workflow is to tokenise and tag the “parts-of-speech” (POS) in a document. We can do this using two nltk functions: # main.py ... continued def tokenise_and_pos_tag(doc): return nltk.pos_tag(nltk.word_tokenize(doc)) A function to sequentially tokenise and POS-tag the contents of a corpus of books can be written: # main.py ... continued def tokenise_seq(corpus): tokens = { f_id: tokenise_and_pos_tag(doc) for f_id, doc in corpus.items() } return tokens You need to install or build Python in a particular way to make use of “Free-threaded” Python. In the above, we installed Python “3.14t” using uv, so we can compare the speed of free-threaded and sequential, single-core, processing. We will use the timeit package to analyse processing speed, from the command line. # Activate the threaded version of Python 3.14 uv python pin 3.14t # Install the dependencies for our main.py script uv add timeit nltk # Time the `tokenise_seq()` function # -- but do not time any setup code... PYTHON_GIL=0 \\ uv run python -m timeit \\ --setup \"import main; corpus = main.setup()\" \\ \"main.tokenise_seq(corpus)\" # [lots of output messages] # 1 loop, best of 5: 53.1 sec per loop After some initial steps where the nltk datasets were downloaded and the corpus object was created (neither of which were timed, because these steps were part of the timeit --setup block), tokenise_seq(corpus) was run multiple times and the fastest speed was around 53 seconds. A small note: we have used the environment variable PYTHON_GIL=0 here. This makes it explicit that we are using free-threading (turning off the GIL). This wouldn’t normally be necessary to take advantage of free-threading (in Python “3.14t”), but was needed because one of the dependencies of nltk hasn’t been validated for the free-threaded build yet. To write a threaded-version of the same, we introduce two functions. The first is a helper that takes (filename, document-content) pairs and returns (filename, processed-document) pairs: def tupled_tokeniser(pair): file_id, doc = pair return file_id, tokenise_and_pos_tag(doc) The second function creates a Thread-pool, taking advantage of as many CPUs as there are available on my machine (16, counted by multiprocessing.cpu_count()). Each document is processed as a separate thread and we wait for all of the documents to be processed before returning results to the caller: import multiprocessing as mp from concurrent.futures import ThreadPoolExecutor, wait # ... def tokenise_threaded(corpus): with ThreadPoolExecutor(max_workers=mp.cpu_count()) as tpe: try: futures = [ tpe.submit(tupled_tokeniser, pair) for pair in corpus.items() ] wait(futures) finally: # output is a list of (file-id, data) pairs tokens = [f.result() for f in futures] return tokens # Time the `tokenise_threaded()` function # -- but do not time any setup code... PYTHON_GIL=0 \\ uv run python -m timeit \\ --setup \"import main; corpus = main.setup()\" \\ \"main.tokenise_threaded(corpus)\" # [lots of output messages] # 1 loop, best of 5: 32.5 sec per loop I could see that every core was used when processing the documents, using the htop tool on Ubuntu. At points during the run, each of the 16 CPUs was at near to 100% use (whereas only one or two CPUs were busy at any time during the sequential run): But, despite using 16x as many CPUs, the multithreaded version of the processing script was only about 40% faster. There was only 18 books in the dataset and some disparity between the book lengths (the bible, containing millions of words was processed much slower than the others). Maybe the speed up would be greater with a larger or more balanced dataset. In the post on the WZB Data Science blog, there is a multiprocessing implementation of the above. Running their multiprocessing code with 16 CPUs gave a similar speed up to multithreading (minimum time 31.2 seconds). Indeed, if I was writing this code for a real project, multiprocessing would remain my choice, because the analysis for one book can proceed independently of that for any other book and data volumes aren’t that big. Other News Python 3.14 has also introduced some improvements to exception-handling, a new approach to string templating and improvements to the use of concurrent interpreters. See the Python 3.14 release notes for further details. In the wider Python Data Science ecosystem, a few other developments have occurred or are due before the end of 2025: The first stable release of the Positron IDE was made in August; Pandas 3.0 is due before the end of the year, and will introduce strings as a data-type, copy-on-write behaviour, and implicit access to columns in DataFrame-modification code; Tools that ingest DataFrames are becoming agnostic to DataFrame library through the Narwahls project. See the Plotly write-up on this subject. Python data science progresses at such a speed that we can only really scratch the surface here. Have we missed anything in the wider Python ecosystem (2025 edition) that will make a huge difference to your data work? Let us know on LinkedIn or Bluesky. For updates and revisions to this article, see the original post",
      "meta_keywords": null,
      "og_description": "Python 3.14 was released on 7th October 2025. Here we summarise some of the more interesting changes and some trends in Python development and data-science over the past year. We will highlight the following: the colourful Python command-line interface; project-management tool uv; free-threading; and a brief summary of other developments. The Python 3.14 release notes also describe the changes to base Python. Colourful REPL At Jumping Rivers we have taught a lot of people to program in Python. Throughout a programming career you get used to making, and learning from, mistakes. The most common mistakes made in introductory programming lessons may still trip you up in 10 years time: unmatched parentheses, typos, missing quote symbols, unimported dependencies. Our Python training courses are presented using Jupyter. Jupyter notebooks have syntax highlighting that makes it easy to identify an unfinished string, or a mis-spelled keyword. But, most Python learners don’t use Jupyter (or other high-level programming tools) on day one - they experiment with Python at the command line. You can type “python” into your shell/terminal window and start programming into the “REPL” (read-evaluate-print loop). Any effort to make the REPL easier to work with will be beneficial to beginning programmers. So the introduction of syntax highlighting in the Python 3.14 REPL is really beneficial. Whether you want to start from scratch, or improve your skills, Jumping Rivers has a training course for you. uv and package development One of the big trends in Python development within 2025, is the rise of the project management tool uv. This is a Rust-based command-line tool and can be used to initialise a package / project structure, to specify the development and runtime environment of a project, and to publish a package to PyPI. At Jumping Rivers, we have used poetry for many of the jobs that uv excels at. Python is used for the data preparation tasks for diffify.com, and we use poetry to ensure that our developers each use precisely the same package versions when working on that project (See our current blog series on Poetry). But, poetry doesn’t prevent developers using different versions of Python. For that, we need a second tool like pyenv (which allows switching between different Python versions) or for each developer to have the same Python version installed on their machine. uv goes a step further than poetry and allows us to pin Python versions for a project. Let’s use uv to install Python 3.14, so that we can test out features in the new release. First follow the instructions for installing uv. Then at the command line, we will use uv to create a new project where we’ll use Python 3.14. # [bash] cd ~/temp mkdir blog-py3.14 cd blog-py3.14 # Which versions of Python 3.14 are available via uv? uv python list | grep 3.14 # cpython-3.14.0rc2-linux-x86_64-gnu # cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu You’ll see something similar regardless of the operating system that you use. That lists two versions of Python 3.14 - one with an optional system called “Free Threading” (see later). We’ll install both versions of Python: uv python install cpython-3.14.0rc2-linux-x86_64-gnu uv python install cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu Users of pyenv will be able to install Python 3.14 in a similar manner. We can select between the two different Python versions at the command line. First using the version that does not have free threading: uv run --python=3.14 python # Python 3.14.0rc2 (main, Aug 18 2025, 19:19:22) [Clang 20.1.4 ] on linux # ... >>> import sys >>> sys._is_gil_enabled() # True Then using the version with free threading (note the t suffix) uv run --python=3.14t python # ... # Python 3.14.0rc2 free-threading build (main, Aug 18 2025, 19:19:12) [Clang 20.1.4 ] on linux # ... >>> import sys >>> sys._is_gil_enabled() # False Project creation and management with uv uv is capable of much more than allowing us to switch between different versions of Python. The following commands initialise a Python project with uv: # From ~/temp/blog-py3.14 # Indicate the default python version for the project uv python pin 3.14 # Initialise a project in the current directory uv init . # Check the Python version uv run python --version # Python 3.14.0rc2 This adds some files for project metadata (pyproject.toml, README.md) and version control: tree -a -L 1 # . # ├── .git # ├── .gitignore # ├── main.py # ├── pyproject.toml # ├── .python-version # ├── README.md # ├── uv.lock # └── .venv # # 2 directories, 6 files Now we can add package dependencies using uv add and other standard project-management tasks. But one thing I wanted to highlight is that uv allows us to start a Jupyter notebook, using the project’s Python interpreter, without either adding jupyter as a dependency or explicitly defining a kernel for jupyter: uv run --with jupyter jupyter lab Creating a new notebook using the default Python 3 kernel in the JupyterLab session that starts, should ensure you are using the currently active Python 3.14 environment. Threading Python 3.13 introduced an experimental feature, ‘Free-threading’, that is now officially supported as of 3.14. First though, what is a ’thread’? When a program runs on your computer, there are lots of different tasks going on. Some of those tasks could run independently of each other. You, as the programmer, may need to explain to the computer which tasks can run independently. A thread is a way of cordoning-off one of those tasks; it’s a way of telling the computer that your software is running on, that this task here can run separately from those tasks there, and the logic for running this task too. (Basically). Python has allowed developers to define threads for a while. If you have a few tasks that are largely independent of each other, each of these tasks can run in a separate thread. Threads can access the same memory space, meaning that they can access and modify shared variables in a Python session. In general, this also means that a computation in one thread could update a value that is used by another thread, or that two different threads could make conflicting updates to the same variable. This freedom can lead to bugs. The CPython interpreter was originally written with a locking mechanism (the Global Interpreter Lock, GIL) that prevented different threads from running at the same time (even when multiple processors were available) and limited the reach of these bugs. Traditionally, you would have used threads for “non-CPU-bound tasks” in Python. These are the kinds of tasks that would be unaffected by having more, or faster, processors available to the Python instance: network traffic, file access, waiting for user input. For CPU-bound tasks, like calculations and data-processing, you could use Python’s ‘multiprocessing’ library (although some libraries like ‘numpy’ have their own low-level mechanisms for splitting work across cores). This starts multiple Python instances, each doing a portion of the processing, and allows a workload to be partitioned across multiple processors. The main other differences between threading and multiprocessing in Python are in memory and data management. With threading, you have one Python instance, with each thread having access to the same memory space. With multiprocessing, you have multiple Python instances that work independently: the instances do not share memory, so to partition a workload using multiprocessing, Python has to send copies of (subsets of) your data to the new instances. This could mean that you need to store two or more copies of a large dataset in memory when using multiprocessing upon it. Simultaneous processing across threads that share memory-space is now possible using the free-threaded build of Python. Many third-party packages have been rewritten to accommodate this new build and you can learn more about free-threading and the progress of the changes in the “Python Free-Threading Guide”. As a simple-ish example, lets consider natural language processing. There is a wonderful blog post about parallel processing with the nltk package on the “WZB Data Science Blog”. We will extend that example to use free-threading. ntlk provides access to some of the Project Gutenberg books, and we can access this data as follows: # main.py import nltk def setup(): nltk.download(\"gutenberg\") nltk.download(\"punkt_tab\") nltk.download('averaged_perceptron_tagger_eng') corpus = { f_id: nltk.corpus.gutenberg.raw(f_id) for f_id in nltk.corpus.gutenberg.fileids() } return corpus corpus = setup() The key-value pairs in corpus are the abbreviated book-title and contents for 18 books. For example: corpus[\"austen-emma.txt\"] # [Emma by Jane Austen 1816] # # VOLUME I # # CHAPTER I # # # Emma Woodhouse, handsome, clever, and rich, with a comfortable home ... A standard part of a text-processing workflow is to tokenise and tag the “parts-of-speech” (POS) in a document. We can do this using two nltk functions: # main.py ... continued def tokenise_and_pos_tag(doc): return nltk.pos_tag(nltk.word_tokenize(doc)) A function to sequentially tokenise and POS-tag the contents of a corpus of books can be written: # main.py ... continued def tokenise_seq(corpus): tokens = { f_id: tokenise_and_pos_tag(doc) for f_id, doc in corpus.items() } return tokens You need to install or build Python in a particular way to make use of “Free-threaded” Python. In the above, we installed Python “3.14t” using uv, so we can compare the speed of free-threaded and sequential, single-core, processing. We will use the timeit package to analyse processing speed, from the command line. # Activate the threaded version of Python 3.14 uv python pin 3.14t # Install the dependencies for our main.py script uv add timeit nltk # Time the `tokenise_seq()` function # -- but do not time any setup code... PYTHON_GIL=0 \\ uv run python -m timeit \\ --setup \"import main; corpus = main.setup()\" \\ \"main.tokenise_seq(corpus)\" # [lots of output messages] # 1 loop, best of 5: 53.1 sec per loop After some initial steps where the nltk datasets were downloaded and the corpus object was created (neither of which were timed, because these steps were part of the timeit --setup block), tokenise_seq(corpus) was run multiple times and the fastest speed was around 53 seconds. A small note: we have used the environment variable PYTHON_GIL=0 here. This makes it explicit that we are using free-threading (turning off the GIL). This wouldn’t normally be necessary to take advantage of free-threading (in Python “3.14t”), but was needed because one of the dependencies of nltk hasn’t been validated for the free-threaded build yet. To write a threaded-version of the same, we introduce two functions. The first is a helper that takes (filename, document-content) pairs and returns (filename, processed-document) pairs: def tupled_tokeniser(pair): file_id, doc = pair return file_id, tokenise_and_pos_tag(doc) The second function creates a Thread-pool, taking advantage of as many CPUs as there are available on my machine (16, counted by multiprocessing.cpu_count()). Each document is processed as a separate thread and we wait for all of the documents to be processed before returning results to the caller: import multiprocessing as mp from concurrent.futures import ThreadPoolExecutor, wait # ... def tokenise_threaded(corpus): with ThreadPoolExecutor(max_workers=mp.cpu_count()) as tpe: try: futures = [ tpe.submit(tupled_tokeniser, pair) for pair in corpus.items() ] wait(futures) finally: # output is a list of (file-id, data) pairs tokens = [f.result() for f in futures] return tokens # Time the `tokenise_threaded()` function # -- but do not time any setup code... PYTHON_GIL=0 \\ uv run python -m timeit \\ --setup \"import main; corpus = main.setup()\" \\ \"main.tokenise_threaded(corpus)\" # [lots of output messages] # 1 loop, best of 5: 32.5 sec per loop I could see that every core was used when processing the documents, using the htop tool on Ubuntu. At points during the run, each of the 16 CPUs was at near to 100% use (whereas only one or two CPUs were busy at any time during the sequential run): But, despite using 16x as many CPUs, the multithreaded version of the processing script was only about 40% faster. There was only 18 books in the dataset and some disparity between the book lengths (the bible, containing millions of words was processed much slower than the others). Maybe the speed up would be greater with a larger or more balanced dataset. In the post on the WZB Data Science blog, there is a multiprocessing implementation of the above. Running their multiprocessing code with 16 CPUs gave a similar speed up to multithreading (minimum time 31.2 seconds). Indeed, if I was writing this code for a real project, multiprocessing would remain my choice, because the analysis for one book can proceed independently of that for any other book and data volumes aren’t that big. Other News Python 3.14 has also introduced some improvements to exception-handling, a new approach to string templating and improvements to the use of concurrent interpreters. See the Python 3.14 release notes for further details. In the wider Python Data Science ecosystem, a few other developments have occurred or are due before the end of 2025: The first stable release of the Positron IDE was made in August; Pandas 3.0 is due before the end of the year, and will introduce strings as a data-type, copy-on-write behaviour, and implicit access to columns in DataFrame-modification code; Tools that ingest DataFrames are becoming agnostic to DataFrame library through the Narwahls project. See the Plotly write-up on this subject. Python data science progresses at such a speed that we can only really scratch the surface here. Have we missed anything in the wider Python ecosystem (2025 edition) that will make a huge difference to your data work? Let us know on LinkedIn or Bluesky. For updates and revisions to this article, see the original post",
      "og_image": "https://www.jumpingrivers.com/blog/whats-new-py314/featured.png",
      "og_title": "What’s new for Python in 2025? | R-bloggers",
      "raw_jsonld_article": null,
      "reading_time_min": 12.3,
      "sitemap_lastmod": null,
      "twitter_description": "Python 3.14 was released on 7th October 2025. Here we summarise some of the more interesting changes and some trends in Python development and data-science over the past year. We will highlight the following: the colourful Python command-line interface; project-management tool uv; free-threading; and a brief summary of other developments. The Python 3.14 release notes also describe the changes to base Python. Colourful REPL At Jumping Rivers we have taught a lot of people to program in Python. Throughout a programming career you get used to making, and learning from, mistakes. The most common mistakes made in introductory programming lessons may still trip you up in 10 years time: unmatched parentheses, typos, missing quote symbols, unimported dependencies. Our Python training courses are presented using Jupyter. Jupyter notebooks have syntax highlighting that makes it easy to identify an unfinished string, or a mis-spelled keyword. But, most Python learners don’t use Jupyter (or other high-level programming tools) on day one - they experiment with Python at the command line. You can type “python” into your shell/terminal window and start programming into the “REPL” (read-evaluate-print loop). Any effort to make the REPL easier to work with will be beneficial to beginning programmers. So the introduction of syntax highlighting in the Python 3.14 REPL is really beneficial. Whether you want to start from scratch, or improve your skills, Jumping Rivers has a training course for you. uv and package development One of the big trends in Python development within 2025, is the rise of the project management tool uv. This is a Rust-based command-line tool and can be used to initialise a package / project structure, to specify the development and runtime environment of a project, and to publish a package to PyPI. At Jumping Rivers, we have used poetry for many of the jobs that uv excels at. Python is used for the data preparation tasks for diffify.com, and we use poetry to ensure that our developers each use precisely the same package versions when working on that project (See our current blog series on Poetry). But, poetry doesn’t prevent developers using different versions of Python. For that, we need a second tool like pyenv (which allows switching between different Python versions) or for each developer to have the same Python version installed on their machine. uv goes a step further than poetry and allows us to pin Python versions for a project. Let’s use uv to install Python 3.14, so that we can test out features in the new release. First follow the instructions for installing uv. Then at the command line, we will use uv to create a new project where we’ll use Python 3.14. # [bash] cd ~/temp mkdir blog-py3.14 cd blog-py3.14 # Which versions of Python 3.14 are available via uv? uv python list | grep 3.14 # cpython-3.14.0rc2-linux-x86_64-gnu # cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu You’ll see something similar regardless of the operating system that you use. That lists two versions of Python 3.14 - one with an optional system called “Free Threading” (see later). We’ll install both versions of Python: uv python install cpython-3.14.0rc2-linux-x86_64-gnu uv python install cpython-3.14.0rc2+freethreaded-linux-x86_64-gnu Users of pyenv will be able to install Python 3.14 in a similar manner. We can select between the two different Python versions at the command line. First using the version that does not have free threading: uv run --python=3.14 python # Python 3.14.0rc2 (main, Aug 18 2025, 19:19:22) [Clang 20.1.4 ] on linux # ... >>> import sys >>> sys._is_gil_enabled() # True Then using the version with free threading (note the t suffix) uv run --python=3.14t python # ... # Python 3.14.0rc2 free-threading build (main, Aug 18 2025, 19:19:12) [Clang 20.1.4 ] on linux # ... >>> import sys >>> sys._is_gil_enabled() # False Project creation and management with uv uv is capable of much more than allowing us to switch between different versions of Python. The following commands initialise a Python project with uv: # From ~/temp/blog-py3.14 # Indicate the default python version for the project uv python pin 3.14 # Initialise a project in the current directory uv init . # Check the Python version uv run python --version # Python 3.14.0rc2 This adds some files for project metadata (pyproject.toml, README.md) and version control: tree -a -L 1 # . # ├── .git # ├── .gitignore # ├── main.py # ├── pyproject.toml # ├── .python-version # ├── README.md # ├── uv.lock # └── .venv # # 2 directories, 6 files Now we can add package dependencies using uv add and other standard project-management tasks. But one thing I wanted to highlight is that uv allows us to start a Jupyter notebook, using the project’s Python interpreter, without either adding jupyter as a dependency or explicitly defining a kernel for jupyter: uv run --with jupyter jupyter lab Creating a new notebook using the default Python 3 kernel in the JupyterLab session that starts, should ensure you are using the currently active Python 3.14 environment. Threading Python 3.13 introduced an experimental feature, ‘Free-threading’, that is now officially supported as of 3.14. First though, what is a ’thread’? When a program runs on your computer, there are lots of different tasks going on. Some of those tasks could run independently of each other. You, as the programmer, may need to explain to the computer which tasks can run independently. A thread is a way of cordoning-off one of those tasks; it’s a way of telling the computer that your software is running on, that this task here can run separately from those tasks there, and the logic for running this task too. (Basically). Python has allowed developers to define threads for a while. If you have a few tasks that are largely independent of each other, each of these tasks can run in a separate thread. Threads can access the same memory space, meaning that they can access and modify shared variables in a Python session. In general, this also means that a computation in one thread could update a value that is used by another thread, or that two different threads could make conflicting updates to the same variable. This freedom can lead to bugs. The CPython interpreter was originally written with a locking mechanism (the Global Interpreter Lock, GIL) that prevented different threads from running at the same time (even when multiple processors were available) and limited the reach of these bugs. Traditionally, you would have used threads for “non-CPU-bound tasks” in Python. These are the kinds of tasks that would be unaffected by having more, or faster, processors available to the Python instance: network traffic, file access, waiting for user input. For CPU-bound tasks, like calculations and data-processing, you could use Python’s ‘multiprocessing’ library (although some libraries like ‘numpy’ have their own low-level mechanisms for splitting work across cores). This starts multiple Python instances, each doing a portion of the processing, and allows a workload to be partitioned across multiple processors. The main other differences between threading and multiprocessing in Python are in memory and data management. With threading, you have one Python instance, with each thread having access to the same memory space. With multiprocessing, you have multiple Python instances that work independently: the instances do not share memory, so to partition a workload using multiprocessing, Python has to send copies of (subsets of) your data to the new instances. This could mean that you need to store two or more copies of a large dataset in memory when using multiprocessing upon it. Simultaneous processing across threads that share memory-space is now possible using the free-threaded build of Python. Many third-party packages have been rewritten to accommodate this new build and you can learn more about free-threading and the progress of the changes in the “Python Free-Threading Guide”. As a simple-ish example, lets consider natural language processing. There is a wonderful blog post about parallel processing with the nltk package on the “WZB Data Science Blog”. We will extend that example to use free-threading. ntlk provides access to some of the Project Gutenberg books, and we can access this data as follows: # main.py import nltk def setup(): nltk.download(\"gutenberg\") nltk.download(\"punkt_tab\") nltk.download('averaged_perceptron_tagger_eng') corpus = { f_id: nltk.corpus.gutenberg.raw(f_id) for f_id in nltk.corpus.gutenberg.fileids() } return corpus corpus = setup() The key-value pairs in corpus are the abbreviated book-title and contents for 18 books. For example: corpus[\"austen-emma.txt\"] # [Emma by Jane Austen 1816] # # VOLUME I # # CHAPTER I # # # Emma Woodhouse, handsome, clever, and rich, with a comfortable home ... A standard part of a text-processing workflow is to tokenise and tag the “parts-of-speech” (POS) in a document. We can do this using two nltk functions: # main.py ... continued def tokenise_and_pos_tag(doc): return nltk.pos_tag(nltk.word_tokenize(doc)) A function to sequentially tokenise and POS-tag the contents of a corpus of books can be written: # main.py ... continued def tokenise_seq(corpus): tokens = { f_id: tokenise_and_pos_tag(doc) for f_id, doc in corpus.items() } return tokens You need to install or build Python in a particular way to make use of “Free-threaded” Python. In the above, we installed Python “3.14t” using uv, so we can compare the speed of free-threaded and sequential, single-core, processing. We will use the timeit package to analyse processing speed, from the command line. # Activate the threaded version of Python 3.14 uv python pin 3.14t # Install the dependencies for our main.py script uv add timeit nltk # Time the `tokenise_seq()` function # -- but do not time any setup code... PYTHON_GIL=0 \\ uv run python -m timeit \\ --setup \"import main; corpus = main.setup()\" \\ \"main.tokenise_seq(corpus)\" # [lots of output messages] # 1 loop, best of 5: 53.1 sec per loop After some initial steps where the nltk datasets were downloaded and the corpus object was created (neither of which were timed, because these steps were part of the timeit --setup block), tokenise_seq(corpus) was run multiple times and the fastest speed was around 53 seconds. A small note: we have used the environment variable PYTHON_GIL=0 here. This makes it explicit that we are using free-threading (turning off the GIL). This wouldn’t normally be necessary to take advantage of free-threading (in Python “3.14t”), but was needed because one of the dependencies of nltk hasn’t been validated for the free-threaded build yet. To write a threaded-version of the same, we introduce two functions. The first is a helper that takes (filename, document-content) pairs and returns (filename, processed-document) pairs: def tupled_tokeniser(pair): file_id, doc = pair return file_id, tokenise_and_pos_tag(doc) The second function creates a Thread-pool, taking advantage of as many CPUs as there are available on my machine (16, counted by multiprocessing.cpu_count()). Each document is processed as a separate thread and we wait for all of the documents to be processed before returning results to the caller: import multiprocessing as mp from concurrent.futures import ThreadPoolExecutor, wait # ... def tokenise_threaded(corpus): with ThreadPoolExecutor(max_workers=mp.cpu_count()) as tpe: try: futures = [ tpe.submit(tupled_tokeniser, pair) for pair in corpus.items() ] wait(futures) finally: # output is a list of (file-id, data) pairs tokens = [f.result() for f in futures] return tokens # Time the `tokenise_threaded()` function # -- but do not time any setup code... PYTHON_GIL=0 \\ uv run python -m timeit \\ --setup \"import main; corpus = main.setup()\" \\ \"main.tokenise_threaded(corpus)\" # [lots of output messages] # 1 loop, best of 5: 32.5 sec per loop I could see that every core was used when processing the documents, using the htop tool on Ubuntu. At points during the run, each of the 16 CPUs was at near to 100% use (whereas only one or two CPUs were busy at any time during the sequential run): But, despite using 16x as many CPUs, the multithreaded version of the processing script was only about 40% faster. There was only 18 books in the dataset and some disparity between the book lengths (the bible, containing millions of words was processed much slower than the others). Maybe the speed up would be greater with a larger or more balanced dataset. In the post on the WZB Data Science blog, there is a multiprocessing implementation of the above. Running their multiprocessing code with 16 CPUs gave a similar speed up to multithreading (minimum time 31.2 seconds). Indeed, if I was writing this code for a real project, multiprocessing would remain my choice, because the analysis for one book can proceed independently of that for any other book and data volumes aren’t that big. Other News Python 3.14 has also introduced some improvements to exception-handling, a new approach to string templating and improvements to the use of concurrent interpreters. See the Python 3.14 release notes for further details. In the wider Python Data Science ecosystem, a few other developments have occurred or are due before the end of 2025: The first stable release of the Positron IDE was made in August; Pandas 3.0 is due before the end of the year, and will introduce strings as a data-type, copy-on-write behaviour, and implicit access to columns in DataFrame-modification code; Tools that ingest DataFrames are becoming agnostic to DataFrame library through the Narwahls project. See the Plotly write-up on this subject. Python data science progresses at such a speed that we can only really scratch the surface here. Have we missed anything in the wider Python ecosystem (2025 edition) that will make a huge difference to your data work? Let us know on LinkedIn or Bluesky. For updates and revisions to this article, see the original post",
      "twitter_title": "What’s new for Python in 2025? | R-bloggers",
      "url": "https://www.r-bloggers.com/2025/10/whats-new-for-python-in-2025/",
      "word_count": 2460
    }
  }
}