name: Crawl R-Bloggers (hourly)

on:
  schedule:
    - cron: "0 * * * *"  # every hour
  workflow_dispatch:

concurrency:
  group: rbloggers-crawl-main
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  crawl-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          clean: true

      - name: Sync to origin/main (before generating files)
        shell: bash
        run: |
          set -euo pipefail
          git fetch origin main
          git checkout main
          git reset --hard origin/main
          git clean -fd

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Crawl R-Bloggers -> by_created
        shell: bash
        run: |
          set -euo pipefail
          python scripts/crawl_rbloggers.py

      - name: Update repo stats (RBLOGGERS_REPO_STATS.md)
        shell: bash
        run: |
          set -euo pipefail
          python scripts/update_repo_stats.py

      - name: Commit & push (json + stats)
        id: commit_push
        shell: bash
        run: |
          set -euo pipefail

          BEFORE_SHA="${GITHUB_SHA}"

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add -A

          COMMITTED="0"
          if git diff --cached --quiet; then
            echo "Nothing to commit."
          else
            git commit -m "chore: update rbloggers crawl + stats"
            git push origin main
            COMMITTED="1"
          fi

          AFTER_SHA="$(git rev-parse HEAD)"

          echo "committed=${COMMITTED}" >> "$GITHUB_OUTPUT"
          echo "before_sha=${BEFORE_SHA}" >> "$GITHUB_OUTPUT"
          echo "after_sha=${AFTER_SHA}" >> "$GITHUB_OUTPUT"
          echo "ref_name=${GITHUB_REF_NAME}" >> "$GITHUB_OUTPUT"
          echo "repo=${GITHUB_REPOSITORY}" >> "$GITHUB_OUTPUT"

          # crawler가 만들어 둔 .action_result.json에서 new_files_relpaths / files 읽기
          FILES_JSON="[]"
          if [ -f ".action_result.json" ]; then
            FILES_JSON="$(python -c 'import json; obj=json.load(open(".action_result.json","r",encoding="utf-8")); files=obj.get("new_files_relpaths") or obj.get("files") or []; files=[f for f in files if isinstance(f,str) and f.startswith("by_created/") and f.endswith(".json")]; import json as _j; print(_j.dumps(files, ensure_ascii=False))' || echo '[]')"
          fi

          # action_result가 비어있는데(commit이 있었던 경우)에만 diff fallback
          if [ "$COMMITTED" = "1" ] && [ "$FILES_JSON" = "[]" ]; then
            FILES_JSON="$(git diff --name-only "$BEFORE_SHA" "$AFTER_SHA" -- 'by_created/**/*.json' \
              | python -c 'import sys,json; files=[l.strip() for l in sys.stdin if l.strip()]; print(json.dumps(files, ensure_ascii=False))')"
          fi

          echo "files_json<<EOF" >> "$GITHUB_OUTPUT"
          echo "$FILES_JSON" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Sync new JSON files to DB (skip if none)
        shell: bash
        env:
          WEBR_SYNC_URL: ${{ secrets.WEBR_SYNC_URL }}
          WEBR_SYNC_TOKEN: ${{ secrets.WEBR_SYNC_TOKEN }}

          REPO: ${{ steps.commit_push.outputs.repo }}
          SHA:  ${{ steps.commit_push.outputs.after_sha }}
          REF:  ${{ steps.commit_push.outputs.ref_name }}
          FILES_JSON: ${{ steps.commit_push.outputs.files_json }}

          # 504 회피용 배치(필요하면 5로 낮추기)
          WEBR_SYNC_BATCH: "10"
        run: |
          set -euo pipefail

          if [ -z "${WEBR_SYNC_URL:-}" ] || [ -z "${WEBR_SYNC_TOKEN:-}" ]; then
            echo "WEBR_SYNC_URL or WEBR_SYNC_TOKEN not set. Skip DB sync."
            exit 0
          fi

          FILES_COUNT="$(python -c 'import os,json; print(len(json.loads(os.environ.get("FILES_JSON","[]"))))')"
          echo "files_count=${FILES_COUNT}"

          # ✅ 핵심: 파일이 없으면 webhook 호출 자체를 하지 않는다 (504 원천 차단)
          if [ "${FILES_COUNT}" = "0" ]; then
            echo "No files to sync. Skip webhook."
            exit 0
          fi

          python - <<'PY'
          import os, json, math, subprocess, tempfile, time

          def post_with_retry(cmd, retries=3):
              for attempt in range(1, retries+1):
                  try:
                      subprocess.check_call(cmd)
                      return
                  except subprocess.CalledProcessError:
                      if attempt == retries:
                          raise
                      wait = 10 * attempt
                      print(f"curl failed (attempt {attempt}/{retries}). retry in {wait}s")
                      time.sleep(wait)

          url   = os.environ["WEBR_SYNC_URL"]
          token = os.environ["WEBR_SYNC_TOKEN"]
          repo  = os.environ.get("REPO","")
          sha   = os.environ.get("SHA","")
          ref   = os.environ.get("REF","")

          files = json.loads(os.environ.get("FILES_JSON","[]"))
          batch = int(os.environ.get("WEBR_SYNC_BATCH","10") or "10")
          if batch <= 0:
              batch = 10

          total = math.ceil(len(files)/batch)

          for idx, i in enumerate(range(0, len(files), batch), start=1):
              part = files[i:i+batch]
              payload = {"repo": repo, "sha": sha, "ref": ref, "files": part}

              with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8", suffix=".json") as f:
                  json.dump(payload, f, ensure_ascii=False)
                  path = f.name

              print(f"batch {idx}/{total} size={len(part)}")

              cmd = [
                  "curl", "--fail-with-body", "-sS", "-L", "-X", "POST", url,
                  "-H", "Content-Type: application/json",
                  "-H", f"X-WEBR-SYNC-TOKEN: {token}",
                  "--data-binary", f"@{path}"
              ]
              post_with_retry(cmd, retries=3)

              # 서버 부담 완화
              time.sleep(1)
          PY
