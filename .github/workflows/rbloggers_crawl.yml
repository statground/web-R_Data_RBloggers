name: Crawl R-Bloggers (twice daily)

'on':
  # 테스트를 위해 push도 넣어둠 (원하면 안정화 후 제거 가능)
  push:
    branches: [ "main" ]

  # UTC 00:00 / 12:00 (KST 09:00 / 21:00)
  schedule:
    - cron: "0 0,12 * * *"

  # Actions 탭에서 수동 실행 버튼
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: rbloggers-crawl
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Crawl
        env:
          MAX_PAGES_FROM_HOME: "1"
          SLEEP_SEC: "1.0"
        run: |
          python scripts/crawl_rbloggers.py

      - name: Commit & Push (if changed)
        run: |
          if git status --porcelain | grep .; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add by_created .action_result.json
            git commit -m "rbloggers: add new posts"
            git push
          else
            echo "No changes. Skip commit."
          fi
