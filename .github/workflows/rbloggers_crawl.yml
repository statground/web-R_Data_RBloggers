name: Crawl R-Bloggers (hourly)

on:
  schedule:
    - cron: "0 * * * *"
  workflow_dispatch:
    inputs:
      resync:
        description: "Force resync by_created files even if no new files"
        required: false
        default: "false"
      resync_month:
        description: "Optional: YYYY/MM (e.g. 2026/01). If empty, resync all by_created"
        required: false
        default: ""
      resync_latest_n:
        description: "Optional: when resync=true, sync only latest N files by recent commits. 0 means all."
        required: false
        default: "0"
      resync_commits_scan:
        description: "How many recent commits to scan for file changes when resync_latest_n>0"
        required: false
        default: "500"
      sync_batch:
        description: "Batch size for webhook (smaller reduces 504 risk)"
        required: false
        default: "10"

concurrency:
  group: rbloggers-crawl-main
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  crawl-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f scripts/requirements.txt ]; then
            pip install -r scripts/requirements.txt
          else
            pip install requests beautifulsoup4 lxml
          fi

      - name: Crawl R-Bloggers
        run: |
          python scripts/crawl_rbloggers.py

      - name: Update Repository Stats
        run: |
          python scripts/update_repo_stats.py

      - name: Commit and Push changes
        id: commit_push
        env:
          BEFORE_SHA: ${{ github.sha }}
        run: |
          set -e

          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          git add by_created/
          git add RBLOGGERS_COUNTS.json RBLOGGERS_REPO_STATS.md
          git add .action_result.json || true

          COMMITTED="0"
          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Auto Update: R-Bloggers Data & Stats ($(date +'%Y-%m-%d %H:%M'))"
            git push
            COMMITTED="1"
          fi

          AFTER_SHA="$(git rev-parse HEAD)"
          echo "committed=${COMMITTED}" >> "$GITHUB_OUTPUT"
          echo "sha=${AFTER_SHA}" >> "$GITHUB_OUTPUT"
          echo "ref=${GITHUB_REF_NAME}" >> "$GITHUB_OUTPUT"
          echo "repo=${GITHUB_REPOSITORY}" >> "$GITHUB_OUTPUT"

          if [ "${COMMITTED}" = "1" ]; then
            git diff --name-only "${BEFORE_SHA}" "${AFTER_SHA}" -- 'by_created/**/*.json' > /tmp/changed_files.txt || true
          else
            : > /tmp/changed_files.txt
          fi

          python - <<'PY'
          import json
          files=[]
          with open('/tmp/changed_files.txt','r',encoding='utf-8') as f:
            for line in f:
              p=line.strip()
              if p:
                files.append(p)
          print("changed by_created json files:", len(files))
          with open('/tmp/changed_files.json','w',encoding='utf-8') as f:
            json.dump(files, f, ensure_ascii=False)
          PY

          echo "files_json<<EOF" >> "$GITHUB_OUTPUT"
          cat /tmp/changed_files.json >> "$GITHUB_OUTPUT"
          echo "" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      # ✅ resync_latest_n: "최근 커밋된 순서" 기준으로 N개 뽑기
      - name: Build file list for webhook
        id: build_files
        env:
          RESYNC: ${{ github.event.inputs.resync }}
          RESYNC_MONTH: ${{ github.event.inputs.resync_month }}
          RESYNC_LATEST_N: ${{ github.event.inputs.resync_latest_n }}
          RESYNC_COMMITS_SCAN: ${{ github.event.inputs.resync_commits_scan }}
          DIFF_FILES_JSON: ${{ steps.commit_push.outputs.files_json }}
        run: |
          python - <<'PY'
          import os, json, subprocess

          resync = (os.environ.get("RESYNC","false") or "").lower() == "true"
          month  = (os.environ.get("RESYNC_MONTH","") or "").strip()

          def month_ok(p: str) -> bool:
            if not month:
              return True
            return p.startswith(f"by_created/{month}/")

          def is_target(p: str) -> bool:
            return p.startswith("by_created/") and p.endswith(".json") and month_ok(p)

          # 기본(diff 기반)
          diff_files = []
          try:
            diff_files = json.loads(os.environ.get("DIFF_FILES_JSON","[]"))
          except Exception:
            diff_files = []
          diff_files = [p for p in diff_files if is_target(p)]

          if not resync:
            files = diff_files
          else:
            # resync 모드
            try:
              latest_n = int((os.environ.get("RESYNC_LATEST_N","0") or "0").strip() or "0")
            except Exception:
              latest_n = 0

            try:
              commits_scan = int((os.environ.get("RESYNC_COMMITS_SCAN","500") or "500").strip() or "500")
            except Exception:
              commits_scan = 500

            if latest_n <= 0:
              # 전체(월/전체) 파일을 다 보내는 건 위험할 수 있어서,
              # 여기서는 '최근 커밋 스캔'으로 "변경된 파일들"만 넓게 모으는 방식으로 구성
              # (필요하면 commits_scan을 크게)
              latest_n = 10**9

            # 최근 커밋부터 파일 수집
            cmd = ["git", "log", f"-n{commits_scan}", "--name-only", "--pretty=format:"]
            out = subprocess.check_output(cmd, text=True)
            seen = set()
            files = []
            for line in out.splitlines():
              p = line.strip()
              if not p:
                continue
              if not is_target(p):
                continue
              if p in seen:
                continue
              seen.add(p)
              files.append(p)
              if len(files) >= latest_n:
                break

          with open("files.json","w",encoding="utf-8") as f:
            json.dump(files, f, ensure_ascii=False)

          print("files_count:", len(files))
          if resync:
            print("resync_month:", month or "(all)")
            print("first_file:", files[0] if files else None)
          PY

          echo "files_json<<EOF" >> "$GITHUB_OUTPUT"
          cat files.json >> "$GITHUB_OUTPUT"
          echo "" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Trigger Web-R Sync (batched)
        if: always()
        env:
          WEBR_SYNC_URL: ${{ secrets.WEBR_SYNC_URL }}
          WEBR_SYNC_TOKEN: ${{ secrets.WEBR_SYNC_TOKEN }}
          GH_SHA: ${{ steps.commit_push.outputs.sha }}
          GH_REF_NAME: ${{ steps.commit_push.outputs.ref }}
          GH_REPO: ${{ steps.commit_push.outputs.repo }}
          FILES_JSON: ${{ steps.build_files.outputs.files_json }}
          WEBR_SYNC_BATCH: ${{ github.event.inputs.sync_batch }}
        run: |
          if [ -z "${WEBR_SYNC_URL:-}" ] || [ -z "${WEBR_SYNC_TOKEN:-}" ]; then
            echo "WEBR_SYNC_URL or WEBR_SYNC_TOKEN not set. Skipping sync."
            exit 0
          fi

          python - <<'PY'
          import os, json, math, subprocess, tempfile, time

          def post_with_retry(cmd, retries=3):
            for attempt in range(1, retries+1):
              try:
                subprocess.check_call(cmd)
                return
              except subprocess.CalledProcessError:
                if attempt == retries:
                  raise
                wait = 10 * attempt
                print(f"curl failed (attempt {attempt}/{retries}). retry in {wait}s")
                time.sleep(wait)

          files = json.loads(os.environ.get("FILES_JSON","[]") or "[]")
          print("total_files:", len(files))
          if not files:
            print("No files. Skip webhook.")
            raise SystemExit(0)

          try:
            BATCH = int(os.environ.get("WEBR_SYNC_BATCH","10") or "10")
          except Exception:
            BATCH = 10
          if BATCH <= 0:
            BATCH = 10

          repo = os.environ.get("GH_REPO","")
          sha  = os.environ.get("GH_SHA","")
          ref  = os.environ.get("GH_REF_NAME","")
          url  = os.environ.get("WEBR_SYNC_URL","")
          token= os.environ.get("WEBR_SYNC_TOKEN","")

          total_batches = math.ceil(len(files)/BATCH)
          for idx, i in enumerate(range(0, len(files), BATCH), start=1):
            part = files[i:i+BATCH]
            payload = {"repo": repo, "sha": sha, "ref": ref, "files": part}
            with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8", suffix=".json") as f:
              json.dump(payload, f, ensure_ascii=False)
              path = f.name

            print(f"batch {idx}/{total_batches} size={len(part)}")

            cmd = [
              "curl", "--fail-with-body", "-sS", "-X", "POST",
              "-H", "Content-Type: application/json",
              "-H", f"X-WEBR-SYNC-TOKEN: {token}",
              "-d", f"@{path}",
              url
            ]
            post_with_retry(cmd, retries=3)
            time.sleep(1)
          PY
