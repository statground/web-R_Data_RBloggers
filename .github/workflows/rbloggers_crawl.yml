name: Crawl R-Bloggers (hourly)

on:
  schedule:
    - cron: "0 * * * *"  # 매 시간 실행
  workflow_dispatch:      # 수동 실행 버튼

concurrency:
  group: rbloggers-crawl-main
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  crawl-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f scripts/requirements.txt ]; then
            pip install -r scripts/requirements.txt
          else
            # 여기에 lxml을 추가했습니다.
            pip install requests beautifulsoup4 lxml
          fi

      # 1. 크롤링 수행
      - name: Crawl R-Bloggers
        env:
          GH_SHA: ${{ github.sha }}
          GH_REF_NAME: ${{ github.ref_name }}
        run: |
          python scripts/crawl_rbloggers.py

      # 2. 통계 파일 강제 갱신 (전수 조사)
      - name: Update Repository Stats
        run: |
          python scripts/update_repo_stats.py

      # 3. 변경 사항 커밋 및 푸시
      - name: Commit and Push changes
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # 데이터 폴더와 통계 파일 명시적 추가
          git add by_created/
          git add RBLOGGERS_COUNTS.json RBLOGGERS_REPO_STATS.md
          
          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Auto Update: R-Bloggers Data & Stats ($(date +'%Y-%m-%d %H:%M'))"
            git push
          fi

      # 4. Web-R 플랫폼 등으로 데이터 싱크 (Webhook)
      # 4. Web-R 플랫폼 등으로 데이터 싱크 (Webhook)
      - name: Trigger Web-R Sync
        if: always()
        env:
          WEBR_SYNC_URL: ${{ secrets.WEBR_SYNC_URL }}
          WEBR_SYNC_TOKEN: ${{ secrets.WEBR_SYNC_TOKEN }}
          GH_SHA: ${{ github.sha }}
          GH_REF_NAME: ${{ github.ref_name }}
        run: |
          if [ -z "${WEBR_SYNC_URL:-}" ] || [ -z "${WEBR_SYNC_TOKEN:-}" ]; then
            echo "WEBR_SYNC_URL or WEBR_SYNC_TOKEN not set. Skipping sync."
            exit 0
          fi

          if [ -f ".action_result.json" ]; then
            # crawl_rbloggers.py가 쓰는 keys(files) 또는 workflow용 keys(new_files_relpaths) 모두 지원
            FILES_JSON="$(python -c 'import json; d=json.load(open(".action_result.json","r",encoding="utf-8")); k = "new_files_relpaths" if "new_files_relpaths" in d else "files"; print(json.dumps(d.get(k, []), ensure_ascii=False))')"
          else
            FILES_JSON="[]"
          fi

          # 중요: python에서 env로 읽을 수 있게 export
          export FILES_JSON

          python -c "
          import json, os
          payload = {
            'sha': os.environ.get('GH_SHA',''),
            'ref': os.environ.get('GH_REF_NAME',''),
            'files': json.loads(os.environ.get('FILES_JSON','[]'))
          }
          with open('payload.json','w',encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False)
          print('payload:', payload)
          "

          echo "Sending webhook to $WEBR_SYNC_URL..."
          curl --fail-with-body -sS -X POST \
               -H "Content-Type: application/json" \
               -H "Authorization: Bearer $WEBR_SYNC_TOKEN" \
               -d @payload.json \
               "$WEBR_SYNC_URL"
